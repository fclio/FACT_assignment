diff --git a/halfcheetah/environment.yml b/halfcheetah/environment.yml
deleted file mode 100644
index e81007c..0000000
--- a/halfcheetah/environment.yml
+++ /dev/null
@@ -1,27 +0,0 @@
-name: trajectory
-channels:
-- defaults
-- conda-forge
-dependencies:
-- python=3.8
-- pip
-- patchelf
-- pip:
-    - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
-    - wheel==0.38.4
-    - setuptools==65.5.0
-    - gym==0.20.0
-    - matplotlib==3.3.4
-    - torch==1.9.1+cu111
-    - typed-argument-parser
-    # - git+https://github.com/Farama-Foundation/d4rl@f2a05c0d66722499bf8031b094d9af3aea7c372b#egg=d4rl
-    - scikit-image==0.17.2
-    - scikit-video==1.1.11
-    - gitpython
-    - os
-    - d3rlpy
-    - pyclustering
-    - moviepy
-    - scipy
-    - scikit-learn
diff --git a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json
index 18993c1..54bfeec 100644
--- a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json
+++ b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json
@@ -1,23 +1,23 @@
 {
     "add_extras": {
         "_type": "python_object (type = method)",
-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgfhpRSlC4="
+        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMC3JlYWRfY29uZmlnlGgCaAZoCIaUUpSMCHZpc19mcmVxlEsyjAdjZGZfb2JzlE6MCHNhdmVwYXRolIw7bG9ncy9oYWxmY2hlZXRhaC1tZWRpdW0tdjIvcGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMyLzCUjAphZGRfZXh0cmFzlGgCaAZoD4aUUpSMB3ZlcmJvc2WUiIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjApiZWFtX3dpZHRolEsgjAVta2RpcpRoAmgGaBaGlFKUjAVrX29ic5RLAYwFa19hY3SUTowIbl9leHBhbmSUSwKMB2xvZ2Jhc2WUjAVsb2dzL5SMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjAZwcmVmaXiUjA9wbGFucy9kZWZhdWx0cy+UjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwJZ3B0X2Vwb2NolIwGbGF0ZXN0lIwIc2V0X3NlZWSUaAJoBmglhpRSlIwKZ2V0X2NvbW1pdJRoAmgGaCiGlFKUjAlzYXZlX2RpZmaUaAJoBmgrhpRSlIwGZGV2aWNllIwDY3B1lIwMZ3B0X2xvYWRwYXRolIwOZ3B0L3ByZXRyYWluZWSUjAZjb25maWeUjA5jb25maWcub2ZmbGluZZSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMCnBlcmNlbnRpbGWUjARtZWFulIwHZGF0YXNldJSMFWhhbGZjaGVldGFoLW1lZGl1bS12MpSMB2hvcml6b26USwWMDnByZWZpeF9jb250ZXh0lIiMB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaD6GlFKUjAZjb21taXSUjC1mNzdhMTk0OTk2ODcyODc5YmQwNDE0ZmNiM2RiYzE1YjNkYTI5Y2I4IG1haW6UdWJoD4aUUpQu"
     },
     "beam_width": 32,
     "cdf_act": 0.6,
     "cdf_obs": null,
-    "commit": "cca8d898e10f9f6102a8c33dac7758e2993dfc60 halfcheetah-xrl",
+    "commit": "f77a194996872879bd0414fcb3dbc15b3da29cb8 main",
     "config": "config.offline",
     "dataset": "halfcheetah-medium-v2",
     "device": "cpu",
     "exp_name": "plans/defaults/freq1_H5_beam32",
     "generate_exp_name": {
         "_type": "python_object (type = method)",
-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgxhpRSlC4="
+        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMC3JlYWRfY29uZmlnlGgCaAZoCIaUUpSMCHZpc19mcmVxlEsyjAdjZGZfb2JzlE6MCHNhdmVwYXRolIw7bG9ncy9oYWxmY2hlZXRhaC1tZWRpdW0tdjIvcGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMyLzCUjAphZGRfZXh0cmFzlGgCaAZoD4aUUpSMB3ZlcmJvc2WUiIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjApiZWFtX3dpZHRolEsgjAVta2RpcpRoAmgGaBaGlFKUjAVrX29ic5RLAYwFa19hY3SUTowIbl9leHBhbmSUSwKMB2xvZ2Jhc2WUjAVsb2dzL5SMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjAZwcmVmaXiUjA9wbGFucy9kZWZhdWx0cy+UjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwJZ3B0X2Vwb2NolIwGbGF0ZXN0lIwIc2V0X3NlZWSUaAJoBmglhpRSlIwKZ2V0X2NvbW1pdJRoAmgGaCiGlFKUjAlzYXZlX2RpZmaUaAJoBmgrhpRSlIwGZGV2aWNllIwDY3B1lIwMZ3B0X2xvYWRwYXRolIwOZ3B0L3ByZXRyYWluZWSUjAZjb25maWeUjA5jb25maWcub2ZmbGluZZSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMCnBlcmNlbnRpbGWUjARtZWFulIwHZGF0YXNldJSMFWhhbGZjaGVldGFoLW1lZGl1bS12MpSMB2hvcml6b26USwWMDnByZWZpeF9jb250ZXh0lIiMB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaD6GlFKUjAZjb21taXSUjC1mNzdhMTk0OTk2ODcyODc5YmQwNDE0ZmNiM2RiYzE1YjNkYTI5Y2I4IG1haW6UdWJoPoaUUpQu"
     },
     "get_commit": {
         "_type": "python_object (type = method)",
-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmglhpRSlC4="
+        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMC3JlYWRfY29uZmlnlGgCaAZoCIaUUpSMCHZpc19mcmVxlEsyjAdjZGZfb2JzlE6MCHNhdmVwYXRolIw7bG9ncy9oYWxmY2hlZXRhaC1tZWRpdW0tdjIvcGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMyLzCUjAphZGRfZXh0cmFzlGgCaAZoD4aUUpSMB3ZlcmJvc2WUiIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjApiZWFtX3dpZHRolEsgjAVta2RpcpRoAmgGaBaGlFKUjAVrX29ic5RLAYwFa19hY3SUTowIbl9leHBhbmSUSwKMB2xvZ2Jhc2WUjAVsb2dzL5SMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjAZwcmVmaXiUjA9wbGFucy9kZWZhdWx0cy+UjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwJZ3B0X2Vwb2NolIwGbGF0ZXN0lIwIc2V0X3NlZWSUaAJoBmglhpRSlIwKZ2V0X2NvbW1pdJRoAmgGaCiGlFKUjAlzYXZlX2RpZmaUaAJoBmgrhpRSlIwGZGV2aWNllIwDY3B1lIwMZ3B0X2xvYWRwYXRolIwOZ3B0L3ByZXRyYWluZWSUjAZjb25maWeUjA5jb25maWcub2ZmbGluZZSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMCnBlcmNlbnRpbGWUjARtZWFulIwHZGF0YXNldJSMFWhhbGZjaGVldGFoLW1lZGl1bS12MpSMB2hvcml6b26USwWMDnByZWZpeF9jb250ZXh0lIiMB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaD6GlFKUjAZjb21taXSUjC1mNzdhMTk0OTk2ODcyODc5YmQwNDE0ZmNiM2RiYzE1YjNkYTI5Y2I4IG1haW6UdWJoKIaUUpQu"
     },
     "gpt_epoch": "latest",
     "gpt_loadpath": "gpt/pretrained",
@@ -28,7 +28,7 @@
     "max_context_transitions": 5,
     "mkdir": {
         "_type": "python_object (type = method)",
-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgihpRSlC4="
+        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMC3JlYWRfY29uZmlnlGgCaAZoCIaUUpSMCHZpc19mcmVxlEsyjAdjZGZfb2JzlE6MCHNhdmVwYXRolIw7bG9ncy9oYWxmY2hlZXRhaC1tZWRpdW0tdjIvcGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMyLzCUjAphZGRfZXh0cmFzlGgCaAZoD4aUUpSMB3ZlcmJvc2WUiIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjApiZWFtX3dpZHRolEsgjAVta2RpcpRoAmgGaBaGlFKUjAVrX29ic5RLAYwFa19hY3SUTowIbl9leHBhbmSUSwKMB2xvZ2Jhc2WUjAVsb2dzL5SMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjAZwcmVmaXiUjA9wbGFucy9kZWZhdWx0cy+UjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwJZ3B0X2Vwb2NolIwGbGF0ZXN0lIwIc2V0X3NlZWSUaAJoBmglhpRSlIwKZ2V0X2NvbW1pdJRoAmgGaCiGlFKUjAlzYXZlX2RpZmaUaAJoBmgrhpRSlIwGZGV2aWNllIwDY3B1lIwMZ3B0X2xvYWRwYXRolIwOZ3B0L3ByZXRyYWluZWSUjAZjb25maWeUjA5jb25maWcub2ZmbGluZZSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMCnBlcmNlbnRpbGWUjARtZWFulIwHZGF0YXNldJSMFWhhbGZjaGVldGFoLW1lZGl1bS12MpSMB2hvcml6b26USwWMDnByZWZpeF9jb250ZXh0lIiMB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaD6GlFKUjAZjb21taXSUjC1mNzdhMTk0OTk2ODcyODc5YmQwNDE0ZmNiM2RiYzE1YjNkYTI5Y2I4IG1haW6UdWJoFoaUUpQu"
     },
     "n_expand": 2,
     "percentile": "mean",
@@ -37,24 +37,24 @@
     "prefix_context": true,
     "read_config": {
         "_type": "python_object (type = method)",
-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgZhpRSlC4="
+        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMC3JlYWRfY29uZmlnlGgCaAZoCIaUUpSMCHZpc19mcmVxlEsyjAdjZGZfb2JzlE6MCHNhdmVwYXRolIw7bG9ncy9oYWxmY2hlZXRhaC1tZWRpdW0tdjIvcGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMyLzCUjAphZGRfZXh0cmFzlGgCaAZoD4aUUpSMB3ZlcmJvc2WUiIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjApiZWFtX3dpZHRolEsgjAVta2RpcpRoAmgGaBaGlFKUjAVrX29ic5RLAYwFa19hY3SUTowIbl9leHBhbmSUSwKMB2xvZ2Jhc2WUjAVsb2dzL5SMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjAZwcmVmaXiUjA9wbGFucy9kZWZhdWx0cy+UjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwJZ3B0X2Vwb2NolIwGbGF0ZXN0lIwIc2V0X3NlZWSUaAJoBmglhpRSlIwKZ2V0X2NvbW1pdJRoAmgGaCiGlFKUjAlzYXZlX2RpZmaUaAJoBmgrhpRSlIwGZGV2aWNllIwDY3B1lIwMZ3B0X2xvYWRwYXRolIwOZ3B0L3ByZXRyYWluZWSUjAZjb25maWeUjA5jb25maWcub2ZmbGluZZSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMCnBlcmNlbnRpbGWUjARtZWFulIwHZGF0YXNldJSMFWhhbGZjaGVldGFoLW1lZGl1bS12MpSMB2hvcml6b26USwWMDnByZWZpeF9jb250ZXh0lIiMB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaD6GlFKUjAZjb21taXSUjC1mNzdhMTk0OTk2ODcyODc5YmQwNDE0ZmNiM2RiYzE1YjNkYTI5Y2I4IG1haW6UdWJoCIaUUpQu"
     },
     "renderer": "Renderer",
     "reproducibility": {
-        "command_line": "python scripts/xrl_v2.py --dataset halfcheetah-medium-v2 --gpt_loadpath gpt/pretrained",
+        "command_line": "python experiment.py --dataset halfcheetah-medium-v2 --gpt_loadpath gpt/pretrained",
         "git_has_uncommitted_changes": true,
         "git_root": "/home/colin/Desktop/FACT/FACT_assignment",
-        "git_url": "https://github.com/fclio/FACT_assignment/tree/cca8d898e10f9f6102a8c33dac7758e2993dfc60",
-        "time": "Fri Feb  2 11:39:41 2024"
+        "git_url": "https://github.com/fclio/FACT_assignment/tree/f77a194996872879bd0414fcb3dbc15b3da29cb8",
+        "time": "Sat Feb  3 22:24:52 2024"
     },
     "save_diff": {
         "_type": "python_object (type = method)",
-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1Ymg0hpRSlC4="
+        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMC3JlYWRfY29uZmlnlGgCaAZoCIaUUpSMCHZpc19mcmVxlEsyjAdjZGZfb2JzlE6MCHNhdmVwYXRolIw7bG9ncy9oYWxmY2hlZXRhaC1tZWRpdW0tdjIvcGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMyLzCUjAphZGRfZXh0cmFzlGgCaAZoD4aUUpSMB3ZlcmJvc2WUiIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjApiZWFtX3dpZHRolEsgjAVta2RpcpRoAmgGaBaGlFKUjAVrX29ic5RLAYwFa19hY3SUTowIbl9leHBhbmSUSwKMB2xvZ2Jhc2WUjAVsb2dzL5SMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjAZwcmVmaXiUjA9wbGFucy9kZWZhdWx0cy+UjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwJZ3B0X2Vwb2NolIwGbGF0ZXN0lIwIc2V0X3NlZWSUaAJoBmglhpRSlIwKZ2V0X2NvbW1pdJRoAmgGaCiGlFKUjAlzYXZlX2RpZmaUaAJoBmgrhpRSlIwGZGV2aWNllIwDY3B1lIwMZ3B0X2xvYWRwYXRolIwOZ3B0L3ByZXRyYWluZWSUjAZjb25maWeUjA5jb25maWcub2ZmbGluZZSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMCnBlcmNlbnRpbGWUjARtZWFulIwHZGF0YXNldJSMFWhhbGZjaGVldGFoLW1lZGl1bS12MpSMB2hvcml6b26USwWMDnByZWZpeF9jb250ZXh0lIiMB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaD6GlFKUjAZjb21taXSUjC1mNzdhMTk0OTk2ODcyODc5YmQwNDE0ZmNiM2RiYzE1YjNkYTI5Y2I4IG1haW6UdWJoK4aUUpQu"
     },
     "savepath": "logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0",
     "set_seed": {
         "_type": "python_object (type = method)",
-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgqhpRSlC4="
+        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMC3JlYWRfY29uZmlnlGgCaAZoCIaUUpSMCHZpc19mcmVxlEsyjAdjZGZfb2JzlE6MCHNhdmVwYXRolIw7bG9ncy9oYWxmY2hlZXRhaC1tZWRpdW0tdjIvcGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMyLzCUjAphZGRfZXh0cmFzlGgCaAZoD4aUUpSMB3ZlcmJvc2WUiIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjApiZWFtX3dpZHRolEsgjAVta2RpcpRoAmgGaBaGlFKUjAVrX29ic5RLAYwFa19hY3SUTowIbl9leHBhbmSUSwKMB2xvZ2Jhc2WUjAVsb2dzL5SMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjAZwcmVmaXiUjA9wbGFucy9kZWZhdWx0cy+UjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwJZ3B0X2Vwb2NolIwGbGF0ZXN0lIwIc2V0X3NlZWSUaAJoBmglhpRSlIwKZ2V0X2NvbW1pdJRoAmgGaCiGlFKUjAlzYXZlX2RpZmaUaAJoBmgrhpRSlIwGZGV2aWNllIwDY3B1lIwMZ3B0X2xvYWRwYXRolIwOZ3B0L3ByZXRyYWluZWSUjAZjb25maWeUjA5jb25maWcub2ZmbGluZZSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMCnBlcmNlbnRpbGWUjARtZWFulIwHZGF0YXNldJSMFWhhbGZjaGVldGFoLW1lZGl1bS12MpSMB2hvcml6b26USwWMDnByZWZpeF9jb250ZXh0lIiMB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaD6GlFKUjAZjb21taXSUjC1mNzdhMTk0OTk2ODcyODc5YmQwNDE0ZmNiM2RiYzE1YjNkYTI5Y2I4IG1haW6UdWJoJYaUUpQu"
     },
     "suffix": "0",
     "verbose": true,
diff --git a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt
index 2c00d59..5e239c3 100644
--- a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt
+++ b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt
@@ -1,848 +1,21949 @@
-diff --git a/.gitignore b/.gitignore
-index 30d226d..a18312a 100644
---- a/.gitignore
-+++ b/.gitignore
-@@ -1,6 +1,5 @@
- __pycache__/
- *.mp4
--halfcheetah/logs/
- .ipynb_checkpoints/
- __pycache__/
- Open Notebook.onetoc2 
-diff --git a/gridworld/gridworld_expts.ipynb b/gridworld/gridworld_expts.ipynb
-index e2a261b..bd0b720 100644
---- a/gridworld/gridworld_expts.ipynb
-+++ b/gridworld/gridworld_expts.ipynb
-@@ -1570,7 +1570,7 @@
-    "name": "python",
-    "nbconvert_exporter": "python",
-    "pygments_lexer": "ipython3",
--   "version": "3.8.10"
-+   "version": "3.10.12"
-   }
-  },
-  "nbformat": 4,
-diff --git a/halfcheetah/clusters.npy b/halfcheetah/clusters.npy
+diff --git a/halfcheetah/environment.yml b/halfcheetah/environment.yml
 deleted file mode 100644
-index 3c198aa..0000000
-Binary files a/halfcheetah/clusters.npy and /dev/null differ
-diff --git a/halfcheetah/embeddings.npy b/halfcheetah/embeddings.npy
-deleted file mode 100644
-index 912d68c..0000000
-Binary files a/halfcheetah/embeddings.npy and /dev/null differ
-diff --git a/halfcheetah/halfcheetah.py b/halfcheetah/halfcheetah.py
-deleted file mode 100644
-index 626c42d..0000000
---- a/halfcheetah/halfcheetah.py
+index e81007c..0000000
+--- a/halfcheetah/environment.yml
 +++ /dev/null
-@@ -1,112 +0,0 @@
--import gym
--import d4rl # Import required to register environments, you may need to also import the submodule
--import numpy as np
--import d3rlpy
--
--def main():
--    dataset_d3, env = d3rlpy.datasets.get_dataset("halfcheetah-medium-v2")
--
--    print(dataset_d3.observations.shape)
--    print(dataset_d3.actions.shape)
--    print(dataset_d3.rewards.shape)
--        # print(dataset_d3.next_observations.shape)
--    print(dataset_d3.terminals.shape)
--    print(dataset_d3.terminals.sum()) # no
--
--    env = gym.make('halfcheetah-medium-v2')
--    dataset_d4 = d4rl.qlearning_dataset(env)
--
--    print(dataset_d4['observations'].shape)
--    print(dataset_d4['rewards'].shape)
--    print(dataset_d4['terminals'].shape)
--    print(dataset_d4['actions'].shape)
--
--    print(dataset_d4['rewards'][1])
--    print(dataset_d3.rewards[1])
--
--
--    print(np.allclose(dataset_d3.actions[100], dataset_d4['actions'][100]))
--
--    for j in range(1000):
--        for i in range(999):
--            if dataset_d4['rewards'][j * 999 + i] != dataset_d3.rewards[j * 1000 + i]: print("yo", i)
--        # if not np.allclose(dataset_d3.observations[i], dataset_d4['observations'][i]): print('obs ongelijk')
--        # if not np.allclose(dataset_d3.rewards[i], dataset_d4['rewards'][i]): print('obs ongelijk')
--        # if not np.allclose(dataset_d3.actions[i], dataset_d4['actions'][i]): print('obs ongelijk')
--
--    sac = d3rlpy.algos.SAC(
--        actor_learning_rate=3e-4,
--        critic_learning_rate=3e-4,
--        temp_learning_rate=3e-4,
--        batch_size=256)
--
--    print(sac)
--    sac.fit(dataset_d3, n_steps=10000)
--
--    actions = sac.predict(dataset_d3.observations[0])
--
--    print(actions)
--
--
--    return
--    print('yo!')
--
--    # Create the environment
--    env = gym.make('halfcheetah-medium-v2')
--
--    # d4rl abides by the OpenAI gym interface
--    env.reset()
--    env.step(env.action_space.sample())
--
--    # Each task is associated with a dataset
--    # dataset contains observations, actions, rewards, terminals, and infos
--    # dataset = env.get_dataset()
--    dataset = d4rl.qlearning_dataset(env)
--
--    print(dataset.keys()) # An N x dim_observation Numpy array of observations
--    print(dataset['rewards'].shape) # An N x dim_observation Numpy array of observations
--
--
--    first_traj = []
--    for i in range(50000):
--        if not np.allclose(dataset['next_observations'][i],dataset['observations'][i+1]): print("yo", i, dataset['terminals'][i])
--        # if dataset['terminals'][i] == True:
--        #     print('traj ended at', i)
--        #     break
--
--        # first_traj.append((dataset['observations'][i],
--        #                    dataset['actions'][i],
--        #                    dataset['rewards'][i],
--        #                    dataset['next_observations'][i]))
--    # print(first_traj)
--
--
--    # print(dataset['rewards'].shape) # An N x dim_observation Numpy array of observations
--
--    # Alternatively, use d4rl.qlearning_dataset which
--    # also adds next_observations.
--
--    # import d3rlpy
--
--    # # dataset, env = d3rlpy.datasets.get_dataset("halfcheetah-medium")
--
--    # # prepare algorithm
--    # # sac = d3rlpy.algos.SAC().create(device="cpu")
--
--    # sac = d3rlpy.algos.SACConfig(
--    #     actor_learning_rate=3e-4,
--    #     critic_learning_rate=3e-4,
--    #     temp_learning_rate=3e-4,
--    #     batch_size=256,
--    # ).create(device='cpu')
--
--
--    # # train offline
--    # # sac.fit(dataset, n_steps=1000)
--
--
--    # # ready to control
--    # actions = sac.predict(0)
--
--if __name__ == "__main__":
--    main()
+@@ -1,27 +0,0 @@
+-name: trajectory
+-channels:
+-- defaults
+-- conda-forge
+-dependencies:
+-- python=3.8
+-- pip
+-- patchelf
+-- pip:
+-    - -f https://download.pytorch.org/whl/torch_stable.html
+-    - numpy
+-    - wheel==0.38.4
+-    - setuptools==65.5.0
+-    - gym==0.20.0
+-    - matplotlib==3.3.4
+-    - torch==1.9.1+cu111
+-    - typed-argument-parser
+-    # - git+https://github.com/Farama-Foundation/d4rl@f2a05c0d66722499bf8031b094d9af3aea7c372b#egg=d4rl
+-    - scikit-image==0.17.2
+-    - scikit-video==1.1.11
+-    - gitpython
+-    - os
+-    - d3rlpy
+-    - pyclustering
+-    - moviepy
+-    - scipy
+-    - scikit-learn
+diff --git a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json
+index 18993c1..5a2086f 100644
+--- a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json
++++ b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json
+@@ -1,23 +1,23 @@
+ {
+     "add_extras": {
+         "_type": "python_object (type = method)",
+-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgfhpRSlC4="
++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjAh2aXNfZnJlcZRLMowGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAZkZXZpY2WUjANjcHWUjAVrX2FjdJROjAxncHRfbG9hZHBhdGiUjA5ncHQvcHJldHJhaW5lZJSMCXBsYW5fZnJlcZRLAYwHbG9nYmFzZZSMBWxvZ3MvlIwLcmVhZF9jb25maWeUaAJoBmgUhpRSlIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZwcmVmaXiUjA9wbGFucy9kZWZhdWx0cy+UjAhuX2V4cGFuZJRLAowOcHJlZml4X2NvbnRleHSUiIwFbWtkaXKUaAJoBmgdhpRSlIwIc2V0X3NlZWSUaAJoBmgghpRSlIwKZ2V0X2NvbW1pdJRoAmgGaCOGlFKUjAhzYXZlcGF0aJSMO2xvZ3MvaGFsZmNoZWV0YWgtbWVkaXVtLXYyL3BsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMi8wlIwJc2F2ZV9kaWZmlGgCaAZoKIaUUpSMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMCmFkZF9leHRyYXOUaAJoBmgthpRSlIwHZGF0YXNldJSMFWhhbGZjaGVldGFoLW1lZGl1bS12MpSMEWdlbmVyYXRlX2V4cF9uYW1llGgCaAZoMoaUUpSMCnBlcmNlbnRpbGWUjARtZWFulIwKYmVhbV93aWR0aJRLIIwFa19vYnOUSwGMB3ZlcmJvc2WUiIwHY2RmX2FjdJRHP+MzMzMzMzOMCWdwdF9lcG9jaJSMBmxhdGVzdJSMB2NkZl9vYnOUTowIcmVuZGVyZXKUjAhSZW5kZXJlcpSMB2hvcml6b26USwWMBnN1ZmZpeJSMATCUdWJoLYaUUpQu"
+     },
+     "beam_width": 32,
+     "cdf_act": 0.6,
+     "cdf_obs": null,
+-    "commit": "cca8d898e10f9f6102a8c33dac7758e2993dfc60 halfcheetah-xrl",
++    "commit": "f77a194996872879bd0414fcb3dbc15b3da29cb8 main",
+     "config": "config.offline",
+     "dataset": "halfcheetah-medium-v2",
+     "device": "cpu",
+     "exp_name": "plans/defaults/freq1_H5_beam32",
+     "generate_exp_name": {
+         "_type": "python_object (type = method)",
+-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgxhpRSlC4="
++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjAh2aXNfZnJlcZRLMowGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAZkZXZpY2WUjANjcHWUjAVrX2FjdJROjAxncHRfbG9hZHBhdGiUjA5ncHQvcHJldHJhaW5lZJSMCXBsYW5fZnJlcZRLAYwHbG9nYmFzZZSMBWxvZ3MvlIwLcmVhZF9jb25maWeUaAJoBmgUhpRSlIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZwcmVmaXiUjA9wbGFucy9kZWZhdWx0cy+UjAhuX2V4cGFuZJRLAowOcHJlZml4X2NvbnRleHSUiIwFbWtkaXKUaAJoBmgdhpRSlIwIc2V0X3NlZWSUaAJoBmgghpRSlIwKZ2V0X2NvbW1pdJRoAmgGaCOGlFKUjAhzYXZlcGF0aJSMO2xvZ3MvaGFsZmNoZWV0YWgtbWVkaXVtLXYyL3BsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMi8wlIwJc2F2ZV9kaWZmlGgCaAZoKIaUUpSMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMCmFkZF9leHRyYXOUaAJoBmgthpRSlIwHZGF0YXNldJSMFWhhbGZjaGVldGFoLW1lZGl1bS12MpSMEWdlbmVyYXRlX2V4cF9uYW1llGgCaAZoMoaUUpSMCnBlcmNlbnRpbGWUjARtZWFulIwKYmVhbV93aWR0aJRLIIwFa19vYnOUSwGMB3ZlcmJvc2WUiIwHY2RmX2FjdJRHP+MzMzMzMzOMCWdwdF9lcG9jaJSMBmxhdGVzdJSMB2NkZl9vYnOUTowIcmVuZGVyZXKUjAhSZW5kZXJlcpSMB2hvcml6b26USwWMBnN1ZmZpeJSMATCUdWJoMoaUUpQu"
+     },
+     "get_commit": {
+         "_type": "python_object (type = method)",
+-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmglhpRSlC4="
++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjAh2aXNfZnJlcZRLMowGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAZkZXZpY2WUjANjcHWUjAVrX2FjdJROjAxncHRfbG9hZHBhdGiUjA5ncHQvcHJldHJhaW5lZJSMCXBsYW5fZnJlcZRLAYwHbG9nYmFzZZSMBWxvZ3MvlIwLcmVhZF9jb25maWeUaAJoBmgUhpRSlIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZwcmVmaXiUjA9wbGFucy9kZWZhdWx0cy+UjAhuX2V4cGFuZJRLAowOcHJlZml4X2NvbnRleHSUiIwFbWtkaXKUaAJoBmgdhpRSlIwIc2V0X3NlZWSUaAJoBmgghpRSlIwKZ2V0X2NvbW1pdJRoAmgGaCOGlFKUjAhzYXZlcGF0aJSMO2xvZ3MvaGFsZmNoZWV0YWgtbWVkaXVtLXYyL3BsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMi8wlIwJc2F2ZV9kaWZmlGgCaAZoKIaUUpSMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMCmFkZF9leHRyYXOUaAJoBmgthpRSlIwHZGF0YXNldJSMFWhhbGZjaGVldGFoLW1lZGl1bS12MpSMEWdlbmVyYXRlX2V4cF9uYW1llGgCaAZoMoaUUpSMCnBlcmNlbnRpbGWUjARtZWFulIwKYmVhbV93aWR0aJRLIIwFa19vYnOUSwGMB3ZlcmJvc2WUiIwHY2RmX2FjdJRHP+MzMzMzMzOMCWdwdF9lcG9jaJSMBmxhdGVzdJSMB2NkZl9vYnOUTowIcmVuZGVyZXKUjAhSZW5kZXJlcpSMB2hvcml6b26USwWMBnN1ZmZpeJSMATCUdWJoI4aUUpQu"
+     },
+     "gpt_epoch": "latest",
+     "gpt_loadpath": "gpt/pretrained",
+@@ -28,7 +28,7 @@
+     "max_context_transitions": 5,
+     "mkdir": {
+         "_type": "python_object (type = method)",
+-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgihpRSlC4="
++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjAh2aXNfZnJlcZRLMowGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAZkZXZpY2WUjANjcHWUjAVrX2FjdJROjAxncHRfbG9hZHBhdGiUjA5ncHQvcHJldHJhaW5lZJSMCXBsYW5fZnJlcZRLAYwHbG9nYmFzZZSMBWxvZ3MvlIwLcmVhZF9jb25maWeUaAJoBmgUhpRSlIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZwcmVmaXiUjA9wbGFucy9kZWZhdWx0cy+UjAhuX2V4cGFuZJRLAowOcHJlZml4X2NvbnRleHSUiIwFbWtkaXKUaAJoBmgdhpRSlIwIc2V0X3NlZWSUaAJoBmgghpRSlIwKZ2V0X2NvbW1pdJRoAmgGaCOGlFKUjAhzYXZlcGF0aJSMO2xvZ3MvaGFsZmNoZWV0YWgtbWVkaXVtLXYyL3BsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMi8wlIwJc2F2ZV9kaWZmlGgCaAZoKIaUUpSMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMCmFkZF9leHRyYXOUaAJoBmgthpRSlIwHZGF0YXNldJSMFWhhbGZjaGVldGFoLW1lZGl1bS12MpSMEWdlbmVyYXRlX2V4cF9uYW1llGgCaAZoMoaUUpSMCnBlcmNlbnRpbGWUjARtZWFulIwKYmVhbV93aWR0aJRLIIwFa19vYnOUSwGMB3ZlcmJvc2WUiIwHY2RmX2FjdJRHP+MzMzMzMzOMCWdwdF9lcG9jaJSMBmxhdGVzdJSMB2NkZl9vYnOUTowIcmVuZGVyZXKUjAhSZW5kZXJlcpSMB2hvcml6b26USwWMBnN1ZmZpeJSMATCUdWJoHYaUUpQu"
+     },
+     "n_expand": 2,
+     "percentile": "mean",
+@@ -37,24 +37,24 @@
+     "prefix_context": true,
+     "read_config": {
+         "_type": "python_object (type = method)",
+-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgZhpRSlC4="
++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjAh2aXNfZnJlcZRLMowGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAZkZXZpY2WUjANjcHWUjAVrX2FjdJROjAxncHRfbG9hZHBhdGiUjA5ncHQvcHJldHJhaW5lZJSMCXBsYW5fZnJlcZRLAYwHbG9nYmFzZZSMBWxvZ3MvlIwLcmVhZF9jb25maWeUaAJoBmgUhpRSlIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZwcmVmaXiUjA9wbGFucy9kZWZhdWx0cy+UjAhuX2V4cGFuZJRLAowOcHJlZml4X2NvbnRleHSUiIwFbWtkaXKUaAJoBmgdhpRSlIwIc2V0X3NlZWSUaAJoBmgghpRSlIwKZ2V0X2NvbW1pdJRoAmgGaCOGlFKUjAhzYXZlcGF0aJSMO2xvZ3MvaGFsZmNoZWV0YWgtbWVkaXVtLXYyL3BsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMi8wlIwJc2F2ZV9kaWZmlGgCaAZoKIaUUpSMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMCmFkZF9leHRyYXOUaAJoBmgthpRSlIwHZGF0YXNldJSMFWhhbGZjaGVldGFoLW1lZGl1bS12MpSMEWdlbmVyYXRlX2V4cF9uYW1llGgCaAZoMoaUUpSMCnBlcmNlbnRpbGWUjARtZWFulIwKYmVhbV93aWR0aJRLIIwFa19vYnOUSwGMB3ZlcmJvc2WUiIwHY2RmX2FjdJRHP+MzMzMzMzOMCWdwdF9lcG9jaJSMBmxhdGVzdJSMB2NkZl9vYnOUTowIcmVuZGVyZXKUjAhSZW5kZXJlcpSMB2hvcml6b26USwWMBnN1ZmZpeJSMATCUdWJoFIaUUpQu"
+     },
+     "renderer": "Renderer",
+     "reproducibility": {
+-        "command_line": "python scripts/xrl_v2.py --dataset halfcheetah-medium-v2 --gpt_loadpath gpt/pretrained",
++        "command_line": "python experiment.py --dataset halfcheetah-medium-v2 --gpt_loadpath gpt/pretrained",
+         "git_has_uncommitted_changes": true,
+         "git_root": "/home/colin/Desktop/FACT/FACT_assignment",
+-        "git_url": "https://github.com/fclio/FACT_assignment/tree/cca8d898e10f9f6102a8c33dac7758e2993dfc60",
+-        "time": "Fri Feb  2 11:39:41 2024"
++        "git_url": "https://github.com/fclio/FACT_assignment/tree/f77a194996872879bd0414fcb3dbc15b3da29cb8",
++        "time": "Sat Feb  3 22:24:32 2024"
+     },
+     "save_diff": {
+         "_type": "python_object (type = method)",
+-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1Ymg0hpRSlC4="
++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjAh2aXNfZnJlcZRLMowGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAZkZXZpY2WUjANjcHWUjAVrX2FjdJROjAxncHRfbG9hZHBhdGiUjA5ncHQvcHJldHJhaW5lZJSMCXBsYW5fZnJlcZRLAYwHbG9nYmFzZZSMBWxvZ3MvlIwLcmVhZF9jb25maWeUaAJoBmgUhpRSlIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZwcmVmaXiUjA9wbGFucy9kZWZhdWx0cy+UjAhuX2V4cGFuZJRLAowOcHJlZml4X2NvbnRleHSUiIwFbWtkaXKUaAJoBmgdhpRSlIwIc2V0X3NlZWSUaAJoBmgghpRSlIwKZ2V0X2NvbW1pdJRoAmgGaCOGlFKUjAhzYXZlcGF0aJSMO2xvZ3MvaGFsZmNoZWV0YWgtbWVkaXVtLXYyL3BsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMi8wlIwJc2F2ZV9kaWZmlGgCaAZoKIaUUpSMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMCmFkZF9leHRyYXOUaAJoBmgthpRSlIwHZGF0YXNldJSMFWhhbGZjaGVldGFoLW1lZGl1bS12MpSMEWdlbmVyYXRlX2V4cF9uYW1llGgCaAZoMoaUUpSMCnBlcmNlbnRpbGWUjARtZWFulIwKYmVhbV93aWR0aJRLIIwFa19vYnOUSwGMB3ZlcmJvc2WUiIwHY2RmX2FjdJRHP+MzMzMzMzOMCWdwdF9lcG9jaJSMBmxhdGVzdJSMB2NkZl9vYnOUTowIcmVuZGVyZXKUjAhSZW5kZXJlcpSMB2hvcml6b26USwWMBnN1ZmZpeJSMATCUdWJoKIaUUpQu"
+     },
+     "savepath": "logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0",
+     "set_seed": {
+         "_type": "python_object (type = method)",
+-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgqhpRSlC4="
++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjAh2aXNfZnJlcZRLMowGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAZkZXZpY2WUjANjcHWUjAVrX2FjdJROjAxncHRfbG9hZHBhdGiUjA5ncHQvcHJldHJhaW5lZJSMCXBsYW5fZnJlcZRLAYwHbG9nYmFzZZSMBWxvZ3MvlIwLcmVhZF9jb25maWeUaAJoBmgUhpRSlIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZwcmVmaXiUjA9wbGFucy9kZWZhdWx0cy+UjAhuX2V4cGFuZJRLAowOcHJlZml4X2NvbnRleHSUiIwFbWtkaXKUaAJoBmgdhpRSlIwIc2V0X3NlZWSUaAJoBmgghpRSlIwKZ2V0X2NvbW1pdJRoAmgGaCOGlFKUjAhzYXZlcGF0aJSMO2xvZ3MvaGFsZmNoZWV0YWgtbWVkaXVtLXYyL3BsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMi8wlIwJc2F2ZV9kaWZmlGgCaAZoKIaUUpSMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMCmFkZF9leHRyYXOUaAJoBmgthpRSlIwHZGF0YXNldJSMFWhhbGZjaGVldGFoLW1lZGl1bS12MpSMEWdlbmVyYXRlX2V4cF9uYW1llGgCaAZoMoaUUpSMCnBlcmNlbnRpbGWUjARtZWFulIwKYmVhbV93aWR0aJRLIIwFa19vYnOUSwGMB3ZlcmJvc2WUiIwHY2RmX2FjdJRHP+MzMzMzMzOMCWdwdF9lcG9jaJSMBmxhdGVzdJSMB2NkZl9vYnOUTowIcmVuZGVyZXKUjAhSZW5kZXJlcpSMB2hvcml6b26USwWMBnN1ZmZpeJSMATCUdWJoIIaUUpQu"
+     },
+     "suffix": "0",
+     "verbose": true,
+diff --git a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt
+index 2c00d59..8a48bbf 100644
+--- a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt
++++ b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt
+@@ -1,848 +1,20274 @@
+-diff --git a/.gitignore b/.gitignore
+-index 30d226d..a18312a 100644
+---- a/.gitignore
+-+++ b/.gitignore
+-@@ -1,6 +1,5 @@
+- __pycache__/
+- *.mp4
+--halfcheetah/logs/
+- .ipynb_checkpoints/
+- __pycache__/
+- Open Notebook.onetoc2 
+-diff --git a/gridworld/gridworld_expts.ipynb b/gridworld/gridworld_expts.ipynb
+-index e2a261b..bd0b720 100644
+---- a/gridworld/gridworld_expts.ipynb
+-+++ b/gridworld/gridworld_expts.ipynb
+-@@ -1570,7 +1570,7 @@
+-    "name": "python",
+-    "nbconvert_exporter": "python",
+-    "pygments_lexer": "ipython3",
+--   "version": "3.8.10"
+-+   "version": "3.10.12"
+-   }
+-  },
+-  "nbformat": 4,
+-diff --git a/halfcheetah/clusters.npy b/halfcheetah/clusters.npy
++diff --git a/halfcheetah/environment.yml b/halfcheetah/environment.yml
+ deleted file mode 100644
+-index 3c198aa..0000000
+-Binary files a/halfcheetah/clusters.npy and /dev/null differ
+-diff --git a/halfcheetah/embeddings.npy b/halfcheetah/embeddings.npy
+-deleted file mode 100644
+-index 912d68c..0000000
+-Binary files a/halfcheetah/embeddings.npy and /dev/null differ
+-diff --git a/halfcheetah/halfcheetah.py b/halfcheetah/halfcheetah.py
+-deleted file mode 100644
+-index 626c42d..0000000
+---- a/halfcheetah/halfcheetah.py
++index e81007c..0000000
++--- a/halfcheetah/environment.yml
+ +++ /dev/null
+-@@ -1,112 +0,0 @@
+--import gym
+--import d4rl # Import required to register environments, you may need to also import the submodule
+--import numpy as np
+--import d3rlpy
+--
+--def main():
+--    dataset_d3, env = d3rlpy.datasets.get_dataset("halfcheetah-medium-v2")
+--
+--    print(dataset_d3.observations.shape)
+--    print(dataset_d3.actions.shape)
+--    print(dataset_d3.rewards.shape)
+--        # print(dataset_d3.next_observations.shape)
+--    print(dataset_d3.terminals.shape)
+--    print(dataset_d3.terminals.sum()) # no
+--
+--    env = gym.make('halfcheetah-medium-v2')
+--    dataset_d4 = d4rl.qlearning_dataset(env)
+--
+--    print(dataset_d4['observations'].shape)
+--    print(dataset_d4['rewards'].shape)
+--    print(dataset_d4['terminals'].shape)
+--    print(dataset_d4['actions'].shape)
+--
+--    print(dataset_d4['rewards'][1])
+--    print(dataset_d3.rewards[1])
+--
+--
+--    print(np.allclose(dataset_d3.actions[100], dataset_d4['actions'][100]))
+--
+--    for j in range(1000):
+--        for i in range(999):
+--            if dataset_d4['rewards'][j * 999 + i] != dataset_d3.rewards[j * 1000 + i]: print("yo", i)
+--        # if not np.allclose(dataset_d3.observations[i], dataset_d4['observations'][i]): print('obs ongelijk')
+--        # if not np.allclose(dataset_d3.rewards[i], dataset_d4['rewards'][i]): print('obs ongelijk')
+--        # if not np.allclose(dataset_d3.actions[i], dataset_d4['actions'][i]): print('obs ongelijk')
+--
+--    sac = d3rlpy.algos.SAC(
+--        actor_learning_rate=3e-4,
+--        critic_learning_rate=3e-4,
+--        temp_learning_rate=3e-4,
+--        batch_size=256)
+--
+--    print(sac)
+--    sac.fit(dataset_d3, n_steps=10000)
+--
+--    actions = sac.predict(dataset_d3.observations[0])
+--
+--    print(actions)
+--
+--
+--    return
+--    print('yo!')
+--
+--    # Create the environment
+--    env = gym.make('halfcheetah-medium-v2')
+--
+--    # d4rl abides by the OpenAI gym interface
+--    env.reset()
+--    env.step(env.action_space.sample())
+--
+--    # Each task is associated with a dataset
+--    # dataset contains observations, actions, rewards, terminals, and infos
+--    # dataset = env.get_dataset()
+--    dataset = d4rl.qlearning_dataset(env)
+--
+--    print(dataset.keys()) # An N x dim_observation Numpy array of observations
+--    print(dataset['rewards'].shape) # An N x dim_observation Numpy array of observations
+--
+--
+--    first_traj = []
+--    for i in range(50000):
+--        if not np.allclose(dataset['next_observations'][i],dataset['observations'][i+1]): print("yo", i, dataset['terminals'][i])
+--        # if dataset['terminals'][i] == True:
+--        #     print('traj ended at', i)
+--        #     break
+--
+--        # first_traj.append((dataset['observations'][i],
+--        #                    dataset['actions'][i],
+--        #                    dataset['rewards'][i],
+--        #                    dataset['next_observations'][i]))
+--    # print(first_traj)
+--
+--
+--    # print(dataset['rewards'].shape) # An N x dim_observation Numpy array of observations
+--
+--    # Alternatively, use d4rl.qlearning_dataset which
+--    # also adds next_observations.
+--
+--    # import d3rlpy
+--
+--    # # dataset, env = d3rlpy.datasets.get_dataset("halfcheetah-medium")
+--
+--    # # prepare algorithm
+--    # # sac = d3rlpy.algos.SAC().create(device="cpu")
+--
+--    # sac = d3rlpy.algos.SACConfig(
+--    #     actor_learning_rate=3e-4,
+--    #     critic_learning_rate=3e-4,
+--    #     temp_learning_rate=3e-4,
+--    #     batch_size=256,
+--    # ).create(device='cpu')
+--
+--
+--    # # train offline
+--    # # sac.fit(dataset, n_steps=1000)
+--
+--
+--    # # ready to control
+--    # actions = sac.predict(0)
+--
+--if __name__ == "__main__":
+--    main()
++@@ -1,27 +0,0 @@
++-name: trajectory
++-channels:
++-- defaults
++-- conda-forge
++-dependencies:
++-- python=3.8
++-- pip
++-- patchelf
++-- pip:
++-    - -f https://download.pytorch.org/whl/torch_stable.html
++-    - numpy
++-    - wheel==0.38.4
++-    - setuptools==65.5.0
++-    - gym==0.20.0
++-    - matplotlib==3.3.4
++-    - torch==1.9.1+cu111
++-    - typed-argument-parser
++-    # - git+https://github.com/Farama-Foundation/d4rl@f2a05c0d66722499bf8031b094d9af3aea7c372b#egg=d4rl
++-    - scikit-image==0.17.2
++-    - scikit-video==1.1.11
++-    - gitpython
++-    - os
++-    - d3rlpy
++-    - pyclustering
++-    - moviepy
++-    - scipy
++-    - scikit-learn
++diff --git a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json
++index 18993c1..f6a6621 100644
++--- a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json
+++++ b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json
++@@ -1,23 +1,23 @@
++ {
++     "add_extras": {
++         "_type": "python_object (type = method)",
++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgfhpRSlC4="
+++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMCHJlbmRlcmVylIwIUmVuZGVyZXKUjAVrX29ic5RLAYwGY29tbWl0lIwtZjc3YTE5NDk5Njg3Mjg3OWJkMDQxNGZjYjNkYmMxNWIzZGEyOWNiOCBtYWlulIwJcGxhbl9mcmVxlEsBjAhzYXZlcGF0aJSMO2xvZ3MvaGFsZmNoZWV0YWgtbWVkaXVtLXYyL3BsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMi8wlIwKZ2V0X2NvbW1pdJRoAmgGaBCGlFKUjAZkZXZpY2WUjANjcHWUjAdjZGZfYWN0lEc/4zMzMzMzM4wHaG9yaXpvbpRLBYwJZ3B0X2Vwb2NolIwGbGF0ZXN0lIwGc3VmZml4lIwBMJSMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjAphZGRfZXh0cmFzlGgCaAZoHIaUUpSMCHNldF9zZWVklGgCaAZoH4aUUpSMCGV4cF9uYW1llIwecGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMylIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjApiZWFtX3dpZHRolEsgjAdjZGZfb2JzlE6MDnByZWZpeF9jb250ZXh0lIiMB3ZlcmJvc2WUiIwFa19hY3SUTowLcmVhZF9jb25maWeUaAJoBmgrhpRSlIwFbWtkaXKUaAJoBmguhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwMZ3B0X2xvYWRwYXRolIwOZ3B0L3ByZXRyYWluZWSUjAlzYXZlX2RpZmaUaAJoBmg1hpRSlIwHbG9nYmFzZZSMBWxvZ3MvlIwIbl9leHBhbmSUSwKMEWdlbmVyYXRlX2V4cF9uYW1llGgCaAZoO4aUUpSMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjApwZXJjZW50aWxllIwEbWVhbpSMCHZpc19mcmVxlEsydWJoHIaUUpQu"
++     },
++     "beam_width": 32,
++     "cdf_act": 0.6,
++     "cdf_obs": null,
++-    "commit": "cca8d898e10f9f6102a8c33dac7758e2993dfc60 halfcheetah-xrl",
+++    "commit": "f77a194996872879bd0414fcb3dbc15b3da29cb8 main",
++     "config": "config.offline",
++     "dataset": "halfcheetah-medium-v2",
++     "device": "cpu",
++     "exp_name": "plans/defaults/freq1_H5_beam32",
++     "generate_exp_name": {
++         "_type": "python_object (type = method)",
++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgxhpRSlC4="
+++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMCHJlbmRlcmVylIwIUmVuZGVyZXKUjAVrX29ic5RLAYwGY29tbWl0lIwtZjc3YTE5NDk5Njg3Mjg3OWJkMDQxNGZjYjNkYmMxNWIzZGEyOWNiOCBtYWlulIwJcGxhbl9mcmVxlEsBjAhzYXZlcGF0aJSMO2xvZ3MvaGFsZmNoZWV0YWgtbWVkaXVtLXYyL3BsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMi8wlIwKZ2V0X2NvbW1pdJRoAmgGaBCGlFKUjAZkZXZpY2WUjANjcHWUjAdjZGZfYWN0lEc/4zMzMzMzM4wHaG9yaXpvbpRLBYwJZ3B0X2Vwb2NolIwGbGF0ZXN0lIwGc3VmZml4lIwBMJSMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjAphZGRfZXh0cmFzlGgCaAZoHIaUUpSMCHNldF9zZWVklGgCaAZoH4aUUpSMCGV4cF9uYW1llIwecGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMylIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjApiZWFtX3dpZHRolEsgjAdjZGZfb2JzlE6MDnByZWZpeF9jb250ZXh0lIiMB3ZlcmJvc2WUiIwFa19hY3SUTowLcmVhZF9jb25maWeUaAJoBmgrhpRSlIwFbWtkaXKUaAJoBmguhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwMZ3B0X2xvYWRwYXRolIwOZ3B0L3ByZXRyYWluZWSUjAlzYXZlX2RpZmaUaAJoBmg1hpRSlIwHbG9nYmFzZZSMBWxvZ3MvlIwIbl9leHBhbmSUSwKMEWdlbmVyYXRlX2V4cF9uYW1llGgCaAZoO4aUUpSMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjApwZXJjZW50aWxllIwEbWVhbpSMCHZpc19mcmVxlEsydWJoO4aUUpQu"
++     },
++     "get_commit": {
++         "_type": "python_object (type = method)",
++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmglhpRSlC4="
+++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMCHJlbmRlcmVylIwIUmVuZGVyZXKUjAVrX29ic5RLAYwGY29tbWl0lIwtZjc3YTE5NDk5Njg3Mjg3OWJkMDQxNGZjYjNkYmMxNWIzZGEyOWNiOCBtYWlulIwJcGxhbl9mcmVxlEsBjAhzYXZlcGF0aJSMO2xvZ3MvaGFsZmNoZWV0YWgtbWVkaXVtLXYyL3BsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMi8wlIwKZ2V0X2NvbW1pdJRoAmgGaBCGlFKUjAZkZXZpY2WUjANjcHWUjAdjZGZfYWN0lEc/4zMzMzMzM4wHaG9yaXpvbpRLBYwJZ3B0X2Vwb2NolIwGbGF0ZXN0lIwGc3VmZml4lIwBMJSMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjAphZGRfZXh0cmFzlGgCaAZoHIaUUpSMCHNldF9zZWVklGgCaAZoH4aUUpSMCGV4cF9uYW1llIwecGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMylIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjApiZWFtX3dpZHRolEsgjAdjZGZfb2JzlE6MDnByZWZpeF9jb250ZXh0lIiMB3ZlcmJvc2WUiIwFa19hY3SUTowLcmVhZF9jb25maWeUaAJoBmgrhpRSlIwFbWtkaXKUaAJoBmguhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwMZ3B0X2xvYWRwYXRolIwOZ3B0L3ByZXRyYWluZWSUjAlzYXZlX2RpZmaUaAJoBmg1hpRSlIwHbG9nYmFzZZSMBWxvZ3MvlIwIbl9leHBhbmSUSwKMEWdlbmVyYXRlX2V4cF9uYW1llGgCaAZoO4aUUpSMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjApwZXJjZW50aWxllIwEbWVhbpSMCHZpc19mcmVxlEsydWJoEIaUUpQu"
++     },
++     "gpt_epoch": "latest",
++     "gpt_loadpath": "gpt/pretrained",
++@@ -28,7 +28,7 @@
++     "max_context_transitions": 5,
++     "mkdir": {
++         "_type": "python_object (type = method)",
++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgihpRSlC4="
+++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMCHJlbmRlcmVylIwIUmVuZGVyZXKUjAVrX29ic5RLAYwGY29tbWl0lIwtZjc3YTE5NDk5Njg3Mjg3OWJkMDQxNGZjYjNkYmMxNWIzZGEyOWNiOCBtYWlulIwJcGxhbl9mcmVxlEsBjAhzYXZlcGF0aJSMO2xvZ3MvaGFsZmNoZWV0YWgtbWVkaXVtLXYyL3BsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMi8wlIwKZ2V0X2NvbW1pdJRoAmgGaBCGlFKUjAZkZXZpY2WUjANjcHWUjAdjZGZfYWN0lEc/4zMzMzMzM4wHaG9yaXpvbpRLBYwJZ3B0X2Vwb2NolIwGbGF0ZXN0lIwGc3VmZml4lIwBMJSMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjAphZGRfZXh0cmFzlGgCaAZoHIaUUpSMCHNldF9zZWVklGgCaAZoH4aUUpSMCGV4cF9uYW1llIwecGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMylIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjApiZWFtX3dpZHRolEsgjAdjZGZfb2JzlE6MDnByZWZpeF9jb250ZXh0lIiMB3ZlcmJvc2WUiIwFa19hY3SUTowLcmVhZF9jb25maWeUaAJoBmgrhpRSlIwFbWtkaXKUaAJoBmguhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwMZ3B0X2xvYWRwYXRolIwOZ3B0L3ByZXRyYWluZWSUjAlzYXZlX2RpZmaUaAJoBmg1hpRSlIwHbG9nYmFzZZSMBWxvZ3MvlIwIbl9leHBhbmSUSwKMEWdlbmVyYXRlX2V4cF9uYW1llGgCaAZoO4aUUpSMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjApwZXJjZW50aWxllIwEbWVhbpSMCHZpc19mcmVxlEsydWJoLoaUUpQu"
++     },
++     "n_expand": 2,
++     "percentile": "mean",
++@@ -37,24 +37,24 @@
++     "prefix_context": true,
++     "read_config": {
++         "_type": "python_object (type = method)",
++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgZhpRSlC4="
+++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMCHJlbmRlcmVylIwIUmVuZGVyZXKUjAVrX29ic5RLAYwGY29tbWl0lIwtZjc3YTE5NDk5Njg3Mjg3OWJkMDQxNGZjYjNkYmMxNWIzZGEyOWNiOCBtYWlulIwJcGxhbl9mcmVxlEsBjAhzYXZlcGF0aJSMO2xvZ3MvaGFsZmNoZWV0YWgtbWVkaXVtLXYyL3BsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMi8wlIwKZ2V0X2NvbW1pdJRoAmgGaBCGlFKUjAZkZXZpY2WUjANjcHWUjAdjZGZfYWN0lEc/4zMzMzMzM4wHaG9yaXpvbpRLBYwJZ3B0X2Vwb2NolIwGbGF0ZXN0lIwGc3VmZml4lIwBMJSMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjAphZGRfZXh0cmFzlGgCaAZoHIaUUpSMCHNldF9zZWVklGgCaAZoH4aUUpSMCGV4cF9uYW1llIwecGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMylIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjApiZWFtX3dpZHRolEsgjAdjZGZfb2JzlE6MDnByZWZpeF9jb250ZXh0lIiMB3ZlcmJvc2WUiIwFa19hY3SUTowLcmVhZF9jb25maWeUaAJoBmgrhpRSlIwFbWtkaXKUaAJoBmguhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwMZ3B0X2xvYWRwYXRolIwOZ3B0L3ByZXRyYWluZWSUjAlzYXZlX2RpZmaUaAJoBmg1hpRSlIwHbG9nYmFzZZSMBWxvZ3MvlIwIbl9leHBhbmSUSwKMEWdlbmVyYXRlX2V4cF9uYW1llGgCaAZoO4aUUpSMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjApwZXJjZW50aWxllIwEbWVhbpSMCHZpc19mcmVxlEsydWJoK4aUUpQu"
++     },
++     "renderer": "Renderer",
++     "reproducibility": {
++-        "command_line": "python scripts/xrl_v2.py --dataset halfcheetah-medium-v2 --gpt_loadpath gpt/pretrained",
+++        "command_line": "python xrl_v2.py --dataset halfcheetah-medium-v2 --gpt_loadpath gpt/pretrained",
++         "git_has_uncommitted_changes": true,
++         "git_root": "/home/colin/Desktop/FACT/FACT_assignment",
++-        "git_url": "https://github.com/fclio/FACT_assignment/tree/cca8d898e10f9f6102a8c33dac7758e2993dfc60",
++-        "time": "Fri Feb  2 11:39:41 2024"
+++        "git_url": "https://github.com/fclio/FACT_assignment/tree/f77a194996872879bd0414fcb3dbc15b3da29cb8",
+++        "time": "Sat Feb  3 22:11:10 2024"
++     },
++     "save_diff": {
++         "_type": "python_object (type = method)",
++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1Ymg0hpRSlC4="
+++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMCHJlbmRlcmVylIwIUmVuZGVyZXKUjAVrX29ic5RLAYwGY29tbWl0lIwtZjc3YTE5NDk5Njg3Mjg3OWJkMDQxNGZjYjNkYmMxNWIzZGEyOWNiOCBtYWlulIwJcGxhbl9mcmVxlEsBjAhzYXZlcGF0aJSMO2xvZ3MvaGFsZmNoZWV0YWgtbWVkaXVtLXYyL3BsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMi8wlIwKZ2V0X2NvbW1pdJRoAmgGaBCGlFKUjAZkZXZpY2WUjANjcHWUjAdjZGZfYWN0lEc/4zMzMzMzM4wHaG9yaXpvbpRLBYwJZ3B0X2Vwb2NolIwGbGF0ZXN0lIwGc3VmZml4lIwBMJSMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjAphZGRfZXh0cmFzlGgCaAZoHIaUUpSMCHNldF9zZWVklGgCaAZoH4aUUpSMCGV4cF9uYW1llIwecGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMylIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjApiZWFtX3dpZHRolEsgjAdjZGZfb2JzlE6MDnByZWZpeF9jb250ZXh0lIiMB3ZlcmJvc2WUiIwFa19hY3SUTowLcmVhZF9jb25maWeUaAJoBmgrhpRSlIwFbWtkaXKUaAJoBmguhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwMZ3B0X2xvYWRwYXRolIwOZ3B0L3ByZXRyYWluZWSUjAlzYXZlX2RpZmaUaAJoBmg1hpRSlIwHbG9nYmFzZZSMBWxvZ3MvlIwIbl9leHBhbmSUSwKMEWdlbmVyYXRlX2V4cF9uYW1llGgCaAZoO4aUUpSMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjApwZXJjZW50aWxllIwEbWVhbpSMCHZpc19mcmVxlEsydWJoNYaUUpQu"
++     },
++     "savepath": "logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0",
++     "set_seed": {
++         "_type": "python_object (type = method)",
++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgqhpRSlC4="
+++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMCHJlbmRlcmVylIwIUmVuZGVyZXKUjAVrX29ic5RLAYwGY29tbWl0lIwtZjc3YTE5NDk5Njg3Mjg3OWJkMDQxNGZjYjNkYmMxNWIzZGEyOWNiOCBtYWlulIwJcGxhbl9mcmVxlEsBjAhzYXZlcGF0aJSMO2xvZ3MvaGFsZmNoZWV0YWgtbWVkaXVtLXYyL3BsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMi8wlIwKZ2V0X2NvbW1pdJRoAmgGaBCGlFKUjAZkZXZpY2WUjANjcHWUjAdjZGZfYWN0lEc/4zMzMzMzM4wHaG9yaXpvbpRLBYwJZ3B0X2Vwb2NolIwGbGF0ZXN0lIwGc3VmZml4lIwBMJSMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjAphZGRfZXh0cmFzlGgCaAZoHIaUUpSMCHNldF9zZWVklGgCaAZoH4aUUpSMCGV4cF9uYW1llIwecGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMylIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjApiZWFtX3dpZHRolEsgjAdjZGZfb2JzlE6MDnByZWZpeF9jb250ZXh0lIiMB3ZlcmJvc2WUiIwFa19hY3SUTowLcmVhZF9jb25maWeUaAJoBmgrhpRSlIwFbWtkaXKUaAJoBmguhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwMZ3B0X2xvYWRwYXRolIwOZ3B0L3ByZXRyYWluZWSUjAlzYXZlX2RpZmaUaAJoBmg1hpRSlIwHbG9nYmFzZZSMBWxvZ3MvlIwIbl9leHBhbmSUSwKMEWdlbmVyYXRlX2V4cF9uYW1llGgCaAZoO4aUUpSMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjApwZXJjZW50aWxllIwEbWVhbpSMCHZpc19mcmVxlEsydWJoH4aUUpQu"
++     },
++     "suffix": "0",
++     "verbose": true,
++diff --git a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt
++index 2c00d59..03e9b04 100644
++--- a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt
+++++ b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt
++@@ -1,848 +1,18599 @@
++-diff --git a/.gitignore b/.gitignore
++-index 30d226d..a18312a 100644
++---- a/.gitignore
++-+++ b/.gitignore
++-@@ -1,6 +1,5 @@
++- __pycache__/
++- *.mp4
++--halfcheetah/logs/
++- .ipynb_checkpoints/
++- __pycache__/
++- Open Notebook.onetoc2 
++-diff --git a/gridworld/gridworld_expts.ipynb b/gridworld/gridworld_expts.ipynb
++-index e2a261b..bd0b720 100644
++---- a/gridworld/gridworld_expts.ipynb
++-+++ b/gridworld/gridworld_expts.ipynb
++-@@ -1570,7 +1570,7 @@
++-    "name": "python",
++-    "nbconvert_exporter": "python",
++-    "pygments_lexer": "ipython3",
++--   "version": "3.8.10"
++-+   "version": "3.10.12"
++-   }
++-  },
++-  "nbformat": 4,
++-diff --git a/halfcheetah/clusters.npy b/halfcheetah/clusters.npy
+++diff --git a/halfcheetah/environment.yml b/halfcheetah/environment.yml
++ deleted file mode 100644
++-index 3c198aa..0000000
++-Binary files a/halfcheetah/clusters.npy and /dev/null differ
++-diff --git a/halfcheetah/embeddings.npy b/halfcheetah/embeddings.npy
++-deleted file mode 100644
++-index 912d68c..0000000
++-Binary files a/halfcheetah/embeddings.npy and /dev/null differ
++-diff --git a/halfcheetah/halfcheetah.py b/halfcheetah/halfcheetah.py
++-deleted file mode 100644
++-index 626c42d..0000000
++---- a/halfcheetah/halfcheetah.py
+++index e81007c..0000000
+++--- a/halfcheetah/environment.yml
++ +++ /dev/null
++-@@ -1,112 +0,0 @@
++--import gym
++--import d4rl # Import required to register environments, you may need to also import the submodule
++--import numpy as np
++--import d3rlpy
++--
++--def main():
++--    dataset_d3, env = d3rlpy.datasets.get_dataset("halfcheetah-medium-v2")
++--
++--    print(dataset_d3.observations.shape)
++--    print(dataset_d3.actions.shape)
++--    print(dataset_d3.rewards.shape)
++--        # print(dataset_d3.next_observations.shape)
++--    print(dataset_d3.terminals.shape)
++--    print(dataset_d3.terminals.sum()) # no
++--
++--    env = gym.make('halfcheetah-medium-v2')
++--    dataset_d4 = d4rl.qlearning_dataset(env)
++--
++--    print(dataset_d4['observations'].shape)
++--    print(dataset_d4['rewards'].shape)
++--    print(dataset_d4['terminals'].shape)
++--    print(dataset_d4['actions'].shape)
++--
++--    print(dataset_d4['rewards'][1])
++--    print(dataset_d3.rewards[1])
++--
++--
++--    print(np.allclose(dataset_d3.actions[100], dataset_d4['actions'][100]))
++--
++--    for j in range(1000):
++--        for i in range(999):
++--            if dataset_d4['rewards'][j * 999 + i] != dataset_d3.rewards[j * 1000 + i]: print("yo", i)
++--        # if not np.allclose(dataset_d3.observations[i], dataset_d4['observations'][i]): print('obs ongelijk')
++--        # if not np.allclose(dataset_d3.rewards[i], dataset_d4['rewards'][i]): print('obs ongelijk')
++--        # if not np.allclose(dataset_d3.actions[i], dataset_d4['actions'][i]): print('obs ongelijk')
++--
++--    sac = d3rlpy.algos.SAC(
++--        actor_learning_rate=3e-4,
++--        critic_learning_rate=3e-4,
++--        temp_learning_rate=3e-4,
++--        batch_size=256)
++--
++--    print(sac)
++--    sac.fit(dataset_d3, n_steps=10000)
++--
++--    actions = sac.predict(dataset_d3.observations[0])
++--
++--    print(actions)
++--
++--
++--    return
++--    print('yo!')
++--
++--    # Create the environment
++--    env = gym.make('halfcheetah-medium-v2')
++--
++--    # d4rl abides by the OpenAI gym interface
++--    env.reset()
++--    env.step(env.action_space.sample())
++--
++--    # Each task is associated with a dataset
++--    # dataset contains observations, actions, rewards, terminals, and infos
++--    # dataset = env.get_dataset()
++--    dataset = d4rl.qlearning_dataset(env)
++--
++--    print(dataset.keys()) # An N x dim_observation Numpy array of observations
++--    print(dataset['rewards'].shape) # An N x dim_observation Numpy array of observations
++--
++--
++--    first_traj = []
++--    for i in range(50000):
++--        if not np.allclose(dataset['next_observations'][i],dataset['observations'][i+1]): print("yo", i, dataset['terminals'][i])
++--        # if dataset['terminals'][i] == True:
++--        #     print('traj ended at', i)
++--        #     break
++--
++--        # first_traj.append((dataset['observations'][i],
++--        #                    dataset['actions'][i],
++--        #                    dataset['rewards'][i],
++--        #                    dataset['next_observations'][i]))
++--    # print(first_traj)
++--
++--
++--    # print(dataset['rewards'].shape) # An N x dim_observation Numpy array of observations
++--
++--    # Alternatively, use d4rl.qlearning_dataset which
++--    # also adds next_observations.
++--
++--    # import d3rlpy
++--
++--    # # dataset, env = d3rlpy.datasets.get_dataset("halfcheetah-medium")
++--
++--    # # prepare algorithm
++--    # # sac = d3rlpy.algos.SAC().create(device="cpu")
++--
++--    # sac = d3rlpy.algos.SACConfig(
++--    #     actor_learning_rate=3e-4,
++--    #     critic_learning_rate=3e-4,
++--    #     temp_learning_rate=3e-4,
++--    #     batch_size=256,
++--    # ).create(device='cpu')
++--
++--
++--    # # train offline
++--    # # sac.fit(dataset, n_steps=1000)
++--
++--
++--    # # ready to control
++--    # actions = sac.predict(0)
++--
++--if __name__ == "__main__":
++--    main()
+++@@ -1,27 +0,0 @@
+++-name: trajectory
+++-channels:
+++-- defaults
+++-- conda-forge
+++-dependencies:
+++-- python=3.8
+++-- pip
+++-- patchelf
+++-- pip:
+++-    - -f https://download.pytorch.org/whl/torch_stable.html
+++-    - numpy
+++-    - wheel==0.38.4
+++-    - setuptools==65.5.0
+++-    - gym==0.20.0
+++-    - matplotlib==3.3.4
+++-    - torch==1.9.1+cu111
+++-    - typed-argument-parser
+++-    # - git+https://github.com/Farama-Foundation/d4rl@f2a05c0d66722499bf8031b094d9af3aea7c372b#egg=d4rl
+++-    - scikit-image==0.17.2
+++-    - scikit-video==1.1.11
+++-    - gitpython
+++-    - os
+++-    - d3rlpy
+++-    - pyclustering
+++-    - moviepy
+++-    - scipy
+++-    - scikit-learn
+++diff --git a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json
+++index 18993c1..2fdc374 100644
+++--- a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json
++++++ b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json
+++@@ -1,23 +1,23 @@
+++ {
+++     "add_extras": {
+++         "_type": "python_object (type = method)",
+++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgfhpRSlC4="
++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMB2NkZl9hY3SURz/jMzMzMzMzjAhzZXRfc2VlZJRoAmgGaAmGlFKUjA5wcmVmaXhfY29udGV4dJSIjAlzYXZlX2RpZmaUaAJoBmgNhpRSlIwHdmVyYm9zZZSIjAphZGRfZXh0cmFzlGgCaAZoEYaUUpSMBWtfb2JzlEsBjAZzdWZmaXiUjAEwlIwXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMBWtfYWN0lE6MB2xvZ2Jhc2WUjAVsb2dzL5SMB2hvcml6b26USwWMCnBlcmNlbnRpbGWUjARtZWFulIwJZ3B0X2Vwb2NolIwGbGF0ZXN0lIwFbWtkaXKUaAJoBmgghpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAZkZXZpY2WUjANjcHWUjAdjZGZfb2JzlE6MB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAhleHBfbmFtZZSMHnBsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMpSMBnByZWZpeJSMD3BsYW5zL2RlZmF1bHRzL5SMCmJlYW1fd2lkdGiUSyCMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIcmVuZGVyZXKUjAhSZW5kZXJlcpSMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMCHZpc19mcmVxlEsyjAlwbGFuX2ZyZXGUSwGMCmdldF9jb21taXSUaAJoBmg3hpRSlIwIbl9leHBhbmSUSwKMCHNhdmVwYXRolIw7bG9ncy9oYWxmY2hlZXRhaC1tZWRpdW0tdjIvcGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMyLzCUjAtyZWFkX2NvbmZpZ5RoAmgGaD2GlFKUjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaECGlFKUdWJoEYaUUpQu"
+++     },
+++     "beam_width": 32,
+++     "cdf_act": 0.6,
+++     "cdf_obs": null,
+++-    "commit": "cca8d898e10f9f6102a8c33dac7758e2993dfc60 halfcheetah-xrl",
++++    "commit": "f77a194996872879bd0414fcb3dbc15b3da29cb8 main",
+++     "config": "config.offline",
+++     "dataset": "halfcheetah-medium-v2",
+++     "device": "cpu",
+++     "exp_name": "plans/defaults/freq1_H5_beam32",
+++     "generate_exp_name": {
+++         "_type": "python_object (type = method)",
+++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgxhpRSlC4="
++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMB2NkZl9hY3SURz/jMzMzMzMzjAhzZXRfc2VlZJRoAmgGaAmGlFKUjA5wcmVmaXhfY29udGV4dJSIjAlzYXZlX2RpZmaUaAJoBmgNhpRSlIwHdmVyYm9zZZSIjAphZGRfZXh0cmFzlGgCaAZoEYaUUpSMBWtfb2JzlEsBjAZzdWZmaXiUjAEwlIwXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMBWtfYWN0lE6MB2xvZ2Jhc2WUjAVsb2dzL5SMB2hvcml6b26USwWMCnBlcmNlbnRpbGWUjARtZWFulIwJZ3B0X2Vwb2NolIwGbGF0ZXN0lIwFbWtkaXKUaAJoBmgghpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAZkZXZpY2WUjANjcHWUjAdjZGZfb2JzlE6MB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAhleHBfbmFtZZSMHnBsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMpSMBnByZWZpeJSMD3BsYW5zL2RlZmF1bHRzL5SMCmJlYW1fd2lkdGiUSyCMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIcmVuZGVyZXKUjAhSZW5kZXJlcpSMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMCHZpc19mcmVxlEsyjAlwbGFuX2ZyZXGUSwGMCmdldF9jb21taXSUaAJoBmg3hpRSlIwIbl9leHBhbmSUSwKMCHNhdmVwYXRolIw7bG9ncy9oYWxmY2hlZXRhaC1tZWRpdW0tdjIvcGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMyLzCUjAtyZWFkX2NvbmZpZ5RoAmgGaD2GlFKUjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaECGlFKUdWJoQIaUUpQu"
+++     },
+++     "get_commit": {
+++         "_type": "python_object (type = method)",
+++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmglhpRSlC4="
++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMB2NkZl9hY3SURz/jMzMzMzMzjAhzZXRfc2VlZJRoAmgGaAmGlFKUjA5wcmVmaXhfY29udGV4dJSIjAlzYXZlX2RpZmaUaAJoBmgNhpRSlIwHdmVyYm9zZZSIjAphZGRfZXh0cmFzlGgCaAZoEYaUUpSMBWtfb2JzlEsBjAZzdWZmaXiUjAEwlIwXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMBWtfYWN0lE6MB2xvZ2Jhc2WUjAVsb2dzL5SMB2hvcml6b26USwWMCnBlcmNlbnRpbGWUjARtZWFulIwJZ3B0X2Vwb2NolIwGbGF0ZXN0lIwFbWtkaXKUaAJoBmgghpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAZkZXZpY2WUjANjcHWUjAdjZGZfb2JzlE6MB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAhleHBfbmFtZZSMHnBsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMpSMBnByZWZpeJSMD3BsYW5zL2RlZmF1bHRzL5SMCmJlYW1fd2lkdGiUSyCMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIcmVuZGVyZXKUjAhSZW5kZXJlcpSMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMCHZpc19mcmVxlEsyjAlwbGFuX2ZyZXGUSwGMCmdldF9jb21taXSUaAJoBmg3hpRSlIwIbl9leHBhbmSUSwKMCHNhdmVwYXRolIw7bG9ncy9oYWxmY2hlZXRhaC1tZWRpdW0tdjIvcGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMyLzCUjAtyZWFkX2NvbmZpZ5RoAmgGaD2GlFKUjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaECGlFKUdWJoN4aUUpQu"
+++     },
+++     "gpt_epoch": "latest",
+++     "gpt_loadpath": "gpt/pretrained",
+++@@ -28,7 +28,7 @@
+++     "max_context_transitions": 5,
+++     "mkdir": {
+++         "_type": "python_object (type = method)",
+++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgihpRSlC4="
++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMB2NkZl9hY3SURz/jMzMzMzMzjAhzZXRfc2VlZJRoAmgGaAmGlFKUjA5wcmVmaXhfY29udGV4dJSIjAlzYXZlX2RpZmaUaAJoBmgNhpRSlIwHdmVyYm9zZZSIjAphZGRfZXh0cmFzlGgCaAZoEYaUUpSMBWtfb2JzlEsBjAZzdWZmaXiUjAEwlIwXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMBWtfYWN0lE6MB2xvZ2Jhc2WUjAVsb2dzL5SMB2hvcml6b26USwWMCnBlcmNlbnRpbGWUjARtZWFulIwJZ3B0X2Vwb2NolIwGbGF0ZXN0lIwFbWtkaXKUaAJoBmgghpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAZkZXZpY2WUjANjcHWUjAdjZGZfb2JzlE6MB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAhleHBfbmFtZZSMHnBsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMpSMBnByZWZpeJSMD3BsYW5zL2RlZmF1bHRzL5SMCmJlYW1fd2lkdGiUSyCMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIcmVuZGVyZXKUjAhSZW5kZXJlcpSMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMCHZpc19mcmVxlEsyjAlwbGFuX2ZyZXGUSwGMCmdldF9jb21taXSUaAJoBmg3hpRSlIwIbl9leHBhbmSUSwKMCHNhdmVwYXRolIw7bG9ncy9oYWxmY2hlZXRhaC1tZWRpdW0tdjIvcGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMyLzCUjAtyZWFkX2NvbmZpZ5RoAmgGaD2GlFKUjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaECGlFKUdWJoIIaUUpQu"
+++     },
+++     "n_expand": 2,
+++     "percentile": "mean",
+++@@ -37,24 +37,24 @@
+++     "prefix_context": true,
+++     "read_config": {
+++         "_type": "python_object (type = method)",
+++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgZhpRSlC4="
++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMB2NkZl9hY3SURz/jMzMzMzMzjAhzZXRfc2VlZJRoAmgGaAmGlFKUjA5wcmVmaXhfY29udGV4dJSIjAlzYXZlX2RpZmaUaAJoBmgNhpRSlIwHdmVyYm9zZZSIjAphZGRfZXh0cmFzlGgCaAZoEYaUUpSMBWtfb2JzlEsBjAZzdWZmaXiUjAEwlIwXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMBWtfYWN0lE6MB2xvZ2Jhc2WUjAVsb2dzL5SMB2hvcml6b26USwWMCnBlcmNlbnRpbGWUjARtZWFulIwJZ3B0X2Vwb2NolIwGbGF0ZXN0lIwFbWtkaXKUaAJoBmgghpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAZkZXZpY2WUjANjcHWUjAdjZGZfb2JzlE6MB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAhleHBfbmFtZZSMHnBsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMpSMBnByZWZpeJSMD3BsYW5zL2RlZmF1bHRzL5SMCmJlYW1fd2lkdGiUSyCMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIcmVuZGVyZXKUjAhSZW5kZXJlcpSMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMCHZpc19mcmVxlEsyjAlwbGFuX2ZyZXGUSwGMCmdldF9jb21taXSUaAJoBmg3hpRSlIwIbl9leHBhbmSUSwKMCHNhdmVwYXRolIw7bG9ncy9oYWxmY2hlZXRhaC1tZWRpdW0tdjIvcGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMyLzCUjAtyZWFkX2NvbmZpZ5RoAmgGaD2GlFKUjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaECGlFKUdWJoPYaUUpQu"
+++     },
+++     "renderer": "Renderer",
+++     "reproducibility": {
+++-        "command_line": "python scripts/xrl_v2.py --dataset halfcheetah-medium-v2 --gpt_loadpath gpt/pretrained",
++++        "command_line": "python xrl_v2.py --dataset halfcheetah-medium-v2 --gpt_loadpath gpt/pretrained",
+++         "git_has_uncommitted_changes": true,
+++         "git_root": "/home/colin/Desktop/FACT/FACT_assignment",
+++-        "git_url": "https://github.com/fclio/FACT_assignment/tree/cca8d898e10f9f6102a8c33dac7758e2993dfc60",
+++-        "time": "Fri Feb  2 11:39:41 2024"
++++        "git_url": "https://github.com/fclio/FACT_assignment/tree/f77a194996872879bd0414fcb3dbc15b3da29cb8",
++++        "time": "Sat Feb  3 22:05:42 2024"
+++     },
+++     "save_diff": {
+++         "_type": "python_object (type = method)",
+++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1Ymg0hpRSlC4="
++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMB2NkZl9hY3SURz/jMzMzMzMzjAhzZXRfc2VlZJRoAmgGaAmGlFKUjA5wcmVmaXhfY29udGV4dJSIjAlzYXZlX2RpZmaUaAJoBmgNhpRSlIwHdmVyYm9zZZSIjAphZGRfZXh0cmFzlGgCaAZoEYaUUpSMBWtfb2JzlEsBjAZzdWZmaXiUjAEwlIwXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMBWtfYWN0lE6MB2xvZ2Jhc2WUjAVsb2dzL5SMB2hvcml6b26USwWMCnBlcmNlbnRpbGWUjARtZWFulIwJZ3B0X2Vwb2NolIwGbGF0ZXN0lIwFbWtkaXKUaAJoBmgghpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAZkZXZpY2WUjANjcHWUjAdjZGZfb2JzlE6MB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAhleHBfbmFtZZSMHnBsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMpSMBnByZWZpeJSMD3BsYW5zL2RlZmF1bHRzL5SMCmJlYW1fd2lkdGiUSyCMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIcmVuZGVyZXKUjAhSZW5kZXJlcpSMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMCHZpc19mcmVxlEsyjAlwbGFuX2ZyZXGUSwGMCmdldF9jb21taXSUaAJoBmg3hpRSlIwIbl9leHBhbmSUSwKMCHNhdmVwYXRolIw7bG9ncy9oYWxmY2hlZXRhaC1tZWRpdW0tdjIvcGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMyLzCUjAtyZWFkX2NvbmZpZ5RoAmgGaD2GlFKUjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaECGlFKUdWJoDYaUUpQu"
+++     },
+++     "savepath": "logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0",
+++     "set_seed": {
+++         "_type": "python_object (type = method)",
+++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgqhpRSlC4="
++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMB2NkZl9hY3SURz/jMzMzMzMzjAhzZXRfc2VlZJRoAmgGaAmGlFKUjA5wcmVmaXhfY29udGV4dJSIjAlzYXZlX2RpZmaUaAJoBmgNhpRSlIwHdmVyYm9zZZSIjAphZGRfZXh0cmFzlGgCaAZoEYaUUpSMBWtfb2JzlEsBjAZzdWZmaXiUjAEwlIwXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMBWtfYWN0lE6MB2xvZ2Jhc2WUjAVsb2dzL5SMB2hvcml6b26USwWMCnBlcmNlbnRpbGWUjARtZWFulIwJZ3B0X2Vwb2NolIwGbGF0ZXN0lIwFbWtkaXKUaAJoBmgghpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAZkZXZpY2WUjANjcHWUjAdjZGZfb2JzlE6MB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAhleHBfbmFtZZSMHnBsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMpSMBnByZWZpeJSMD3BsYW5zL2RlZmF1bHRzL5SMCmJlYW1fd2lkdGiUSyCMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIcmVuZGVyZXKUjAhSZW5kZXJlcpSMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMCHZpc19mcmVxlEsyjAlwbGFuX2ZyZXGUSwGMCmdldF9jb21taXSUaAJoBmg3hpRSlIwIbl9leHBhbmSUSwKMCHNhdmVwYXRolIw7bG9ncy9oYWxmY2hlZXRhaC1tZWRpdW0tdjIvcGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMyLzCUjAtyZWFkX2NvbmZpZ5RoAmgGaD2GlFKUjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaECGlFKUdWJoCYaUUpQu"
+++     },
+++     "suffix": "0",
+++     "verbose": true,
+++diff --git a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt
+++index 2c00d59..07b19f2 100644
+++--- a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt
++++++ b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt
+++@@ -1,848 +1,16924 @@
+++-diff --git a/.gitignore b/.gitignore
+++-index 30d226d..a18312a 100644
+++---- a/.gitignore
+++-+++ b/.gitignore
+++-@@ -1,6 +1,5 @@
+++- __pycache__/
+++- *.mp4
+++--halfcheetah/logs/
+++- .ipynb_checkpoints/
+++- __pycache__/
+++- Open Notebook.onetoc2 
+++-diff --git a/gridworld/gridworld_expts.ipynb b/gridworld/gridworld_expts.ipynb
+++-index e2a261b..bd0b720 100644
+++---- a/gridworld/gridworld_expts.ipynb
+++-+++ b/gridworld/gridworld_expts.ipynb
+++-@@ -1570,7 +1570,7 @@
+++-    "name": "python",
+++-    "nbconvert_exporter": "python",
+++-    "pygments_lexer": "ipython3",
+++--   "version": "3.8.10"
+++-+   "version": "3.10.12"
+++-   }
+++-  },
+++-  "nbformat": 4,
+++-diff --git a/halfcheetah/clusters.npy b/halfcheetah/clusters.npy
++++diff --git a/halfcheetah/environment.yml b/halfcheetah/environment.yml
+++ deleted file mode 100644
+++-index 3c198aa..0000000
+++-Binary files a/halfcheetah/clusters.npy and /dev/null differ
+++-diff --git a/halfcheetah/embeddings.npy b/halfcheetah/embeddings.npy
+++-deleted file mode 100644
+++-index 912d68c..0000000
+++-Binary files a/halfcheetah/embeddings.npy and /dev/null differ
+++-diff --git a/halfcheetah/halfcheetah.py b/halfcheetah/halfcheetah.py
+++-deleted file mode 100644
+++-index 626c42d..0000000
+++---- a/halfcheetah/halfcheetah.py
++++index e81007c..0000000
++++--- a/halfcheetah/environment.yml
+++ +++ /dev/null
+++-@@ -1,112 +0,0 @@
+++--import gym
+++--import d4rl # Import required to register environments, you may need to also import the submodule
+++--import numpy as np
+++--import d3rlpy
+++--
+++--def main():
+++--    dataset_d3, env = d3rlpy.datasets.get_dataset("halfcheetah-medium-v2")
+++--
+++--    print(dataset_d3.observations.shape)
+++--    print(dataset_d3.actions.shape)
+++--    print(dataset_d3.rewards.shape)
+++--        # print(dataset_d3.next_observations.shape)
+++--    print(dataset_d3.terminals.shape)
+++--    print(dataset_d3.terminals.sum()) # no
+++--
+++--    env = gym.make('halfcheetah-medium-v2')
+++--    dataset_d4 = d4rl.qlearning_dataset(env)
+++--
+++--    print(dataset_d4['observations'].shape)
+++--    print(dataset_d4['rewards'].shape)
+++--    print(dataset_d4['terminals'].shape)
+++--    print(dataset_d4['actions'].shape)
+++--
+++--    print(dataset_d4['rewards'][1])
+++--    print(dataset_d3.rewards[1])
+++--
+++--
+++--    print(np.allclose(dataset_d3.actions[100], dataset_d4['actions'][100]))
+++--
+++--    for j in range(1000):
+++--        for i in range(999):
+++--            if dataset_d4['rewards'][j * 999 + i] != dataset_d3.rewards[j * 1000 + i]: print("yo", i)
+++--        # if not np.allclose(dataset_d3.observations[i], dataset_d4['observations'][i]): print('obs ongelijk')
+++--        # if not np.allclose(dataset_d3.rewards[i], dataset_d4['rewards'][i]): print('obs ongelijk')
+++--        # if not np.allclose(dataset_d3.actions[i], dataset_d4['actions'][i]): print('obs ongelijk')
+++--
+++--    sac = d3rlpy.algos.SAC(
+++--        actor_learning_rate=3e-4,
+++--        critic_learning_rate=3e-4,
+++--        temp_learning_rate=3e-4,
+++--        batch_size=256)
+++--
+++--    print(sac)
+++--    sac.fit(dataset_d3, n_steps=10000)
+++--
+++--    actions = sac.predict(dataset_d3.observations[0])
+++--
+++--    print(actions)
+++--
+++--
+++--    return
+++--    print('yo!')
+++--
+++--    # Create the environment
+++--    env = gym.make('halfcheetah-medium-v2')
+++--
+++--    # d4rl abides by the OpenAI gym interface
+++--    env.reset()
+++--    env.step(env.action_space.sample())
+++--
+++--    # Each task is associated with a dataset
+++--    # dataset contains observations, actions, rewards, terminals, and infos
+++--    # dataset = env.get_dataset()
+++--    dataset = d4rl.qlearning_dataset(env)
+++--
+++--    print(dataset.keys()) # An N x dim_observation Numpy array of observations
+++--    print(dataset['rewards'].shape) # An N x dim_observation Numpy array of observations
+++--
+++--
+++--    first_traj = []
+++--    for i in range(50000):
+++--        if not np.allclose(dataset['next_observations'][i],dataset['observations'][i+1]): print("yo", i, dataset['terminals'][i])
+++--        # if dataset['terminals'][i] == True:
+++--        #     print('traj ended at', i)
+++--        #     break
+++--
+++--        # first_traj.append((dataset['observations'][i],
+++--        #                    dataset['actions'][i],
+++--        #                    dataset['rewards'][i],
+++--        #                    dataset['next_observations'][i]))
+++--    # print(first_traj)
+++--
+++--
+++--    # print(dataset['rewards'].shape) # An N x dim_observation Numpy array of observations
+++--
+++--    # Alternatively, use d4rl.qlearning_dataset which
+++--    # also adds next_observations.
+++--
+++--    # import d3rlpy
+++--
+++--    # # dataset, env = d3rlpy.datasets.get_dataset("halfcheetah-medium")
+++--
+++--    # # prepare algorithm
+++--    # # sac = d3rlpy.algos.SAC().create(device="cpu")
+++--
+++--    # sac = d3rlpy.algos.SACConfig(
+++--    #     actor_learning_rate=3e-4,
+++--    #     critic_learning_rate=3e-4,
+++--    #     temp_learning_rate=3e-4,
+++--    #     batch_size=256,
+++--    # ).create(device='cpu')
+++--
+++--
+++--    # # train offline
+++--    # # sac.fit(dataset, n_steps=1000)
+++--
+++--
+++--    # # ready to control
+++--    # actions = sac.predict(0)
+++--
+++--if __name__ == "__main__":
+++--    main()
++++@@ -1,27 +0,0 @@
++++-name: trajectory
++++-channels:
++++-- defaults
++++-- conda-forge
++++-dependencies:
++++-- python=3.8
++++-- pip
++++-- patchelf
++++-- pip:
++++-    - -f https://download.pytorch.org/whl/torch_stable.html
++++-    - numpy
++++-    - wheel==0.38.4
++++-    - setuptools==65.5.0
++++-    - gym==0.20.0
++++-    - matplotlib==3.3.4
++++-    - torch==1.9.1+cu111
++++-    - typed-argument-parser
++++-    # - git+https://github.com/Farama-Foundation/d4rl@f2a05c0d66722499bf8031b094d9af3aea7c372b#egg=d4rl
++++-    - scikit-image==0.17.2
++++-    - scikit-video==1.1.11
++++-    - gitpython
++++-    - os
++++-    - d3rlpy
++++-    - pyclustering
++++-    - moviepy
++++-    - scipy
++++-    - scikit-learn
++++diff --git a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json
++++index 18993c1..249be28 100644
++++--- a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json
+++++++ b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json
++++@@ -1,23 +1,23 @@
++++ {
++++     "add_extras": {
++++         "_type": "python_object (type = method)",
++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgfhpRSlC4="
+++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMBnByZWZpeJSMD3BsYW5zL2RlZmF1bHRzL5SMB2NkZl9hY3SURz/jMzMzMzMzjAVrX29ic5RLAYwRZ2VuZXJhdGVfZXhwX25hbWWUaAJoBmgMhpRSlIwHbG9nYmFzZZSMBWxvZ3MvlIwGc3VmZml4lIwBMJSMB2NkZl9vYnOUTowMZ3B0X2xvYWRwYXRolIwOZ3B0L3ByZXRyYWluZWSUjAtyZWFkX2NvbmZpZ5RoAmgGaBaGlFKUjAdkYXRhc2V0lIwVaGFsZmNoZWV0YWgtbWVkaXVtLXYylIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAh2aXNfZnJlcZRLMowIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMCXBsYW5fZnJlcZRLAYwHaG9yaXpvbpRLBYwKYmVhbV93aWR0aJRLIIwFa19hY3SUTowHdmVyYm9zZZSIjAphZGRfZXh0cmFzlGgCaAZoJYaUUpSMCHNldF9zZWVklGgCaAZoKIaUUpSMCXNhdmVfZGlmZpRoAmgGaCuGlFKUjApnZXRfY29tbWl0lGgCaAZoLoaUUpSMBmRldmljZZSMA2NwdZSMCG5fZXhwYW5klEsCjA5wcmVmaXhfY29udGV4dJSIjAlncHRfZXBvY2iUjAZsYXRlc3SUjAZjb21taXSUjC1mNzdhMTk0OTk2ODcyODc5YmQwNDE0ZmNiM2RiYzE1YjNkYTI5Y2I4IG1haW6UjAVta2RpcpRoAmgGaDmGlFKUjAZjb25maWeUjA5jb25maWcub2ZmbGluZZSMCHJlbmRlcmVylIwIUmVuZGVyZXKUjApwZXJjZW50aWxllIwEbWVhbpSMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFdWJoJYaUUpQu"
++++     },
++++     "beam_width": 32,
++++     "cdf_act": 0.6,
++++     "cdf_obs": null,
++++-    "commit": "cca8d898e10f9f6102a8c33dac7758e2993dfc60 halfcheetah-xrl",
+++++    "commit": "f77a194996872879bd0414fcb3dbc15b3da29cb8 main",
++++     "config": "config.offline",
++++     "dataset": "halfcheetah-medium-v2",
++++     "device": "cpu",
++++     "exp_name": "plans/defaults/freq1_H5_beam32",
++++     "generate_exp_name": {
++++         "_type": "python_object (type = method)",
++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgxhpRSlC4="
+++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMBnByZWZpeJSMD3BsYW5zL2RlZmF1bHRzL5SMB2NkZl9hY3SURz/jMzMzMzMzjAVrX29ic5RLAYwRZ2VuZXJhdGVfZXhwX25hbWWUaAJoBmgMhpRSlIwHbG9nYmFzZZSMBWxvZ3MvlIwGc3VmZml4lIwBMJSMB2NkZl9vYnOUTowMZ3B0X2xvYWRwYXRolIwOZ3B0L3ByZXRyYWluZWSUjAtyZWFkX2NvbmZpZ5RoAmgGaBaGlFKUjAdkYXRhc2V0lIwVaGFsZmNoZWV0YWgtbWVkaXVtLXYylIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAh2aXNfZnJlcZRLMowIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMCXBsYW5fZnJlcZRLAYwHaG9yaXpvbpRLBYwKYmVhbV93aWR0aJRLIIwFa19hY3SUTowHdmVyYm9zZZSIjAphZGRfZXh0cmFzlGgCaAZoJYaUUpSMCHNldF9zZWVklGgCaAZoKIaUUpSMCXNhdmVfZGlmZpRoAmgGaCuGlFKUjApnZXRfY29tbWl0lGgCaAZoLoaUUpSMBmRldmljZZSMA2NwdZSMCG5fZXhwYW5klEsCjA5wcmVmaXhfY29udGV4dJSIjAlncHRfZXBvY2iUjAZsYXRlc3SUjAZjb21taXSUjC1mNzdhMTk0OTk2ODcyODc5YmQwNDE0ZmNiM2RiYzE1YjNkYTI5Y2I4IG1haW6UjAVta2RpcpRoAmgGaDmGlFKUjAZjb25maWeUjA5jb25maWcub2ZmbGluZZSMCHJlbmRlcmVylIwIUmVuZGVyZXKUjApwZXJjZW50aWxllIwEbWVhbpSMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFdWJoDIaUUpQu"
++++     },
++++     "get_commit": {
++++         "_type": "python_object (type = method)",
++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmglhpRSlC4="
+++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMBnByZWZpeJSMD3BsYW5zL2RlZmF1bHRzL5SMB2NkZl9hY3SURz/jMzMzMzMzjAVrX29ic5RLAYwRZ2VuZXJhdGVfZXhwX25hbWWUaAJoBmgMhpRSlIwHbG9nYmFzZZSMBWxvZ3MvlIwGc3VmZml4lIwBMJSMB2NkZl9vYnOUTowMZ3B0X2xvYWRwYXRolIwOZ3B0L3ByZXRyYWluZWSUjAtyZWFkX2NvbmZpZ5RoAmgGaBaGlFKUjAdkYXRhc2V0lIwVaGFsZmNoZWV0YWgtbWVkaXVtLXYylIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAh2aXNfZnJlcZRLMowIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMCXBsYW5fZnJlcZRLAYwHaG9yaXpvbpRLBYwKYmVhbV93aWR0aJRLIIwFa19hY3SUTowHdmVyYm9zZZSIjAphZGRfZXh0cmFzlGgCaAZoJYaUUpSMCHNldF9zZWVklGgCaAZoKIaUUpSMCXNhdmVfZGlmZpRoAmgGaCuGlFKUjApnZXRfY29tbWl0lGgCaAZoLoaUUpSMBmRldmljZZSMA2NwdZSMCG5fZXhwYW5klEsCjA5wcmVmaXhfY29udGV4dJSIjAlncHRfZXBvY2iUjAZsYXRlc3SUjAZjb21taXSUjC1mNzdhMTk0OTk2ODcyODc5YmQwNDE0ZmNiM2RiYzE1YjNkYTI5Y2I4IG1haW6UjAVta2RpcpRoAmgGaDmGlFKUjAZjb25maWeUjA5jb25maWcub2ZmbGluZZSMCHJlbmRlcmVylIwIUmVuZGVyZXKUjApwZXJjZW50aWxllIwEbWVhbpSMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFdWJoLoaUUpQu"
++++     },
++++     "gpt_epoch": "latest",
++++     "gpt_loadpath": "gpt/pretrained",
++++@@ -28,7 +28,7 @@
++++     "max_context_transitions": 5,
++++     "mkdir": {
++++         "_type": "python_object (type = method)",
++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgihpRSlC4="
+++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMBnByZWZpeJSMD3BsYW5zL2RlZmF1bHRzL5SMB2NkZl9hY3SURz/jMzMzMzMzjAVrX29ic5RLAYwRZ2VuZXJhdGVfZXhwX25hbWWUaAJoBmgMhpRSlIwHbG9nYmFzZZSMBWxvZ3MvlIwGc3VmZml4lIwBMJSMB2NkZl9vYnOUTowMZ3B0X2xvYWRwYXRolIwOZ3B0L3ByZXRyYWluZWSUjAtyZWFkX2NvbmZpZ5RoAmgGaBaGlFKUjAdkYXRhc2V0lIwVaGFsZmNoZWV0YWgtbWVkaXVtLXYylIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAh2aXNfZnJlcZRLMowIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMCXBsYW5fZnJlcZRLAYwHaG9yaXpvbpRLBYwKYmVhbV93aWR0aJRLIIwFa19hY3SUTowHdmVyYm9zZZSIjAphZGRfZXh0cmFzlGgCaAZoJYaUUpSMCHNldF9zZWVklGgCaAZoKIaUUpSMCXNhdmVfZGlmZpRoAmgGaCuGlFKUjApnZXRfY29tbWl0lGgCaAZoLoaUUpSMBmRldmljZZSMA2NwdZSMCG5fZXhwYW5klEsCjA5wcmVmaXhfY29udGV4dJSIjAlncHRfZXBvY2iUjAZsYXRlc3SUjAZjb21taXSUjC1mNzdhMTk0OTk2ODcyODc5YmQwNDE0ZmNiM2RiYzE1YjNkYTI5Y2I4IG1haW6UjAVta2RpcpRoAmgGaDmGlFKUjAZjb25maWeUjA5jb25maWcub2ZmbGluZZSMCHJlbmRlcmVylIwIUmVuZGVyZXKUjApwZXJjZW50aWxllIwEbWVhbpSMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFdWJoOYaUUpQu"
++++     },
++++     "n_expand": 2,
++++     "percentile": "mean",
++++@@ -37,24 +37,24 @@
++++     "prefix_context": true,
++++     "read_config": {
++++         "_type": "python_object (type = method)",
++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgZhpRSlC4="
+++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMBnByZWZpeJSMD3BsYW5zL2RlZmF1bHRzL5SMB2NkZl9hY3SURz/jMzMzMzMzjAVrX29ic5RLAYwRZ2VuZXJhdGVfZXhwX25hbWWUaAJoBmgMhpRSlIwHbG9nYmFzZZSMBWxvZ3MvlIwGc3VmZml4lIwBMJSMB2NkZl9vYnOUTowMZ3B0X2xvYWRwYXRolIwOZ3B0L3ByZXRyYWluZWSUjAtyZWFkX2NvbmZpZ5RoAmgGaBaGlFKUjAdkYXRhc2V0lIwVaGFsZmNoZWV0YWgtbWVkaXVtLXYylIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAh2aXNfZnJlcZRLMowIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMCXBsYW5fZnJlcZRLAYwHaG9yaXpvbpRLBYwKYmVhbV93aWR0aJRLIIwFa19hY3SUTowHdmVyYm9zZZSIjAphZGRfZXh0cmFzlGgCaAZoJYaUUpSMCHNldF9zZWVklGgCaAZoKIaUUpSMCXNhdmVfZGlmZpRoAmgGaCuGlFKUjApnZXRfY29tbWl0lGgCaAZoLoaUUpSMBmRldmljZZSMA2NwdZSMCG5fZXhwYW5klEsCjA5wcmVmaXhfY29udGV4dJSIjAlncHRfZXBvY2iUjAZsYXRlc3SUjAZjb21taXSUjC1mNzdhMTk0OTk2ODcyODc5YmQwNDE0ZmNiM2RiYzE1YjNkYTI5Y2I4IG1haW6UjAVta2RpcpRoAmgGaDmGlFKUjAZjb25maWeUjA5jb25maWcub2ZmbGluZZSMCHJlbmRlcmVylIwIUmVuZGVyZXKUjApwZXJjZW50aWxllIwEbWVhbpSMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFdWJoFoaUUpQu"
++++     },
++++     "renderer": "Renderer",
++++     "reproducibility": {
++++-        "command_line": "python scripts/xrl_v2.py --dataset halfcheetah-medium-v2 --gpt_loadpath gpt/pretrained",
+++++        "command_line": "python xrl_v2.py --dataset halfcheetah-medium-v2 --gpt_loadpath gpt/pretrained",
++++         "git_has_uncommitted_changes": true,
++++         "git_root": "/home/colin/Desktop/FACT/FACT_assignment",
++++-        "git_url": "https://github.com/fclio/FACT_assignment/tree/cca8d898e10f9f6102a8c33dac7758e2993dfc60",
++++-        "time": "Fri Feb  2 11:39:41 2024"
+++++        "git_url": "https://github.com/fclio/FACT_assignment/tree/f77a194996872879bd0414fcb3dbc15b3da29cb8",
+++++        "time": "Sat Feb  3 22:05:17 2024"
++++     },
++++     "save_diff": {
++++         "_type": "python_object (type = method)",
++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1Ymg0hpRSlC4="
+++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMBnByZWZpeJSMD3BsYW5zL2RlZmF1bHRzL5SMB2NkZl9hY3SURz/jMzMzMzMzjAVrX29ic5RLAYwRZ2VuZXJhdGVfZXhwX25hbWWUaAJoBmgMhpRSlIwHbG9nYmFzZZSMBWxvZ3MvlIwGc3VmZml4lIwBMJSMB2NkZl9vYnOUTowMZ3B0X2xvYWRwYXRolIwOZ3B0L3ByZXRyYWluZWSUjAtyZWFkX2NvbmZpZ5RoAmgGaBaGlFKUjAdkYXRhc2V0lIwVaGFsZmNoZWV0YWgtbWVkaXVtLXYylIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAh2aXNfZnJlcZRLMowIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMCXBsYW5fZnJlcZRLAYwHaG9yaXpvbpRLBYwKYmVhbV93aWR0aJRLIIwFa19hY3SUTowHdmVyYm9zZZSIjAphZGRfZXh0cmFzlGgCaAZoJYaUUpSMCHNldF9zZWVklGgCaAZoKIaUUpSMCXNhdmVfZGlmZpRoAmgGaCuGlFKUjApnZXRfY29tbWl0lGgCaAZoLoaUUpSMBmRldmljZZSMA2NwdZSMCG5fZXhwYW5klEsCjA5wcmVmaXhfY29udGV4dJSIjAlncHRfZXBvY2iUjAZsYXRlc3SUjAZjb21taXSUjC1mNzdhMTk0OTk2ODcyODc5YmQwNDE0ZmNiM2RiYzE1YjNkYTI5Y2I4IG1haW6UjAVta2RpcpRoAmgGaDmGlFKUjAZjb25maWeUjA5jb25maWcub2ZmbGluZZSMCHJlbmRlcmVylIwIUmVuZGVyZXKUjApwZXJjZW50aWxllIwEbWVhbpSMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFdWJoK4aUUpQu"
++++     },
++++     "savepath": "logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0",
++++     "set_seed": {
++++         "_type": "python_object (type = method)",
++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgqhpRSlC4="
+++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMBnByZWZpeJSMD3BsYW5zL2RlZmF1bHRzL5SMB2NkZl9hY3SURz/jMzMzMzMzjAVrX29ic5RLAYwRZ2VuZXJhdGVfZXhwX25hbWWUaAJoBmgMhpRSlIwHbG9nYmFzZZSMBWxvZ3MvlIwGc3VmZml4lIwBMJSMB2NkZl9vYnOUTowMZ3B0X2xvYWRwYXRolIwOZ3B0L3ByZXRyYWluZWSUjAtyZWFkX2NvbmZpZ5RoAmgGaBaGlFKUjAdkYXRhc2V0lIwVaGFsZmNoZWV0YWgtbWVkaXVtLXYylIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAh2aXNfZnJlcZRLMowIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMCXBsYW5fZnJlcZRLAYwHaG9yaXpvbpRLBYwKYmVhbV93aWR0aJRLIIwFa19hY3SUTowHdmVyYm9zZZSIjAphZGRfZXh0cmFzlGgCaAZoJYaUUpSMCHNldF9zZWVklGgCaAZoKIaUUpSMCXNhdmVfZGlmZpRoAmgGaCuGlFKUjApnZXRfY29tbWl0lGgCaAZoLoaUUpSMBmRldmljZZSMA2NwdZSMCG5fZXhwYW5klEsCjA5wcmVmaXhfY29udGV4dJSIjAlncHRfZXBvY2iUjAZsYXRlc3SUjAZjb21taXSUjC1mNzdhMTk0OTk2ODcyODc5YmQwNDE0ZmNiM2RiYzE1YjNkYTI5Y2I4IG1haW6UjAVta2RpcpRoAmgGaDmGlFKUjAZjb25maWeUjA5jb25maWcub2ZmbGluZZSMCHJlbmRlcmVylIwIUmVuZGVyZXKUjApwZXJjZW50aWxllIwEbWVhbpSMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFdWJoKIaUUpQu"
++++     },
++++     "suffix": "0",
++++     "verbose": true,
++++diff --git a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt
++++index 2c00d59..99a9b3d 100644
++++--- a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt
+++++++ b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt
++++@@ -1,1250 +1,15572 @@
++++-diff --git a/.gitignore b/.gitignore
++++-index 30d226d..a18312a 100644
++++---- a/.gitignore
++++-+++ b/.gitignore
++++-@@ -1,6 +1,5 @@
++++- __pycache__/
++++- *.mp4
++++--halfcheetah/logs/
++++- .ipynb_checkpoints/
++++- __pycache__/
++++- Open Notebook.onetoc2 
++++-diff --git a/gridworld/gridworld_expts.ipynb b/gridworld/gridworld_expts.ipynb
++++-index e2a261b..bd0b720 100644
++++---- a/gridworld/gridworld_expts.ipynb
++++-+++ b/gridworld/gridworld_expts.ipynb
++++-@@ -1570,7 +1570,7 @@
++++-    "name": "python",
++++-    "nbconvert_exporter": "python",
++++-    "pygments_lexer": "ipython3",
++++--   "version": "3.8.10"
++++-+   "version": "3.10.12"
++++-   }
++++-  },
++++-  "nbformat": 4,
++++-diff --git a/halfcheetah/clusters.npy b/halfcheetah/clusters.npy
+++++diff --git a/halfcheetah/environment.yml b/halfcheetah/environment.yml
++++ deleted file mode 100644
++++-index 3c198aa..0000000
++++-Binary files a/halfcheetah/clusters.npy and /dev/null differ
++++-diff --git a/halfcheetah/embeddings.npy b/halfcheetah/embeddings.npy
++++-deleted file mode 100644
++++-index 912d68c..0000000
++++-Binary files a/halfcheetah/embeddings.npy and /dev/null differ
++++-diff --git a/halfcheetah/halfcheetah.py b/halfcheetah/halfcheetah.py
++++-deleted file mode 100644
++++-index 626c42d..0000000
++++---- a/halfcheetah/halfcheetah.py
+++++index e81007c..0000000
+++++--- a/halfcheetah/environment.yml
++++ +++ /dev/null
++++-@@ -1,112 +0,0 @@
++++--import gym
++++--import d4rl # Import required to register environments, you may need to also import the submodule
++++--import numpy as np
++++--import d3rlpy
++++--
++++--def main():
++++--    dataset_d3, env = d3rlpy.datasets.get_dataset("halfcheetah-medium-v2")
++++--
++++--    print(dataset_d3.observations.shape)
++++--    print(dataset_d3.actions.shape)
++++--    print(dataset_d3.rewards.shape)
++++--        # print(dataset_d3.next_observations.shape)
++++--    print(dataset_d3.terminals.shape)
++++--    print(dataset_d3.terminals.sum()) # no
++++--
++++--    env = gym.make('halfcheetah-medium-v2')
++++--    dataset_d4 = d4rl.qlearning_dataset(env)
++++--
++++--    print(dataset_d4['observations'].shape)
++++--    print(dataset_d4['rewards'].shape)
++++--    print(dataset_d4['terminals'].shape)
++++--    print(dataset_d4['actions'].shape)
++++--
++++--    print(dataset_d4['rewards'][1])
++++--    print(dataset_d3.rewards[1])
++++--
++++--
++++--    print(np.allclose(dataset_d3.actions[100], dataset_d4['actions'][100]))
++++--
++++--    for j in range(1000):
++++--        for i in range(999):
++++--            if dataset_d4['rewards'][j * 999 + i] != dataset_d3.rewards[j * 1000 + i]: print("yo", i)
++++--        # if not np.allclose(dataset_d3.observations[i], dataset_d4['observations'][i]): print('obs ongelijk')
++++--        # if not np.allclose(dataset_d3.rewards[i], dataset_d4['rewards'][i]): print('obs ongelijk')
++++--        # if not np.allclose(dataset_d3.actions[i], dataset_d4['actions'][i]): print('obs ongelijk')
++++--
++++--    sac = d3rlpy.algos.SAC(
++++--        actor_learning_rate=3e-4,
++++--        critic_learning_rate=3e-4,
++++--        temp_learning_rate=3e-4,
++++--        batch_size=256)
++++--
++++--    print(sac)
++++--    sac.fit(dataset_d3, n_steps=10000)
++++--
++++--    actions = sac.predict(dataset_d3.observations[0])
++++--
++++--    print(actions)
++++--
++++--
++++--    return
++++--    print('yo!')
++++--
++++--    # Create the environment
++++--    env = gym.make('halfcheetah-medium-v2')
++++--
++++--    # d4rl abides by the OpenAI gym interface
++++--    env.reset()
++++--    env.step(env.action_space.sample())
++++--
++++--    # Each task is associated with a dataset
++++--    # dataset contains observations, actions, rewards, terminals, and infos
++++--    # dataset = env.get_dataset()
++++--    dataset = d4rl.qlearning_dataset(env)
++++--
++++--    print(dataset.keys()) # An N x dim_observation Numpy array of observations
++++--    print(dataset['rewards'].shape) # An N x dim_observation Numpy array of observations
++++--
++++--
++++--    first_traj = []
++++--    for i in range(50000):
++++--        if not np.allclose(dataset['next_observations'][i],dataset['observations'][i+1]): print("yo", i, dataset['terminals'][i])
++++--        # if dataset['terminals'][i] == True:
++++--        #     print('traj ended at', i)
++++--        #     break
++++--
++++--        # first_traj.append((dataset['observations'][i],
++++--        #                    dataset['actions'][i],
++++--        #                    dataset['rewards'][i],
++++--        #                    dataset['next_observations'][i]))
++++--    # print(first_traj)
++++--
++++--
++++--    # print(dataset['rewards'].shape) # An N x dim_observation Numpy array of observations
++++--
++++--    # Alternatively, use d4rl.qlearning_dataset which
++++--    # also adds next_observations.
++++--
++++--    # import d3rlpy
++++--
++++--    # # dataset, env = d3rlpy.datasets.get_dataset("halfcheetah-medium")
++++--
++++--    # # prepare algorithm
++++--    # # sac = d3rlpy.algos.SAC().create(device="cpu")
++++--
++++--    # sac = d3rlpy.algos.SACConfig(
++++--    #     actor_learning_rate=3e-4,
++++--    #     critic_learning_rate=3e-4,
++++--    #     temp_learning_rate=3e-4,
++++--    #     batch_size=256,
++++--    # ).create(device='cpu')
++++--
++++--
++++--    # # train offline
++++--    # # sac.fit(dataset, n_steps=1000)
++++--
++++--
++++--    # # ready to control
++++--    # actions = sac.predict(0)
++++--
++++--if __name__ == "__main__":
++++--    main()
+++++@@ -1,27 +0,0 @@
+++++-name: trajectory
+++++-channels:
+++++-- defaults
+++++-- conda-forge
+++++-dependencies:
+++++-- python=3.8
+++++-- pip
+++++-- patchelf
+++++-- pip:
+++++-    - -f https://download.pytorch.org/whl/torch_stable.html
+++++-    - numpy
+++++-    - wheel==0.38.4
+++++-    - setuptools==65.5.0
+++++-    - gym==0.20.0
+++++-    - matplotlib==3.3.4
+++++-    - torch==1.9.1+cu111
+++++-    - typed-argument-parser
+++++-    # - git+https://github.com/Farama-Foundation/d4rl@f2a05c0d66722499bf8031b094d9af3aea7c372b#egg=d4rl
+++++-    - scikit-image==0.17.2
+++++-    - scikit-video==1.1.11
+++++-    - gitpython
+++++-    - os
+++++-    - d3rlpy
+++++-    - pyclustering
+++++-    - moviepy
+++++-    - scipy
+++++-    - scikit-learn
+++++diff --git a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json
+++++index 18993c1..73a98f2 100644
+++++--- a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json
++++++++ b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json
+++++@@ -1,23 +1,23 @@
+++++ {
+++++     "add_extras": {
+++++         "_type": "python_object (type = method)",
+++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgfhpRSlC4="
++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMB3ZlcmJvc2WUiIwRZ2VuZXJhdGVfZXhwX25hbWWUaAJoBmgJhpRSlIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAVrX2FjdJROjAlzYXZlX2RpZmaUaAJoBmgPhpRSlIwKZ2V0X2NvbW1pdJRoAmgGaBKGlFKUjApiZWFtX3dpZHRolEsgjAZkZXZpY2WUjANjcHWUjAdjZGZfYWN0lEc/4zMzMzMzM4wGc3VmZml4lIwBMJSMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMBW1rZGlylGgCaAZoHYaUUpSMB2hvcml6b26USwWMCHZpc19mcmVxlEsyjA5wcmVmaXhfY29udGV4dJSIjAphZGRfZXh0cmFzlGgCaAZoI4aUUpSMB2NkZl9vYnOUTowIbl9leHBhbmSUSwKMBnByZWZpeJSMD3BsYW5zL2RlZmF1bHRzL5SMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwJcGxhbl9mcmVxlEsBjAtyZWFkX2NvbmZpZ5RoAmgGaC6GlFKUjAZjb25maWeUjA5jb25maWcub2ZmbGluZZSMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAdsb2diYXNllIwFbG9ncy+UjAxncHRfbG9hZHBhdGiUjA5ncHQvcHJldHJhaW5lZJSMCWdwdF9lcG9jaJSMBmxhdGVzdJSMCHNldF9zZWVklGgCaAZoO4aUUpSMCnBlcmNlbnRpbGWUjARtZWFulIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfb2JzlEsBdWJoI4aUUpQu"
+++++     },
+++++     "beam_width": 32,
+++++     "cdf_act": 0.6,
+++++     "cdf_obs": null,
+++++-    "commit": "cca8d898e10f9f6102a8c33dac7758e2993dfc60 halfcheetah-xrl",
++++++    "commit": "f77a194996872879bd0414fcb3dbc15b3da29cb8 main",
+++++     "config": "config.offline",
+++++     "dataset": "halfcheetah-medium-v2",
+++++     "device": "cpu",
+++++     "exp_name": "plans/defaults/freq1_H5_beam32",
+++++     "generate_exp_name": {
+++++         "_type": "python_object (type = method)",
+++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgxhpRSlC4="
++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMB3ZlcmJvc2WUiIwRZ2VuZXJhdGVfZXhwX25hbWWUaAJoBmgJhpRSlIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAVrX2FjdJROjAlzYXZlX2RpZmaUaAJoBmgPhpRSlIwKZ2V0X2NvbW1pdJRoAmgGaBKGlFKUjApiZWFtX3dpZHRolEsgjAZkZXZpY2WUjANjcHWUjAdjZGZfYWN0lEc/4zMzMzMzM4wGc3VmZml4lIwBMJSMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMBW1rZGlylGgCaAZoHYaUUpSMB2hvcml6b26USwWMCHZpc19mcmVxlEsyjA5wcmVmaXhfY29udGV4dJSIjAphZGRfZXh0cmFzlGgCaAZoI4aUUpSMB2NkZl9vYnOUTowIbl9leHBhbmSUSwKMBnByZWZpeJSMD3BsYW5zL2RlZmF1bHRzL5SMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwJcGxhbl9mcmVxlEsBjAtyZWFkX2NvbmZpZ5RoAmgGaC6GlFKUjAZjb25maWeUjA5jb25maWcub2ZmbGluZZSMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAdsb2diYXNllIwFbG9ncy+UjAxncHRfbG9hZHBhdGiUjA5ncHQvcHJldHJhaW5lZJSMCWdwdF9lcG9jaJSMBmxhdGVzdJSMCHNldF9zZWVklGgCaAZoO4aUUpSMCnBlcmNlbnRpbGWUjARtZWFulIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfb2JzlEsBdWJoCYaUUpQu"
+++++     },
+++++     "get_commit": {
+++++         "_type": "python_object (type = method)",
+++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmglhpRSlC4="
++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMB3ZlcmJvc2WUiIwRZ2VuZXJhdGVfZXhwX25hbWWUaAJoBmgJhpRSlIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAVrX2FjdJROjAlzYXZlX2RpZmaUaAJoBmgPhpRSlIwKZ2V0X2NvbW1pdJRoAmgGaBKGlFKUjApiZWFtX3dpZHRolEsgjAZkZXZpY2WUjANjcHWUjAdjZGZfYWN0lEc/4zMzMzMzM4wGc3VmZml4lIwBMJSMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMBW1rZGlylGgCaAZoHYaUUpSMB2hvcml6b26USwWMCHZpc19mcmVxlEsyjA5wcmVmaXhfY29udGV4dJSIjAphZGRfZXh0cmFzlGgCaAZoI4aUUpSMB2NkZl9vYnOUTowIbl9leHBhbmSUSwKMBnByZWZpeJSMD3BsYW5zL2RlZmF1bHRzL5SMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwJcGxhbl9mcmVxlEsBjAtyZWFkX2NvbmZpZ5RoAmgGaC6GlFKUjAZjb25maWeUjA5jb25maWcub2ZmbGluZZSMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAdsb2diYXNllIwFbG9ncy+UjAxncHRfbG9hZHBhdGiUjA5ncHQvcHJldHJhaW5lZJSMCWdwdF9lcG9jaJSMBmxhdGVzdJSMCHNldF9zZWVklGgCaAZoO4aUUpSMCnBlcmNlbnRpbGWUjARtZWFulIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfb2JzlEsBdWJoEoaUUpQu"
+++++     },
+++++     "gpt_epoch": "latest",
+++++     "gpt_loadpath": "gpt/pretrained",
+++++@@ -28,7 +28,7 @@
+++++     "max_context_transitions": 5,
+++++     "mkdir": {
+++++         "_type": "python_object (type = method)",
+++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgihpRSlC4="
++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMB3ZlcmJvc2WUiIwRZ2VuZXJhdGVfZXhwX25hbWWUaAJoBmgJhpRSlIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAVrX2FjdJROjAlzYXZlX2RpZmaUaAJoBmgPhpRSlIwKZ2V0X2NvbW1pdJRoAmgGaBKGlFKUjApiZWFtX3dpZHRolEsgjAZkZXZpY2WUjANjcHWUjAdjZGZfYWN0lEc/4zMzMzMzM4wGc3VmZml4lIwBMJSMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMBW1rZGlylGgCaAZoHYaUUpSMB2hvcml6b26USwWMCHZpc19mcmVxlEsyjA5wcmVmaXhfY29udGV4dJSIjAphZGRfZXh0cmFzlGgCaAZoI4aUUpSMB2NkZl9vYnOUTowIbl9leHBhbmSUSwKMBnByZWZpeJSMD3BsYW5zL2RlZmF1bHRzL5SMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwJcGxhbl9mcmVxlEsBjAtyZWFkX2NvbmZpZ5RoAmgGaC6GlFKUjAZjb25maWeUjA5jb25maWcub2ZmbGluZZSMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAdsb2diYXNllIwFbG9ncy+UjAxncHRfbG9hZHBhdGiUjA5ncHQvcHJldHJhaW5lZJSMCWdwdF9lcG9jaJSMBmxhdGVzdJSMCHNldF9zZWVklGgCaAZoO4aUUpSMCnBlcmNlbnRpbGWUjARtZWFulIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfb2JzlEsBdWJoHYaUUpQu"
+++++     },
+++++     "n_expand": 2,
+++++     "percentile": "mean",
+++++@@ -37,24 +37,24 @@
+++++     "prefix_context": true,
+++++     "read_config": {
+++++         "_type": "python_object (type = method)",
+++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgZhpRSlC4="
++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMB3ZlcmJvc2WUiIwRZ2VuZXJhdGVfZXhwX25hbWWUaAJoBmgJhpRSlIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAVrX2FjdJROjAlzYXZlX2RpZmaUaAJoBmgPhpRSlIwKZ2V0X2NvbW1pdJRoAmgGaBKGlFKUjApiZWFtX3dpZHRolEsgjAZkZXZpY2WUjANjcHWUjAdjZGZfYWN0lEc/4zMzMzMzM4wGc3VmZml4lIwBMJSMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMBW1rZGlylGgCaAZoHYaUUpSMB2hvcml6b26USwWMCHZpc19mcmVxlEsyjA5wcmVmaXhfY29udGV4dJSIjAphZGRfZXh0cmFzlGgCaAZoI4aUUpSMB2NkZl9vYnOUTowIbl9leHBhbmSUSwKMBnByZWZpeJSMD3BsYW5zL2RlZmF1bHRzL5SMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwJcGxhbl9mcmVxlEsBjAtyZWFkX2NvbmZpZ5RoAmgGaC6GlFKUjAZjb25maWeUjA5jb25maWcub2ZmbGluZZSMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAdsb2diYXNllIwFbG9ncy+UjAxncHRfbG9hZHBhdGiUjA5ncHQvcHJldHJhaW5lZJSMCWdwdF9lcG9jaJSMBmxhdGVzdJSMCHNldF9zZWVklGgCaAZoO4aUUpSMCnBlcmNlbnRpbGWUjARtZWFulIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfb2JzlEsBdWJoLoaUUpQu"
+++++     },
+++++     "renderer": "Renderer",
+++++     "reproducibility": {
+++++-        "command_line": "python scripts/xrl_v2.py --dataset halfcheetah-medium-v2 --gpt_loadpath gpt/pretrained",
++++++        "command_line": "python xrl_v2.py --dataset halfcheetah-medium-v2 --gpt_loadpath gpt/pretrained",
+++++         "git_has_uncommitted_changes": true,
+++++         "git_root": "/home/colin/Desktop/FACT/FACT_assignment",
+++++-        "git_url": "https://github.com/fclio/FACT_assignment/tree/cca8d898e10f9f6102a8c33dac7758e2993dfc60",
+++++-        "time": "Fri Feb  2 11:39:41 2024"
++++++        "git_url": "https://github.com/fclio/FACT_assignment/tree/f77a194996872879bd0414fcb3dbc15b3da29cb8",
++++++        "time": "Sat Feb  3 21:31:17 2024"
+++++     },
+++++     "save_diff": {
+++++         "_type": "python_object (type = method)",
+++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1Ymg0hpRSlC4="
++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMB3ZlcmJvc2WUiIwRZ2VuZXJhdGVfZXhwX25hbWWUaAJoBmgJhpRSlIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAVrX2FjdJROjAlzYXZlX2RpZmaUaAJoBmgPhpRSlIwKZ2V0X2NvbW1pdJRoAmgGaBKGlFKUjApiZWFtX3dpZHRolEsgjAZkZXZpY2WUjANjcHWUjAdjZGZfYWN0lEc/4zMzMzMzM4wGc3VmZml4lIwBMJSMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMBW1rZGlylGgCaAZoHYaUUpSMB2hvcml6b26USwWMCHZpc19mcmVxlEsyjA5wcmVmaXhfY29udGV4dJSIjAphZGRfZXh0cmFzlGgCaAZoI4aUUpSMB2NkZl9vYnOUTowIbl9leHBhbmSUSwKMBnByZWZpeJSMD3BsYW5zL2RlZmF1bHRzL5SMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwJcGxhbl9mcmVxlEsBjAtyZWFkX2NvbmZpZ5RoAmgGaC6GlFKUjAZjb25maWeUjA5jb25maWcub2ZmbGluZZSMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAdsb2diYXNllIwFbG9ncy+UjAxncHRfbG9hZHBhdGiUjA5ncHQvcHJldHJhaW5lZJSMCWdwdF9lcG9jaJSMBmxhdGVzdJSMCHNldF9zZWVklGgCaAZoO4aUUpSMCnBlcmNlbnRpbGWUjARtZWFulIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfb2JzlEsBdWJoD4aUUpQu"
+++++     },
+++++     "savepath": "logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0",
+++++     "set_seed": {
+++++         "_type": "python_object (type = method)",
+++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgqhpRSlC4="
++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMB3ZlcmJvc2WUiIwRZ2VuZXJhdGVfZXhwX25hbWWUaAJoBmgJhpRSlIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAVrX2FjdJROjAlzYXZlX2RpZmaUaAJoBmgPhpRSlIwKZ2V0X2NvbW1pdJRoAmgGaBKGlFKUjApiZWFtX3dpZHRolEsgjAZkZXZpY2WUjANjcHWUjAdjZGZfYWN0lEc/4zMzMzMzM4wGc3VmZml4lIwBMJSMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMBW1rZGlylGgCaAZoHYaUUpSMB2hvcml6b26USwWMCHZpc19mcmVxlEsyjA5wcmVmaXhfY29udGV4dJSIjAphZGRfZXh0cmFzlGgCaAZoI4aUUpSMB2NkZl9vYnOUTowIbl9leHBhbmSUSwKMBnByZWZpeJSMD3BsYW5zL2RlZmF1bHRzL5SMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwJcGxhbl9mcmVxlEsBjAtyZWFkX2NvbmZpZ5RoAmgGaC6GlFKUjAZjb25maWeUjA5jb25maWcub2ZmbGluZZSMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAdsb2diYXNllIwFbG9ncy+UjAxncHRfbG9hZHBhdGiUjA5ncHQvcHJldHJhaW5lZJSMCWdwdF9lcG9jaJSMBmxhdGVzdJSMCHNldF9zZWVklGgCaAZoO4aUUpSMCnBlcmNlbnRpbGWUjARtZWFulIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfb2JzlEsBdWJoO4aUUpQu"
+++++     },
+++++     "suffix": "0",
+++++     "verbose": true,
+++++diff --git a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt
+++++index 2c00d59..8b26677 100644
+++++--- a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt
++++++++ b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt
+++++@@ -1,1250 +1,14003 @@
+++++-diff --git a/.gitignore b/.gitignore
+++++-index 30d226d..a18312a 100644
+++++---- a/.gitignore
+++++-+++ b/.gitignore
+++++-@@ -1,6 +1,5 @@
+++++- __pycache__/
+++++- *.mp4
+++++--halfcheetah/logs/
+++++- .ipynb_checkpoints/
+++++- __pycache__/
+++++- Open Notebook.onetoc2 
+++++-diff --git a/gridworld/gridworld_expts.ipynb b/gridworld/gridworld_expts.ipynb
+++++-index e2a261b..bd0b720 100644
+++++---- a/gridworld/gridworld_expts.ipynb
+++++-+++ b/gridworld/gridworld_expts.ipynb
+++++-@@ -1570,7 +1570,7 @@
+++++-    "name": "python",
+++++-    "nbconvert_exporter": "python",
+++++-    "pygments_lexer": "ipython3",
+++++--   "version": "3.8.10"
+++++-+   "version": "3.10.12"
+++++-   }
+++++-  },
+++++-  "nbformat": 4,
+++++-diff --git a/halfcheetah/clusters.npy b/halfcheetah/clusters.npy
++++++diff --git a/halfcheetah/environment.yml b/halfcheetah/environment.yml
+++++ deleted file mode 100644
+++++-index 3c198aa..0000000
+++++-Binary files a/halfcheetah/clusters.npy and /dev/null differ
+++++-diff --git a/halfcheetah/embeddings.npy b/halfcheetah/embeddings.npy
+++++-deleted file mode 100644
+++++-index 912d68c..0000000
+++++-Binary files a/halfcheetah/embeddings.npy and /dev/null differ
+++++-diff --git a/halfcheetah/halfcheetah.py b/halfcheetah/halfcheetah.py
+++++-deleted file mode 100644
+++++-index 626c42d..0000000
+++++---- a/halfcheetah/halfcheetah.py
++++++index e81007c..0000000
++++++--- a/halfcheetah/environment.yml
+++++ +++ /dev/null
+++++-@@ -1,112 +0,0 @@
+++++--import gym
+++++--import d4rl # Import required to register environments, you may need to also import the submodule
+++++--import numpy as np
+++++--import d3rlpy
+++++--
+++++--def main():
+++++--    dataset_d3, env = d3rlpy.datasets.get_dataset("halfcheetah-medium-v2")
+++++--
+++++--    print(dataset_d3.observations.shape)
+++++--    print(dataset_d3.actions.shape)
+++++--    print(dataset_d3.rewards.shape)
+++++--        # print(dataset_d3.next_observations.shape)
+++++--    print(dataset_d3.terminals.shape)
+++++--    print(dataset_d3.terminals.sum()) # no
+++++--
+++++--    env = gym.make('halfcheetah-medium-v2')
+++++--    dataset_d4 = d4rl.qlearning_dataset(env)
+++++--
+++++--    print(dataset_d4['observations'].shape)
+++++--    print(dataset_d4['rewards'].shape)
+++++--    print(dataset_d4['terminals'].shape)
+++++--    print(dataset_d4['actions'].shape)
+++++--
+++++--    print(dataset_d4['rewards'][1])
+++++--    print(dataset_d3.rewards[1])
+++++--
+++++--
+++++--    print(np.allclose(dataset_d3.actions[100], dataset_d4['actions'][100]))
+++++--
+++++--    for j in range(1000):
+++++--        for i in range(999):
+++++--            if dataset_d4['rewards'][j * 999 + i] != dataset_d3.rewards[j * 1000 + i]: print("yo", i)
+++++--        # if not np.allclose(dataset_d3.observations[i], dataset_d4['observations'][i]): print('obs ongelijk')
+++++--        # if not np.allclose(dataset_d3.rewards[i], dataset_d4['rewards'][i]): print('obs ongelijk')
+++++--        # if not np.allclose(dataset_d3.actions[i], dataset_d4['actions'][i]): print('obs ongelijk')
+++++--
+++++--    sac = d3rlpy.algos.SAC(
+++++--        actor_learning_rate=3e-4,
+++++--        critic_learning_rate=3e-4,
+++++--        temp_learning_rate=3e-4,
+++++--        batch_size=256)
+++++--
+++++--    print(sac)
+++++--    sac.fit(dataset_d3, n_steps=10000)
+++++--
+++++--    actions = sac.predict(dataset_d3.observations[0])
+++++--
+++++--    print(actions)
+++++--
+++++--
+++++--    return
+++++--    print('yo!')
+++++--
+++++--    # Create the environment
+++++--    env = gym.make('halfcheetah-medium-v2')
+++++--
+++++--    # d4rl abides by the OpenAI gym interface
+++++--    env.reset()
+++++--    env.step(env.action_space.sample())
+++++--
+++++--    # Each task is associated with a dataset
+++++--    # dataset contains observations, actions, rewards, terminals, and infos
+++++--    # dataset = env.get_dataset()
+++++--    dataset = d4rl.qlearning_dataset(env)
+++++--
+++++--    print(dataset.keys()) # An N x dim_observation Numpy array of observations
+++++--    print(dataset['rewards'].shape) # An N x dim_observation Numpy array of observations
+++++--
+++++--
+++++--    first_traj = []
+++++--    for i in range(50000):
+++++--        if not np.allclose(dataset['next_observations'][i],dataset['observations'][i+1]): print("yo", i, dataset['terminals'][i])
+++++--        # if dataset['terminals'][i] == True:
+++++--        #     print('traj ended at', i)
+++++--        #     break
+++++--
+++++--        # first_traj.append((dataset['observations'][i],
+++++--        #                    dataset['actions'][i],
+++++--        #                    dataset['rewards'][i],
+++++--        #                    dataset['next_observations'][i]))
+++++--    # print(first_traj)
+++++--
+++++--
+++++--    # print(dataset['rewards'].shape) # An N x dim_observation Numpy array of observations
+++++--
+++++--    # Alternatively, use d4rl.qlearning_dataset which
+++++--    # also adds next_observations.
+++++--
+++++--    # import d3rlpy
+++++--
+++++--    # # dataset, env = d3rlpy.datasets.get_dataset("halfcheetah-medium")
+++++--
+++++--    # # prepare algorithm
+++++--    # # sac = d3rlpy.algos.SAC().create(device="cpu")
+++++--
+++++--    # sac = d3rlpy.algos.SACConfig(
+++++--    #     actor_learning_rate=3e-4,
+++++--    #     critic_learning_rate=3e-4,
+++++--    #     temp_learning_rate=3e-4,
+++++--    #     batch_size=256,
+++++--    # ).create(device='cpu')
+++++--
+++++--
+++++--    # # train offline
+++++--    # # sac.fit(dataset, n_steps=1000)
+++++--
+++++--
+++++--    # # ready to control
+++++--    # actions = sac.predict(0)
+++++--
+++++--if __name__ == "__main__":
+++++--    main()
++++++@@ -1,27 +0,0 @@
++++++-name: trajectory
++++++-channels:
++++++-- defaults
++++++-- conda-forge
++++++-dependencies:
++++++-- python=3.8
++++++-- pip
++++++-- patchelf
++++++-- pip:
++++++-    - -f https://download.pytorch.org/whl/torch_stable.html
++++++-    - numpy
++++++-    - wheel==0.38.4
++++++-    - setuptools==65.5.0
++++++-    - gym==0.20.0
++++++-    - matplotlib==3.3.4
++++++-    - torch==1.9.1+cu111
++++++-    - typed-argument-parser
++++++-    # - git+https://github.com/Farama-Foundation/d4rl@f2a05c0d66722499bf8031b094d9af3aea7c372b#egg=d4rl
++++++-    - scikit-image==0.17.2
++++++-    - scikit-video==1.1.11
++++++-    - gitpython
++++++-    - os
++++++-    - d3rlpy
++++++-    - pyclustering
++++++-    - moviepy
++++++-    - scipy
++++++-    - scikit-learn
++++++diff --git a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json
++++++index 18993c1..29ab0b3 100644
++++++--- a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json
+++++++++ b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json
++++++@@ -1,23 +1,23 @@
++++++ {
++++++     "add_extras": {
++++++         "_type": "python_object (type = method)",
++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgfhpRSlC4="
+++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMCmFkZF9leHRyYXOUaAJoBmgIhpRSlIwOcHJlZml4X2NvbnRleHSUiIwHaG9yaXpvbpRLBYwFa19hY3SUTowGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwJc2F2ZV9kaWZmlGgCaAZoEIaUUpSMB2NkZl9vYnOUTowXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMCHNldF9zZWVklGgCaAZoFYaUUpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIcmVuZGVyZXKUjAhSZW5kZXJlcpSMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAtyZWFkX2NvbmZpZ5RoAmgGaB6GlFKUjAh2aXNfZnJlcZRLMowKZ2V0X2NvbW1pdJRoAmgGaCKGlFKUjAhleHBfbmFtZZSMHnBsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMpSMB2xvZ2Jhc2WUjAVsb2dzL5SMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjAd2ZXJib3NllIiMBmRldmljZZSMA2NwdZSMBnN1ZmZpeJSMATCUjAlncHRfZXBvY2iUjAZsYXRlc3SUjAVrX29ic5RLAYwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAlwbGFuX2ZyZXGUSwGMBW1rZGlylGgCaAZoNoaUUpSMCnBlcmNlbnRpbGWUjARtZWFulIwRZ2VuZXJhdGVfZXhwX25hbWWUaAJoBmg7hpRSlIwGY29tbWl0lIwtZjc3YTE5NDk5Njg3Mjg3OWJkMDQxNGZjYjNkYmMxNWIzZGEyOWNiOCBtYWlulIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMB2NkZl9hY3SURz/jMzMzMzMzdWJoCIaUUpQu"
++++++     },
++++++     "beam_width": 32,
++++++     "cdf_act": 0.6,
++++++     "cdf_obs": null,
++++++-    "commit": "cca8d898e10f9f6102a8c33dac7758e2993dfc60 halfcheetah-xrl",
+++++++    "commit": "f77a194996872879bd0414fcb3dbc15b3da29cb8 main",
++++++     "config": "config.offline",
++++++     "dataset": "halfcheetah-medium-v2",
++++++     "device": "cpu",
++++++     "exp_name": "plans/defaults/freq1_H5_beam32",
++++++     "generate_exp_name": {
++++++         "_type": "python_object (type = method)",
++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgxhpRSlC4="
+++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMCmFkZF9leHRyYXOUaAJoBmgIhpRSlIwOcHJlZml4X2NvbnRleHSUiIwHaG9yaXpvbpRLBYwFa19hY3SUTowGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwJc2F2ZV9kaWZmlGgCaAZoEIaUUpSMB2NkZl9vYnOUTowXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMCHNldF9zZWVklGgCaAZoFYaUUpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIcmVuZGVyZXKUjAhSZW5kZXJlcpSMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAtyZWFkX2NvbmZpZ5RoAmgGaB6GlFKUjAh2aXNfZnJlcZRLMowKZ2V0X2NvbW1pdJRoAmgGaCKGlFKUjAhleHBfbmFtZZSMHnBsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMpSMB2xvZ2Jhc2WUjAVsb2dzL5SMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjAd2ZXJib3NllIiMBmRldmljZZSMA2NwdZSMBnN1ZmZpeJSMATCUjAlncHRfZXBvY2iUjAZsYXRlc3SUjAVrX29ic5RLAYwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAlwbGFuX2ZyZXGUSwGMBW1rZGlylGgCaAZoNoaUUpSMCnBlcmNlbnRpbGWUjARtZWFulIwRZ2VuZXJhdGVfZXhwX25hbWWUaAJoBmg7hpRSlIwGY29tbWl0lIwtZjc3YTE5NDk5Njg3Mjg3OWJkMDQxNGZjYjNkYmMxNWIzZGEyOWNiOCBtYWlulIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMB2NkZl9hY3SURz/jMzMzMzMzdWJoO4aUUpQu"
++++++     },
++++++     "get_commit": {
++++++         "_type": "python_object (type = method)",
++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmglhpRSlC4="
+++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMCmFkZF9leHRyYXOUaAJoBmgIhpRSlIwOcHJlZml4X2NvbnRleHSUiIwHaG9yaXpvbpRLBYwFa19hY3SUTowGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwJc2F2ZV9kaWZmlGgCaAZoEIaUUpSMB2NkZl9vYnOUTowXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMCHNldF9zZWVklGgCaAZoFYaUUpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIcmVuZGVyZXKUjAhSZW5kZXJlcpSMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAtyZWFkX2NvbmZpZ5RoAmgGaB6GlFKUjAh2aXNfZnJlcZRLMowKZ2V0X2NvbW1pdJRoAmgGaCKGlFKUjAhleHBfbmFtZZSMHnBsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMpSMB2xvZ2Jhc2WUjAVsb2dzL5SMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjAd2ZXJib3NllIiMBmRldmljZZSMA2NwdZSMBnN1ZmZpeJSMATCUjAlncHRfZXBvY2iUjAZsYXRlc3SUjAVrX29ic5RLAYwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAlwbGFuX2ZyZXGUSwGMBW1rZGlylGgCaAZoNoaUUpSMCnBlcmNlbnRpbGWUjARtZWFulIwRZ2VuZXJhdGVfZXhwX25hbWWUaAJoBmg7hpRSlIwGY29tbWl0lIwtZjc3YTE5NDk5Njg3Mjg3OWJkMDQxNGZjYjNkYmMxNWIzZGEyOWNiOCBtYWlulIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMB2NkZl9hY3SURz/jMzMzMzMzdWJoIoaUUpQu"
++++++     },
++++++     "gpt_epoch": "latest",
++++++     "gpt_loadpath": "gpt/pretrained",
++++++@@ -28,7 +28,7 @@
++++++     "max_context_transitions": 5,
++++++     "mkdir": {
++++++         "_type": "python_object (type = method)",
++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgihpRSlC4="
+++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMCmFkZF9leHRyYXOUaAJoBmgIhpRSlIwOcHJlZml4X2NvbnRleHSUiIwHaG9yaXpvbpRLBYwFa19hY3SUTowGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwJc2F2ZV9kaWZmlGgCaAZoEIaUUpSMB2NkZl9vYnOUTowXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMCHNldF9zZWVklGgCaAZoFYaUUpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIcmVuZGVyZXKUjAhSZW5kZXJlcpSMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAtyZWFkX2NvbmZpZ5RoAmgGaB6GlFKUjAh2aXNfZnJlcZRLMowKZ2V0X2NvbW1pdJRoAmgGaCKGlFKUjAhleHBfbmFtZZSMHnBsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMpSMB2xvZ2Jhc2WUjAVsb2dzL5SMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjAd2ZXJib3NllIiMBmRldmljZZSMA2NwdZSMBnN1ZmZpeJSMATCUjAlncHRfZXBvY2iUjAZsYXRlc3SUjAVrX29ic5RLAYwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAlwbGFuX2ZyZXGUSwGMBW1rZGlylGgCaAZoNoaUUpSMCnBlcmNlbnRpbGWUjARtZWFulIwRZ2VuZXJhdGVfZXhwX25hbWWUaAJoBmg7hpRSlIwGY29tbWl0lIwtZjc3YTE5NDk5Njg3Mjg3OWJkMDQxNGZjYjNkYmMxNWIzZGEyOWNiOCBtYWlulIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMB2NkZl9hY3SURz/jMzMzMzMzdWJoNoaUUpQu"
++++++     },
++++++     "n_expand": 2,
++++++     "percentile": "mean",
++++++@@ -37,24 +37,24 @@
++++++     "prefix_context": true,
++++++     "read_config": {
++++++         "_type": "python_object (type = method)",
++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgZhpRSlC4="
+++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMCmFkZF9leHRyYXOUaAJoBmgIhpRSlIwOcHJlZml4X2NvbnRleHSUiIwHaG9yaXpvbpRLBYwFa19hY3SUTowGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwJc2F2ZV9kaWZmlGgCaAZoEIaUUpSMB2NkZl9vYnOUTowXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMCHNldF9zZWVklGgCaAZoFYaUUpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIcmVuZGVyZXKUjAhSZW5kZXJlcpSMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAtyZWFkX2NvbmZpZ5RoAmgGaB6GlFKUjAh2aXNfZnJlcZRLMowKZ2V0X2NvbW1pdJRoAmgGaCKGlFKUjAhleHBfbmFtZZSMHnBsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMpSMB2xvZ2Jhc2WUjAVsb2dzL5SMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjAd2ZXJib3NllIiMBmRldmljZZSMA2NwdZSMBnN1ZmZpeJSMATCUjAlncHRfZXBvY2iUjAZsYXRlc3SUjAVrX29ic5RLAYwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAlwbGFuX2ZyZXGUSwGMBW1rZGlylGgCaAZoNoaUUpSMCnBlcmNlbnRpbGWUjARtZWFulIwRZ2VuZXJhdGVfZXhwX25hbWWUaAJoBmg7hpRSlIwGY29tbWl0lIwtZjc3YTE5NDk5Njg3Mjg3OWJkMDQxNGZjYjNkYmMxNWIzZGEyOWNiOCBtYWlulIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMB2NkZl9hY3SURz/jMzMzMzMzdWJoHoaUUpQu"
++++++     },
++++++     "renderer": "Renderer",
++++++     "reproducibility": {
++++++-        "command_line": "python scripts/xrl_v2.py --dataset halfcheetah-medium-v2 --gpt_loadpath gpt/pretrained",
+++++++        "command_line": "python xrl_v2.py --dataset halfcheetah-medium-v2 --gpt_loadpath gpt/pretrained",
++++++         "git_has_uncommitted_changes": true,
++++++         "git_root": "/home/colin/Desktop/FACT/FACT_assignment",
++++++-        "git_url": "https://github.com/fclio/FACT_assignment/tree/cca8d898e10f9f6102a8c33dac7758e2993dfc60",
++++++-        "time": "Fri Feb  2 11:39:41 2024"
+++++++        "git_url": "https://github.com/fclio/FACT_assignment/tree/f77a194996872879bd0414fcb3dbc15b3da29cb8",
+++++++        "time": "Sat Feb  3 21:30:14 2024"
++++++     },
++++++     "save_diff": {
++++++         "_type": "python_object (type = method)",
++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1Ymg0hpRSlC4="
+++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMCmFkZF9leHRyYXOUaAJoBmgIhpRSlIwOcHJlZml4X2NvbnRleHSUiIwHaG9yaXpvbpRLBYwFa19hY3SUTowGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwJc2F2ZV9kaWZmlGgCaAZoEIaUUpSMB2NkZl9vYnOUTowXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMCHNldF9zZWVklGgCaAZoFYaUUpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIcmVuZGVyZXKUjAhSZW5kZXJlcpSMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAtyZWFkX2NvbmZpZ5RoAmgGaB6GlFKUjAh2aXNfZnJlcZRLMowKZ2V0X2NvbW1pdJRoAmgGaCKGlFKUjAhleHBfbmFtZZSMHnBsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMpSMB2xvZ2Jhc2WUjAVsb2dzL5SMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjAd2ZXJib3NllIiMBmRldmljZZSMA2NwdZSMBnN1ZmZpeJSMATCUjAlncHRfZXBvY2iUjAZsYXRlc3SUjAVrX29ic5RLAYwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAlwbGFuX2ZyZXGUSwGMBW1rZGlylGgCaAZoNoaUUpSMCnBlcmNlbnRpbGWUjARtZWFulIwRZ2VuZXJhdGVfZXhwX25hbWWUaAJoBmg7hpRSlIwGY29tbWl0lIwtZjc3YTE5NDk5Njg3Mjg3OWJkMDQxNGZjYjNkYmMxNWIzZGEyOWNiOCBtYWlulIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMB2NkZl9hY3SURz/jMzMzMzMzdWJoEIaUUpQu"
++++++     },
++++++     "savepath": "logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0",
++++++     "set_seed": {
++++++         "_type": "python_object (type = method)",
++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgqhpRSlC4="
+++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMCmFkZF9leHRyYXOUaAJoBmgIhpRSlIwOcHJlZml4X2NvbnRleHSUiIwHaG9yaXpvbpRLBYwFa19hY3SUTowGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwJc2F2ZV9kaWZmlGgCaAZoEIaUUpSMB2NkZl9vYnOUTowXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMCHNldF9zZWVklGgCaAZoFYaUUpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIcmVuZGVyZXKUjAhSZW5kZXJlcpSMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAtyZWFkX2NvbmZpZ5RoAmgGaB6GlFKUjAh2aXNfZnJlcZRLMowKZ2V0X2NvbW1pdJRoAmgGaCKGlFKUjAhleHBfbmFtZZSMHnBsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMpSMB2xvZ2Jhc2WUjAVsb2dzL5SMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjAd2ZXJib3NllIiMBmRldmljZZSMA2NwdZSMBnN1ZmZpeJSMATCUjAlncHRfZXBvY2iUjAZsYXRlc3SUjAVrX29ic5RLAYwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAlwbGFuX2ZyZXGUSwGMBW1rZGlylGgCaAZoNoaUUpSMCnBlcmNlbnRpbGWUjARtZWFulIwRZ2VuZXJhdGVfZXhwX25hbWWUaAJoBmg7hpRSlIwGY29tbWl0lIwtZjc3YTE5NDk5Njg3Mjg3OWJkMDQxNGZjYjNkYmMxNWIzZGEyOWNiOCBtYWlulIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMB2NkZl9hY3SURz/jMzMzMzMzdWJoFYaUUpQu"
++++++     },
++++++     "suffix": "0",
++++++     "verbose": true,
++++++diff --git a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt
++++++index 2c00d59..75a583b 100644
++++++--- a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt
+++++++++ b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt
++++++@@ -1,1250 +1,12429 @@
++++++-diff --git a/.gitignore b/.gitignore
++++++-index 30d226d..a18312a 100644
++++++---- a/.gitignore
++++++-+++ b/.gitignore
++++++-@@ -1,6 +1,5 @@
++++++- __pycache__/
++++++- *.mp4
++++++--halfcheetah/logs/
++++++- .ipynb_checkpoints/
++++++- __pycache__/
++++++- Open Notebook.onetoc2 
++++++-diff --git a/gridworld/gridworld_expts.ipynb b/gridworld/gridworld_expts.ipynb
++++++-index e2a261b..bd0b720 100644
++++++---- a/gridworld/gridworld_expts.ipynb
++++++-+++ b/gridworld/gridworld_expts.ipynb
++++++-@@ -1570,7 +1570,7 @@
++++++-    "name": "python",
++++++-    "nbconvert_exporter": "python",
++++++-    "pygments_lexer": "ipython3",
++++++--   "version": "3.8.10"
++++++-+   "version": "3.10.12"
++++++-   }
++++++-  },
++++++-  "nbformat": 4,
++++++-diff --git a/halfcheetah/clusters.npy b/halfcheetah/clusters.npy
+++++++diff --git a/halfcheetah/environment.yml b/halfcheetah/environment.yml
++++++ deleted file mode 100644
++++++-index 3c198aa..0000000
++++++-Binary files a/halfcheetah/clusters.npy and /dev/null differ
++++++-diff --git a/halfcheetah/embeddings.npy b/halfcheetah/embeddings.npy
++++++-deleted file mode 100644
++++++-index 912d68c..0000000
++++++-Binary files a/halfcheetah/embeddings.npy and /dev/null differ
++++++-diff --git a/halfcheetah/halfcheetah.py b/halfcheetah/halfcheetah.py
++++++-deleted file mode 100644
++++++-index 626c42d..0000000
++++++---- a/halfcheetah/halfcheetah.py
+++++++index e81007c..0000000
+++++++--- a/halfcheetah/environment.yml
++++++ +++ /dev/null
++++++-@@ -1,112 +0,0 @@
++++++--import gym
++++++--import d4rl # Import required to register environments, you may need to also import the submodule
++++++--import numpy as np
++++++--import d3rlpy
++++++--
++++++--def main():
++++++--    dataset_d3, env = d3rlpy.datasets.get_dataset("halfcheetah-medium-v2")
++++++--
++++++--    print(dataset_d3.observations.shape)
++++++--    print(dataset_d3.actions.shape)
++++++--    print(dataset_d3.rewards.shape)
++++++--        # print(dataset_d3.next_observations.shape)
++++++--    print(dataset_d3.terminals.shape)
++++++--    print(dataset_d3.terminals.sum()) # no
++++++--
++++++--    env = gym.make('halfcheetah-medium-v2')
++++++--    dataset_d4 = d4rl.qlearning_dataset(env)
++++++--
++++++--    print(dataset_d4['observations'].shape)
++++++--    print(dataset_d4['rewards'].shape)
++++++--    print(dataset_d4['terminals'].shape)
++++++--    print(dataset_d4['actions'].shape)
++++++--
++++++--    print(dataset_d4['rewards'][1])
++++++--    print(dataset_d3.rewards[1])
++++++--
++++++--
++++++--    print(np.allclose(dataset_d3.actions[100], dataset_d4['actions'][100]))
++++++--
++++++--    for j in range(1000):
++++++--        for i in range(999):
++++++--            if dataset_d4['rewards'][j * 999 + i] != dataset_d3.rewards[j * 1000 + i]: print("yo", i)
++++++--        # if not np.allclose(dataset_d3.observations[i], dataset_d4['observations'][i]): print('obs ongelijk')
++++++--        # if not np.allclose(dataset_d3.rewards[i], dataset_d4['rewards'][i]): print('obs ongelijk')
++++++--        # if not np.allclose(dataset_d3.actions[i], dataset_d4['actions'][i]): print('obs ongelijk')
++++++--
++++++--    sac = d3rlpy.algos.SAC(
++++++--        actor_learning_rate=3e-4,
++++++--        critic_learning_rate=3e-4,
++++++--        temp_learning_rate=3e-4,
++++++--        batch_size=256)
++++++--
++++++--    print(sac)
++++++--    sac.fit(dataset_d3, n_steps=10000)
++++++--
++++++--    actions = sac.predict(dataset_d3.observations[0])
++++++--
++++++--    print(actions)
++++++--
++++++--
++++++--    return
++++++--    print('yo!')
++++++--
++++++--    # Create the environment
++++++--    env = gym.make('halfcheetah-medium-v2')
++++++--
++++++--    # d4rl abides by the OpenAI gym interface
++++++--    env.reset()
++++++--    env.step(env.action_space.sample())
++++++--
++++++--    # Each task is associated with a dataset
++++++--    # dataset contains observations, actions, rewards, terminals, and infos
++++++--    # dataset = env.get_dataset()
++++++--    dataset = d4rl.qlearning_dataset(env)
++++++--
++++++--    print(dataset.keys()) # An N x dim_observation Numpy array of observations
++++++--    print(dataset['rewards'].shape) # An N x dim_observation Numpy array of observations
++++++--
++++++--
++++++--    first_traj = []
++++++--    for i in range(50000):
++++++--        if not np.allclose(dataset['next_observations'][i],dataset['observations'][i+1]): print("yo", i, dataset['terminals'][i])
++++++--        # if dataset['terminals'][i] == True:
++++++--        #     print('traj ended at', i)
++++++--        #     break
++++++--
++++++--        # first_traj.append((dataset['observations'][i],
++++++--        #                    dataset['actions'][i],
++++++--        #                    dataset['rewards'][i],
++++++--        #                    dataset['next_observations'][i]))
++++++--    # print(first_traj)
++++++--
++++++--
++++++--    # print(dataset['rewards'].shape) # An N x dim_observation Numpy array of observations
++++++--
++++++--    # Alternatively, use d4rl.qlearning_dataset which
++++++--    # also adds next_observations.
++++++--
++++++--    # import d3rlpy
++++++--
++++++--    # # dataset, env = d3rlpy.datasets.get_dataset("halfcheetah-medium")
++++++--
++++++--    # # prepare algorithm
++++++--    # # sac = d3rlpy.algos.SAC().create(device="cpu")
++++++--
++++++--    # sac = d3rlpy.algos.SACConfig(
++++++--    #     actor_learning_rate=3e-4,
++++++--    #     critic_learning_rate=3e-4,
++++++--    #     temp_learning_rate=3e-4,
++++++--    #     batch_size=256,
++++++--    # ).create(device='cpu')
++++++--
++++++--
++++++--    # # train offline
++++++--    # # sac.fit(dataset, n_steps=1000)
++++++--
++++++--
++++++--    # # ready to control
++++++--    # actions = sac.predict(0)
++++++--
++++++--if __name__ == "__main__":
++++++--    main()
+++++++@@ -1,27 +0,0 @@
+++++++-name: trajectory
+++++++-channels:
+++++++-- defaults
+++++++-- conda-forge
+++++++-dependencies:
+++++++-- python=3.8
+++++++-- pip
+++++++-- patchelf
+++++++-- pip:
+++++++-    - -f https://download.pytorch.org/whl/torch_stable.html
+++++++-    - numpy
+++++++-    - wheel==0.38.4
+++++++-    - setuptools==65.5.0
+++++++-    - gym==0.20.0
+++++++-    - matplotlib==3.3.4
+++++++-    - torch==1.9.1+cu111
+++++++-    - typed-argument-parser
+++++++-    # - git+https://github.com/Farama-Foundation/d4rl@f2a05c0d66722499bf8031b094d9af3aea7c372b#egg=d4rl
+++++++-    - scikit-image==0.17.2
+++++++-    - scikit-video==1.1.11
+++++++-    - gitpython
+++++++-    - os
+++++++-    - d3rlpy
+++++++-    - pyclustering
+++++++-    - moviepy
+++++++-    - scipy
+++++++-    - scikit-learn
+++++++diff --git a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json
+++++++index 18993c1..a831bba 100644
+++++++--- a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json
++++++++++ b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json
+++++++@@ -1,23 +1,23 @@
+++++++ {
+++++++     "add_extras": {
+++++++         "_type": "python_object (type = method)",
+++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgfhpRSlC4="
++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMCG5fZXhwYW5klEsCjAZjb21taXSUjC1mNzdhMTk0OTk2ODcyODc5YmQwNDE0ZmNiM2RiYzE1YjNkYTI5Y2I4IG1haW6UjBdtYXhfY29udGV4dF90cmFuc2l0aW9uc5RLBYwKZ2V0X2NvbW1pdJRoAmgGaAyGlFKUjAdjZGZfYWN0lEc/4zMzMzMzM4wFa19vYnOUSwGMCmFkZF9leHRyYXOUaAJoBmgRhpRSlIwHdmVyYm9zZZSIjAdob3Jpem9ulEsFjAhzYXZlcGF0aJSMO2xvZ3MvaGFsZmNoZWV0YWgtbWVkaXVtLXYyL3BsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMi8wlIwGc3VmZml4lIwBMJSMCGV4cF9uYW1llIwecGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMylIwKcGVyY2VudGlsZZSMBG1lYW6UjAh2aXNfZnJlcZRLMowGZGV2aWNllIwDY3B1lIwFbWtkaXKUaAJoBmghhpRSlIwRZ2VuZXJhdGVfZXhwX25hbWWUaAJoBmgkhpRSlIwKYmVhbV93aWR0aJRLIIwHZGF0YXNldJSMFWhhbGZjaGVldGFoLW1lZGl1bS12MpSMCHJlbmRlcmVylIwIUmVuZGVyZXKUjAtyZWFkX2NvbmZpZ5RoAmgGaCyGlFKUjA5wcmVmaXhfY29udGV4dJSIjAhzZXRfc2VlZJRoAmgGaDCGlFKUjAVrX2FjdJROjAZwcmVmaXiUjA9wbGFucy9kZWZhdWx0cy+UjAlzYXZlX2RpZmaUaAJoBmg2hpRSlIwMZ3B0X2xvYWRwYXRolIwOZ3B0L3ByZXRyYWluZWSUjAdsb2diYXNllIwFbG9ncy+UjAlncHRfZXBvY2iUjAZsYXRlc3SUjAZjb25maWeUjA5jb25maWcub2ZmbGluZZSMCXBsYW5fZnJlcZRLAYwHY2RmX29ic5ROdWJoEYaUUpQu"
+++++++     },
+++++++     "beam_width": 32,
+++++++     "cdf_act": 0.6,
+++++++     "cdf_obs": null,
+++++++-    "commit": "cca8d898e10f9f6102a8c33dac7758e2993dfc60 halfcheetah-xrl",
++++++++    "commit": "f77a194996872879bd0414fcb3dbc15b3da29cb8 main",
+++++++     "config": "config.offline",
+++++++     "dataset": "halfcheetah-medium-v2",
+++++++     "device": "cpu",
+++++++     "exp_name": "plans/defaults/freq1_H5_beam32",
+++++++     "generate_exp_name": {
+++++++         "_type": "python_object (type = method)",
+++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgxhpRSlC4="
++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMCG5fZXhwYW5klEsCjAZjb21taXSUjC1mNzdhMTk0OTk2ODcyODc5YmQwNDE0ZmNiM2RiYzE1YjNkYTI5Y2I4IG1haW6UjBdtYXhfY29udGV4dF90cmFuc2l0aW9uc5RLBYwKZ2V0X2NvbW1pdJRoAmgGaAyGlFKUjAdjZGZfYWN0lEc/4zMzMzMzM4wFa19vYnOUSwGMCmFkZF9leHRyYXOUaAJoBmgRhpRSlIwHdmVyYm9zZZSIjAdob3Jpem9ulEsFjAhzYXZlcGF0aJSMO2xvZ3MvaGFsZmNoZWV0YWgtbWVkaXVtLXYyL3BsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMi8wlIwGc3VmZml4lIwBMJSMCGV4cF9uYW1llIwecGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMylIwKcGVyY2VudGlsZZSMBG1lYW6UjAh2aXNfZnJlcZRLMowGZGV2aWNllIwDY3B1lIwFbWtkaXKUaAJoBmghhpRSlIwRZ2VuZXJhdGVfZXhwX25hbWWUaAJoBmgkhpRSlIwKYmVhbV93aWR0aJRLIIwHZGF0YXNldJSMFWhhbGZjaGVldGFoLW1lZGl1bS12MpSMCHJlbmRlcmVylIwIUmVuZGVyZXKUjAtyZWFkX2NvbmZpZ5RoAmgGaCyGlFKUjA5wcmVmaXhfY29udGV4dJSIjAhzZXRfc2VlZJRoAmgGaDCGlFKUjAVrX2FjdJROjAZwcmVmaXiUjA9wbGFucy9kZWZhdWx0cy+UjAlzYXZlX2RpZmaUaAJoBmg2hpRSlIwMZ3B0X2xvYWRwYXRolIwOZ3B0L3ByZXRyYWluZWSUjAdsb2diYXNllIwFbG9ncy+UjAlncHRfZXBvY2iUjAZsYXRlc3SUjAZjb25maWeUjA5jb25maWcub2ZmbGluZZSMCXBsYW5fZnJlcZRLAYwHY2RmX29ic5ROdWJoJIaUUpQu"
+++++++     },
+++++++     "get_commit": {
+++++++         "_type": "python_object (type = method)",
+++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmglhpRSlC4="
++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMCG5fZXhwYW5klEsCjAZjb21taXSUjC1mNzdhMTk0OTk2ODcyODc5YmQwNDE0ZmNiM2RiYzE1YjNkYTI5Y2I4IG1haW6UjBdtYXhfY29udGV4dF90cmFuc2l0aW9uc5RLBYwKZ2V0X2NvbW1pdJRoAmgGaAyGlFKUjAdjZGZfYWN0lEc/4zMzMzMzM4wFa19vYnOUSwGMCmFkZF9leHRyYXOUaAJoBmgRhpRSlIwHdmVyYm9zZZSIjAdob3Jpem9ulEsFjAhzYXZlcGF0aJSMO2xvZ3MvaGFsZmNoZWV0YWgtbWVkaXVtLXYyL3BsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMi8wlIwGc3VmZml4lIwBMJSMCGV4cF9uYW1llIwecGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMylIwKcGVyY2VudGlsZZSMBG1lYW6UjAh2aXNfZnJlcZRLMowGZGV2aWNllIwDY3B1lIwFbWtkaXKUaAJoBmghhpRSlIwRZ2VuZXJhdGVfZXhwX25hbWWUaAJoBmgkhpRSlIwKYmVhbV93aWR0aJRLIIwHZGF0YXNldJSMFWhhbGZjaGVldGFoLW1lZGl1bS12MpSMCHJlbmRlcmVylIwIUmVuZGVyZXKUjAtyZWFkX2NvbmZpZ5RoAmgGaCyGlFKUjA5wcmVmaXhfY29udGV4dJSIjAhzZXRfc2VlZJRoAmgGaDCGlFKUjAVrX2FjdJROjAZwcmVmaXiUjA9wbGFucy9kZWZhdWx0cy+UjAlzYXZlX2RpZmaUaAJoBmg2hpRSlIwMZ3B0X2xvYWRwYXRolIwOZ3B0L3ByZXRyYWluZWSUjAdsb2diYXNllIwFbG9ncy+UjAlncHRfZXBvY2iUjAZsYXRlc3SUjAZjb25maWeUjA5jb25maWcub2ZmbGluZZSMCXBsYW5fZnJlcZRLAYwHY2RmX29ic5ROdWJoDIaUUpQu"
+++++++     },
+++++++     "gpt_epoch": "latest",
+++++++     "gpt_loadpath": "gpt/pretrained",
+++++++@@ -28,7 +28,7 @@
+++++++     "max_context_transitions": 5,
+++++++     "mkdir": {
+++++++         "_type": "python_object (type = method)",
+++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgihpRSlC4="
++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMCG5fZXhwYW5klEsCjAZjb21taXSUjC1mNzdhMTk0OTk2ODcyODc5YmQwNDE0ZmNiM2RiYzE1YjNkYTI5Y2I4IG1haW6UjBdtYXhfY29udGV4dF90cmFuc2l0aW9uc5RLBYwKZ2V0X2NvbW1pdJRoAmgGaAyGlFKUjAdjZGZfYWN0lEc/4zMzMzMzM4wFa19vYnOUSwGMCmFkZF9leHRyYXOUaAJoBmgRhpRSlIwHdmVyYm9zZZSIjAdob3Jpem9ulEsFjAhzYXZlcGF0aJSMO2xvZ3MvaGFsZmNoZWV0YWgtbWVkaXVtLXYyL3BsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMi8wlIwGc3VmZml4lIwBMJSMCGV4cF9uYW1llIwecGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMylIwKcGVyY2VudGlsZZSMBG1lYW6UjAh2aXNfZnJlcZRLMowGZGV2aWNllIwDY3B1lIwFbWtkaXKUaAJoBmghhpRSlIwRZ2VuZXJhdGVfZXhwX25hbWWUaAJoBmgkhpRSlIwKYmVhbV93aWR0aJRLIIwHZGF0YXNldJSMFWhhbGZjaGVldGFoLW1lZGl1bS12MpSMCHJlbmRlcmVylIwIUmVuZGVyZXKUjAtyZWFkX2NvbmZpZ5RoAmgGaCyGlFKUjA5wcmVmaXhfY29udGV4dJSIjAhzZXRfc2VlZJRoAmgGaDCGlFKUjAVrX2FjdJROjAZwcmVmaXiUjA9wbGFucy9kZWZhdWx0cy+UjAlzYXZlX2RpZmaUaAJoBmg2hpRSlIwMZ3B0X2xvYWRwYXRolIwOZ3B0L3ByZXRyYWluZWSUjAdsb2diYXNllIwFbG9ncy+UjAlncHRfZXBvY2iUjAZsYXRlc3SUjAZjb25maWeUjA5jb25maWcub2ZmbGluZZSMCXBsYW5fZnJlcZRLAYwHY2RmX29ic5ROdWJoIYaUUpQu"
+++++++     },
+++++++     "n_expand": 2,
+++++++     "percentile": "mean",
+++++++@@ -37,24 +37,24 @@
+++++++     "prefix_context": true,
+++++++     "read_config": {
+++++++         "_type": "python_object (type = method)",
+++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgZhpRSlC4="
++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMCG5fZXhwYW5klEsCjAZjb21taXSUjC1mNzdhMTk0OTk2ODcyODc5YmQwNDE0ZmNiM2RiYzE1YjNkYTI5Y2I4IG1haW6UjBdtYXhfY29udGV4dF90cmFuc2l0aW9uc5RLBYwKZ2V0X2NvbW1pdJRoAmgGaAyGlFKUjAdjZGZfYWN0lEc/4zMzMzMzM4wFa19vYnOUSwGMCmFkZF9leHRyYXOUaAJoBmgRhpRSlIwHdmVyYm9zZZSIjAdob3Jpem9ulEsFjAhzYXZlcGF0aJSMO2xvZ3MvaGFsZmNoZWV0YWgtbWVkaXVtLXYyL3BsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMi8wlIwGc3VmZml4lIwBMJSMCGV4cF9uYW1llIwecGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMylIwKcGVyY2VudGlsZZSMBG1lYW6UjAh2aXNfZnJlcZRLMowGZGV2aWNllIwDY3B1lIwFbWtkaXKUaAJoBmghhpRSlIwRZ2VuZXJhdGVfZXhwX25hbWWUaAJoBmgkhpRSlIwKYmVhbV93aWR0aJRLIIwHZGF0YXNldJSMFWhhbGZjaGVldGFoLW1lZGl1bS12MpSMCHJlbmRlcmVylIwIUmVuZGVyZXKUjAtyZWFkX2NvbmZpZ5RoAmgGaCyGlFKUjA5wcmVmaXhfY29udGV4dJSIjAhzZXRfc2VlZJRoAmgGaDCGlFKUjAVrX2FjdJROjAZwcmVmaXiUjA9wbGFucy9kZWZhdWx0cy+UjAlzYXZlX2RpZmaUaAJoBmg2hpRSlIwMZ3B0X2xvYWRwYXRolIwOZ3B0L3ByZXRyYWluZWSUjAdsb2diYXNllIwFbG9ncy+UjAlncHRfZXBvY2iUjAZsYXRlc3SUjAZjb25maWeUjA5jb25maWcub2ZmbGluZZSMCXBsYW5fZnJlcZRLAYwHY2RmX29ic5ROdWJoLIaUUpQu"
+++++++     },
+++++++     "renderer": "Renderer",
+++++++     "reproducibility": {
+++++++-        "command_line": "python scripts/xrl_v2.py --dataset halfcheetah-medium-v2 --gpt_loadpath gpt/pretrained",
++++++++        "command_line": "python xrl_v2.py --dataset halfcheetah-medium-v2 --gpt_loadpath gpt/pretrained",
+++++++         "git_has_uncommitted_changes": true,
+++++++         "git_root": "/home/colin/Desktop/FACT/FACT_assignment",
+++++++-        "git_url": "https://github.com/fclio/FACT_assignment/tree/cca8d898e10f9f6102a8c33dac7758e2993dfc60",
+++++++-        "time": "Fri Feb  2 11:39:41 2024"
++++++++        "git_url": "https://github.com/fclio/FACT_assignment/tree/f77a194996872879bd0414fcb3dbc15b3da29cb8",
++++++++        "time": "Sat Feb  3 21:29:46 2024"
+++++++     },
+++++++     "save_diff": {
+++++++         "_type": "python_object (type = method)",
+++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1Ymg0hpRSlC4="
++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMCG5fZXhwYW5klEsCjAZjb21taXSUjC1mNzdhMTk0OTk2ODcyODc5YmQwNDE0ZmNiM2RiYzE1YjNkYTI5Y2I4IG1haW6UjBdtYXhfY29udGV4dF90cmFuc2l0aW9uc5RLBYwKZ2V0X2NvbW1pdJRoAmgGaAyGlFKUjAdjZGZfYWN0lEc/4zMzMzMzM4wFa19vYnOUSwGMCmFkZF9leHRyYXOUaAJoBmgRhpRSlIwHdmVyYm9zZZSIjAdob3Jpem9ulEsFjAhzYXZlcGF0aJSMO2xvZ3MvaGFsZmNoZWV0YWgtbWVkaXVtLXYyL3BsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMi8wlIwGc3VmZml4lIwBMJSMCGV4cF9uYW1llIwecGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMylIwKcGVyY2VudGlsZZSMBG1lYW6UjAh2aXNfZnJlcZRLMowGZGV2aWNllIwDY3B1lIwFbWtkaXKUaAJoBmghhpRSlIwRZ2VuZXJhdGVfZXhwX25hbWWUaAJoBmgkhpRSlIwKYmVhbV93aWR0aJRLIIwHZGF0YXNldJSMFWhhbGZjaGVldGFoLW1lZGl1bS12MpSMCHJlbmRlcmVylIwIUmVuZGVyZXKUjAtyZWFkX2NvbmZpZ5RoAmgGaCyGlFKUjA5wcmVmaXhfY29udGV4dJSIjAhzZXRfc2VlZJRoAmgGaDCGlFKUjAVrX2FjdJROjAZwcmVmaXiUjA9wbGFucy9kZWZhdWx0cy+UjAlzYXZlX2RpZmaUaAJoBmg2hpRSlIwMZ3B0X2xvYWRwYXRolIwOZ3B0L3ByZXRyYWluZWSUjAdsb2diYXNllIwFbG9ncy+UjAlncHRfZXBvY2iUjAZsYXRlc3SUjAZjb25maWeUjA5jb25maWcub2ZmbGluZZSMCXBsYW5fZnJlcZRLAYwHY2RmX29ic5ROdWJoNoaUUpQu"
+++++++     },
+++++++     "savepath": "logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0",
+++++++     "set_seed": {
+++++++         "_type": "python_object (type = method)",
+++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgqhpRSlC4="
++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMCG5fZXhwYW5klEsCjAZjb21taXSUjC1mNzdhMTk0OTk2ODcyODc5YmQwNDE0ZmNiM2RiYzE1YjNkYTI5Y2I4IG1haW6UjBdtYXhfY29udGV4dF90cmFuc2l0aW9uc5RLBYwKZ2V0X2NvbW1pdJRoAmgGaAyGlFKUjAdjZGZfYWN0lEc/4zMzMzMzM4wFa19vYnOUSwGMCmFkZF9leHRyYXOUaAJoBmgRhpRSlIwHdmVyYm9zZZSIjAdob3Jpem9ulEsFjAhzYXZlcGF0aJSMO2xvZ3MvaGFsZmNoZWV0YWgtbWVkaXVtLXYyL3BsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMi8wlIwGc3VmZml4lIwBMJSMCGV4cF9uYW1llIwecGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMylIwKcGVyY2VudGlsZZSMBG1lYW6UjAh2aXNfZnJlcZRLMowGZGV2aWNllIwDY3B1lIwFbWtkaXKUaAJoBmghhpRSlIwRZ2VuZXJhdGVfZXhwX25hbWWUaAJoBmgkhpRSlIwKYmVhbV93aWR0aJRLIIwHZGF0YXNldJSMFWhhbGZjaGVldGFoLW1lZGl1bS12MpSMCHJlbmRlcmVylIwIUmVuZGVyZXKUjAtyZWFkX2NvbmZpZ5RoAmgGaCyGlFKUjA5wcmVmaXhfY29udGV4dJSIjAhzZXRfc2VlZJRoAmgGaDCGlFKUjAVrX2FjdJROjAZwcmVmaXiUjA9wbGFucy9kZWZhdWx0cy+UjAlzYXZlX2RpZmaUaAJoBmg2hpRSlIwMZ3B0X2xvYWRwYXRolIwOZ3B0L3ByZXRyYWluZWSUjAdsb2diYXNllIwFbG9ncy+UjAlncHRfZXBvY2iUjAZsYXRlc3SUjAZjb25maWeUjA5jb25maWcub2ZmbGluZZSMCXBsYW5fZnJlcZRLAYwHY2RmX29ic5ROdWJoMIaUUpQu"
+++++++     },
+++++++     "suffix": "0",
+++++++     "verbose": true,
+++++++diff --git a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt
+++++++index 2c00d59..ed6adb5 100644
+++++++--- a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt
++++++++++ b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt
+++++++@@ -1,1250 +1,10860 @@
+++++++-diff --git a/.gitignore b/.gitignore
+++++++-index 30d226d..a18312a 100644
+++++++---- a/.gitignore
+++++++-+++ b/.gitignore
+++++++-@@ -1,6 +1,5 @@
+++++++- __pycache__/
+++++++- *.mp4
+++++++--halfcheetah/logs/
+++++++- .ipynb_checkpoints/
+++++++- __pycache__/
+++++++- Open Notebook.onetoc2 
+++++++-diff --git a/gridworld/gridworld_expts.ipynb b/gridworld/gridworld_expts.ipynb
+++++++-index e2a261b..bd0b720 100644
+++++++---- a/gridworld/gridworld_expts.ipynb
+++++++-+++ b/gridworld/gridworld_expts.ipynb
+++++++-@@ -1570,7 +1570,7 @@
+++++++-    "name": "python",
+++++++-    "nbconvert_exporter": "python",
+++++++-    "pygments_lexer": "ipython3",
+++++++--   "version": "3.8.10"
+++++++-+   "version": "3.10.12"
+++++++-   }
+++++++-  },
+++++++-  "nbformat": 4,
+++++++-diff --git a/halfcheetah/clusters.npy b/halfcheetah/clusters.npy
++++++++diff --git a/halfcheetah/environment.yml b/halfcheetah/environment.yml
+++++++ deleted file mode 100644
+++++++-index 3c198aa..0000000
+++++++-Binary files a/halfcheetah/clusters.npy and /dev/null differ
+++++++-diff --git a/halfcheetah/embeddings.npy b/halfcheetah/embeddings.npy
+++++++-deleted file mode 100644
+++++++-index 912d68c..0000000
+++++++-Binary files a/halfcheetah/embeddings.npy and /dev/null differ
+++++++-diff --git a/halfcheetah/halfcheetah.py b/halfcheetah/halfcheetah.py
+++++++-deleted file mode 100644
+++++++-index 626c42d..0000000
+++++++---- a/halfcheetah/halfcheetah.py
++++++++index e81007c..0000000
++++++++--- a/halfcheetah/environment.yml
+++++++ +++ /dev/null
+++++++-@@ -1,112 +0,0 @@
+++++++--import gym
+++++++--import d4rl # Import required to register environments, you may need to also import the submodule
+++++++--import numpy as np
+++++++--import d3rlpy
+++++++--
+++++++--def main():
+++++++--    dataset_d3, env = d3rlpy.datasets.get_dataset("halfcheetah-medium-v2")
+++++++--
+++++++--    print(dataset_d3.observations.shape)
+++++++--    print(dataset_d3.actions.shape)
+++++++--    print(dataset_d3.rewards.shape)
+++++++--        # print(dataset_d3.next_observations.shape)
+++++++--    print(dataset_d3.terminals.shape)
+++++++--    print(dataset_d3.terminals.sum()) # no
+++++++--
+++++++--    env = gym.make('halfcheetah-medium-v2')
+++++++--    dataset_d4 = d4rl.qlearning_dataset(env)
+++++++--
+++++++--    print(dataset_d4['observations'].shape)
+++++++--    print(dataset_d4['rewards'].shape)
+++++++--    print(dataset_d4['terminals'].shape)
+++++++--    print(dataset_d4['actions'].shape)
+++++++--
+++++++--    print(dataset_d4['rewards'][1])
+++++++--    print(dataset_d3.rewards[1])
+++++++--
+++++++--
+++++++--    print(np.allclose(dataset_d3.actions[100], dataset_d4['actions'][100]))
+++++++--
+++++++--    for j in range(1000):
+++++++--        for i in range(999):
+++++++--            if dataset_d4['rewards'][j * 999 + i] != dataset_d3.rewards[j * 1000 + i]: print("yo", i)
+++++++--        # if not np.allclose(dataset_d3.observations[i], dataset_d4['observations'][i]): print('obs ongelijk')
+++++++--        # if not np.allclose(dataset_d3.rewards[i], dataset_d4['rewards'][i]): print('obs ongelijk')
+++++++--        # if not np.allclose(dataset_d3.actions[i], dataset_d4['actions'][i]): print('obs ongelijk')
+++++++--
+++++++--    sac = d3rlpy.algos.SAC(
+++++++--        actor_learning_rate=3e-4,
+++++++--        critic_learning_rate=3e-4,
+++++++--        temp_learning_rate=3e-4,
+++++++--        batch_size=256)
+++++++--
+++++++--    print(sac)
+++++++--    sac.fit(dataset_d3, n_steps=10000)
+++++++--
+++++++--    actions = sac.predict(dataset_d3.observations[0])
+++++++--
+++++++--    print(actions)
+++++++--
+++++++--
+++++++--    return
+++++++--    print('yo!')
+++++++--
+++++++--    # Create the environment
+++++++--    env = gym.make('halfcheetah-medium-v2')
+++++++--
+++++++--    # d4rl abides by the OpenAI gym interface
+++++++--    env.reset()
+++++++--    env.step(env.action_space.sample())
+++++++--
+++++++--    # Each task is associated with a dataset
+++++++--    # dataset contains observations, actions, rewards, terminals, and infos
+++++++--    # dataset = env.get_dataset()
+++++++--    dataset = d4rl.qlearning_dataset(env)
+++++++--
+++++++--    print(dataset.keys()) # An N x dim_observation Numpy array of observations
+++++++--    print(dataset['rewards'].shape) # An N x dim_observation Numpy array of observations
+++++++--
+++++++--
+++++++--    first_traj = []
+++++++--    for i in range(50000):
+++++++--        if not np.allclose(dataset['next_observations'][i],dataset['observations'][i+1]): print("yo", i, dataset['terminals'][i])
+++++++--        # if dataset['terminals'][i] == True:
+++++++--        #     print('traj ended at', i)
+++++++--        #     break
+++++++--
+++++++--        # first_traj.append((dataset['observations'][i],
+++++++--        #                    dataset['actions'][i],
+++++++--        #                    dataset['rewards'][i],
+++++++--        #                    dataset['next_observations'][i]))
+++++++--    # print(first_traj)
+++++++--
+++++++--
+++++++--    # print(dataset['rewards'].shape) # An N x dim_observation Numpy array of observations
+++++++--
+++++++--    # Alternatively, use d4rl.qlearning_dataset which
+++++++--    # also adds next_observations.
+++++++--
+++++++--    # import d3rlpy
+++++++--
+++++++--    # # dataset, env = d3rlpy.datasets.get_dataset("halfcheetah-medium")
+++++++--
+++++++--    # # prepare algorithm
+++++++--    # # sac = d3rlpy.algos.SAC().create(device="cpu")
+++++++--
+++++++--    # sac = d3rlpy.algos.SACConfig(
+++++++--    #     actor_learning_rate=3e-4,
+++++++--    #     critic_learning_rate=3e-4,
+++++++--    #     temp_learning_rate=3e-4,
+++++++--    #     batch_size=256,
+++++++--    # ).create(device='cpu')
+++++++--
+++++++--
+++++++--    # # train offline
+++++++--    # # sac.fit(dataset, n_steps=1000)
+++++++--
+++++++--
+++++++--    # # ready to control
+++++++--    # actions = sac.predict(0)
+++++++--
+++++++--if __name__ == "__main__":
+++++++--    main()
++++++++@@ -1,27 +0,0 @@
++++++++-name: trajectory
++++++++-channels:
++++++++-- defaults
++++++++-- conda-forge
++++++++-dependencies:
++++++++-- python=3.8
++++++++-- pip
++++++++-- patchelf
++++++++-- pip:
++++++++-    - -f https://download.pytorch.org/whl/torch_stable.html
++++++++-    - numpy
++++++++-    - wheel==0.38.4
++++++++-    - setuptools==65.5.0
++++++++-    - gym==0.20.0
++++++++-    - matplotlib==3.3.4
++++++++-    - torch==1.9.1+cu111
++++++++-    - typed-argument-parser
++++++++-    # - git+https://github.com/Farama-Foundation/d4rl@f2a05c0d66722499bf8031b094d9af3aea7c372b#egg=d4rl
++++++++-    - scikit-image==0.17.2
++++++++-    - scikit-video==1.1.11
++++++++-    - gitpython
++++++++-    - os
++++++++-    - d3rlpy
++++++++-    - pyclustering
++++++++-    - moviepy
++++++++-    - scipy
++++++++-    - scikit-learn
++++++++diff --git a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json
++++++++index 18993c1..97d7fc4 100644
++++++++--- a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json
+++++++++++ b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json
++++++++@@ -1,23 +1,23 @@
++++++++ {
++++++++     "add_extras": {
++++++++         "_type": "python_object (type = method)",
++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgfhpRSlC4="
+++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMCHNhdmVwYXRolIw7bG9ncy9oYWxmY2hlZXRhaC1tZWRpdW0tdjIvcGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMyLzCUjAhuX2V4cGFuZJRLAowXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMCmdldF9jb21taXSUaAJoBmgMhpRSlIwKYWRkX2V4dHJhc5RoAmgGaA+GlFKUjAxncHRfbG9hZHBhdGiUjA5ncHQvcHJldHJhaW5lZJSMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHaG9yaXpvbpRLBYwOcHJlZml4X2NvbnRleHSUiIwHY2RmX29ic5ROjAVta2RpcpRoAmgGaBuGlFKUjAlncHRfZXBvY2iUjAZsYXRlc3SUjAtyZWFkX2NvbmZpZ5RoAmgGaCCGlFKUjAVrX29ic5RLAYwGZGV2aWNllIwDY3B1lIwKcGVyY2VudGlsZZSMBG1lYW6UjAh2aXNfZnJlcZRLMowJc2F2ZV9kaWZmlGgCaAZoKYaUUpSMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMCmJlYW1fd2lkdGiUSyCMBWtfYWN0lE6MBnByZWZpeJSMD3BsYW5zL2RlZmF1bHRzL5SMCHNldF9zZWVklGgCaAZoMoaUUpSMB2NkZl9hY3SURz/jMzMzMzMzjAZjb25maWeUjA5jb25maWcub2ZmbGluZZSMB3ZlcmJvc2WUiIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAlwbGFuX2ZyZXGUSwGMBnN1ZmZpeJSMATCUjAdsb2diYXNllIwFbG9ncy+UjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaECGlFKUdWJoD4aUUpQu"
++++++++     },
++++++++     "beam_width": 32,
++++++++     "cdf_act": 0.6,
++++++++     "cdf_obs": null,
++++++++-    "commit": "cca8d898e10f9f6102a8c33dac7758e2993dfc60 halfcheetah-xrl",
+++++++++    "commit": "f77a194996872879bd0414fcb3dbc15b3da29cb8 main",
++++++++     "config": "config.offline",
++++++++     "dataset": "halfcheetah-medium-v2",
++++++++     "device": "cpu",
++++++++     "exp_name": "plans/defaults/freq1_H5_beam32",
++++++++     "generate_exp_name": {
++++++++         "_type": "python_object (type = method)",
++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgxhpRSlC4="
+++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMCHNhdmVwYXRolIw7bG9ncy9oYWxmY2hlZXRhaC1tZWRpdW0tdjIvcGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMyLzCUjAhuX2V4cGFuZJRLAowXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMCmdldF9jb21taXSUaAJoBmgMhpRSlIwKYWRkX2V4dHJhc5RoAmgGaA+GlFKUjAxncHRfbG9hZHBhdGiUjA5ncHQvcHJldHJhaW5lZJSMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHaG9yaXpvbpRLBYwOcHJlZml4X2NvbnRleHSUiIwHY2RmX29ic5ROjAVta2RpcpRoAmgGaBuGlFKUjAlncHRfZXBvY2iUjAZsYXRlc3SUjAtyZWFkX2NvbmZpZ5RoAmgGaCCGlFKUjAVrX29ic5RLAYwGZGV2aWNllIwDY3B1lIwKcGVyY2VudGlsZZSMBG1lYW6UjAh2aXNfZnJlcZRLMowJc2F2ZV9kaWZmlGgCaAZoKYaUUpSMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMCmJlYW1fd2lkdGiUSyCMBWtfYWN0lE6MBnByZWZpeJSMD3BsYW5zL2RlZmF1bHRzL5SMCHNldF9zZWVklGgCaAZoMoaUUpSMB2NkZl9hY3SURz/jMzMzMzMzjAZjb25maWeUjA5jb25maWcub2ZmbGluZZSMB3ZlcmJvc2WUiIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAlwbGFuX2ZyZXGUSwGMBnN1ZmZpeJSMATCUjAdsb2diYXNllIwFbG9ncy+UjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaECGlFKUdWJoQIaUUpQu"
++++++++     },
++++++++     "get_commit": {
++++++++         "_type": "python_object (type = method)",
++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmglhpRSlC4="
+++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMCHNhdmVwYXRolIw7bG9ncy9oYWxmY2hlZXRhaC1tZWRpdW0tdjIvcGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMyLzCUjAhuX2V4cGFuZJRLAowXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMCmdldF9jb21taXSUaAJoBmgMhpRSlIwKYWRkX2V4dHJhc5RoAmgGaA+GlFKUjAxncHRfbG9hZHBhdGiUjA5ncHQvcHJldHJhaW5lZJSMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHaG9yaXpvbpRLBYwOcHJlZml4X2NvbnRleHSUiIwHY2RmX29ic5ROjAVta2RpcpRoAmgGaBuGlFKUjAlncHRfZXBvY2iUjAZsYXRlc3SUjAtyZWFkX2NvbmZpZ5RoAmgGaCCGlFKUjAVrX29ic5RLAYwGZGV2aWNllIwDY3B1lIwKcGVyY2VudGlsZZSMBG1lYW6UjAh2aXNfZnJlcZRLMowJc2F2ZV9kaWZmlGgCaAZoKYaUUpSMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMCmJlYW1fd2lkdGiUSyCMBWtfYWN0lE6MBnByZWZpeJSMD3BsYW5zL2RlZmF1bHRzL5SMCHNldF9zZWVklGgCaAZoMoaUUpSMB2NkZl9hY3SURz/jMzMzMzMzjAZjb25maWeUjA5jb25maWcub2ZmbGluZZSMB3ZlcmJvc2WUiIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAlwbGFuX2ZyZXGUSwGMBnN1ZmZpeJSMATCUjAdsb2diYXNllIwFbG9ncy+UjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaECGlFKUdWJoDIaUUpQu"
++++++++     },
++++++++     "gpt_epoch": "latest",
++++++++     "gpt_loadpath": "gpt/pretrained",
++++++++@@ -28,7 +28,7 @@
++++++++     "max_context_transitions": 5,
++++++++     "mkdir": {
++++++++         "_type": "python_object (type = method)",
++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgihpRSlC4="
+++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMCHNhdmVwYXRolIw7bG9ncy9oYWxmY2hlZXRhaC1tZWRpdW0tdjIvcGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMyLzCUjAhuX2V4cGFuZJRLAowXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMCmdldF9jb21taXSUaAJoBmgMhpRSlIwKYWRkX2V4dHJhc5RoAmgGaA+GlFKUjAxncHRfbG9hZHBhdGiUjA5ncHQvcHJldHJhaW5lZJSMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHaG9yaXpvbpRLBYwOcHJlZml4X2NvbnRleHSUiIwHY2RmX29ic5ROjAVta2RpcpRoAmgGaBuGlFKUjAlncHRfZXBvY2iUjAZsYXRlc3SUjAtyZWFkX2NvbmZpZ5RoAmgGaCCGlFKUjAVrX29ic5RLAYwGZGV2aWNllIwDY3B1lIwKcGVyY2VudGlsZZSMBG1lYW6UjAh2aXNfZnJlcZRLMowJc2F2ZV9kaWZmlGgCaAZoKYaUUpSMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMCmJlYW1fd2lkdGiUSyCMBWtfYWN0lE6MBnByZWZpeJSMD3BsYW5zL2RlZmF1bHRzL5SMCHNldF9zZWVklGgCaAZoMoaUUpSMB2NkZl9hY3SURz/jMzMzMzMzjAZjb25maWeUjA5jb25maWcub2ZmbGluZZSMB3ZlcmJvc2WUiIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAlwbGFuX2ZyZXGUSwGMBnN1ZmZpeJSMATCUjAdsb2diYXNllIwFbG9ncy+UjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaECGlFKUdWJoG4aUUpQu"
++++++++     },
++++++++     "n_expand": 2,
++++++++     "percentile": "mean",
++++++++@@ -37,24 +37,24 @@
++++++++     "prefix_context": true,
++++++++     "read_config": {
++++++++         "_type": "python_object (type = method)",
++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgZhpRSlC4="
+++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMCHNhdmVwYXRolIw7bG9ncy9oYWxmY2hlZXRhaC1tZWRpdW0tdjIvcGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMyLzCUjAhuX2V4cGFuZJRLAowXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMCmdldF9jb21taXSUaAJoBmgMhpRSlIwKYWRkX2V4dHJhc5RoAmgGaA+GlFKUjAxncHRfbG9hZHBhdGiUjA5ncHQvcHJldHJhaW5lZJSMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHaG9yaXpvbpRLBYwOcHJlZml4X2NvbnRleHSUiIwHY2RmX29ic5ROjAVta2RpcpRoAmgGaBuGlFKUjAlncHRfZXBvY2iUjAZsYXRlc3SUjAtyZWFkX2NvbmZpZ5RoAmgGaCCGlFKUjAVrX29ic5RLAYwGZGV2aWNllIwDY3B1lIwKcGVyY2VudGlsZZSMBG1lYW6UjAh2aXNfZnJlcZRLMowJc2F2ZV9kaWZmlGgCaAZoKYaUUpSMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMCmJlYW1fd2lkdGiUSyCMBWtfYWN0lE6MBnByZWZpeJSMD3BsYW5zL2RlZmF1bHRzL5SMCHNldF9zZWVklGgCaAZoMoaUUpSMB2NkZl9hY3SURz/jMzMzMzMzjAZjb25maWeUjA5jb25maWcub2ZmbGluZZSMB3ZlcmJvc2WUiIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAlwbGFuX2ZyZXGUSwGMBnN1ZmZpeJSMATCUjAdsb2diYXNllIwFbG9ncy+UjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaECGlFKUdWJoIIaUUpQu"
++++++++     },
++++++++     "renderer": "Renderer",
++++++++     "reproducibility": {
++++++++-        "command_line": "python scripts/xrl_v2.py --dataset halfcheetah-medium-v2 --gpt_loadpath gpt/pretrained",
+++++++++        "command_line": "python xrl_v2.py --dataset halfcheetah-medium-v2 --gpt_loadpath gpt/pretrained",
++++++++         "git_has_uncommitted_changes": true,
++++++++         "git_root": "/home/colin/Desktop/FACT/FACT_assignment",
++++++++-        "git_url": "https://github.com/fclio/FACT_assignment/tree/cca8d898e10f9f6102a8c33dac7758e2993dfc60",
++++++++-        "time": "Fri Feb  2 11:39:41 2024"
+++++++++        "git_url": "https://github.com/fclio/FACT_assignment/tree/f77a194996872879bd0414fcb3dbc15b3da29cb8",
+++++++++        "time": "Sat Feb  3 21:28:42 2024"
++++++++     },
++++++++     "save_diff": {
++++++++         "_type": "python_object (type = method)",
++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1Ymg0hpRSlC4="
+++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMCHNhdmVwYXRolIw7bG9ncy9oYWxmY2hlZXRhaC1tZWRpdW0tdjIvcGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMyLzCUjAhuX2V4cGFuZJRLAowXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMCmdldF9jb21taXSUaAJoBmgMhpRSlIwKYWRkX2V4dHJhc5RoAmgGaA+GlFKUjAxncHRfbG9hZHBhdGiUjA5ncHQvcHJldHJhaW5lZJSMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHaG9yaXpvbpRLBYwOcHJlZml4X2NvbnRleHSUiIwHY2RmX29ic5ROjAVta2RpcpRoAmgGaBuGlFKUjAlncHRfZXBvY2iUjAZsYXRlc3SUjAtyZWFkX2NvbmZpZ5RoAmgGaCCGlFKUjAVrX29ic5RLAYwGZGV2aWNllIwDY3B1lIwKcGVyY2VudGlsZZSMBG1lYW6UjAh2aXNfZnJlcZRLMowJc2F2ZV9kaWZmlGgCaAZoKYaUUpSMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMCmJlYW1fd2lkdGiUSyCMBWtfYWN0lE6MBnByZWZpeJSMD3BsYW5zL2RlZmF1bHRzL5SMCHNldF9zZWVklGgCaAZoMoaUUpSMB2NkZl9hY3SURz/jMzMzMzMzjAZjb25maWeUjA5jb25maWcub2ZmbGluZZSMB3ZlcmJvc2WUiIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAlwbGFuX2ZyZXGUSwGMBnN1ZmZpeJSMATCUjAdsb2diYXNllIwFbG9ncy+UjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaECGlFKUdWJoKYaUUpQu"
++++++++     },
++++++++     "savepath": "logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0",
++++++++     "set_seed": {
++++++++         "_type": "python_object (type = method)",
++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgqhpRSlC4="
+++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMCHNhdmVwYXRolIw7bG9ncy9oYWxmY2hlZXRhaC1tZWRpdW0tdjIvcGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMyLzCUjAhuX2V4cGFuZJRLAowXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMCmdldF9jb21taXSUaAJoBmgMhpRSlIwKYWRkX2V4dHJhc5RoAmgGaA+GlFKUjAxncHRfbG9hZHBhdGiUjA5ncHQvcHJldHJhaW5lZJSMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHaG9yaXpvbpRLBYwOcHJlZml4X2NvbnRleHSUiIwHY2RmX29ic5ROjAVta2RpcpRoAmgGaBuGlFKUjAlncHRfZXBvY2iUjAZsYXRlc3SUjAtyZWFkX2NvbmZpZ5RoAmgGaCCGlFKUjAVrX29ic5RLAYwGZGV2aWNllIwDY3B1lIwKcGVyY2VudGlsZZSMBG1lYW6UjAh2aXNfZnJlcZRLMowJc2F2ZV9kaWZmlGgCaAZoKYaUUpSMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMCmJlYW1fd2lkdGiUSyCMBWtfYWN0lE6MBnByZWZpeJSMD3BsYW5zL2RlZmF1bHRzL5SMCHNldF9zZWVklGgCaAZoMoaUUpSMB2NkZl9hY3SURz/jMzMzMzMzjAZjb25maWeUjA5jb25maWcub2ZmbGluZZSMB3ZlcmJvc2WUiIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAlwbGFuX2ZyZXGUSwGMBnN1ZmZpeJSMATCUjAdsb2diYXNllIwFbG9ncy+UjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaECGlFKUdWJoMoaUUpQu"
++++++++     },
++++++++     "suffix": "0",
++++++++     "verbose": true,
++++++++diff --git a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt
++++++++index 2c00d59..4879322 100644
++++++++--- a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt
+++++++++++ b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt
++++++++@@ -1,1250 +1,9291 @@
++++++++-diff --git a/.gitignore b/.gitignore
++++++++-index 30d226d..a18312a 100644
++++++++---- a/.gitignore
++++++++-+++ b/.gitignore
++++++++-@@ -1,6 +1,5 @@
++++++++- __pycache__/
++++++++- *.mp4
++++++++--halfcheetah/logs/
++++++++- .ipynb_checkpoints/
++++++++- __pycache__/
++++++++- Open Notebook.onetoc2 
++++++++-diff --git a/gridworld/gridworld_expts.ipynb b/gridworld/gridworld_expts.ipynb
++++++++-index e2a261b..bd0b720 100644
++++++++---- a/gridworld/gridworld_expts.ipynb
++++++++-+++ b/gridworld/gridworld_expts.ipynb
++++++++-@@ -1570,7 +1570,7 @@
++++++++-    "name": "python",
++++++++-    "nbconvert_exporter": "python",
++++++++-    "pygments_lexer": "ipython3",
++++++++--   "version": "3.8.10"
++++++++-+   "version": "3.10.12"
++++++++-   }
++++++++-  },
++++++++-  "nbformat": 4,
++++++++-diff --git a/halfcheetah/clusters.npy b/halfcheetah/clusters.npy
+++++++++diff --git a/halfcheetah/environment.yml b/halfcheetah/environment.yml
++++++++ deleted file mode 100644
++++++++-index 3c198aa..0000000
++++++++-Binary files a/halfcheetah/clusters.npy and /dev/null differ
++++++++-diff --git a/halfcheetah/embeddings.npy b/halfcheetah/embeddings.npy
++++++++-deleted file mode 100644
++++++++-index 912d68c..0000000
++++++++-Binary files a/halfcheetah/embeddings.npy and /dev/null differ
++++++++-diff --git a/halfcheetah/halfcheetah.py b/halfcheetah/halfcheetah.py
++++++++-deleted file mode 100644
++++++++-index 626c42d..0000000
++++++++---- a/halfcheetah/halfcheetah.py
+++++++++index e81007c..0000000
+++++++++--- a/halfcheetah/environment.yml
++++++++ +++ /dev/null
++++++++-@@ -1,112 +0,0 @@
++++++++--import gym
++++++++--import d4rl # Import required to register environments, you may need to also import the submodule
++++++++--import numpy as np
++++++++--import d3rlpy
++++++++--
++++++++--def main():
++++++++--    dataset_d3, env = d3rlpy.datasets.get_dataset("halfcheetah-medium-v2")
++++++++--
++++++++--    print(dataset_d3.observations.shape)
++++++++--    print(dataset_d3.actions.shape)
++++++++--    print(dataset_d3.rewards.shape)
++++++++--        # print(dataset_d3.next_observations.shape)
++++++++--    print(dataset_d3.terminals.shape)
++++++++--    print(dataset_d3.terminals.sum()) # no
++++++++--
++++++++--    env = gym.make('halfcheetah-medium-v2')
++++++++--    dataset_d4 = d4rl.qlearning_dataset(env)
++++++++--
++++++++--    print(dataset_d4['observations'].shape)
++++++++--    print(dataset_d4['rewards'].shape)
++++++++--    print(dataset_d4['terminals'].shape)
++++++++--    print(dataset_d4['actions'].shape)
++++++++--
++++++++--    print(dataset_d4['rewards'][1])
++++++++--    print(dataset_d3.rewards[1])
++++++++--
++++++++--
++++++++--    print(np.allclose(dataset_d3.actions[100], dataset_d4['actions'][100]))
++++++++--
++++++++--    for j in range(1000):
++++++++--        for i in range(999):
++++++++--            if dataset_d4['rewards'][j * 999 + i] != dataset_d3.rewards[j * 1000 + i]: print("yo", i)
++++++++--        # if not np.allclose(dataset_d3.observations[i], dataset_d4['observations'][i]): print('obs ongelijk')
++++++++--        # if not np.allclose(dataset_d3.rewards[i], dataset_d4['rewards'][i]): print('obs ongelijk')
++++++++--        # if not np.allclose(dataset_d3.actions[i], dataset_d4['actions'][i]): print('obs ongelijk')
++++++++--
++++++++--    sac = d3rlpy.algos.SAC(
++++++++--        actor_learning_rate=3e-4,
++++++++--        critic_learning_rate=3e-4,
++++++++--        temp_learning_rate=3e-4,
++++++++--        batch_size=256)
++++++++--
++++++++--    print(sac)
++++++++--    sac.fit(dataset_d3, n_steps=10000)
++++++++--
++++++++--    actions = sac.predict(dataset_d3.observations[0])
++++++++--
++++++++--    print(actions)
++++++++--
++++++++--
++++++++--    return
++++++++--    print('yo!')
++++++++--
++++++++--    # Create the environment
++++++++--    env = gym.make('halfcheetah-medium-v2')
++++++++--
++++++++--    # d4rl abides by the OpenAI gym interface
++++++++--    env.reset()
++++++++--    env.step(env.action_space.sample())
++++++++--
++++++++--    # Each task is associated with a dataset
++++++++--    # dataset contains observations, actions, rewards, terminals, and infos
++++++++--    # dataset = env.get_dataset()
++++++++--    dataset = d4rl.qlearning_dataset(env)
++++++++--
++++++++--    print(dataset.keys()) # An N x dim_observation Numpy array of observations
++++++++--    print(dataset['rewards'].shape) # An N x dim_observation Numpy array of observations
++++++++--
++++++++--
++++++++--    first_traj = []
++++++++--    for i in range(50000):
++++++++--        if not np.allclose(dataset['next_observations'][i],dataset['observations'][i+1]): print("yo", i, dataset['terminals'][i])
++++++++--        # if dataset['terminals'][i] == True:
++++++++--        #     print('traj ended at', i)
++++++++--        #     break
++++++++--
++++++++--        # first_traj.append((dataset['observations'][i],
++++++++--        #                    dataset['actions'][i],
++++++++--        #                    dataset['rewards'][i],
++++++++--        #                    dataset['next_observations'][i]))
++++++++--    # print(first_traj)
++++++++--
++++++++--
++++++++--    # print(dataset['rewards'].shape) # An N x dim_observation Numpy array of observations
++++++++--
++++++++--    # Alternatively, use d4rl.qlearning_dataset which
++++++++--    # also adds next_observations.
++++++++--
++++++++--    # import d3rlpy
++++++++--
++++++++--    # # dataset, env = d3rlpy.datasets.get_dataset("halfcheetah-medium")
++++++++--
++++++++--    # # prepare algorithm
++++++++--    # # sac = d3rlpy.algos.SAC().create(device="cpu")
++++++++--
++++++++--    # sac = d3rlpy.algos.SACConfig(
++++++++--    #     actor_learning_rate=3e-4,
++++++++--    #     critic_learning_rate=3e-4,
++++++++--    #     temp_learning_rate=3e-4,
++++++++--    #     batch_size=256,
++++++++--    # ).create(device='cpu')
++++++++--
++++++++--
++++++++--    # # train offline
++++++++--    # # sac.fit(dataset, n_steps=1000)
++++++++--
++++++++--
++++++++--    # # ready to control
++++++++--    # actions = sac.predict(0)
++++++++--
++++++++--if __name__ == "__main__":
++++++++--    main()
+++++++++@@ -1,27 +0,0 @@
+++++++++-name: trajectory
+++++++++-channels:
+++++++++-- defaults
+++++++++-- conda-forge
+++++++++-dependencies:
+++++++++-- python=3.8
+++++++++-- pip
+++++++++-- patchelf
+++++++++-- pip:
+++++++++-    - -f https://download.pytorch.org/whl/torch_stable.html
+++++++++-    - numpy
+++++++++-    - wheel==0.38.4
+++++++++-    - setuptools==65.5.0
+++++++++-    - gym==0.20.0
+++++++++-    - matplotlib==3.3.4
+++++++++-    - torch==1.9.1+cu111
+++++++++-    - typed-argument-parser
+++++++++-    # - git+https://github.com/Farama-Foundation/d4rl@f2a05c0d66722499bf8031b094d9af3aea7c372b#egg=d4rl
+++++++++-    - scikit-image==0.17.2
+++++++++-    - scikit-video==1.1.11
+++++++++-    - gitpython
+++++++++-    - os
+++++++++-    - d3rlpy
+++++++++-    - pyclustering
+++++++++-    - moviepy
+++++++++-    - scipy
+++++++++-    - scikit-learn
+++++++++diff --git a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json
+++++++++index 18993c1..7918db1 100644
+++++++++--- a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json
++++++++++++ b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json
+++++++++@@ -1,23 +1,23 @@
+++++++++ {
+++++++++     "add_extras": {
+++++++++         "_type": "python_object (type = method)",
+++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgfhpRSlC4="
++++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMB2NkZl9hY3SURz/jMzMzMzMzjAdjZGZfb2JzlE6MCGV4cF9uYW1llIwecGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMylIwFa19vYnOUSwGMCmJlYW1fd2lkdGiUSyCMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMBnN1ZmZpeJSMATCUjAlncHRfZXBvY2iUjAZsYXRlc3SUjApwZXJjZW50aWxllIwEbWVhbpSMB3ZlcmJvc2WUiIwIcmVuZGVyZXKUjAhSZW5kZXJlcpSMCXNhdmVfZGlmZpRoAmgGaBmGlFKUjAdkYXRhc2V0lIwVaGFsZmNoZWV0YWgtbWVkaXVtLXYylIwHaG9yaXpvbpRLBYwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAhuX2V4cGFuZJRLAowKYWRkX2V4dHJhc5RoAmgGaCKGlFKUjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaCWGlFKUjAh2aXNfZnJlcZRLMowOcHJlZml4X2NvbnRleHSUiIwLcmVhZF9jb25maWeUaAJoBmgqhpRSlIwXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMCHNhdmVwYXRolIw7bG9ncy9oYWxmY2hlZXRhaC1tZWRpdW0tdjIvcGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMyLzCUjAlwbGFuX2ZyZXGUSwGMBnByZWZpeJSMD3BsYW5zL2RlZmF1bHRzL5SMBWtfYWN0lE6MBmRldmljZZSMA2NwdZSMCHNldF9zZWVklGgCaAZoNoaUUpSMCmdldF9jb21taXSUaAJoBmg5hpRSlIwFbWtkaXKUaAJoBmg8hpRSlIwMZ3B0X2xvYWRwYXRolIwOZ3B0L3ByZXRyYWluZWSUjAdsb2diYXNllIwFbG9ncy+UdWJoIoaUUpQu"
+++++++++     },
+++++++++     "beam_width": 32,
+++++++++     "cdf_act": 0.6,
+++++++++     "cdf_obs": null,
+++++++++-    "commit": "cca8d898e10f9f6102a8c33dac7758e2993dfc60 halfcheetah-xrl",
++++++++++    "commit": "f77a194996872879bd0414fcb3dbc15b3da29cb8 main",
+++++++++     "config": "config.offline",
+++++++++     "dataset": "halfcheetah-medium-v2",
+++++++++     "device": "cpu",
+++++++++     "exp_name": "plans/defaults/freq1_H5_beam32",
+++++++++     "generate_exp_name": {
+++++++++         "_type": "python_object (type = method)",
+++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgxhpRSlC4="
++++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMB2NkZl9hY3SURz/jMzMzMzMzjAdjZGZfb2JzlE6MCGV4cF9uYW1llIwecGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMylIwFa19vYnOUSwGMCmJlYW1fd2lkdGiUSyCMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMBnN1ZmZpeJSMATCUjAlncHRfZXBvY2iUjAZsYXRlc3SUjApwZXJjZW50aWxllIwEbWVhbpSMB3ZlcmJvc2WUiIwIcmVuZGVyZXKUjAhSZW5kZXJlcpSMCXNhdmVfZGlmZpRoAmgGaBmGlFKUjAdkYXRhc2V0lIwVaGFsZmNoZWV0YWgtbWVkaXVtLXYylIwHaG9yaXpvbpRLBYwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAhuX2V4cGFuZJRLAowKYWRkX2V4dHJhc5RoAmgGaCKGlFKUjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaCWGlFKUjAh2aXNfZnJlcZRLMowOcHJlZml4X2NvbnRleHSUiIwLcmVhZF9jb25maWeUaAJoBmgqhpRSlIwXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMCHNhdmVwYXRolIw7bG9ncy9oYWxmY2hlZXRhaC1tZWRpdW0tdjIvcGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMyLzCUjAlwbGFuX2ZyZXGUSwGMBnByZWZpeJSMD3BsYW5zL2RlZmF1bHRzL5SMBWtfYWN0lE6MBmRldmljZZSMA2NwdZSMCHNldF9zZWVklGgCaAZoNoaUUpSMCmdldF9jb21taXSUaAJoBmg5hpRSlIwFbWtkaXKUaAJoBmg8hpRSlIwMZ3B0X2xvYWRwYXRolIwOZ3B0L3ByZXRyYWluZWSUjAdsb2diYXNllIwFbG9ncy+UdWJoJYaUUpQu"
+++++++++     },
+++++++++     "get_commit": {
+++++++++         "_type": "python_object (type = method)",
+++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmglhpRSlC4="
++++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMB2NkZl9hY3SURz/jMzMzMzMzjAdjZGZfb2JzlE6MCGV4cF9uYW1llIwecGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMylIwFa19vYnOUSwGMCmJlYW1fd2lkdGiUSyCMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMBnN1ZmZpeJSMATCUjAlncHRfZXBvY2iUjAZsYXRlc3SUjApwZXJjZW50aWxllIwEbWVhbpSMB3ZlcmJvc2WUiIwIcmVuZGVyZXKUjAhSZW5kZXJlcpSMCXNhdmVfZGlmZpRoAmgGaBmGlFKUjAdkYXRhc2V0lIwVaGFsZmNoZWV0YWgtbWVkaXVtLXYylIwHaG9yaXpvbpRLBYwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAhuX2V4cGFuZJRLAowKYWRkX2V4dHJhc5RoAmgGaCKGlFKUjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaCWGlFKUjAh2aXNfZnJlcZRLMowOcHJlZml4X2NvbnRleHSUiIwLcmVhZF9jb25maWeUaAJoBmgqhpRSlIwXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMCHNhdmVwYXRolIw7bG9ncy9oYWxmY2hlZXRhaC1tZWRpdW0tdjIvcGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMyLzCUjAlwbGFuX2ZyZXGUSwGMBnByZWZpeJSMD3BsYW5zL2RlZmF1bHRzL5SMBWtfYWN0lE6MBmRldmljZZSMA2NwdZSMCHNldF9zZWVklGgCaAZoNoaUUpSMCmdldF9jb21taXSUaAJoBmg5hpRSlIwFbWtkaXKUaAJoBmg8hpRSlIwMZ3B0X2xvYWRwYXRolIwOZ3B0L3ByZXRyYWluZWSUjAdsb2diYXNllIwFbG9ncy+UdWJoOYaUUpQu"
+++++++++     },
+++++++++     "gpt_epoch": "latest",
+++++++++     "gpt_loadpath": "gpt/pretrained",
+++++++++@@ -28,7 +28,7 @@
+++++++++     "max_context_transitions": 5,
+++++++++     "mkdir": {
+++++++++         "_type": "python_object (type = method)",
+++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgihpRSlC4="
++++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMB2NkZl9hY3SURz/jMzMzMzMzjAdjZGZfb2JzlE6MCGV4cF9uYW1llIwecGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMylIwFa19vYnOUSwGMCmJlYW1fd2lkdGiUSyCMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMBnN1ZmZpeJSMATCUjAlncHRfZXBvY2iUjAZsYXRlc3SUjApwZXJjZW50aWxllIwEbWVhbpSMB3ZlcmJvc2WUiIwIcmVuZGVyZXKUjAhSZW5kZXJlcpSMCXNhdmVfZGlmZpRoAmgGaBmGlFKUjAdkYXRhc2V0lIwVaGFsZmNoZWV0YWgtbWVkaXVtLXYylIwHaG9yaXpvbpRLBYwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAhuX2V4cGFuZJRLAowKYWRkX2V4dHJhc5RoAmgGaCKGlFKUjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaCWGlFKUjAh2aXNfZnJlcZRLMowOcHJlZml4X2NvbnRleHSUiIwLcmVhZF9jb25maWeUaAJoBmgqhpRSlIwXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMCHNhdmVwYXRolIw7bG9ncy9oYWxmY2hlZXRhaC1tZWRpdW0tdjIvcGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMyLzCUjAlwbGFuX2ZyZXGUSwGMBnByZWZpeJSMD3BsYW5zL2RlZmF1bHRzL5SMBWtfYWN0lE6MBmRldmljZZSMA2NwdZSMCHNldF9zZWVklGgCaAZoNoaUUpSMCmdldF9jb21taXSUaAJoBmg5hpRSlIwFbWtkaXKUaAJoBmg8hpRSlIwMZ3B0X2xvYWRwYXRolIwOZ3B0L3ByZXRyYWluZWSUjAdsb2diYXNllIwFbG9ncy+UdWJoPIaUUpQu"
+++++++++     },
+++++++++     "n_expand": 2,
+++++++++     "percentile": "mean",
+++++++++@@ -37,24 +37,24 @@
+++++++++     "prefix_context": true,
+++++++++     "read_config": {
+++++++++         "_type": "python_object (type = method)",
+++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgZhpRSlC4="
++++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMB2NkZl9hY3SURz/jMzMzMzMzjAdjZGZfb2JzlE6MCGV4cF9uYW1llIwecGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMylIwFa19vYnOUSwGMCmJlYW1fd2lkdGiUSyCMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMBnN1ZmZpeJSMATCUjAlncHRfZXBvY2iUjAZsYXRlc3SUjApwZXJjZW50aWxllIwEbWVhbpSMB3ZlcmJvc2WUiIwIcmVuZGVyZXKUjAhSZW5kZXJlcpSMCXNhdmVfZGlmZpRoAmgGaBmGlFKUjAdkYXRhc2V0lIwVaGFsZmNoZWV0YWgtbWVkaXVtLXYylIwHaG9yaXpvbpRLBYwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAhuX2V4cGFuZJRLAowKYWRkX2V4dHJhc5RoAmgGaCKGlFKUjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaCWGlFKUjAh2aXNfZnJlcZRLMowOcHJlZml4X2NvbnRleHSUiIwLcmVhZF9jb25maWeUaAJoBmgqhpRSlIwXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMCHNhdmVwYXRolIw7bG9ncy9oYWxmY2hlZXRhaC1tZWRpdW0tdjIvcGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMyLzCUjAlwbGFuX2ZyZXGUSwGMBnByZWZpeJSMD3BsYW5zL2RlZmF1bHRzL5SMBWtfYWN0lE6MBmRldmljZZSMA2NwdZSMCHNldF9zZWVklGgCaAZoNoaUUpSMCmdldF9jb21taXSUaAJoBmg5hpRSlIwFbWtkaXKUaAJoBmg8hpRSlIwMZ3B0X2xvYWRwYXRolIwOZ3B0L3ByZXRyYWluZWSUjAdsb2diYXNllIwFbG9ncy+UdWJoKoaUUpQu"
+++++++++     },
+++++++++     "renderer": "Renderer",
+++++++++     "reproducibility": {
+++++++++-        "command_line": "python scripts/xrl_v2.py --dataset halfcheetah-medium-v2 --gpt_loadpath gpt/pretrained",
++++++++++        "command_line": "python xrl_v2.py --dataset halfcheetah-medium-v2 --gpt_loadpath gpt/pretrained",
+++++++++         "git_has_uncommitted_changes": true,
+++++++++         "git_root": "/home/colin/Desktop/FACT/FACT_assignment",
+++++++++-        "git_url": "https://github.com/fclio/FACT_assignment/tree/cca8d898e10f9f6102a8c33dac7758e2993dfc60",
+++++++++-        "time": "Fri Feb  2 11:39:41 2024"
++++++++++        "git_url": "https://github.com/fclio/FACT_assignment/tree/f77a194996872879bd0414fcb3dbc15b3da29cb8",
++++++++++        "time": "Sat Feb  3 21:24:44 2024"
+++++++++     },
+++++++++     "save_diff": {
+++++++++         "_type": "python_object (type = method)",
+++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1Ymg0hpRSlC4="
++++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMB2NkZl9hY3SURz/jMzMzMzMzjAdjZGZfb2JzlE6MCGV4cF9uYW1llIwecGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMylIwFa19vYnOUSwGMCmJlYW1fd2lkdGiUSyCMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMBnN1ZmZpeJSMATCUjAlncHRfZXBvY2iUjAZsYXRlc3SUjApwZXJjZW50aWxllIwEbWVhbpSMB3ZlcmJvc2WUiIwIcmVuZGVyZXKUjAhSZW5kZXJlcpSMCXNhdmVfZGlmZpRoAmgGaBmGlFKUjAdkYXRhc2V0lIwVaGFsZmNoZWV0YWgtbWVkaXVtLXYylIwHaG9yaXpvbpRLBYwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAhuX2V4cGFuZJRLAowKYWRkX2V4dHJhc5RoAmgGaCKGlFKUjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaCWGlFKUjAh2aXNfZnJlcZRLMowOcHJlZml4X2NvbnRleHSUiIwLcmVhZF9jb25maWeUaAJoBmgqhpRSlIwXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMCHNhdmVwYXRolIw7bG9ncy9oYWxmY2hlZXRhaC1tZWRpdW0tdjIvcGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMyLzCUjAlwbGFuX2ZyZXGUSwGMBnByZWZpeJSMD3BsYW5zL2RlZmF1bHRzL5SMBWtfYWN0lE6MBmRldmljZZSMA2NwdZSMCHNldF9zZWVklGgCaAZoNoaUUpSMCmdldF9jb21taXSUaAJoBmg5hpRSlIwFbWtkaXKUaAJoBmg8hpRSlIwMZ3B0X2xvYWRwYXRolIwOZ3B0L3ByZXRyYWluZWSUjAdsb2diYXNllIwFbG9ncy+UdWJoGYaUUpQu"
+++++++++     },
+++++++++     "savepath": "logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0",
+++++++++     "set_seed": {
+++++++++         "_type": "python_object (type = method)",
+++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgqhpRSlC4="
++++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMB2NkZl9hY3SURz/jMzMzMzMzjAdjZGZfb2JzlE6MCGV4cF9uYW1llIwecGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMylIwFa19vYnOUSwGMCmJlYW1fd2lkdGiUSyCMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMBnN1ZmZpeJSMATCUjAlncHRfZXBvY2iUjAZsYXRlc3SUjApwZXJjZW50aWxllIwEbWVhbpSMB3ZlcmJvc2WUiIwIcmVuZGVyZXKUjAhSZW5kZXJlcpSMCXNhdmVfZGlmZpRoAmgGaBmGlFKUjAdkYXRhc2V0lIwVaGFsZmNoZWV0YWgtbWVkaXVtLXYylIwHaG9yaXpvbpRLBYwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAhuX2V4cGFuZJRLAowKYWRkX2V4dHJhc5RoAmgGaCKGlFKUjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaCWGlFKUjAh2aXNfZnJlcZRLMowOcHJlZml4X2NvbnRleHSUiIwLcmVhZF9jb25maWeUaAJoBmgqhpRSlIwXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMCHNhdmVwYXRolIw7bG9ncy9oYWxmY2hlZXRhaC1tZWRpdW0tdjIvcGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMyLzCUjAlwbGFuX2ZyZXGUSwGMBnByZWZpeJSMD3BsYW5zL2RlZmF1bHRzL5SMBWtfYWN0lE6MBmRldmljZZSMA2NwdZSMCHNldF9zZWVklGgCaAZoNoaUUpSMCmdldF9jb21taXSUaAJoBmg5hpRSlIwFbWtkaXKUaAJoBmg8hpRSlIwMZ3B0X2xvYWRwYXRolIwOZ3B0L3ByZXRyYWluZWSUjAdsb2diYXNllIwFbG9ncy+UdWJoNoaUUpQu"
+++++++++     },
+++++++++     "suffix": "0",
+++++++++     "verbose": true,
+++++++++diff --git a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt
+++++++++index 2c00d59..c25f468 100644
+++++++++--- a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt
++++++++++++ b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt
+++++++++@@ -1,1250 +1,7721 @@
+++++++++-diff --git a/.gitignore b/.gitignore
+++++++++-index 30d226d..a18312a 100644
+++++++++---- a/.gitignore
+++++++++-+++ b/.gitignore
+++++++++-@@ -1,6 +1,5 @@
+++++++++- __pycache__/
+++++++++- *.mp4
+++++++++--halfcheetah/logs/
+++++++++- .ipynb_checkpoints/
+++++++++- __pycache__/
+++++++++- Open Notebook.onetoc2 
+++++++++-diff --git a/gridworld/gridworld_expts.ipynb b/gridworld/gridworld_expts.ipynb
+++++++++-index e2a261b..bd0b720 100644
+++++++++---- a/gridworld/gridworld_expts.ipynb
+++++++++-+++ b/gridworld/gridworld_expts.ipynb
+++++++++-@@ -1570,7 +1570,7 @@
+++++++++-    "name": "python",
+++++++++-    "nbconvert_exporter": "python",
+++++++++-    "pygments_lexer": "ipython3",
+++++++++--   "version": "3.8.10"
+++++++++-+   "version": "3.10.12"
+++++++++-   }
+++++++++-  },
+++++++++-  "nbformat": 4,
+++++++++-diff --git a/halfcheetah/clusters.npy b/halfcheetah/clusters.npy
++++++++++diff --git a/halfcheetah/environment.yml b/halfcheetah/environment.yml
+++++++++ deleted file mode 100644
+++++++++-index 3c198aa..0000000
+++++++++-Binary files a/halfcheetah/clusters.npy and /dev/null differ
+++++++++-diff --git a/halfcheetah/embeddings.npy b/halfcheetah/embeddings.npy
+++++++++-deleted file mode 100644
+++++++++-index 912d68c..0000000
+++++++++-Binary files a/halfcheetah/embeddings.npy and /dev/null differ
+++++++++-diff --git a/halfcheetah/halfcheetah.py b/halfcheetah/halfcheetah.py
+++++++++-deleted file mode 100644
+++++++++-index 626c42d..0000000
+++++++++---- a/halfcheetah/halfcheetah.py
++++++++++index e81007c..0000000
++++++++++--- a/halfcheetah/environment.yml
+++++++++ +++ /dev/null
+++++++++-@@ -1,112 +0,0 @@
+++++++++--import gym
+++++++++--import d4rl # Import required to register environments, you may need to also import the submodule
+++++++++--import numpy as np
+++++++++--import d3rlpy
+++++++++--
+++++++++--def main():
+++++++++--    dataset_d3, env = d3rlpy.datasets.get_dataset("halfcheetah-medium-v2")
+++++++++--
+++++++++--    print(dataset_d3.observations.shape)
+++++++++--    print(dataset_d3.actions.shape)
+++++++++--    print(dataset_d3.rewards.shape)
+++++++++--        # print(dataset_d3.next_observations.shape)
+++++++++--    print(dataset_d3.terminals.shape)
+++++++++--    print(dataset_d3.terminals.sum()) # no
+++++++++--
+++++++++--    env = gym.make('halfcheetah-medium-v2')
+++++++++--    dataset_d4 = d4rl.qlearning_dataset(env)
+++++++++--
+++++++++--    print(dataset_d4['observations'].shape)
+++++++++--    print(dataset_d4['rewards'].shape)
+++++++++--    print(dataset_d4['terminals'].shape)
+++++++++--    print(dataset_d4['actions'].shape)
+++++++++--
+++++++++--    print(dataset_d4['rewards'][1])
+++++++++--    print(dataset_d3.rewards[1])
+++++++++--
+++++++++--
+++++++++--    print(np.allclose(dataset_d3.actions[100], dataset_d4['actions'][100]))
+++++++++--
+++++++++--    for j in range(1000):
+++++++++--        for i in range(999):
+++++++++--            if dataset_d4['rewards'][j * 999 + i] != dataset_d3.rewards[j * 1000 + i]: print("yo", i)
+++++++++--        # if not np.allclose(dataset_d3.observations[i], dataset_d4['observations'][i]): print('obs ongelijk')
+++++++++--        # if not np.allclose(dataset_d3.rewards[i], dataset_d4['rewards'][i]): print('obs ongelijk')
+++++++++--        # if not np.allclose(dataset_d3.actions[i], dataset_d4['actions'][i]): print('obs ongelijk')
+++++++++--
+++++++++--    sac = d3rlpy.algos.SAC(
+++++++++--        actor_learning_rate=3e-4,
+++++++++--        critic_learning_rate=3e-4,
+++++++++--        temp_learning_rate=3e-4,
+++++++++--        batch_size=256)
+++++++++--
+++++++++--    print(sac)
+++++++++--    sac.fit(dataset_d3, n_steps=10000)
+++++++++--
+++++++++--    actions = sac.predict(dataset_d3.observations[0])
+++++++++--
+++++++++--    print(actions)
+++++++++--
+++++++++--
+++++++++--    return
+++++++++--    print('yo!')
+++++++++--
+++++++++--    # Create the environment
+++++++++--    env = gym.make('halfcheetah-medium-v2')
+++++++++--
+++++++++--    # d4rl abides by the OpenAI gym interface
+++++++++--    env.reset()
+++++++++--    env.step(env.action_space.sample())
+++++++++--
+++++++++--    # Each task is associated with a dataset
+++++++++--    # dataset contains observations, actions, rewards, terminals, and infos
+++++++++--    # dataset = env.get_dataset()
+++++++++--    dataset = d4rl.qlearning_dataset(env)
+++++++++--
+++++++++--    print(dataset.keys()) # An N x dim_observation Numpy array of observations
+++++++++--    print(dataset['rewards'].shape) # An N x dim_observation Numpy array of observations
+++++++++--
+++++++++--
+++++++++--    first_traj = []
+++++++++--    for i in range(50000):
+++++++++--        if not np.allclose(dataset['next_observations'][i],dataset['observations'][i+1]): print("yo", i, dataset['terminals'][i])
+++++++++--        # if dataset['terminals'][i] == True:
+++++++++--        #     print('traj ended at', i)
+++++++++--        #     break
+++++++++--
+++++++++--        # first_traj.append((dataset['observations'][i],
+++++++++--        #                    dataset['actions'][i],
+++++++++--        #                    dataset['rewards'][i],
+++++++++--        #                    dataset['next_observations'][i]))
+++++++++--    # print(first_traj)
+++++++++--
+++++++++--
+++++++++--    # print(dataset['rewards'].shape) # An N x dim_observation Numpy array of observations
+++++++++--
+++++++++--    # Alternatively, use d4rl.qlearning_dataset which
+++++++++--    # also adds next_observations.
+++++++++--
+++++++++--    # import d3rlpy
+++++++++--
+++++++++--    # # dataset, env = d3rlpy.datasets.get_dataset("halfcheetah-medium")
+++++++++--
+++++++++--    # # prepare algorithm
+++++++++--    # # sac = d3rlpy.algos.SAC().create(device="cpu")
+++++++++--
+++++++++--    # sac = d3rlpy.algos.SACConfig(
+++++++++--    #     actor_learning_rate=3e-4,
+++++++++--    #     critic_learning_rate=3e-4,
+++++++++--    #     temp_learning_rate=3e-4,
+++++++++--    #     batch_size=256,
+++++++++--    # ).create(device='cpu')
+++++++++--
+++++++++--
+++++++++--    # # train offline
+++++++++--    # # sac.fit(dataset, n_steps=1000)
+++++++++--
+++++++++--
+++++++++--    # # ready to control
+++++++++--    # actions = sac.predict(0)
+++++++++--
+++++++++--if __name__ == "__main__":
+++++++++--    main()
++++++++++@@ -1,27 +0,0 @@
++++++++++-name: trajectory
++++++++++-channels:
++++++++++-- defaults
++++++++++-- conda-forge
++++++++++-dependencies:
++++++++++-- python=3.8
++++++++++-- pip
++++++++++-- patchelf
++++++++++-- pip:
++++++++++-    - -f https://download.pytorch.org/whl/torch_stable.html
++++++++++-    - numpy
++++++++++-    - wheel==0.38.4
++++++++++-    - setuptools==65.5.0
++++++++++-    - gym==0.20.0
++++++++++-    - matplotlib==3.3.4
++++++++++-    - torch==1.9.1+cu111
++++++++++-    - typed-argument-parser
++++++++++-    # - git+https://github.com/Farama-Foundation/d4rl@f2a05c0d66722499bf8031b094d9af3aea7c372b#egg=d4rl
++++++++++-    - scikit-image==0.17.2
++++++++++-    - scikit-video==1.1.11
++++++++++-    - gitpython
++++++++++-    - os
++++++++++-    - d3rlpy
++++++++++-    - pyclustering
++++++++++-    - moviepy
++++++++++-    - scipy
++++++++++-    - scikit-learn
++++++++++diff --git a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json
++++++++++index 18993c1..5c86445 100644
++++++++++--- a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json
+++++++++++++ b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json
++++++++++@@ -1,23 +1,23 @@
++++++++++ {
++++++++++     "add_extras": {
++++++++++         "_type": "python_object (type = method)",
++++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgfhpRSlC4="
+++++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMCXBsYW5fZnJlcZRLAYwGY29tbWl0lIwtZjc3YTE5NDk5Njg3Mjg3OWJkMDQxNGZjYjNkYmMxNWIzZGEyOWNiOCBtYWlulIwHbG9nYmFzZZSMBWxvZ3MvlIwXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMCmFkZF9leHRyYXOUaAJoBmgOhpRSlIwHaG9yaXpvbpRLBYwFbWtkaXKUaAJoBmgShpRSlIwMZ3B0X2xvYWRwYXRolIwOZ3B0L3ByZXRyYWluZWSUjAlncHRfZXBvY2iUjAZsYXRlc3SUjA5wcmVmaXhfY29udGV4dJSIjAhleHBfbmFtZZSMHnBsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMpSMB2NkZl9hY3SURz/jMzMzMzMzjAVrX2FjdJROjAtyZWFkX2NvbmZpZ5RoAmgGaB6GlFKUjAZjb25maWeUjA5jb25maWcub2ZmbGluZZSMBnByZWZpeJSMD3BsYW5zL2RlZmF1bHRzL5SMCHNldF9zZWVklGgCaAZoJYaUUpSMEWdlbmVyYXRlX2V4cF9uYW1llGgCaAZoKIaUUpSMCnBlcmNlbnRpbGWUjARtZWFulIwHdmVyYm9zZZSIjAhuX2V4cGFuZJRLAowGZGV2aWNllIwDY3B1lIwHY2RmX29ic5ROjAh2aXNfZnJlcZRLMowKZ2V0X2NvbW1pdJRoAmgGaDOGlFKUjAlzYXZlX2RpZmaUaAJoBmg2hpRSlIwFa19vYnOUSwGMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwGc3VmZml4lIwBMJSMCHNhdmVwYXRolIw7bG9ncy9oYWxmY2hlZXRhaC1tZWRpdW0tdjIvcGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMyLzCUjApiZWFtX3dpZHRolEsgdWJoDoaUUpQu"
++++++++++     },
++++++++++     "beam_width": 32,
++++++++++     "cdf_act": 0.6,
++++++++++     "cdf_obs": null,
++++++++++-    "commit": "cca8d898e10f9f6102a8c33dac7758e2993dfc60 halfcheetah-xrl",
+++++++++++    "commit": "f77a194996872879bd0414fcb3dbc15b3da29cb8 main",
++++++++++     "config": "config.offline",
++++++++++     "dataset": "halfcheetah-medium-v2",
++++++++++     "device": "cpu",
++++++++++     "exp_name": "plans/defaults/freq1_H5_beam32",
++++++++++     "generate_exp_name": {
++++++++++         "_type": "python_object (type = method)",
++++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgxhpRSlC4="
+++++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMCXBsYW5fZnJlcZRLAYwGY29tbWl0lIwtZjc3YTE5NDk5Njg3Mjg3OWJkMDQxNGZjYjNkYmMxNWIzZGEyOWNiOCBtYWlulIwHbG9nYmFzZZSMBWxvZ3MvlIwXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMCmFkZF9leHRyYXOUaAJoBmgOhpRSlIwHaG9yaXpvbpRLBYwFbWtkaXKUaAJoBmgShpRSlIwMZ3B0X2xvYWRwYXRolIwOZ3B0L3ByZXRyYWluZWSUjAlncHRfZXBvY2iUjAZsYXRlc3SUjA5wcmVmaXhfY29udGV4dJSIjAhleHBfbmFtZZSMHnBsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMpSMB2NkZl9hY3SURz/jMzMzMzMzjAVrX2FjdJROjAtyZWFkX2NvbmZpZ5RoAmgGaB6GlFKUjAZjb25maWeUjA5jb25maWcub2ZmbGluZZSMBnByZWZpeJSMD3BsYW5zL2RlZmF1bHRzL5SMCHNldF9zZWVklGgCaAZoJYaUUpSMEWdlbmVyYXRlX2V4cF9uYW1llGgCaAZoKIaUUpSMCnBlcmNlbnRpbGWUjARtZWFulIwHdmVyYm9zZZSIjAhuX2V4cGFuZJRLAowGZGV2aWNllIwDY3B1lIwHY2RmX29ic5ROjAh2aXNfZnJlcZRLMowKZ2V0X2NvbW1pdJRoAmgGaDOGlFKUjAlzYXZlX2RpZmaUaAJoBmg2hpRSlIwFa19vYnOUSwGMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwGc3VmZml4lIwBMJSMCHNhdmVwYXRolIw7bG9ncy9oYWxmY2hlZXRhaC1tZWRpdW0tdjIvcGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMyLzCUjApiZWFtX3dpZHRolEsgdWJoKIaUUpQu"
++++++++++     },
++++++++++     "get_commit": {
++++++++++         "_type": "python_object (type = method)",
++++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmglhpRSlC4="
+++++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMCXBsYW5fZnJlcZRLAYwGY29tbWl0lIwtZjc3YTE5NDk5Njg3Mjg3OWJkMDQxNGZjYjNkYmMxNWIzZGEyOWNiOCBtYWlulIwHbG9nYmFzZZSMBWxvZ3MvlIwXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMCmFkZF9leHRyYXOUaAJoBmgOhpRSlIwHaG9yaXpvbpRLBYwFbWtkaXKUaAJoBmgShpRSlIwMZ3B0X2xvYWRwYXRolIwOZ3B0L3ByZXRyYWluZWSUjAlncHRfZXBvY2iUjAZsYXRlc3SUjA5wcmVmaXhfY29udGV4dJSIjAhleHBfbmFtZZSMHnBsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMpSMB2NkZl9hY3SURz/jMzMzMzMzjAVrX2FjdJROjAtyZWFkX2NvbmZpZ5RoAmgGaB6GlFKUjAZjb25maWeUjA5jb25maWcub2ZmbGluZZSMBnByZWZpeJSMD3BsYW5zL2RlZmF1bHRzL5SMCHNldF9zZWVklGgCaAZoJYaUUpSMEWdlbmVyYXRlX2V4cF9uYW1llGgCaAZoKIaUUpSMCnBlcmNlbnRpbGWUjARtZWFulIwHdmVyYm9zZZSIjAhuX2V4cGFuZJRLAowGZGV2aWNllIwDY3B1lIwHY2RmX29ic5ROjAh2aXNfZnJlcZRLMowKZ2V0X2NvbW1pdJRoAmgGaDOGlFKUjAlzYXZlX2RpZmaUaAJoBmg2hpRSlIwFa19vYnOUSwGMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwGc3VmZml4lIwBMJSMCHNhdmVwYXRolIw7bG9ncy9oYWxmY2hlZXRhaC1tZWRpdW0tdjIvcGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMyLzCUjApiZWFtX3dpZHRolEsgdWJoM4aUUpQu"
++++++++++     },
++++++++++     "gpt_epoch": "latest",
++++++++++     "gpt_loadpath": "gpt/pretrained",
++++++++++@@ -28,7 +28,7 @@
++++++++++     "max_context_transitions": 5,
++++++++++     "mkdir": {
++++++++++         "_type": "python_object (type = method)",
++++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgihpRSlC4="
+++++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMCXBsYW5fZnJlcZRLAYwGY29tbWl0lIwtZjc3YTE5NDk5Njg3Mjg3OWJkMDQxNGZjYjNkYmMxNWIzZGEyOWNiOCBtYWlulIwHbG9nYmFzZZSMBWxvZ3MvlIwXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMCmFkZF9leHRyYXOUaAJoBmgOhpRSlIwHaG9yaXpvbpRLBYwFbWtkaXKUaAJoBmgShpRSlIwMZ3B0X2xvYWRwYXRolIwOZ3B0L3ByZXRyYWluZWSUjAlncHRfZXBvY2iUjAZsYXRlc3SUjA5wcmVmaXhfY29udGV4dJSIjAhleHBfbmFtZZSMHnBsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMpSMB2NkZl9hY3SURz/jMzMzMzMzjAVrX2FjdJROjAtyZWFkX2NvbmZpZ5RoAmgGaB6GlFKUjAZjb25maWeUjA5jb25maWcub2ZmbGluZZSMBnByZWZpeJSMD3BsYW5zL2RlZmF1bHRzL5SMCHNldF9zZWVklGgCaAZoJYaUUpSMEWdlbmVyYXRlX2V4cF9uYW1llGgCaAZoKIaUUpSMCnBlcmNlbnRpbGWUjARtZWFulIwHdmVyYm9zZZSIjAhuX2V4cGFuZJRLAowGZGV2aWNllIwDY3B1lIwHY2RmX29ic5ROjAh2aXNfZnJlcZRLMowKZ2V0X2NvbW1pdJRoAmgGaDOGlFKUjAlzYXZlX2RpZmaUaAJoBmg2hpRSlIwFa19vYnOUSwGMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwGc3VmZml4lIwBMJSMCHNhdmVwYXRolIw7bG9ncy9oYWxmY2hlZXRhaC1tZWRpdW0tdjIvcGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMyLzCUjApiZWFtX3dpZHRolEsgdWJoEoaUUpQu"
++++++++++     },
++++++++++     "n_expand": 2,
++++++++++     "percentile": "mean",
++++++++++@@ -37,24 +37,24 @@
++++++++++     "prefix_context": true,
++++++++++     "read_config": {
++++++++++         "_type": "python_object (type = method)",
++++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgZhpRSlC4="
+++++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMCXBsYW5fZnJlcZRLAYwGY29tbWl0lIwtZjc3YTE5NDk5Njg3Mjg3OWJkMDQxNGZjYjNkYmMxNWIzZGEyOWNiOCBtYWlulIwHbG9nYmFzZZSMBWxvZ3MvlIwXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMCmFkZF9leHRyYXOUaAJoBmgOhpRSlIwHaG9yaXpvbpRLBYwFbWtkaXKUaAJoBmgShpRSlIwMZ3B0X2xvYWRwYXRolIwOZ3B0L3ByZXRyYWluZWSUjAlncHRfZXBvY2iUjAZsYXRlc3SUjA5wcmVmaXhfY29udGV4dJSIjAhleHBfbmFtZZSMHnBsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMpSMB2NkZl9hY3SURz/jMzMzMzMzjAVrX2FjdJROjAtyZWFkX2NvbmZpZ5RoAmgGaB6GlFKUjAZjb25maWeUjA5jb25maWcub2ZmbGluZZSMBnByZWZpeJSMD3BsYW5zL2RlZmF1bHRzL5SMCHNldF9zZWVklGgCaAZoJYaUUpSMEWdlbmVyYXRlX2V4cF9uYW1llGgCaAZoKIaUUpSMCnBlcmNlbnRpbGWUjARtZWFulIwHdmVyYm9zZZSIjAhuX2V4cGFuZJRLAowGZGV2aWNllIwDY3B1lIwHY2RmX29ic5ROjAh2aXNfZnJlcZRLMowKZ2V0X2NvbW1pdJRoAmgGaDOGlFKUjAlzYXZlX2RpZmaUaAJoBmg2hpRSlIwFa19vYnOUSwGMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwGc3VmZml4lIwBMJSMCHNhdmVwYXRolIw7bG9ncy9oYWxmY2hlZXRhaC1tZWRpdW0tdjIvcGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMyLzCUjApiZWFtX3dpZHRolEsgdWJoHoaUUpQu"
++++++++++     },
++++++++++     "renderer": "Renderer",
++++++++++     "reproducibility": {
++++++++++-        "command_line": "python scripts/xrl_v2.py --dataset halfcheetah-medium-v2 --gpt_loadpath gpt/pretrained",
+++++++++++        "command_line": "python xrl_v2.py --dataset halfcheetah-medium-v2 --gpt_loadpath gpt/pretrained",
++++++++++         "git_has_uncommitted_changes": true,
++++++++++         "git_root": "/home/colin/Desktop/FACT/FACT_assignment",
++++++++++-        "git_url": "https://github.com/fclio/FACT_assignment/tree/cca8d898e10f9f6102a8c33dac7758e2993dfc60",
++++++++++-        "time": "Fri Feb  2 11:39:41 2024"
+++++++++++        "git_url": "https://github.com/fclio/FACT_assignment/tree/f77a194996872879bd0414fcb3dbc15b3da29cb8",
+++++++++++        "time": "Sat Feb  3 21:11:44 2024"
++++++++++     },
++++++++++     "save_diff": {
++++++++++         "_type": "python_object (type = method)",
++++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1Ymg0hpRSlC4="
+++++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMCXBsYW5fZnJlcZRLAYwGY29tbWl0lIwtZjc3YTE5NDk5Njg3Mjg3OWJkMDQxNGZjYjNkYmMxNWIzZGEyOWNiOCBtYWlulIwHbG9nYmFzZZSMBWxvZ3MvlIwXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMCmFkZF9leHRyYXOUaAJoBmgOhpRSlIwHaG9yaXpvbpRLBYwFbWtkaXKUaAJoBmgShpRSlIwMZ3B0X2xvYWRwYXRolIwOZ3B0L3ByZXRyYWluZWSUjAlncHRfZXBvY2iUjAZsYXRlc3SUjA5wcmVmaXhfY29udGV4dJSIjAhleHBfbmFtZZSMHnBsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMpSMB2NkZl9hY3SURz/jMzMzMzMzjAVrX2FjdJROjAtyZWFkX2NvbmZpZ5RoAmgGaB6GlFKUjAZjb25maWeUjA5jb25maWcub2ZmbGluZZSMBnByZWZpeJSMD3BsYW5zL2RlZmF1bHRzL5SMCHNldF9zZWVklGgCaAZoJYaUUpSMEWdlbmVyYXRlX2V4cF9uYW1llGgCaAZoKIaUUpSMCnBlcmNlbnRpbGWUjARtZWFulIwHdmVyYm9zZZSIjAhuX2V4cGFuZJRLAowGZGV2aWNllIwDY3B1lIwHY2RmX29ic5ROjAh2aXNfZnJlcZRLMowKZ2V0X2NvbW1pdJRoAmgGaDOGlFKUjAlzYXZlX2RpZmaUaAJoBmg2hpRSlIwFa19vYnOUSwGMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwGc3VmZml4lIwBMJSMCHNhdmVwYXRolIw7bG9ncy9oYWxmY2hlZXRhaC1tZWRpdW0tdjIvcGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMyLzCUjApiZWFtX3dpZHRolEsgdWJoNoaUUpQu"
++++++++++     },
++++++++++     "savepath": "logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0",
++++++++++     "set_seed": {
++++++++++         "_type": "python_object (type = method)",
++++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgqhpRSlC4="
+++++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMCXBsYW5fZnJlcZRLAYwGY29tbWl0lIwtZjc3YTE5NDk5Njg3Mjg3OWJkMDQxNGZjYjNkYmMxNWIzZGEyOWNiOCBtYWlulIwHbG9nYmFzZZSMBWxvZ3MvlIwXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMCmFkZF9leHRyYXOUaAJoBmgOhpRSlIwHaG9yaXpvbpRLBYwFbWtkaXKUaAJoBmgShpRSlIwMZ3B0X2xvYWRwYXRolIwOZ3B0L3ByZXRyYWluZWSUjAlncHRfZXBvY2iUjAZsYXRlc3SUjA5wcmVmaXhfY29udGV4dJSIjAhleHBfbmFtZZSMHnBsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMpSMB2NkZl9hY3SURz/jMzMzMzMzjAVrX2FjdJROjAtyZWFkX2NvbmZpZ5RoAmgGaB6GlFKUjAZjb25maWeUjA5jb25maWcub2ZmbGluZZSMBnByZWZpeJSMD3BsYW5zL2RlZmF1bHRzL5SMCHNldF9zZWVklGgCaAZoJYaUUpSMEWdlbmVyYXRlX2V4cF9uYW1llGgCaAZoKIaUUpSMCnBlcmNlbnRpbGWUjARtZWFulIwHdmVyYm9zZZSIjAhuX2V4cGFuZJRLAowGZGV2aWNllIwDY3B1lIwHY2RmX29ic5ROjAh2aXNfZnJlcZRLMowKZ2V0X2NvbW1pdJRoAmgGaDOGlFKUjAlzYXZlX2RpZmaUaAJoBmg2hpRSlIwFa19vYnOUSwGMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwGc3VmZml4lIwBMJSMCHNhdmVwYXRolIw7bG9ncy9oYWxmY2hlZXRhaC1tZWRpdW0tdjIvcGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMyLzCUjApiZWFtX3dpZHRolEsgdWJoJYaUUpQu"
++++++++++     },
++++++++++     "suffix": "0",
++++++++++     "verbose": true,
++++++++++diff --git a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt
++++++++++index 2c00d59..e0e0226 100644
++++++++++--- a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt
+++++++++++++ b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt
++++++++++@@ -1,1250 +1,6203 @@
++++++++++-diff --git a/.gitignore b/.gitignore
++++++++++-index 30d226d..a18312a 100644
++++++++++---- a/.gitignore
++++++++++-+++ b/.gitignore
++++++++++-@@ -1,6 +1,5 @@
++++++++++- __pycache__/
++++++++++- *.mp4
++++++++++--halfcheetah/logs/
++++++++++- .ipynb_checkpoints/
++++++++++- __pycache__/
++++++++++- Open Notebook.onetoc2 
++++++++++-diff --git a/gridworld/gridworld_expts.ipynb b/gridworld/gridworld_expts.ipynb
++++++++++-index e2a261b..bd0b720 100644
++++++++++---- a/gridworld/gridworld_expts.ipynb
++++++++++-+++ b/gridworld/gridworld_expts.ipynb
++++++++++-@@ -1570,7 +1570,7 @@
++++++++++-    "name": "python",
++++++++++-    "nbconvert_exporter": "python",
++++++++++-    "pygments_lexer": "ipython3",
++++++++++--   "version": "3.8.10"
++++++++++-+   "version": "3.10.12"
++++++++++-   }
++++++++++-  },
++++++++++-  "nbformat": 4,
++++++++++-diff --git a/halfcheetah/clusters.npy b/halfcheetah/clusters.npy
+++++++++++diff --git a/halfcheetah/environment.yml b/halfcheetah/environment.yml
++++++++++ deleted file mode 100644
++++++++++-index 3c198aa..0000000
++++++++++-Binary files a/halfcheetah/clusters.npy and /dev/null differ
++++++++++-diff --git a/halfcheetah/embeddings.npy b/halfcheetah/embeddings.npy
++++++++++-deleted file mode 100644
++++++++++-index 912d68c..0000000
++++++++++-Binary files a/halfcheetah/embeddings.npy and /dev/null differ
++++++++++-diff --git a/halfcheetah/halfcheetah.py b/halfcheetah/halfcheetah.py
++++++++++-deleted file mode 100644
++++++++++-index 626c42d..0000000
++++++++++---- a/halfcheetah/halfcheetah.py
+++++++++++index e81007c..0000000
+++++++++++--- a/halfcheetah/environment.yml
++++++++++ +++ /dev/null
++++++++++-@@ -1,112 +0,0 @@
++++++++++--import gym
++++++++++--import d4rl # Import required to register environments, you may need to also import the submodule
++++++++++--import numpy as np
++++++++++--import d3rlpy
++++++++++--
++++++++++--def main():
++++++++++--    dataset_d3, env = d3rlpy.datasets.get_dataset("halfcheetah-medium-v2")
++++++++++--
++++++++++--    print(dataset_d3.observations.shape)
++++++++++--    print(dataset_d3.actions.shape)
++++++++++--    print(dataset_d3.rewards.shape)
++++++++++--        # print(dataset_d3.next_observations.shape)
++++++++++--    print(dataset_d3.terminals.shape)
++++++++++--    print(dataset_d3.terminals.sum()) # no
++++++++++--
++++++++++--    env = gym.make('halfcheetah-medium-v2')
++++++++++--    dataset_d4 = d4rl.qlearning_dataset(env)
++++++++++--
++++++++++--    print(dataset_d4['observations'].shape)
++++++++++--    print(dataset_d4['rewards'].shape)
++++++++++--    print(dataset_d4['terminals'].shape)
++++++++++--    print(dataset_d4['actions'].shape)
++++++++++--
++++++++++--    print(dataset_d4['rewards'][1])
++++++++++--    print(dataset_d3.rewards[1])
++++++++++--
++++++++++--
++++++++++--    print(np.allclose(dataset_d3.actions[100], dataset_d4['actions'][100]))
++++++++++--
++++++++++--    for j in range(1000):
++++++++++--        for i in range(999):
++++++++++--            if dataset_d4['rewards'][j * 999 + i] != dataset_d3.rewards[j * 1000 + i]: print("yo", i)
++++++++++--        # if not np.allclose(dataset_d3.observations[i], dataset_d4['observations'][i]): print('obs ongelijk')
++++++++++--        # if not np.allclose(dataset_d3.rewards[i], dataset_d4['rewards'][i]): print('obs ongelijk')
++++++++++--        # if not np.allclose(dataset_d3.actions[i], dataset_d4['actions'][i]): print('obs ongelijk')
++++++++++--
++++++++++--    sac = d3rlpy.algos.SAC(
++++++++++--        actor_learning_rate=3e-4,
++++++++++--        critic_learning_rate=3e-4,
++++++++++--        temp_learning_rate=3e-4,
++++++++++--        batch_size=256)
++++++++++--
++++++++++--    print(sac)
++++++++++--    sac.fit(dataset_d3, n_steps=10000)
++++++++++--
++++++++++--    actions = sac.predict(dataset_d3.observations[0])
++++++++++--
++++++++++--    print(actions)
++++++++++--
++++++++++--
++++++++++--    return
++++++++++--    print('yo!')
++++++++++--
++++++++++--    # Create the environment
++++++++++--    env = gym.make('halfcheetah-medium-v2')
++++++++++--
++++++++++--    # d4rl abides by the OpenAI gym interface
++++++++++--    env.reset()
++++++++++--    env.step(env.action_space.sample())
++++++++++--
++++++++++--    # Each task is associated with a dataset
++++++++++--    # dataset contains observations, actions, rewards, terminals, and infos
++++++++++--    # dataset = env.get_dataset()
++++++++++--    dataset = d4rl.qlearning_dataset(env)
++++++++++--
++++++++++--    print(dataset.keys()) # An N x dim_observation Numpy array of observations
++++++++++--    print(dataset['rewards'].shape) # An N x dim_observation Numpy array of observations
++++++++++--
++++++++++--
++++++++++--    first_traj = []
++++++++++--    for i in range(50000):
++++++++++--        if not np.allclose(dataset['next_observations'][i],dataset['observations'][i+1]): print("yo", i, dataset['terminals'][i])
++++++++++--        # if dataset['terminals'][i] == True:
++++++++++--        #     print('traj ended at', i)
++++++++++--        #     break
++++++++++--
++++++++++--        # first_traj.append((dataset['observations'][i],
++++++++++--        #                    dataset['actions'][i],
++++++++++--        #                    dataset['rewards'][i],
++++++++++--        #                    dataset['next_observations'][i]))
++++++++++--    # print(first_traj)
++++++++++--
++++++++++--
++++++++++--    # print(dataset['rewards'].shape) # An N x dim_observation Numpy array of observations
++++++++++--
++++++++++--    # Alternatively, use d4rl.qlearning_dataset which
++++++++++--    # also adds next_observations.
++++++++++--
++++++++++--    # import d3rlpy
++++++++++--
++++++++++--    # # dataset, env = d3rlpy.datasets.get_dataset("halfcheetah-medium")
++++++++++--
++++++++++--    # # prepare algorithm
++++++++++--    # # sac = d3rlpy.algos.SAC().create(device="cpu")
++++++++++--
++++++++++--    # sac = d3rlpy.algos.SACConfig(
++++++++++--    #     actor_learning_rate=3e-4,
++++++++++--    #     critic_learning_rate=3e-4,
++++++++++--    #     temp_learning_rate=3e-4,
++++++++++--    #     batch_size=256,
++++++++++--    # ).create(device='cpu')
++++++++++--
++++++++++--
++++++++++--    # # train offline
++++++++++--    # # sac.fit(dataset, n_steps=1000)
++++++++++--
++++++++++--
++++++++++--    # # ready to control
++++++++++--    # actions = sac.predict(0)
++++++++++--
++++++++++--if __name__ == "__main__":
++++++++++--    main()
+++++++++++@@ -1,27 +0,0 @@
+++++++++++-name: trajectory
+++++++++++-channels:
+++++++++++-- defaults
+++++++++++-- conda-forge
+++++++++++-dependencies:
+++++++++++-- python=3.8
+++++++++++-- pip
+++++++++++-- patchelf
+++++++++++-- pip:
+++++++++++-    - -f https://download.pytorch.org/whl/torch_stable.html
+++++++++++-    - numpy
+++++++++++-    - wheel==0.38.4
+++++++++++-    - setuptools==65.5.0
+++++++++++-    - gym==0.20.0
+++++++++++-    - matplotlib==3.3.4
+++++++++++-    - torch==1.9.1+cu111
+++++++++++-    - typed-argument-parser
+++++++++++-    # - git+https://github.com/Farama-Foundation/d4rl@f2a05c0d66722499bf8031b094d9af3aea7c372b#egg=d4rl
+++++++++++-    - scikit-image==0.17.2
+++++++++++-    - scikit-video==1.1.11
+++++++++++-    - gitpython
+++++++++++-    - os
+++++++++++-    - d3rlpy
+++++++++++-    - pyclustering
+++++++++++-    - moviepy
+++++++++++-    - scipy
+++++++++++-    - scikit-learn
+++++++++++diff --git a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json
+++++++++++index 18993c1..91630c9 100644
+++++++++++--- a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json
++++++++++++++ b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json
+++++++++++@@ -1,23 +1,23 @@
+++++++++++ {
+++++++++++     "add_extras": {
+++++++++++         "_type": "python_object (type = method)",
+++++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgfhpRSlC4="
++++++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMC3JlYWRfY29uZmlnlGgCaAZoCIaUUpSMCmFkZF9leHRyYXOUaAJoBmgLhpRSlIwFa19vYnOUSwGMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMB3ZlcmJvc2WUiIwHaG9yaXpvbpRLBYwFbWtkaXKUaAJoBmgUhpRSlIwHbG9nYmFzZZSMBWxvZ3MvlIwGZGV2aWNllIwDY3B1lIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwOcHJlZml4X2NvbnRleHSUiIwHY2RmX29ic5ROjAdjZGZfYWN0lEc/4zMzMzMzM4wIdmlzX2ZyZXGUSzKMCWdwdF9lcG9jaJSMBmxhdGVzdJSMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMCGV4cF9uYW1llIwecGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMylIwKYmVhbV93aWR0aJRLIIwIc2V0X3NlZWSUaAJoBmgrhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDCGlFKUjAhzYXZlcGF0aJSMO2xvZ3MvaGFsZmNoZWV0YWgtbWVkaXVtLXYyL3BsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMi8wlIwHZGF0YXNldJSMFWhhbGZjaGVldGFoLW1lZGl1bS12MpSMBWtfYWN0lE6MCXBsYW5fZnJlcZRLAYwGc3VmZml4lIwBMJSMCXNhdmVfZGlmZpRoAmgGaDuGlFKUjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwKZ2V0X2NvbW1pdJRoAmgGaECGlFKUdWJoC4aUUpQu"
+++++++++++     },
+++++++++++     "beam_width": 32,
+++++++++++     "cdf_act": 0.6,
+++++++++++     "cdf_obs": null,
+++++++++++-    "commit": "cca8d898e10f9f6102a8c33dac7758e2993dfc60 halfcheetah-xrl",
++++++++++++    "commit": "f77a194996872879bd0414fcb3dbc15b3da29cb8 main",
+++++++++++     "config": "config.offline",
+++++++++++     "dataset": "halfcheetah-medium-v2",
+++++++++++     "device": "cpu",
+++++++++++     "exp_name": "plans/defaults/freq1_H5_beam32",
+++++++++++     "generate_exp_name": {
+++++++++++         "_type": "python_object (type = method)",
+++++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgxhpRSlC4="
++++++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMC3JlYWRfY29uZmlnlGgCaAZoCIaUUpSMCmFkZF9leHRyYXOUaAJoBmgLhpRSlIwFa19vYnOUSwGMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMB3ZlcmJvc2WUiIwHaG9yaXpvbpRLBYwFbWtkaXKUaAJoBmgUhpRSlIwHbG9nYmFzZZSMBWxvZ3MvlIwGZGV2aWNllIwDY3B1lIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwOcHJlZml4X2NvbnRleHSUiIwHY2RmX29ic5ROjAdjZGZfYWN0lEc/4zMzMzMzM4wIdmlzX2ZyZXGUSzKMCWdwdF9lcG9jaJSMBmxhdGVzdJSMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMCGV4cF9uYW1llIwecGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMylIwKYmVhbV93aWR0aJRLIIwIc2V0X3NlZWSUaAJoBmgrhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDCGlFKUjAhzYXZlcGF0aJSMO2xvZ3MvaGFsZmNoZWV0YWgtbWVkaXVtLXYyL3BsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMi8wlIwHZGF0YXNldJSMFWhhbGZjaGVldGFoLW1lZGl1bS12MpSMBWtfYWN0lE6MCXBsYW5fZnJlcZRLAYwGc3VmZml4lIwBMJSMCXNhdmVfZGlmZpRoAmgGaDuGlFKUjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwKZ2V0X2NvbW1pdJRoAmgGaECGlFKUdWJoMIaUUpQu"
+++++++++++     },
+++++++++++     "get_commit": {
+++++++++++         "_type": "python_object (type = method)",
+++++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmglhpRSlC4="
++++++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMC3JlYWRfY29uZmlnlGgCaAZoCIaUUpSMCmFkZF9leHRyYXOUaAJoBmgLhpRSlIwFa19vYnOUSwGMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMB3ZlcmJvc2WUiIwHaG9yaXpvbpRLBYwFbWtkaXKUaAJoBmgUhpRSlIwHbG9nYmFzZZSMBWxvZ3MvlIwGZGV2aWNllIwDY3B1lIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwOcHJlZml4X2NvbnRleHSUiIwHY2RmX29ic5ROjAdjZGZfYWN0lEc/4zMzMzMzM4wIdmlzX2ZyZXGUSzKMCWdwdF9lcG9jaJSMBmxhdGVzdJSMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMCGV4cF9uYW1llIwecGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMylIwKYmVhbV93aWR0aJRLIIwIc2V0X3NlZWSUaAJoBmgrhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDCGlFKUjAhzYXZlcGF0aJSMO2xvZ3MvaGFsZmNoZWV0YWgtbWVkaXVtLXYyL3BsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMi8wlIwHZGF0YXNldJSMFWhhbGZjaGVldGFoLW1lZGl1bS12MpSMBWtfYWN0lE6MCXBsYW5fZnJlcZRLAYwGc3VmZml4lIwBMJSMCXNhdmVfZGlmZpRoAmgGaDuGlFKUjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwKZ2V0X2NvbW1pdJRoAmgGaECGlFKUdWJoQIaUUpQu"
+++++++++++     },
+++++++++++     "gpt_epoch": "latest",
+++++++++++     "gpt_loadpath": "gpt/pretrained",
+++++++++++@@ -28,7 +28,7 @@
+++++++++++     "max_context_transitions": 5,
+++++++++++     "mkdir": {
+++++++++++         "_type": "python_object (type = method)",
+++++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgihpRSlC4="
++++++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMC3JlYWRfY29uZmlnlGgCaAZoCIaUUpSMCmFkZF9leHRyYXOUaAJoBmgLhpRSlIwFa19vYnOUSwGMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMB3ZlcmJvc2WUiIwHaG9yaXpvbpRLBYwFbWtkaXKUaAJoBmgUhpRSlIwHbG9nYmFzZZSMBWxvZ3MvlIwGZGV2aWNllIwDY3B1lIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwOcHJlZml4X2NvbnRleHSUiIwHY2RmX29ic5ROjAdjZGZfYWN0lEc/4zMzMzMzM4wIdmlzX2ZyZXGUSzKMCWdwdF9lcG9jaJSMBmxhdGVzdJSMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMCGV4cF9uYW1llIwecGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMylIwKYmVhbV93aWR0aJRLIIwIc2V0X3NlZWSUaAJoBmgrhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDCGlFKUjAhzYXZlcGF0aJSMO2xvZ3MvaGFsZmNoZWV0YWgtbWVkaXVtLXYyL3BsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMi8wlIwHZGF0YXNldJSMFWhhbGZjaGVldGFoLW1lZGl1bS12MpSMBWtfYWN0lE6MCXBsYW5fZnJlcZRLAYwGc3VmZml4lIwBMJSMCXNhdmVfZGlmZpRoAmgGaDuGlFKUjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwKZ2V0X2NvbW1pdJRoAmgGaECGlFKUdWJoFIaUUpQu"
+++++++++++     },
+++++++++++     "n_expand": 2,
+++++++++++     "percentile": "mean",
+++++++++++@@ -37,24 +37,24 @@
+++++++++++     "prefix_context": true,
+++++++++++     "read_config": {
+++++++++++         "_type": "python_object (type = method)",
+++++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgZhpRSlC4="
++++++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMC3JlYWRfY29uZmlnlGgCaAZoCIaUUpSMCmFkZF9leHRyYXOUaAJoBmgLhpRSlIwFa19vYnOUSwGMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMB3ZlcmJvc2WUiIwHaG9yaXpvbpRLBYwFbWtkaXKUaAJoBmgUhpRSlIwHbG9nYmFzZZSMBWxvZ3MvlIwGZGV2aWNllIwDY3B1lIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwOcHJlZml4X2NvbnRleHSUiIwHY2RmX29ic5ROjAdjZGZfYWN0lEc/4zMzMzMzM4wIdmlzX2ZyZXGUSzKMCWdwdF9lcG9jaJSMBmxhdGVzdJSMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMCGV4cF9uYW1llIwecGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMylIwKYmVhbV93aWR0aJRLIIwIc2V0X3NlZWSUaAJoBmgrhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDCGlFKUjAhzYXZlcGF0aJSMO2xvZ3MvaGFsZmNoZWV0YWgtbWVkaXVtLXYyL3BsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMi8wlIwHZGF0YXNldJSMFWhhbGZjaGVldGFoLW1lZGl1bS12MpSMBWtfYWN0lE6MCXBsYW5fZnJlcZRLAYwGc3VmZml4lIwBMJSMCXNhdmVfZGlmZpRoAmgGaDuGlFKUjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwKZ2V0X2NvbW1pdJRoAmgGaECGlFKUdWJoCIaUUpQu"
+++++++++++     },
+++++++++++     "renderer": "Renderer",
+++++++++++     "reproducibility": {
+++++++++++-        "command_line": "python scripts/xrl_v2.py --dataset halfcheetah-medium-v2 --gpt_loadpath gpt/pretrained",
++++++++++++        "command_line": "python xrl_v2.py --dataset halfcheetah-medium-v2 --gpt_loadpath gpt/pretrained",
+++++++++++         "git_has_uncommitted_changes": true,
+++++++++++         "git_root": "/home/colin/Desktop/FACT/FACT_assignment",
+++++++++++-        "git_url": "https://github.com/fclio/FACT_assignment/tree/cca8d898e10f9f6102a8c33dac7758e2993dfc60",
+++++++++++-        "time": "Fri Feb  2 11:39:41 2024"
++++++++++++        "git_url": "https://github.com/fclio/FACT_assignment/tree/f77a194996872879bd0414fcb3dbc15b3da29cb8",
++++++++++++        "time": "Sat Feb  3 21:10:19 2024"
+++++++++++     },
+++++++++++     "save_diff": {
+++++++++++         "_type": "python_object (type = method)",
+++++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1Ymg0hpRSlC4="
++++++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMC3JlYWRfY29uZmlnlGgCaAZoCIaUUpSMCmFkZF9leHRyYXOUaAJoBmgLhpRSlIwFa19vYnOUSwGMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMB3ZlcmJvc2WUiIwHaG9yaXpvbpRLBYwFbWtkaXKUaAJoBmgUhpRSlIwHbG9nYmFzZZSMBWxvZ3MvlIwGZGV2aWNllIwDY3B1lIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwOcHJlZml4X2NvbnRleHSUiIwHY2RmX29ic5ROjAdjZGZfYWN0lEc/4zMzMzMzM4wIdmlzX2ZyZXGUSzKMCWdwdF9lcG9jaJSMBmxhdGVzdJSMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMCGV4cF9uYW1llIwecGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMylIwKYmVhbV93aWR0aJRLIIwIc2V0X3NlZWSUaAJoBmgrhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDCGlFKUjAhzYXZlcGF0aJSMO2xvZ3MvaGFsZmNoZWV0YWgtbWVkaXVtLXYyL3BsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMi8wlIwHZGF0YXNldJSMFWhhbGZjaGVldGFoLW1lZGl1bS12MpSMBWtfYWN0lE6MCXBsYW5fZnJlcZRLAYwGc3VmZml4lIwBMJSMCXNhdmVfZGlmZpRoAmgGaDuGlFKUjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwKZ2V0X2NvbW1pdJRoAmgGaECGlFKUdWJoO4aUUpQu"
+++++++++++     },
+++++++++++     "savepath": "logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0",
+++++++++++     "set_seed": {
+++++++++++         "_type": "python_object (type = method)",
+++++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgqhpRSlC4="
++++++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMC3JlYWRfY29uZmlnlGgCaAZoCIaUUpSMCmFkZF9leHRyYXOUaAJoBmgLhpRSlIwFa19vYnOUSwGMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMB3ZlcmJvc2WUiIwHaG9yaXpvbpRLBYwFbWtkaXKUaAJoBmgUhpRSlIwHbG9nYmFzZZSMBWxvZ3MvlIwGZGV2aWNllIwDY3B1lIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwOcHJlZml4X2NvbnRleHSUiIwHY2RmX29ic5ROjAdjZGZfYWN0lEc/4zMzMzMzM4wIdmlzX2ZyZXGUSzKMCWdwdF9lcG9jaJSMBmxhdGVzdJSMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMCGV4cF9uYW1llIwecGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMylIwKYmVhbV93aWR0aJRLIIwIc2V0X3NlZWSUaAJoBmgrhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDCGlFKUjAhzYXZlcGF0aJSMO2xvZ3MvaGFsZmNoZWV0YWgtbWVkaXVtLXYyL3BsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMi8wlIwHZGF0YXNldJSMFWhhbGZjaGVldGFoLW1lZGl1bS12MpSMBWtfYWN0lE6MCXBsYW5fZnJlcZRLAYwGc3VmZml4lIwBMJSMCXNhdmVfZGlmZpRoAmgGaDuGlFKUjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwKZ2V0X2NvbW1pdJRoAmgGaECGlFKUdWJoK4aUUpQu"
+++++++++++     },
+++++++++++     "suffix": "0",
+++++++++++     "verbose": true,
+++++++++++diff --git a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt
+++++++++++index 2c00d59..77ed61d 100644
+++++++++++--- a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt
++++++++++++++ b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt
+++++++++++@@ -1,1250 +1,4685 @@
+++++++++++-diff --git a/.gitignore b/.gitignore
+++++++++++-index 30d226d..a18312a 100644
+++++++++++---- a/.gitignore
+++++++++++-+++ b/.gitignore
+++++++++++-@@ -1,6 +1,5 @@
+++++++++++- __pycache__/
+++++++++++- *.mp4
+++++++++++--halfcheetah/logs/
+++++++++++- .ipynb_checkpoints/
+++++++++++- __pycache__/
+++++++++++- Open Notebook.onetoc2 
+++++++++++-diff --git a/gridworld/gridworld_expts.ipynb b/gridworld/gridworld_expts.ipynb
+++++++++++-index e2a261b..bd0b720 100644
+++++++++++---- a/gridworld/gridworld_expts.ipynb
+++++++++++-+++ b/gridworld/gridworld_expts.ipynb
+++++++++++-@@ -1570,7 +1570,7 @@
+++++++++++-    "name": "python",
+++++++++++-    "nbconvert_exporter": "python",
+++++++++++-    "pygments_lexer": "ipython3",
+++++++++++--   "version": "3.8.10"
+++++++++++-+   "version": "3.10.12"
+++++++++++-   }
+++++++++++-  },
+++++++++++-  "nbformat": 4,
+++++++++++-diff --git a/halfcheetah/clusters.npy b/halfcheetah/clusters.npy
++++++++++++diff --git a/halfcheetah/environment.yml b/halfcheetah/environment.yml
+++++++++++ deleted file mode 100644
+++++++++++-index 3c198aa..0000000
+++++++++++-Binary files a/halfcheetah/clusters.npy and /dev/null differ
+++++++++++-diff --git a/halfcheetah/embeddings.npy b/halfcheetah/embeddings.npy
+++++++++++-deleted file mode 100644
+++++++++++-index 912d68c..0000000
+++++++++++-Binary files a/halfcheetah/embeddings.npy and /dev/null differ
+++++++++++-diff --git a/halfcheetah/halfcheetah.py b/halfcheetah/halfcheetah.py
+++++++++++-deleted file mode 100644
+++++++++++-index 626c42d..0000000
+++++++++++---- a/halfcheetah/halfcheetah.py
++++++++++++index e81007c..0000000
++++++++++++--- a/halfcheetah/environment.yml
+++++++++++ +++ /dev/null
+++++++++++-@@ -1,112 +0,0 @@
+++++++++++--import gym
+++++++++++--import d4rl # Import required to register environments, you may need to also import the submodule
+++++++++++--import numpy as np
+++++++++++--import d3rlpy
+++++++++++--
+++++++++++--def main():
+++++++++++--    dataset_d3, env = d3rlpy.datasets.get_dataset("halfcheetah-medium-v2")
+++++++++++--
+++++++++++--    print(dataset_d3.observations.shape)
+++++++++++--    print(dataset_d3.actions.shape)
+++++++++++--    print(dataset_d3.rewards.shape)
+++++++++++--        # print(dataset_d3.next_observations.shape)
+++++++++++--    print(dataset_d3.terminals.shape)
+++++++++++--    print(dataset_d3.terminals.sum()) # no
+++++++++++--
+++++++++++--    env = gym.make('halfcheetah-medium-v2')
+++++++++++--    dataset_d4 = d4rl.qlearning_dataset(env)
+++++++++++--
+++++++++++--    print(dataset_d4['observations'].shape)
+++++++++++--    print(dataset_d4['rewards'].shape)
+++++++++++--    print(dataset_d4['terminals'].shape)
+++++++++++--    print(dataset_d4['actions'].shape)
+++++++++++--
+++++++++++--    print(dataset_d4['rewards'][1])
+++++++++++--    print(dataset_d3.rewards[1])
+++++++++++--
+++++++++++--
+++++++++++--    print(np.allclose(dataset_d3.actions[100], dataset_d4['actions'][100]))
+++++++++++--
+++++++++++--    for j in range(1000):
+++++++++++--        for i in range(999):
+++++++++++--            if dataset_d4['rewards'][j * 999 + i] != dataset_d3.rewards[j * 1000 + i]: print("yo", i)
+++++++++++--        # if not np.allclose(dataset_d3.observations[i], dataset_d4['observations'][i]): print('obs ongelijk')
+++++++++++--        # if not np.allclose(dataset_d3.rewards[i], dataset_d4['rewards'][i]): print('obs ongelijk')
+++++++++++--        # if not np.allclose(dataset_d3.actions[i], dataset_d4['actions'][i]): print('obs ongelijk')
+++++++++++--
+++++++++++--    sac = d3rlpy.algos.SAC(
+++++++++++--        actor_learning_rate=3e-4,
+++++++++++--        critic_learning_rate=3e-4,
+++++++++++--        temp_learning_rate=3e-4,
+++++++++++--        batch_size=256)
+++++++++++--
+++++++++++--    print(sac)
+++++++++++--    sac.fit(dataset_d3, n_steps=10000)
+++++++++++--
+++++++++++--    actions = sac.predict(dataset_d3.observations[0])
+++++++++++--
+++++++++++--    print(actions)
+++++++++++--
+++++++++++--
+++++++++++--    return
+++++++++++--    print('yo!')
+++++++++++--
+++++++++++--    # Create the environment
+++++++++++--    env = gym.make('halfcheetah-medium-v2')
+++++++++++--
+++++++++++--    # d4rl abides by the OpenAI gym interface
+++++++++++--    env.reset()
+++++++++++--    env.step(env.action_space.sample())
+++++++++++--
+++++++++++--    # Each task is associated with a dataset
+++++++++++--    # dataset contains observations, actions, rewards, terminals, and infos
+++++++++++--    # dataset = env.get_dataset()
+++++++++++--    dataset = d4rl.qlearning_dataset(env)
+++++++++++--
+++++++++++--    print(dataset.keys()) # An N x dim_observation Numpy array of observations
+++++++++++--    print(dataset['rewards'].shape) # An N x dim_observation Numpy array of observations
+++++++++++--
+++++++++++--
+++++++++++--    first_traj = []
+++++++++++--    for i in range(50000):
+++++++++++--        if not np.allclose(dataset['next_observations'][i],dataset['observations'][i+1]): print("yo", i, dataset['terminals'][i])
+++++++++++--        # if dataset['terminals'][i] == True:
+++++++++++--        #     print('traj ended at', i)
+++++++++++--        #     break
+++++++++++--
+++++++++++--        # first_traj.append((dataset['observations'][i],
+++++++++++--        #                    dataset['actions'][i],
+++++++++++--        #                    dataset['rewards'][i],
+++++++++++--        #                    dataset['next_observations'][i]))
+++++++++++--    # print(first_traj)
+++++++++++--
+++++++++++--
+++++++++++--    # print(dataset['rewards'].shape) # An N x dim_observation Numpy array of observations
+++++++++++--
+++++++++++--    # Alternatively, use d4rl.qlearning_dataset which
+++++++++++--    # also adds next_observations.
+++++++++++--
+++++++++++--    # import d3rlpy
+++++++++++--
+++++++++++--    # # dataset, env = d3rlpy.datasets.get_dataset("halfcheetah-medium")
+++++++++++--
+++++++++++--    # # prepare algorithm
+++++++++++--    # # sac = d3rlpy.algos.SAC().create(device="cpu")
+++++++++++--
+++++++++++--    # sac = d3rlpy.algos.SACConfig(
+++++++++++--    #     actor_learning_rate=3e-4,
+++++++++++--    #     critic_learning_rate=3e-4,
+++++++++++--    #     temp_learning_rate=3e-4,
+++++++++++--    #     batch_size=256,
+++++++++++--    # ).create(device='cpu')
+++++++++++--
+++++++++++--
+++++++++++--    # # train offline
+++++++++++--    # # sac.fit(dataset, n_steps=1000)
+++++++++++--
+++++++++++--
+++++++++++--    # # ready to control
+++++++++++--    # actions = sac.predict(0)
+++++++++++--
+++++++++++--if __name__ == "__main__":
+++++++++++--    main()
++++++++++++@@ -1,27 +0,0 @@
++++++++++++-name: trajectory
++++++++++++-channels:
++++++++++++-- defaults
++++++++++++-- conda-forge
++++++++++++-dependencies:
++++++++++++-- python=3.8
++++++++++++-- pip
++++++++++++-- patchelf
++++++++++++-- pip:
++++++++++++-    - -f https://download.pytorch.org/whl/torch_stable.html
++++++++++++-    - numpy
++++++++++++-    - wheel==0.38.4
++++++++++++-    - setuptools==65.5.0
++++++++++++-    - gym==0.20.0
++++++++++++-    - matplotlib==3.3.4
++++++++++++-    - torch==1.9.1+cu111
++++++++++++-    - typed-argument-parser
++++++++++++-    # - git+https://github.com/Farama-Foundation/d4rl@f2a05c0d66722499bf8031b094d9af3aea7c372b#egg=d4rl
++++++++++++-    - scikit-image==0.17.2
++++++++++++-    - scikit-video==1.1.11
++++++++++++-    - gitpython
++++++++++++-    - os
++++++++++++-    - d3rlpy
++++++++++++-    - pyclustering
++++++++++++-    - moviepy
++++++++++++-    - scipy
++++++++++++-    - scikit-learn
++++++++++++diff --git a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json
++++++++++++index 18993c1..85d0998 100644
++++++++++++--- a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json
+++++++++++++++ b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json
++++++++++++@@ -1,23 +1,23 @@
++++++++++++ {
++++++++++++     "add_extras": {
++++++++++++         "_type": "python_object (type = method)",
++++++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgfhpRSlC4="
+++++++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMB2xvZ2Jhc2WUjAVsb2dzL5SMCmJlYW1fd2lkdGiUSyCMCHNhdmVwYXRolIw7bG9ncy9oYWxmY2hlZXRhaC1tZWRpdW0tdjIvcGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMyLzCUjAtyZWFkX2NvbmZpZ5RoAmgGaA2GlFKUjAZkZXZpY2WUjANjcHWUjAhleHBfbmFtZZSMHnBsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMpSMB2hvcml6b26USwWMEWdlbmVyYXRlX2V4cF9uYW1llGgCaAZoFYaUUpSMBWtfYWN0lE6MCmdldF9jb21taXSUaAJoBmgZhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAxncHRfbG9hZHBhdGiUjA5ncHQvcHJldHJhaW5lZJSMCHNldF9zZWVklGgCaAZoIIaUUpSMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMB2NkZl9vYnOUTowJZ3B0X2Vwb2NolIwGbGF0ZXN0lIwXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjApwZXJjZW50aWxllIwEbWVhbpSMCHZpc19mcmVxlEsyjAVrX29ic5RLAYwHdmVyYm9zZZSIjAphZGRfZXh0cmFzlGgCaAZoMIaUUpSMCXBsYW5fZnJlcZRLAYwIbl9leHBhbmSUSwKMBnN1ZmZpeJSMATCUjAZwcmVmaXiUjA9wbGFucy9kZWZhdWx0cy+UjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHY2RmX2FjdJRHP+MzMzMzMzOMBW1rZGlylGgCaAZoPIaUUpSMCXNhdmVfZGlmZpRoAmgGaD+GlFKUjA5wcmVmaXhfY29udGV4dJSIdWJoMIaUUpQu"
++++++++++++     },
++++++++++++     "beam_width": 32,
++++++++++++     "cdf_act": 0.6,
++++++++++++     "cdf_obs": null,
++++++++++++-    "commit": "cca8d898e10f9f6102a8c33dac7758e2993dfc60 halfcheetah-xrl",
+++++++++++++    "commit": "f77a194996872879bd0414fcb3dbc15b3da29cb8 main",
++++++++++++     "config": "config.offline",
++++++++++++     "dataset": "halfcheetah-medium-v2",
++++++++++++     "device": "cpu",
++++++++++++     "exp_name": "plans/defaults/freq1_H5_beam32",
++++++++++++     "generate_exp_name": {
++++++++++++         "_type": "python_object (type = method)",
++++++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgxhpRSlC4="
+++++++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMB2xvZ2Jhc2WUjAVsb2dzL5SMCmJlYW1fd2lkdGiUSyCMCHNhdmVwYXRolIw7bG9ncy9oYWxmY2hlZXRhaC1tZWRpdW0tdjIvcGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMyLzCUjAtyZWFkX2NvbmZpZ5RoAmgGaA2GlFKUjAZkZXZpY2WUjANjcHWUjAhleHBfbmFtZZSMHnBsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMpSMB2hvcml6b26USwWMEWdlbmVyYXRlX2V4cF9uYW1llGgCaAZoFYaUUpSMBWtfYWN0lE6MCmdldF9jb21taXSUaAJoBmgZhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAxncHRfbG9hZHBhdGiUjA5ncHQvcHJldHJhaW5lZJSMCHNldF9zZWVklGgCaAZoIIaUUpSMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMB2NkZl9vYnOUTowJZ3B0X2Vwb2NolIwGbGF0ZXN0lIwXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjApwZXJjZW50aWxllIwEbWVhbpSMCHZpc19mcmVxlEsyjAVrX29ic5RLAYwHdmVyYm9zZZSIjAphZGRfZXh0cmFzlGgCaAZoMIaUUpSMCXBsYW5fZnJlcZRLAYwIbl9leHBhbmSUSwKMBnN1ZmZpeJSMATCUjAZwcmVmaXiUjA9wbGFucy9kZWZhdWx0cy+UjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHY2RmX2FjdJRHP+MzMzMzMzOMBW1rZGlylGgCaAZoPIaUUpSMCXNhdmVfZGlmZpRoAmgGaD+GlFKUjA5wcmVmaXhfY29udGV4dJSIdWJoFYaUUpQu"
++++++++++++     },
++++++++++++     "get_commit": {
++++++++++++         "_type": "python_object (type = method)",
++++++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmglhpRSlC4="
+++++++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMB2xvZ2Jhc2WUjAVsb2dzL5SMCmJlYW1fd2lkdGiUSyCMCHNhdmVwYXRolIw7bG9ncy9oYWxmY2hlZXRhaC1tZWRpdW0tdjIvcGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMyLzCUjAtyZWFkX2NvbmZpZ5RoAmgGaA2GlFKUjAZkZXZpY2WUjANjcHWUjAhleHBfbmFtZZSMHnBsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMpSMB2hvcml6b26USwWMEWdlbmVyYXRlX2V4cF9uYW1llGgCaAZoFYaUUpSMBWtfYWN0lE6MCmdldF9jb21taXSUaAJoBmgZhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAxncHRfbG9hZHBhdGiUjA5ncHQvcHJldHJhaW5lZJSMCHNldF9zZWVklGgCaAZoIIaUUpSMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMB2NkZl9vYnOUTowJZ3B0X2Vwb2NolIwGbGF0ZXN0lIwXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjApwZXJjZW50aWxllIwEbWVhbpSMCHZpc19mcmVxlEsyjAVrX29ic5RLAYwHdmVyYm9zZZSIjAphZGRfZXh0cmFzlGgCaAZoMIaUUpSMCXBsYW5fZnJlcZRLAYwIbl9leHBhbmSUSwKMBnN1ZmZpeJSMATCUjAZwcmVmaXiUjA9wbGFucy9kZWZhdWx0cy+UjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHY2RmX2FjdJRHP+MzMzMzMzOMBW1rZGlylGgCaAZoPIaUUpSMCXNhdmVfZGlmZpRoAmgGaD+GlFKUjA5wcmVmaXhfY29udGV4dJSIdWJoGYaUUpQu"
++++++++++++     },
++++++++++++     "gpt_epoch": "latest",
++++++++++++     "gpt_loadpath": "gpt/pretrained",
++++++++++++@@ -28,7 +28,7 @@
++++++++++++     "max_context_transitions": 5,
++++++++++++     "mkdir": {
++++++++++++         "_type": "python_object (type = method)",
++++++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgihpRSlC4="
+++++++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMB2xvZ2Jhc2WUjAVsb2dzL5SMCmJlYW1fd2lkdGiUSyCMCHNhdmVwYXRolIw7bG9ncy9oYWxmY2hlZXRhaC1tZWRpdW0tdjIvcGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMyLzCUjAtyZWFkX2NvbmZpZ5RoAmgGaA2GlFKUjAZkZXZpY2WUjANjcHWUjAhleHBfbmFtZZSMHnBsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMpSMB2hvcml6b26USwWMEWdlbmVyYXRlX2V4cF9uYW1llGgCaAZoFYaUUpSMBWtfYWN0lE6MCmdldF9jb21taXSUaAJoBmgZhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAxncHRfbG9hZHBhdGiUjA5ncHQvcHJldHJhaW5lZJSMCHNldF9zZWVklGgCaAZoIIaUUpSMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMB2NkZl9vYnOUTowJZ3B0X2Vwb2NolIwGbGF0ZXN0lIwXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjApwZXJjZW50aWxllIwEbWVhbpSMCHZpc19mcmVxlEsyjAVrX29ic5RLAYwHdmVyYm9zZZSIjAphZGRfZXh0cmFzlGgCaAZoMIaUUpSMCXBsYW5fZnJlcZRLAYwIbl9leHBhbmSUSwKMBnN1ZmZpeJSMATCUjAZwcmVmaXiUjA9wbGFucy9kZWZhdWx0cy+UjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHY2RmX2FjdJRHP+MzMzMzMzOMBW1rZGlylGgCaAZoPIaUUpSMCXNhdmVfZGlmZpRoAmgGaD+GlFKUjA5wcmVmaXhfY29udGV4dJSIdWJoPIaUUpQu"
++++++++++++     },
++++++++++++     "n_expand": 2,
++++++++++++     "percentile": "mean",
++++++++++++@@ -37,24 +37,24 @@
++++++++++++     "prefix_context": true,
++++++++++++     "read_config": {
++++++++++++         "_type": "python_object (type = method)",
++++++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgZhpRSlC4="
+++++++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMB2xvZ2Jhc2WUjAVsb2dzL5SMCmJlYW1fd2lkdGiUSyCMCHNhdmVwYXRolIw7bG9ncy9oYWxmY2hlZXRhaC1tZWRpdW0tdjIvcGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMyLzCUjAtyZWFkX2NvbmZpZ5RoAmgGaA2GlFKUjAZkZXZpY2WUjANjcHWUjAhleHBfbmFtZZSMHnBsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMpSMB2hvcml6b26USwWMEWdlbmVyYXRlX2V4cF9uYW1llGgCaAZoFYaUUpSMBWtfYWN0lE6MCmdldF9jb21taXSUaAJoBmgZhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAxncHRfbG9hZHBhdGiUjA5ncHQvcHJldHJhaW5lZJSMCHNldF9zZWVklGgCaAZoIIaUUpSMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMB2NkZl9vYnOUTowJZ3B0X2Vwb2NolIwGbGF0ZXN0lIwXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjApwZXJjZW50aWxllIwEbWVhbpSMCHZpc19mcmVxlEsyjAVrX29ic5RLAYwHdmVyYm9zZZSIjAphZGRfZXh0cmFzlGgCaAZoMIaUUpSMCXBsYW5fZnJlcZRLAYwIbl9leHBhbmSUSwKMBnN1ZmZpeJSMATCUjAZwcmVmaXiUjA9wbGFucy9kZWZhdWx0cy+UjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHY2RmX2FjdJRHP+MzMzMzMzOMBW1rZGlylGgCaAZoPIaUUpSMCXNhdmVfZGlmZpRoAmgGaD+GlFKUjA5wcmVmaXhfY29udGV4dJSIdWJoDYaUUpQu"
++++++++++++     },
++++++++++++     "renderer": "Renderer",
++++++++++++     "reproducibility": {
++++++++++++-        "command_line": "python scripts/xrl_v2.py --dataset halfcheetah-medium-v2 --gpt_loadpath gpt/pretrained",
+++++++++++++        "command_line": "python xrl_v2.py --dataset halfcheetah-medium-v2 --gpt_loadpath gpt/pretrained",
++++++++++++         "git_has_uncommitted_changes": true,
++++++++++++         "git_root": "/home/colin/Desktop/FACT/FACT_assignment",
++++++++++++-        "git_url": "https://github.com/fclio/FACT_assignment/tree/cca8d898e10f9f6102a8c33dac7758e2993dfc60",
++++++++++++-        "time": "Fri Feb  2 11:39:41 2024"
+++++++++++++        "git_url": "https://github.com/fclio/FACT_assignment/tree/f77a194996872879bd0414fcb3dbc15b3da29cb8",
+++++++++++++        "time": "Sat Feb  3 21:04:01 2024"
++++++++++++     },
++++++++++++     "save_diff": {
++++++++++++         "_type": "python_object (type = method)",
++++++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1Ymg0hpRSlC4="
+++++++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMB2xvZ2Jhc2WUjAVsb2dzL5SMCmJlYW1fd2lkdGiUSyCMCHNhdmVwYXRolIw7bG9ncy9oYWxmY2hlZXRhaC1tZWRpdW0tdjIvcGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMyLzCUjAtyZWFkX2NvbmZpZ5RoAmgGaA2GlFKUjAZkZXZpY2WUjANjcHWUjAhleHBfbmFtZZSMHnBsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMpSMB2hvcml6b26USwWMEWdlbmVyYXRlX2V4cF9uYW1llGgCaAZoFYaUUpSMBWtfYWN0lE6MCmdldF9jb21taXSUaAJoBmgZhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAxncHRfbG9hZHBhdGiUjA5ncHQvcHJldHJhaW5lZJSMCHNldF9zZWVklGgCaAZoIIaUUpSMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMB2NkZl9vYnOUTowJZ3B0X2Vwb2NolIwGbGF0ZXN0lIwXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjApwZXJjZW50aWxllIwEbWVhbpSMCHZpc19mcmVxlEsyjAVrX29ic5RLAYwHdmVyYm9zZZSIjAphZGRfZXh0cmFzlGgCaAZoMIaUUpSMCXBsYW5fZnJlcZRLAYwIbl9leHBhbmSUSwKMBnN1ZmZpeJSMATCUjAZwcmVmaXiUjA9wbGFucy9kZWZhdWx0cy+UjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHY2RmX2FjdJRHP+MzMzMzMzOMBW1rZGlylGgCaAZoPIaUUpSMCXNhdmVfZGlmZpRoAmgGaD+GlFKUjA5wcmVmaXhfY29udGV4dJSIdWJoP4aUUpQu"
++++++++++++     },
++++++++++++     "savepath": "logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0",
++++++++++++     "set_seed": {
++++++++++++         "_type": "python_object (type = method)",
++++++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgqhpRSlC4="
+++++++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMB2xvZ2Jhc2WUjAVsb2dzL5SMCmJlYW1fd2lkdGiUSyCMCHNhdmVwYXRolIw7bG9ncy9oYWxmY2hlZXRhaC1tZWRpdW0tdjIvcGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMyLzCUjAtyZWFkX2NvbmZpZ5RoAmgGaA2GlFKUjAZkZXZpY2WUjANjcHWUjAhleHBfbmFtZZSMHnBsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMpSMB2hvcml6b26USwWMEWdlbmVyYXRlX2V4cF9uYW1llGgCaAZoFYaUUpSMBWtfYWN0lE6MCmdldF9jb21taXSUaAJoBmgZhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAxncHRfbG9hZHBhdGiUjA5ncHQvcHJldHJhaW5lZJSMCHNldF9zZWVklGgCaAZoIIaUUpSMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMB2NkZl9vYnOUTowJZ3B0X2Vwb2NolIwGbGF0ZXN0lIwXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjApwZXJjZW50aWxllIwEbWVhbpSMCHZpc19mcmVxlEsyjAVrX29ic5RLAYwHdmVyYm9zZZSIjAphZGRfZXh0cmFzlGgCaAZoMIaUUpSMCXBsYW5fZnJlcZRLAYwIbl9leHBhbmSUSwKMBnN1ZmZpeJSMATCUjAZwcmVmaXiUjA9wbGFucy9kZWZhdWx0cy+UjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHY2RmX2FjdJRHP+MzMzMzMzOMBW1rZGlylGgCaAZoPIaUUpSMCXNhdmVfZGlmZpRoAmgGaD+GlFKUjA5wcmVmaXhfY29udGV4dJSIdWJoIIaUUpQu"
++++++++++++     },
++++++++++++     "suffix": "0",
++++++++++++     "verbose": true,
++++++++++++diff --git a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt
++++++++++++index 2c00d59..028c1c0 100644
++++++++++++--- a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt
+++++++++++++++ b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt
++++++++++++@@ -1,1250 +1,3167 @@
++++++++++++-diff --git a/.gitignore b/.gitignore
++++++++++++-index 30d226d..a18312a 100644
++++++++++++---- a/.gitignore
++++++++++++-+++ b/.gitignore
++++++++++++-@@ -1,6 +1,5 @@
++++++++++++- __pycache__/
++++++++++++- *.mp4
++++++++++++--halfcheetah/logs/
++++++++++++- .ipynb_checkpoints/
++++++++++++- __pycache__/
++++++++++++- Open Notebook.onetoc2 
++++++++++++-diff --git a/gridworld/gridworld_expts.ipynb b/gridworld/gridworld_expts.ipynb
++++++++++++-index e2a261b..bd0b720 100644
++++++++++++---- a/gridworld/gridworld_expts.ipynb
++++++++++++-+++ b/gridworld/gridworld_expts.ipynb
++++++++++++-@@ -1570,7 +1570,7 @@
++++++++++++-    "name": "python",
++++++++++++-    "nbconvert_exporter": "python",
++++++++++++-    "pygments_lexer": "ipython3",
++++++++++++--   "version": "3.8.10"
++++++++++++-+   "version": "3.10.12"
++++++++++++-   }
++++++++++++-  },
++++++++++++-  "nbformat": 4,
++++++++++++-diff --git a/halfcheetah/clusters.npy b/halfcheetah/clusters.npy
+++++++++++++diff --git a/halfcheetah/environment.yml b/halfcheetah/environment.yml
++++++++++++ deleted file mode 100644
++++++++++++-index 3c198aa..0000000
++++++++++++-Binary files a/halfcheetah/clusters.npy and /dev/null differ
++++++++++++-diff --git a/halfcheetah/embeddings.npy b/halfcheetah/embeddings.npy
++++++++++++-deleted file mode 100644
++++++++++++-index 912d68c..0000000
++++++++++++-Binary files a/halfcheetah/embeddings.npy and /dev/null differ
++++++++++++-diff --git a/halfcheetah/halfcheetah.py b/halfcheetah/halfcheetah.py
++++++++++++-deleted file mode 100644
++++++++++++-index 626c42d..0000000
++++++++++++---- a/halfcheetah/halfcheetah.py
+++++++++++++index e81007c..0000000
+++++++++++++--- a/halfcheetah/environment.yml
++++++++++++ +++ /dev/null
++++++++++++-@@ -1,112 +0,0 @@
++++++++++++--import gym
++++++++++++--import d4rl # Import required to register environments, you may need to also import the submodule
++++++++++++--import numpy as np
++++++++++++--import d3rlpy
++++++++++++--
++++++++++++--def main():
++++++++++++--    dataset_d3, env = d3rlpy.datasets.get_dataset("halfcheetah-medium-v2")
++++++++++++--
++++++++++++--    print(dataset_d3.observations.shape)
++++++++++++--    print(dataset_d3.actions.shape)
++++++++++++--    print(dataset_d3.rewards.shape)
++++++++++++--        # print(dataset_d3.next_observations.shape)
++++++++++++--    print(dataset_d3.terminals.shape)
++++++++++++--    print(dataset_d3.terminals.sum()) # no
++++++++++++--
++++++++++++--    env = gym.make('halfcheetah-medium-v2')
++++++++++++--    dataset_d4 = d4rl.qlearning_dataset(env)
++++++++++++--
++++++++++++--    print(dataset_d4['observations'].shape)
++++++++++++--    print(dataset_d4['rewards'].shape)
++++++++++++--    print(dataset_d4['terminals'].shape)
++++++++++++--    print(dataset_d4['actions'].shape)
++++++++++++--
++++++++++++--    print(dataset_d4['rewards'][1])
++++++++++++--    print(dataset_d3.rewards[1])
++++++++++++--
++++++++++++--
++++++++++++--    print(np.allclose(dataset_d3.actions[100], dataset_d4['actions'][100]))
++++++++++++--
++++++++++++--    for j in range(1000):
++++++++++++--        for i in range(999):
++++++++++++--            if dataset_d4['rewards'][j * 999 + i] != dataset_d3.rewards[j * 1000 + i]: print("yo", i)
++++++++++++--        # if not np.allclose(dataset_d3.observations[i], dataset_d4['observations'][i]): print('obs ongelijk')
++++++++++++--        # if not np.allclose(dataset_d3.rewards[i], dataset_d4['rewards'][i]): print('obs ongelijk')
++++++++++++--        # if not np.allclose(dataset_d3.actions[i], dataset_d4['actions'][i]): print('obs ongelijk')
++++++++++++--
++++++++++++--    sac = d3rlpy.algos.SAC(
++++++++++++--        actor_learning_rate=3e-4,
++++++++++++--        critic_learning_rate=3e-4,
++++++++++++--        temp_learning_rate=3e-4,
++++++++++++--        batch_size=256)
++++++++++++--
++++++++++++--    print(sac)
++++++++++++--    sac.fit(dataset_d3, n_steps=10000)
++++++++++++--
++++++++++++--    actions = sac.predict(dataset_d3.observations[0])
++++++++++++--
++++++++++++--    print(actions)
++++++++++++--
++++++++++++--
++++++++++++--    return
++++++++++++--    print('yo!')
++++++++++++--
++++++++++++--    # Create the environment
++++++++++++--    env = gym.make('halfcheetah-medium-v2')
++++++++++++--
++++++++++++--    # d4rl abides by the OpenAI gym interface
++++++++++++--    env.reset()
++++++++++++--    env.step(env.action_space.sample())
++++++++++++--
++++++++++++--    # Each task is associated with a dataset
++++++++++++--    # dataset contains observations, actions, rewards, terminals, and infos
++++++++++++--    # dataset = env.get_dataset()
++++++++++++--    dataset = d4rl.qlearning_dataset(env)
++++++++++++--
++++++++++++--    print(dataset.keys()) # An N x dim_observation Numpy array of observations
++++++++++++--    print(dataset['rewards'].shape) # An N x dim_observation Numpy array of observations
++++++++++++--
++++++++++++--
++++++++++++--    first_traj = []
++++++++++++--    for i in range(50000):
++++++++++++--        if not np.allclose(dataset['next_observations'][i],dataset['observations'][i+1]): print("yo", i, dataset['terminals'][i])
++++++++++++--        # if dataset['terminals'][i] == True:
++++++++++++--        #     print('traj ended at', i)
++++++++++++--        #     break
++++++++++++--
++++++++++++--        # first_traj.append((dataset['observations'][i],
++++++++++++--        #                    dataset['actions'][i],
++++++++++++--        #                    dataset['rewards'][i],
++++++++++++--        #                    dataset['next_observations'][i]))
++++++++++++--    # print(first_traj)
++++++++++++--
++++++++++++--
++++++++++++--    # print(dataset['rewards'].shape) # An N x dim_observation Numpy array of observations
++++++++++++--
++++++++++++--    # Alternatively, use d4rl.qlearning_dataset which
++++++++++++--    # also adds next_observations.
++++++++++++--
++++++++++++--    # import d3rlpy
++++++++++++--
++++++++++++--    # # dataset, env = d3rlpy.datasets.get_dataset("halfcheetah-medium")
++++++++++++--
++++++++++++--    # # prepare algorithm
++++++++++++--    # # sac = d3rlpy.algos.SAC().create(device="cpu")
++++++++++++--
++++++++++++--    # sac = d3rlpy.algos.SACConfig(
++++++++++++--    #     actor_learning_rate=3e-4,
++++++++++++--    #     critic_learning_rate=3e-4,
++++++++++++--    #     temp_learning_rate=3e-4,
++++++++++++--    #     batch_size=256,
++++++++++++--    # ).create(device='cpu')
++++++++++++--
++++++++++++--
++++++++++++--    # # train offline
++++++++++++--    # # sac.fit(dataset, n_steps=1000)
++++++++++++--
++++++++++++--
++++++++++++--    # # ready to control
++++++++++++--    # actions = sac.predict(0)
++++++++++++--
++++++++++++--if __name__ == "__main__":
++++++++++++--    main()
+++++++++++++@@ -1,27 +0,0 @@
+++++++++++++-name: trajectory
+++++++++++++-channels:
+++++++++++++-- defaults
+++++++++++++-- conda-forge
+++++++++++++-dependencies:
+++++++++++++-- python=3.8
+++++++++++++-- pip
+++++++++++++-- patchelf
+++++++++++++-- pip:
+++++++++++++-    - -f https://download.pytorch.org/whl/torch_stable.html
+++++++++++++-    - numpy
+++++++++++++-    - wheel==0.38.4
+++++++++++++-    - setuptools==65.5.0
+++++++++++++-    - gym==0.20.0
+++++++++++++-    - matplotlib==3.3.4
+++++++++++++-    - torch==1.9.1+cu111
+++++++++++++-    - typed-argument-parser
+++++++++++++-    # - git+https://github.com/Farama-Foundation/d4rl@f2a05c0d66722499bf8031b094d9af3aea7c372b#egg=d4rl
+++++++++++++-    - scikit-image==0.17.2
+++++++++++++-    - scikit-video==1.1.11
+++++++++++++-    - gitpython
+++++++++++++-    - os
+++++++++++++-    - d3rlpy
+++++++++++++-    - pyclustering
+++++++++++++-    - moviepy
+++++++++++++-    - scipy
+++++++++++++-    - scikit-learn
+++++++++++++diff --git a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json
+++++++++++++index 18993c1..2dc6557 100644
+++++++++++++--- a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json
++++++++++++++++ b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json
+++++++++++++@@ -1,23 +1,23 @@
+++++++++++++ {
+++++++++++++     "add_extras": {
+++++++++++++         "_type": "python_object (type = method)",
+++++++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgfhpRSlC4="
++++++++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMCHJlbmRlcmVylIwIUmVuZGVyZXKUjAZkZXZpY2WUjANjcHWUjA5wcmVmaXhfY29udGV4dJSIjAhleHBfbmFtZZSMHnBsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMpSMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjAh2aXNfZnJlcZRLMowKZ2V0X2NvbW1pdJRoAmgGaBGGlFKUjAdsb2diYXNllIwFbG9ncy+UjAxncHRfbG9hZHBhdGiUjA5ncHQvcHJldHJhaW5lZJSMCWdwdF9lcG9jaJSMBmxhdGVzdJSMCXBsYW5fZnJlcZRLAYwGc3VmZml4lIwBMJSMBmNvbmZpZ5SMDmNvbmZpZy5vZmZsaW5llIwJc2F2ZV9kaWZmlGgCaAZoH4aUUpSMB2hvcml6b26USwWMB3ZlcmJvc2WUiIwKYmVhbV93aWR0aJRLIIwRZ2VuZXJhdGVfZXhwX25hbWWUaAJoBmglhpRSlIwKYWRkX2V4dHJhc5RoAmgGaCiGlFKUjAVrX29ic5RLAYwIc2V0X3NlZWSUaAJoBmgshpRSlIwFbWtkaXKUaAJoBmgvhpRSlIwHY2RmX29ic5ROjAhzYXZlcGF0aJSMO2xvZ3MvaGFsZmNoZWV0YWgtbWVkaXVtLXYyL3BsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMi8wlIwIbl9leHBhbmSUSwKMB2NkZl9hY3SURz/jMzMzMzMzjAtyZWFkX2NvbmZpZ5RoAmgGaDeGlFKUjAdkYXRhc2V0lIwVaGFsZmNoZWV0YWgtbWVkaXVtLXYylIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwGY29tbWl0lIwtZjc3YTE5NDk5Njg3Mjg3OWJkMDQxNGZjYjNkYmMxNWIzZGEyOWNiOCBtYWlulIwKcGVyY2VudGlsZZSMBG1lYW6UjAVrX2FjdJROdWJoKIaUUpQu"
+++++++++++++     },
+++++++++++++     "beam_width": 32,
+++++++++++++     "cdf_act": 0.6,
+++++++++++++     "cdf_obs": null,
+++++++++++++-    "commit": "cca8d898e10f9f6102a8c33dac7758e2993dfc60 halfcheetah-xrl",
++++++++++++++    "commit": "f77a194996872879bd0414fcb3dbc15b3da29cb8 main",
+++++++++++++     "config": "config.offline",
+++++++++++++     "dataset": "halfcheetah-medium-v2",
+++++++++++++     "device": "cpu",
+++++++++++++     "exp_name": "plans/defaults/freq1_H5_beam32",
+++++++++++++     "generate_exp_name": {
+++++++++++++         "_type": "python_object (type = method)",
+++++++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgxhpRSlC4="
++++++++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMCHJlbmRlcmVylIwIUmVuZGVyZXKUjAZkZXZpY2WUjANjcHWUjA5wcmVmaXhfY29udGV4dJSIjAhleHBfbmFtZZSMHnBsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMpSMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjAh2aXNfZnJlcZRLMowKZ2V0X2NvbW1pdJRoAmgGaBGGlFKUjAdsb2diYXNllIwFbG9ncy+UjAxncHRfbG9hZHBhdGiUjA5ncHQvcHJldHJhaW5lZJSMCWdwdF9lcG9jaJSMBmxhdGVzdJSMCXBsYW5fZnJlcZRLAYwGc3VmZml4lIwBMJSMBmNvbmZpZ5SMDmNvbmZpZy5vZmZsaW5llIwJc2F2ZV9kaWZmlGgCaAZoH4aUUpSMB2hvcml6b26USwWMB3ZlcmJvc2WUiIwKYmVhbV93aWR0aJRLIIwRZ2VuZXJhdGVfZXhwX25hbWWUaAJoBmglhpRSlIwKYWRkX2V4dHJhc5RoAmgGaCiGlFKUjAVrX29ic5RLAYwIc2V0X3NlZWSUaAJoBmgshpRSlIwFbWtkaXKUaAJoBmgvhpRSlIwHY2RmX29ic5ROjAhzYXZlcGF0aJSMO2xvZ3MvaGFsZmNoZWV0YWgtbWVkaXVtLXYyL3BsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMi8wlIwIbl9leHBhbmSUSwKMB2NkZl9hY3SURz/jMzMzMzMzjAtyZWFkX2NvbmZpZ5RoAmgGaDeGlFKUjAdkYXRhc2V0lIwVaGFsZmNoZWV0YWgtbWVkaXVtLXYylIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwGY29tbWl0lIwtZjc3YTE5NDk5Njg3Mjg3OWJkMDQxNGZjYjNkYmMxNWIzZGEyOWNiOCBtYWlulIwKcGVyY2VudGlsZZSMBG1lYW6UjAVrX2FjdJROdWJoJYaUUpQu"
+++++++++++++     },
+++++++++++++     "get_commit": {
+++++++++++++         "_type": "python_object (type = method)",
+++++++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmglhpRSlC4="
++++++++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMCHJlbmRlcmVylIwIUmVuZGVyZXKUjAZkZXZpY2WUjANjcHWUjA5wcmVmaXhfY29udGV4dJSIjAhleHBfbmFtZZSMHnBsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMpSMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjAh2aXNfZnJlcZRLMowKZ2V0X2NvbW1pdJRoAmgGaBGGlFKUjAdsb2diYXNllIwFbG9ncy+UjAxncHRfbG9hZHBhdGiUjA5ncHQvcHJldHJhaW5lZJSMCWdwdF9lcG9jaJSMBmxhdGVzdJSMCXBsYW5fZnJlcZRLAYwGc3VmZml4lIwBMJSMBmNvbmZpZ5SMDmNvbmZpZy5vZmZsaW5llIwJc2F2ZV9kaWZmlGgCaAZoH4aUUpSMB2hvcml6b26USwWMB3ZlcmJvc2WUiIwKYmVhbV93aWR0aJRLIIwRZ2VuZXJhdGVfZXhwX25hbWWUaAJoBmglhpRSlIwKYWRkX2V4dHJhc5RoAmgGaCiGlFKUjAVrX29ic5RLAYwIc2V0X3NlZWSUaAJoBmgshpRSlIwFbWtkaXKUaAJoBmgvhpRSlIwHY2RmX29ic5ROjAhzYXZlcGF0aJSMO2xvZ3MvaGFsZmNoZWV0YWgtbWVkaXVtLXYyL3BsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMi8wlIwIbl9leHBhbmSUSwKMB2NkZl9hY3SURz/jMzMzMzMzjAtyZWFkX2NvbmZpZ5RoAmgGaDeGlFKUjAdkYXRhc2V0lIwVaGFsZmNoZWV0YWgtbWVkaXVtLXYylIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwGY29tbWl0lIwtZjc3YTE5NDk5Njg3Mjg3OWJkMDQxNGZjYjNkYmMxNWIzZGEyOWNiOCBtYWlulIwKcGVyY2VudGlsZZSMBG1lYW6UjAVrX2FjdJROdWJoEYaUUpQu"
+++++++++++++     },
+++++++++++++     "gpt_epoch": "latest",
+++++++++++++     "gpt_loadpath": "gpt/pretrained",
+++++++++++++@@ -28,7 +28,7 @@
+++++++++++++     "max_context_transitions": 5,
+++++++++++++     "mkdir": {
+++++++++++++         "_type": "python_object (type = method)",
+++++++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgihpRSlC4="
++++++++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMCHJlbmRlcmVylIwIUmVuZGVyZXKUjAZkZXZpY2WUjANjcHWUjA5wcmVmaXhfY29udGV4dJSIjAhleHBfbmFtZZSMHnBsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMpSMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjAh2aXNfZnJlcZRLMowKZ2V0X2NvbW1pdJRoAmgGaBGGlFKUjAdsb2diYXNllIwFbG9ncy+UjAxncHRfbG9hZHBhdGiUjA5ncHQvcHJldHJhaW5lZJSMCWdwdF9lcG9jaJSMBmxhdGVzdJSMCXBsYW5fZnJlcZRLAYwGc3VmZml4lIwBMJSMBmNvbmZpZ5SMDmNvbmZpZy5vZmZsaW5llIwJc2F2ZV9kaWZmlGgCaAZoH4aUUpSMB2hvcml6b26USwWMB3ZlcmJvc2WUiIwKYmVhbV93aWR0aJRLIIwRZ2VuZXJhdGVfZXhwX25hbWWUaAJoBmglhpRSlIwKYWRkX2V4dHJhc5RoAmgGaCiGlFKUjAVrX29ic5RLAYwIc2V0X3NlZWSUaAJoBmgshpRSlIwFbWtkaXKUaAJoBmgvhpRSlIwHY2RmX29ic5ROjAhzYXZlcGF0aJSMO2xvZ3MvaGFsZmNoZWV0YWgtbWVkaXVtLXYyL3BsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMi8wlIwIbl9leHBhbmSUSwKMB2NkZl9hY3SURz/jMzMzMzMzjAtyZWFkX2NvbmZpZ5RoAmgGaDeGlFKUjAdkYXRhc2V0lIwVaGFsZmNoZWV0YWgtbWVkaXVtLXYylIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwGY29tbWl0lIwtZjc3YTE5NDk5Njg3Mjg3OWJkMDQxNGZjYjNkYmMxNWIzZGEyOWNiOCBtYWlulIwKcGVyY2VudGlsZZSMBG1lYW6UjAVrX2FjdJROdWJoL4aUUpQu"
+++++++++++++     },
+++++++++++++     "n_expand": 2,
+++++++++++++     "percentile": "mean",
+++++++++++++@@ -37,24 +37,24 @@
+++++++++++++     "prefix_context": true,
+++++++++++++     "read_config": {
+++++++++++++         "_type": "python_object (type = method)",
+++++++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgZhpRSlC4="
++++++++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMCHJlbmRlcmVylIwIUmVuZGVyZXKUjAZkZXZpY2WUjANjcHWUjA5wcmVmaXhfY29udGV4dJSIjAhleHBfbmFtZZSMHnBsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMpSMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjAh2aXNfZnJlcZRLMowKZ2V0X2NvbW1pdJRoAmgGaBGGlFKUjAdsb2diYXNllIwFbG9ncy+UjAxncHRfbG9hZHBhdGiUjA5ncHQvcHJldHJhaW5lZJSMCWdwdF9lcG9jaJSMBmxhdGVzdJSMCXBsYW5fZnJlcZRLAYwGc3VmZml4lIwBMJSMBmNvbmZpZ5SMDmNvbmZpZy5vZmZsaW5llIwJc2F2ZV9kaWZmlGgCaAZoH4aUUpSMB2hvcml6b26USwWMB3ZlcmJvc2WUiIwKYmVhbV93aWR0aJRLIIwRZ2VuZXJhdGVfZXhwX25hbWWUaAJoBmglhpRSlIwKYWRkX2V4dHJhc5RoAmgGaCiGlFKUjAVrX29ic5RLAYwIc2V0X3NlZWSUaAJoBmgshpRSlIwFbWtkaXKUaAJoBmgvhpRSlIwHY2RmX29ic5ROjAhzYXZlcGF0aJSMO2xvZ3MvaGFsZmNoZWV0YWgtbWVkaXVtLXYyL3BsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMi8wlIwIbl9leHBhbmSUSwKMB2NkZl9hY3SURz/jMzMzMzMzjAtyZWFkX2NvbmZpZ5RoAmgGaDeGlFKUjAdkYXRhc2V0lIwVaGFsZmNoZWV0YWgtbWVkaXVtLXYylIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwGY29tbWl0lIwtZjc3YTE5NDk5Njg3Mjg3OWJkMDQxNGZjYjNkYmMxNWIzZGEyOWNiOCBtYWlulIwKcGVyY2VudGlsZZSMBG1lYW6UjAVrX2FjdJROdWJoN4aUUpQu"
+++++++++++++     },
+++++++++++++     "renderer": "Renderer",
+++++++++++++     "reproducibility": {
+++++++++++++         "command_line": "python scripts/xrl_v2.py --dataset halfcheetah-medium-v2 --gpt_loadpath gpt/pretrained",
+++++++++++++         "git_has_uncommitted_changes": true,
+++++++++++++         "git_root": "/home/colin/Desktop/FACT/FACT_assignment",
+++++++++++++-        "git_url": "https://github.com/fclio/FACT_assignment/tree/cca8d898e10f9f6102a8c33dac7758e2993dfc60",
+++++++++++++-        "time": "Fri Feb  2 11:39:41 2024"
++++++++++++++        "git_url": "https://github.com/fclio/FACT_assignment/tree/f77a194996872879bd0414fcb3dbc15b3da29cb8",
++++++++++++++        "time": "Sat Feb  3 20:58:34 2024"
+++++++++++++     },
+++++++++++++     "save_diff": {
+++++++++++++         "_type": "python_object (type = method)",
+++++++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1Ymg0hpRSlC4="
++++++++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMCHJlbmRlcmVylIwIUmVuZGVyZXKUjAZkZXZpY2WUjANjcHWUjA5wcmVmaXhfY29udGV4dJSIjAhleHBfbmFtZZSMHnBsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMpSMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjAh2aXNfZnJlcZRLMowKZ2V0X2NvbW1pdJRoAmgGaBGGlFKUjAdsb2diYXNllIwFbG9ncy+UjAxncHRfbG9hZHBhdGiUjA5ncHQvcHJldHJhaW5lZJSMCWdwdF9lcG9jaJSMBmxhdGVzdJSMCXBsYW5fZnJlcZRLAYwGc3VmZml4lIwBMJSMBmNvbmZpZ5SMDmNvbmZpZy5vZmZsaW5llIwJc2F2ZV9kaWZmlGgCaAZoH4aUUpSMB2hvcml6b26USwWMB3ZlcmJvc2WUiIwKYmVhbV93aWR0aJRLIIwRZ2VuZXJhdGVfZXhwX25hbWWUaAJoBmglhpRSlIwKYWRkX2V4dHJhc5RoAmgGaCiGlFKUjAVrX29ic5RLAYwIc2V0X3NlZWSUaAJoBmgshpRSlIwFbWtkaXKUaAJoBmgvhpRSlIwHY2RmX29ic5ROjAhzYXZlcGF0aJSMO2xvZ3MvaGFsZmNoZWV0YWgtbWVkaXVtLXYyL3BsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMi8wlIwIbl9leHBhbmSUSwKMB2NkZl9hY3SURz/jMzMzMzMzjAtyZWFkX2NvbmZpZ5RoAmgGaDeGlFKUjAdkYXRhc2V0lIwVaGFsZmNoZWV0YWgtbWVkaXVtLXYylIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwGY29tbWl0lIwtZjc3YTE5NDk5Njg3Mjg3OWJkMDQxNGZjYjNkYmMxNWIzZGEyOWNiOCBtYWlulIwKcGVyY2VudGlsZZSMBG1lYW6UjAVrX2FjdJROdWJoH4aUUpQu"
+++++++++++++     },
+++++++++++++     "savepath": "logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0",
+++++++++++++     "set_seed": {
+++++++++++++         "_type": "python_object (type = method)",
+++++++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgqhpRSlC4="
++++++++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMCHJlbmRlcmVylIwIUmVuZGVyZXKUjAZkZXZpY2WUjANjcHWUjA5wcmVmaXhfY29udGV4dJSIjAhleHBfbmFtZZSMHnBsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMpSMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjAh2aXNfZnJlcZRLMowKZ2V0X2NvbW1pdJRoAmgGaBGGlFKUjAdsb2diYXNllIwFbG9ncy+UjAxncHRfbG9hZHBhdGiUjA5ncHQvcHJldHJhaW5lZJSMCWdwdF9lcG9jaJSMBmxhdGVzdJSMCXBsYW5fZnJlcZRLAYwGc3VmZml4lIwBMJSMBmNvbmZpZ5SMDmNvbmZpZy5vZmZsaW5llIwJc2F2ZV9kaWZmlGgCaAZoH4aUUpSMB2hvcml6b26USwWMB3ZlcmJvc2WUiIwKYmVhbV93aWR0aJRLIIwRZ2VuZXJhdGVfZXhwX25hbWWUaAJoBmglhpRSlIwKYWRkX2V4dHJhc5RoAmgGaCiGlFKUjAVrX29ic5RLAYwIc2V0X3NlZWSUaAJoBmgshpRSlIwFbWtkaXKUaAJoBmgvhpRSlIwHY2RmX29ic5ROjAhzYXZlcGF0aJSMO2xvZ3MvaGFsZmNoZWV0YWgtbWVkaXVtLXYyL3BsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMi8wlIwIbl9leHBhbmSUSwKMB2NkZl9hY3SURz/jMzMzMzMzjAtyZWFkX2NvbmZpZ5RoAmgGaDeGlFKUjAdkYXRhc2V0lIwVaGFsZmNoZWV0YWgtbWVkaXVtLXYylIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwGY29tbWl0lIwtZjc3YTE5NDk5Njg3Mjg3OWJkMDQxNGZjYjNkYmMxNWIzZGEyOWNiOCBtYWlulIwKcGVyY2VudGlsZZSMBG1lYW6UjAVrX2FjdJROdWJoLIaUUpQu"
+++++++++++++     },
+++++++++++++     "suffix": "0",
+++++++++++++     "verbose": true,
+++++++++++++diff --git a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt
+++++++++++++index 2c00d59..fbfcda9 100644
+++++++++++++--- a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt
++++++++++++++++ b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt
+++++++++++++@@ -1,1250 +1,1668 @@
+++++++++++++-diff --git a/.gitignore b/.gitignore
+++++++++++++-index 30d226d..a18312a 100644
+++++++++++++---- a/.gitignore
+++++++++++++-+++ b/.gitignore
+++++++++++++-@@ -1,6 +1,5 @@
+++++++++++++- __pycache__/
+++++++++++++- *.mp4
+++++++++++++--halfcheetah/logs/
+++++++++++++- .ipynb_checkpoints/
+++++++++++++- __pycache__/
+++++++++++++- Open Notebook.onetoc2 
+++++++++++++-diff --git a/gridworld/gridworld_expts.ipynb b/gridworld/gridworld_expts.ipynb
+++++++++++++-index e2a261b..bd0b720 100644
+++++++++++++---- a/gridworld/gridworld_expts.ipynb
+++++++++++++-+++ b/gridworld/gridworld_expts.ipynb
+++++++++++++-@@ -1570,7 +1570,7 @@
+++++++++++++-    "name": "python",
+++++++++++++-    "nbconvert_exporter": "python",
+++++++++++++-    "pygments_lexer": "ipython3",
+++++++++++++--   "version": "3.8.10"
+++++++++++++-+   "version": "3.10.12"
+++++++++++++-   }
+++++++++++++-  },
+++++++++++++-  "nbformat": 4,
+++++++++++++-diff --git a/halfcheetah/clusters.npy b/halfcheetah/clusters.npy
++++++++++++++diff --git a/halfcheetah/environment.yml b/halfcheetah/environment.yml
+++++++++++++ deleted file mode 100644
+++++++++++++-index 3c198aa..0000000
+++++++++++++-Binary files a/halfcheetah/clusters.npy and /dev/null differ
+++++++++++++-diff --git a/halfcheetah/embeddings.npy b/halfcheetah/embeddings.npy
+++++++++++++-deleted file mode 100644
+++++++++++++-index 912d68c..0000000
+++++++++++++-Binary files a/halfcheetah/embeddings.npy and /dev/null differ
+++++++++++++-diff --git a/halfcheetah/halfcheetah.py b/halfcheetah/halfcheetah.py
+++++++++++++-deleted file mode 100644
+++++++++++++-index 626c42d..0000000
+++++++++++++---- a/halfcheetah/halfcheetah.py
++++++++++++++index e81007c..0000000
++++++++++++++--- a/halfcheetah/environment.yml
+++++++++++++ +++ /dev/null
+++++++++++++-@@ -1,112 +0,0 @@
+++++++++++++--import gym
+++++++++++++--import d4rl # Import required to register environments, you may need to also import the submodule
+++++++++++++--import numpy as np
+++++++++++++--import d3rlpy
+++++++++++++--
+++++++++++++--def main():
+++++++++++++--    dataset_d3, env = d3rlpy.datasets.get_dataset("halfcheetah-medium-v2")
+++++++++++++--
+++++++++++++--    print(dataset_d3.observations.shape)
+++++++++++++--    print(dataset_d3.actions.shape)
+++++++++++++--    print(dataset_d3.rewards.shape)
+++++++++++++--        # print(dataset_d3.next_observations.shape)
+++++++++++++--    print(dataset_d3.terminals.shape)
+++++++++++++--    print(dataset_d3.terminals.sum()) # no
+++++++++++++--
+++++++++++++--    env = gym.make('halfcheetah-medium-v2')
+++++++++++++--    dataset_d4 = d4rl.qlearning_dataset(env)
+++++++++++++--
+++++++++++++--    print(dataset_d4['observations'].shape)
+++++++++++++--    print(dataset_d4['rewards'].shape)
+++++++++++++--    print(dataset_d4['terminals'].shape)
+++++++++++++--    print(dataset_d4['actions'].shape)
+++++++++++++--
+++++++++++++--    print(dataset_d4['rewards'][1])
+++++++++++++--    print(dataset_d3.rewards[1])
+++++++++++++--
+++++++++++++--
+++++++++++++--    print(np.allclose(dataset_d3.actions[100], dataset_d4['actions'][100]))
+++++++++++++--
+++++++++++++--    for j in range(1000):
+++++++++++++--        for i in range(999):
+++++++++++++--            if dataset_d4['rewards'][j * 999 + i] != dataset_d3.rewards[j * 1000 + i]: print("yo", i)
+++++++++++++--        # if not np.allclose(dataset_d3.observations[i], dataset_d4['observations'][i]): print('obs ongelijk')
+++++++++++++--        # if not np.allclose(dataset_d3.rewards[i], dataset_d4['rewards'][i]): print('obs ongelijk')
+++++++++++++--        # if not np.allclose(dataset_d3.actions[i], dataset_d4['actions'][i]): print('obs ongelijk')
+++++++++++++--
+++++++++++++--    sac = d3rlpy.algos.SAC(
+++++++++++++--        actor_learning_rate=3e-4,
+++++++++++++--        critic_learning_rate=3e-4,
+++++++++++++--        temp_learning_rate=3e-4,
+++++++++++++--        batch_size=256)
+++++++++++++--
+++++++++++++--    print(sac)
+++++++++++++--    sac.fit(dataset_d3, n_steps=10000)
+++++++++++++--
+++++++++++++--    actions = sac.predict(dataset_d3.observations[0])
+++++++++++++--
+++++++++++++--    print(actions)
+++++++++++++--
+++++++++++++--
+++++++++++++--    return
+++++++++++++--    print('yo!')
+++++++++++++--
+++++++++++++--    # Create the environment
+++++++++++++--    env = gym.make('halfcheetah-medium-v2')
+++++++++++++--
+++++++++++++--    # d4rl abides by the OpenAI gym interface
+++++++++++++--    env.reset()
+++++++++++++--    env.step(env.action_space.sample())
+++++++++++++--
+++++++++++++--    # Each task is associated with a dataset
+++++++++++++--    # dataset contains observations, actions, rewards, terminals, and infos
+++++++++++++--    # dataset = env.get_dataset()
+++++++++++++--    dataset = d4rl.qlearning_dataset(env)
+++++++++++++--
+++++++++++++--    print(dataset.keys()) # An N x dim_observation Numpy array of observations
+++++++++++++--    print(dataset['rewards'].shape) # An N x dim_observation Numpy array of observations
+++++++++++++--
+++++++++++++--
+++++++++++++--    first_traj = []
+++++++++++++--    for i in range(50000):
+++++++++++++--        if not np.allclose(dataset['next_observations'][i],dataset['observations'][i+1]): print("yo", i, dataset['terminals'][i])
+++++++++++++--        # if dataset['terminals'][i] == True:
+++++++++++++--        #     print('traj ended at', i)
+++++++++++++--        #     break
+++++++++++++--
+++++++++++++--        # first_traj.append((dataset['observations'][i],
+++++++++++++--        #                    dataset['actions'][i],
+++++++++++++--        #                    dataset['rewards'][i],
+++++++++++++--        #                    dataset['next_observations'][i]))
+++++++++++++--    # print(first_traj)
+++++++++++++--
+++++++++++++--
+++++++++++++--    # print(dataset['rewards'].shape) # An N x dim_observation Numpy array of observations
+++++++++++++--
+++++++++++++--    # Alternatively, use d4rl.qlearning_dataset which
+++++++++++++--    # also adds next_observations.
+++++++++++++--
+++++++++++++--    # import d3rlpy
+++++++++++++--
+++++++++++++--    # # dataset, env = d3rlpy.datasets.get_dataset("halfcheetah-medium")
+++++++++++++--
+++++++++++++--    # # prepare algorithm
+++++++++++++--    # # sac = d3rlpy.algos.SAC().create(device="cpu")
+++++++++++++--
+++++++++++++--    # sac = d3rlpy.algos.SACConfig(
+++++++++++++--    #     actor_learning_rate=3e-4,
+++++++++++++--    #     critic_learning_rate=3e-4,
+++++++++++++--    #     temp_learning_rate=3e-4,
+++++++++++++--    #     batch_size=256,
+++++++++++++--    # ).create(device='cpu')
+++++++++++++--
+++++++++++++--
+++++++++++++--    # # train offline
+++++++++++++--    # # sac.fit(dataset, n_steps=1000)
+++++++++++++--
+++++++++++++--
+++++++++++++--    # # ready to control
+++++++++++++--    # actions = sac.predict(0)
+++++++++++++--
+++++++++++++--if __name__ == "__main__":
+++++++++++++--    main()
++++++++++++++@@ -1,27 +0,0 @@
++++++++++++++-name: trajectory
++++++++++++++-channels:
++++++++++++++-- defaults
++++++++++++++-- conda-forge
++++++++++++++-dependencies:
++++++++++++++-- python=3.8
++++++++++++++-- pip
++++++++++++++-- patchelf
++++++++++++++-- pip:
++++++++++++++-    - -f https://download.pytorch.org/whl/torch_stable.html
++++++++++++++-    - numpy
++++++++++++++-    - wheel==0.38.4
++++++++++++++-    - setuptools==65.5.0
++++++++++++++-    - gym==0.20.0
++++++++++++++-    - matplotlib==3.3.4
++++++++++++++-    - torch==1.9.1+cu111
++++++++++++++-    - typed-argument-parser
++++++++++++++-    # - git+https://github.com/Farama-Foundation/d4rl@f2a05c0d66722499bf8031b094d9af3aea7c372b#egg=d4rl
++++++++++++++-    - scikit-image==0.17.2
++++++++++++++-    - scikit-video==1.1.11
++++++++++++++-    - gitpython
++++++++++++++-    - os
++++++++++++++-    - d3rlpy
++++++++++++++-    - pyclustering
++++++++++++++-    - moviepy
++++++++++++++-    - scipy
++++++++++++++-    - scikit-learn
++++++++++++++diff --git a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json
++++++++++++++index 18993c1..35714c2 100644
++++++++++++++--- a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json
+++++++++++++++++ b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json
++++++++++++++@@ -1,23 +1,23 @@
++++++++++++++ {
++++++++++++++     "add_extras": {
++++++++++++++         "_type": "python_object (type = method)",
++++++++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgfhpRSlC4="
+++++++++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMB2xvZ2Jhc2WUjAVsb2dzL5SMCWdwdF9lcG9jaJSMBmxhdGVzdJSMCHZpc19mcmVxlEsyjAZwcmVmaXiUjA9wbGFucy9kZWZhdWx0cy+UjAtyZWFkX2NvbmZpZ5RoAmgGaA+GlFKUjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwIbl9leHBhbmSUSwKMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjApnZXRfY29tbWl0lGgCaAZoGYaUUpSMCmFkZF9leHRyYXOUaAJoBmgchpRSlIwFbWtkaXKUaAJoBmgfhpRSlIwXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMBmRldmljZZSMA2NwdZSMBWtfb2JzlEsBjAZzdWZmaXiUjAEwlIwJcGxhbl9mcmVxlEsBjApiZWFtX3dpZHRolEsgjAlzYXZlX2RpZmaUaAJoBmgqhpRSlIwHZGF0YXNldJSMFWhhbGZjaGVldGFoLW1lZGl1bS12MpSMEWdlbmVyYXRlX2V4cF9uYW1llGgCaAZoL4aUUpSMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMB2NkZl9hY3SURz/jMzMzMzMzjAhzYXZlcGF0aJSMO2xvZ3MvaGFsZmNoZWV0YWgtbWVkaXVtLXYyL3BsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMi8wlIwFa19hY3SUTowKcGVyY2VudGlsZZSMBG1lYW6UjAhleHBfbmFtZZSMHnBsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMpSMB2NkZl9vYnOUTowHaG9yaXpvbpRLBYwOcHJlZml4X2NvbnRleHSUiIwIc2V0X3NlZWSUaAJoBmg/hpRSlIwHdmVyYm9zZZSIdWJoHIaUUpQu"
++++++++++++++     },
++++++++++++++     "beam_width": 32,
++++++++++++++     "cdf_act": 0.6,
++++++++++++++     "cdf_obs": null,
++++++++++++++-    "commit": "cca8d898e10f9f6102a8c33dac7758e2993dfc60 halfcheetah-xrl",
+++++++++++++++    "commit": "f77a194996872879bd0414fcb3dbc15b3da29cb8 main",
++++++++++++++     "config": "config.offline",
++++++++++++++     "dataset": "halfcheetah-medium-v2",
++++++++++++++     "device": "cpu",
++++++++++++++     "exp_name": "plans/defaults/freq1_H5_beam32",
++++++++++++++     "generate_exp_name": {
++++++++++++++         "_type": "python_object (type = method)",
++++++++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgxhpRSlC4="
+++++++++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMB2xvZ2Jhc2WUjAVsb2dzL5SMCWdwdF9lcG9jaJSMBmxhdGVzdJSMCHZpc19mcmVxlEsyjAZwcmVmaXiUjA9wbGFucy9kZWZhdWx0cy+UjAtyZWFkX2NvbmZpZ5RoAmgGaA+GlFKUjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwIbl9leHBhbmSUSwKMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjApnZXRfY29tbWl0lGgCaAZoGYaUUpSMCmFkZF9leHRyYXOUaAJoBmgchpRSlIwFbWtkaXKUaAJoBmgfhpRSlIwXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMBmRldmljZZSMA2NwdZSMBWtfb2JzlEsBjAZzdWZmaXiUjAEwlIwJcGxhbl9mcmVxlEsBjApiZWFtX3dpZHRolEsgjAlzYXZlX2RpZmaUaAJoBmgqhpRSlIwHZGF0YXNldJSMFWhhbGZjaGVldGFoLW1lZGl1bS12MpSMEWdlbmVyYXRlX2V4cF9uYW1llGgCaAZoL4aUUpSMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMB2NkZl9hY3SURz/jMzMzMzMzjAhzYXZlcGF0aJSMO2xvZ3MvaGFsZmNoZWV0YWgtbWVkaXVtLXYyL3BsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMi8wlIwFa19hY3SUTowKcGVyY2VudGlsZZSMBG1lYW6UjAhleHBfbmFtZZSMHnBsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMpSMB2NkZl9vYnOUTowHaG9yaXpvbpRLBYwOcHJlZml4X2NvbnRleHSUiIwIc2V0X3NlZWSUaAJoBmg/hpRSlIwHdmVyYm9zZZSIdWJoL4aUUpQu"
++++++++++++++     },
++++++++++++++     "get_commit": {
++++++++++++++         "_type": "python_object (type = method)",
++++++++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmglhpRSlC4="
+++++++++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMB2xvZ2Jhc2WUjAVsb2dzL5SMCWdwdF9lcG9jaJSMBmxhdGVzdJSMCHZpc19mcmVxlEsyjAZwcmVmaXiUjA9wbGFucy9kZWZhdWx0cy+UjAtyZWFkX2NvbmZpZ5RoAmgGaA+GlFKUjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwIbl9leHBhbmSUSwKMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjApnZXRfY29tbWl0lGgCaAZoGYaUUpSMCmFkZF9leHRyYXOUaAJoBmgchpRSlIwFbWtkaXKUaAJoBmgfhpRSlIwXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMBmRldmljZZSMA2NwdZSMBWtfb2JzlEsBjAZzdWZmaXiUjAEwlIwJcGxhbl9mcmVxlEsBjApiZWFtX3dpZHRolEsgjAlzYXZlX2RpZmaUaAJoBmgqhpRSlIwHZGF0YXNldJSMFWhhbGZjaGVldGFoLW1lZGl1bS12MpSMEWdlbmVyYXRlX2V4cF9uYW1llGgCaAZoL4aUUpSMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMB2NkZl9hY3SURz/jMzMzMzMzjAhzYXZlcGF0aJSMO2xvZ3MvaGFsZmNoZWV0YWgtbWVkaXVtLXYyL3BsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMi8wlIwFa19hY3SUTowKcGVyY2VudGlsZZSMBG1lYW6UjAhleHBfbmFtZZSMHnBsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMpSMB2NkZl9vYnOUTowHaG9yaXpvbpRLBYwOcHJlZml4X2NvbnRleHSUiIwIc2V0X3NlZWSUaAJoBmg/hpRSlIwHdmVyYm9zZZSIdWJoGYaUUpQu"
++++++++++++++     },
++++++++++++++     "gpt_epoch": "latest",
++++++++++++++     "gpt_loadpath": "gpt/pretrained",
++++++++++++++@@ -28,7 +28,7 @@
++++++++++++++     "max_context_transitions": 5,
++++++++++++++     "mkdir": {
++++++++++++++         "_type": "python_object (type = method)",
++++++++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgihpRSlC4="
+++++++++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMB2xvZ2Jhc2WUjAVsb2dzL5SMCWdwdF9lcG9jaJSMBmxhdGVzdJSMCHZpc19mcmVxlEsyjAZwcmVmaXiUjA9wbGFucy9kZWZhdWx0cy+UjAtyZWFkX2NvbmZpZ5RoAmgGaA+GlFKUjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwIbl9leHBhbmSUSwKMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjApnZXRfY29tbWl0lGgCaAZoGYaUUpSMCmFkZF9leHRyYXOUaAJoBmgchpRSlIwFbWtkaXKUaAJoBmgfhpRSlIwXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMBmRldmljZZSMA2NwdZSMBWtfb2JzlEsBjAZzdWZmaXiUjAEwlIwJcGxhbl9mcmVxlEsBjApiZWFtX3dpZHRolEsgjAlzYXZlX2RpZmaUaAJoBmgqhpRSlIwHZGF0YXNldJSMFWhhbGZjaGVldGFoLW1lZGl1bS12MpSMEWdlbmVyYXRlX2V4cF9uYW1llGgCaAZoL4aUUpSMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMB2NkZl9hY3SURz/jMzMzMzMzjAhzYXZlcGF0aJSMO2xvZ3MvaGFsZmNoZWV0YWgtbWVkaXVtLXYyL3BsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMi8wlIwFa19hY3SUTowKcGVyY2VudGlsZZSMBG1lYW6UjAhleHBfbmFtZZSMHnBsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMpSMB2NkZl9vYnOUTowHaG9yaXpvbpRLBYwOcHJlZml4X2NvbnRleHSUiIwIc2V0X3NlZWSUaAJoBmg/hpRSlIwHdmVyYm9zZZSIdWJoH4aUUpQu"
++++++++++++++     },
++++++++++++++     "n_expand": 2,
++++++++++++++     "percentile": "mean",
++++++++++++++@@ -37,24 +37,24 @@
++++++++++++++     "prefix_context": true,
++++++++++++++     "read_config": {
++++++++++++++         "_type": "python_object (type = method)",
++++++++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgZhpRSlC4="
+++++++++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMB2xvZ2Jhc2WUjAVsb2dzL5SMCWdwdF9lcG9jaJSMBmxhdGVzdJSMCHZpc19mcmVxlEsyjAZwcmVmaXiUjA9wbGFucy9kZWZhdWx0cy+UjAtyZWFkX2NvbmZpZ5RoAmgGaA+GlFKUjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwIbl9leHBhbmSUSwKMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjApnZXRfY29tbWl0lGgCaAZoGYaUUpSMCmFkZF9leHRyYXOUaAJoBmgchpRSlIwFbWtkaXKUaAJoBmgfhpRSlIwXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMBmRldmljZZSMA2NwdZSMBWtfb2JzlEsBjAZzdWZmaXiUjAEwlIwJcGxhbl9mcmVxlEsBjApiZWFtX3dpZHRolEsgjAlzYXZlX2RpZmaUaAJoBmgqhpRSlIwHZGF0YXNldJSMFWhhbGZjaGVldGFoLW1lZGl1bS12MpSMEWdlbmVyYXRlX2V4cF9uYW1llGgCaAZoL4aUUpSMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMB2NkZl9hY3SURz/jMzMzMzMzjAhzYXZlcGF0aJSMO2xvZ3MvaGFsZmNoZWV0YWgtbWVkaXVtLXYyL3BsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMi8wlIwFa19hY3SUTowKcGVyY2VudGlsZZSMBG1lYW6UjAhleHBfbmFtZZSMHnBsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMpSMB2NkZl9vYnOUTowHaG9yaXpvbpRLBYwOcHJlZml4X2NvbnRleHSUiIwIc2V0X3NlZWSUaAJoBmg/hpRSlIwHdmVyYm9zZZSIdWJoD4aUUpQu"
++++++++++++++     },
++++++++++++++     "renderer": "Renderer",
++++++++++++++     "reproducibility": {
++++++++++++++         "command_line": "python scripts/xrl_v2.py --dataset halfcheetah-medium-v2 --gpt_loadpath gpt/pretrained",
++++++++++++++         "git_has_uncommitted_changes": true,
++++++++++++++         "git_root": "/home/colin/Desktop/FACT/FACT_assignment",
++++++++++++++-        "git_url": "https://github.com/fclio/FACT_assignment/tree/cca8d898e10f9f6102a8c33dac7758e2993dfc60",
++++++++++++++-        "time": "Fri Feb  2 11:39:41 2024"
+++++++++++++++        "git_url": "https://github.com/fclio/FACT_assignment/tree/f77a194996872879bd0414fcb3dbc15b3da29cb8",
+++++++++++++++        "time": "Sat Feb  3 20:37:26 2024"
++++++++++++++     },
++++++++++++++     "save_diff": {
++++++++++++++         "_type": "python_object (type = method)",
++++++++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1Ymg0hpRSlC4="
+++++++++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMB2xvZ2Jhc2WUjAVsb2dzL5SMCWdwdF9lcG9jaJSMBmxhdGVzdJSMCHZpc19mcmVxlEsyjAZwcmVmaXiUjA9wbGFucy9kZWZhdWx0cy+UjAtyZWFkX2NvbmZpZ5RoAmgGaA+GlFKUjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwIbl9leHBhbmSUSwKMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjApnZXRfY29tbWl0lGgCaAZoGYaUUpSMCmFkZF9leHRyYXOUaAJoBmgchpRSlIwFbWtkaXKUaAJoBmgfhpRSlIwXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMBmRldmljZZSMA2NwdZSMBWtfb2JzlEsBjAZzdWZmaXiUjAEwlIwJcGxhbl9mcmVxlEsBjApiZWFtX3dpZHRolEsgjAlzYXZlX2RpZmaUaAJoBmgqhpRSlIwHZGF0YXNldJSMFWhhbGZjaGVldGFoLW1lZGl1bS12MpSMEWdlbmVyYXRlX2V4cF9uYW1llGgCaAZoL4aUUpSMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMB2NkZl9hY3SURz/jMzMzMzMzjAhzYXZlcGF0aJSMO2xvZ3MvaGFsZmNoZWV0YWgtbWVkaXVtLXYyL3BsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMi8wlIwFa19hY3SUTowKcGVyY2VudGlsZZSMBG1lYW6UjAhleHBfbmFtZZSMHnBsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMpSMB2NkZl9vYnOUTowHaG9yaXpvbpRLBYwOcHJlZml4X2NvbnRleHSUiIwIc2V0X3NlZWSUaAJoBmg/hpRSlIwHdmVyYm9zZZSIdWJoKoaUUpQu"
++++++++++++++     },
++++++++++++++     "savepath": "logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0",
++++++++++++++     "set_seed": {
++++++++++++++         "_type": "python_object (type = method)",
++++++++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgqhpRSlC4="
+++++++++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMB2xvZ2Jhc2WUjAVsb2dzL5SMCWdwdF9lcG9jaJSMBmxhdGVzdJSMCHZpc19mcmVxlEsyjAZwcmVmaXiUjA9wbGFucy9kZWZhdWx0cy+UjAtyZWFkX2NvbmZpZ5RoAmgGaA+GlFKUjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwIbl9leHBhbmSUSwKMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjApnZXRfY29tbWl0lGgCaAZoGYaUUpSMCmFkZF9leHRyYXOUaAJoBmgchpRSlIwFbWtkaXKUaAJoBmgfhpRSlIwXbWF4X2NvbnRleHRfdHJhbnNpdGlvbnOUSwWMBmRldmljZZSMA2NwdZSMBWtfb2JzlEsBjAZzdWZmaXiUjAEwlIwJcGxhbl9mcmVxlEsBjApiZWFtX3dpZHRolEsgjAlzYXZlX2RpZmaUaAJoBmgqhpRSlIwHZGF0YXNldJSMFWhhbGZjaGVldGFoLW1lZGl1bS12MpSMEWdlbmVyYXRlX2V4cF9uYW1llGgCaAZoL4aUUpSMBmNvbW1pdJSMLWY3N2ExOTQ5OTY4NzI4NzliZDA0MTRmY2IzZGJjMTViM2RhMjljYjggbWFpbpSMB2NkZl9hY3SURz/jMzMzMzMzjAhzYXZlcGF0aJSMO2xvZ3MvaGFsZmNoZWV0YWgtbWVkaXVtLXYyL3BsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMi8wlIwFa19hY3SUTowKcGVyY2VudGlsZZSMBG1lYW6UjAhleHBfbmFtZZSMHnBsYW5zL2RlZmF1bHRzL2ZyZXExX0g1X2JlYW0zMpSMB2NkZl9vYnOUTowHaG9yaXpvbpRLBYwOcHJlZml4X2NvbnRleHSUiIwIc2V0X3NlZWSUaAJoBmg/hpRSlIwHdmVyYm9zZZSIdWJoP4aUUpQu"
++++++++++++++     },
++++++++++++++     "suffix": "0",
++++++++++++++     "verbose": true,
++++++++++++++diff --git a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt
++++++++++++++index 2c00d59..ae6d79f 100644
++++++++++++++--- a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt
+++++++++++++++++ b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/diff.txt
++++++++++++++@@ -1,1250 +1,213 @@
++++++++++++++-diff --git a/.gitignore b/.gitignore
++++++++++++++-index 30d226d..a18312a 100644
++++++++++++++---- a/.gitignore
++++++++++++++-+++ b/.gitignore
++++++++++++++-@@ -1,6 +1,5 @@
++++++++++++++- __pycache__/
++++++++++++++- *.mp4
++++++++++++++--halfcheetah/logs/
++++++++++++++- .ipynb_checkpoints/
++++++++++++++- __pycache__/
++++++++++++++- Open Notebook.onetoc2 
++++++++++++++-diff --git a/gridworld/gridworld_expts.ipynb b/gridworld/gridworld_expts.ipynb
++++++++++++++-index e2a261b..bd0b720 100644
++++++++++++++---- a/gridworld/gridworld_expts.ipynb
++++++++++++++-+++ b/gridworld/gridworld_expts.ipynb
++++++++++++++-@@ -1570,7 +1570,7 @@
++++++++++++++-    "name": "python",
++++++++++++++-    "nbconvert_exporter": "python",
++++++++++++++-    "pygments_lexer": "ipython3",
++++++++++++++--   "version": "3.8.10"
++++++++++++++-+   "version": "3.10.12"
++++++++++++++-   }
++++++++++++++-  },
++++++++++++++-  "nbformat": 4,
++++++++++++++-diff --git a/halfcheetah/clusters.npy b/halfcheetah/clusters.npy
+++++++++++++++diff --git a/halfcheetah/environment.yml b/halfcheetah/environment.yml
++++++++++++++ deleted file mode 100644
++++++++++++++-index 3c198aa..0000000
++++++++++++++-Binary files a/halfcheetah/clusters.npy and /dev/null differ
++++++++++++++-diff --git a/halfcheetah/embeddings.npy b/halfcheetah/embeddings.npy
++++++++++++++-deleted file mode 100644
++++++++++++++-index 912d68c..0000000
++++++++++++++-Binary files a/halfcheetah/embeddings.npy and /dev/null differ
++++++++++++++-diff --git a/halfcheetah/halfcheetah.py b/halfcheetah/halfcheetah.py
++++++++++++++-deleted file mode 100644
++++++++++++++-index 626c42d..0000000
++++++++++++++---- a/halfcheetah/halfcheetah.py
++++++++++++++-+++ /dev/null
++++++++++++++-@@ -1,112 +0,0 @@
++++++++++++++--import gym
++++++++++++++--import d4rl # Import required to register environments, you may need to also import the submodule
++++++++++++++--import numpy as np
++++++++++++++--import d3rlpy
++++++++++++++--
++++++++++++++--def main():
++++++++++++++--    dataset_d3, env = d3rlpy.datasets.get_dataset("halfcheetah-medium-v2")
++++++++++++++--
++++++++++++++--    print(dataset_d3.observations.shape)
++++++++++++++--    print(dataset_d3.actions.shape)
++++++++++++++--    print(dataset_d3.rewards.shape)
++++++++++++++--        # print(dataset_d3.next_observations.shape)
++++++++++++++--    print(dataset_d3.terminals.shape)
++++++++++++++--    print(dataset_d3.terminals.sum()) # no
++++++++++++++--
++++++++++++++--    env = gym.make('halfcheetah-medium-v2')
++++++++++++++--    dataset_d4 = d4rl.qlearning_dataset(env)
++++++++++++++--
++++++++++++++--    print(dataset_d4['observations'].shape)
++++++++++++++--    print(dataset_d4['rewards'].shape)
++++++++++++++--    print(dataset_d4['terminals'].shape)
++++++++++++++--    print(dataset_d4['actions'].shape)
++++++++++++++--
++++++++++++++--    print(dataset_d4['rewards'][1])
++++++++++++++--    print(dataset_d3.rewards[1])
++++++++++++++--
++++++++++++++--
++++++++++++++--    print(np.allclose(dataset_d3.actions[100], dataset_d4['actions'][100]))
++++++++++++++--
++++++++++++++--    for j in range(1000):
++++++++++++++--        for i in range(999):
++++++++++++++--            if dataset_d4['rewards'][j * 999 + i] != dataset_d3.rewards[j * 1000 + i]: print("yo", i)
++++++++++++++--        # if not np.allclose(dataset_d3.observations[i], dataset_d4['observations'][i]): print('obs ongelijk')
++++++++++++++--        # if not np.allclose(dataset_d3.rewards[i], dataset_d4['rewards'][i]): print('obs ongelijk')
++++++++++++++--        # if not np.allclose(dataset_d3.actions[i], dataset_d4['actions'][i]): print('obs ongelijk')
++++++++++++++--
++++++++++++++--    sac = d3rlpy.algos.SAC(
++++++++++++++--        actor_learning_rate=3e-4,
++++++++++++++--        critic_learning_rate=3e-4,
++++++++++++++--        temp_learning_rate=3e-4,
++++++++++++++--        batch_size=256)
++++++++++++++--
++++++++++++++--    print(sac)
++++++++++++++--    sac.fit(dataset_d3, n_steps=10000)
++++++++++++++--
++++++++++++++--    actions = sac.predict(dataset_d3.observations[0])
++++++++++++++--
++++++++++++++--    print(actions)
++++++++++++++--
++++++++++++++--
++++++++++++++--    return
++++++++++++++--    print('yo!')
++++++++++++++--
++++++++++++++--    # Create the environment
++++++++++++++--    env = gym.make('halfcheetah-medium-v2')
++++++++++++++--
++++++++++++++--    # d4rl abides by the OpenAI gym interface
++++++++++++++--    env.reset()
++++++++++++++--    env.step(env.action_space.sample())
++++++++++++++--
++++++++++++++--    # Each task is associated with a dataset
++++++++++++++--    # dataset contains observations, actions, rewards, terminals, and infos
++++++++++++++--    # dataset = env.get_dataset()
++++++++++++++--    dataset = d4rl.qlearning_dataset(env)
++++++++++++++--
++++++++++++++--    print(dataset.keys()) # An N x dim_observation Numpy array of observations
++++++++++++++--    print(dataset['rewards'].shape) # An N x dim_observation Numpy array of observations
++++++++++++++--
++++++++++++++--
++++++++++++++--    first_traj = []
++++++++++++++--    for i in range(50000):
++++++++++++++--        if not np.allclose(dataset['next_observations'][i],dataset['observations'][i+1]): print("yo", i, dataset['terminals'][i])
++++++++++++++--        # if dataset['terminals'][i] == True:
++++++++++++++--        #     print('traj ended at', i)
++++++++++++++--        #     break
++++++++++++++--
++++++++++++++--        # first_traj.append((dataset['observations'][i],
++++++++++++++--        #                    dataset['actions'][i],
++++++++++++++--        #                    dataset['rewards'][i],
++++++++++++++--        #                    dataset['next_observations'][i]))
++++++++++++++--    # print(first_traj)
++++++++++++++--
++++++++++++++--
++++++++++++++--    # print(dataset['rewards'].shape) # An N x dim_observation Numpy array of observations
++++++++++++++--
++++++++++++++--    # Alternatively, use d4rl.qlearning_dataset which
++++++++++++++--    # also adds next_observations.
++++++++++++++--
++++++++++++++--    # import d3rlpy
++++++++++++++--
++++++++++++++--    # # dataset, env = d3rlpy.datasets.get_dataset("halfcheetah-medium")
++++++++++++++--
++++++++++++++--    # # prepare algorithm
++++++++++++++--    # # sac = d3rlpy.algos.SAC().create(device="cpu")
++++++++++++++--
++++++++++++++--    # sac = d3rlpy.algos.SACConfig(
++++++++++++++--    #     actor_learning_rate=3e-4,
++++++++++++++--    #     critic_learning_rate=3e-4,
++++++++++++++--    #     temp_learning_rate=3e-4,
++++++++++++++--    #     batch_size=256,
++++++++++++++--    # ).create(device='cpu')
++++++++++++++--
++++++++++++++--
++++++++++++++--    # # train offline
++++++++++++++--    # # sac.fit(dataset, n_steps=1000)
++++++++++++++--
++++++++++++++--
++++++++++++++--    # # ready to control
++++++++++++++--    # actions = sac.predict(0)
++++++++++++++--
++++++++++++++--if __name__ == "__main__":
++++++++++++++--    main()
++++++++++++++-\ No newline at end of file
++++++++++++++-diff --git a/halfcheetah/pca.py.npy b/halfcheetah/pca.py.npy
++++++++++++++-deleted file mode 100644
++++++++++++++-index bb19150..0000000
++++++++++++++-Binary files a/halfcheetah/pca.py.npy and /dev/null differ
++++++++++++++-diff --git a/halfcheetah/plotting/bar.png b/halfcheetah/plotting/bar.png
++++++++++++++-deleted file mode 100644
++++++++++++++-index 3679667..0000000
++++++++++++++-Binary files a/halfcheetah/plotting/bar.png and /dev/null differ
++++++++++++++-diff --git a/halfcheetah/plotting/plot.py b/halfcheetah/plotting/plot.py
++++++++++++++-deleted file mode 100644
++++++++++++++-index 163d0e4..0000000
++++++++++++++---- a/halfcheetah/plotting/plot.py
++++++++++++++-+++ /dev/null
++++++++++++++-@@ -1,74 +0,0 @@
++++++++++++++--import numpy as np
++++++++++++++--import matplotlib
++++++++++++++--import matplotlib.pyplot as plt
++++++++++++++--import pdb
++++++++++++++--
++++++++++++++--from plotting.scores import means
++++++++++++++--
++++++++++++++--class Colors:
++++++++++++++--	grey = '#B4B4B4'
++++++++++++++--	gold = '#F6C781'
++++++++++++++--	red = '#EC7C7D'
++++++++++++++--	blue = '#70ABCC'
++++++++++++++--
++++++++++++++--LABELS = {
++++++++++++++--	# 'BC': 'Behavior\nCloning',
++++++++++++++--	# 'MBOP': 'Model-Based\nOffline Planning',
++++++++++++++--	# 'BRAC': 'Behavior-Reg.\nActor-Critic',
++++++++++++++--	# 'CQL': 'Conservative\nQ-Learning',
++++++++++++++--}
++++++++++++++--
++++++++++++++--def get_mean(results, exclude=None):
++++++++++++++--	'''
++++++++++++++--		results : { environment: score, ... }
++++++++++++++--	'''
++++++++++++++--	filtered = {
++++++++++++++--		k: v for k, v in results.items()
++++++++++++++--		if (not exclude) or (exclude and exclude not in k)
++++++++++++++--	}
++++++++++++++--	return np.mean(list(filtered.values()))
++++++++++++++--
++++++++++++++--if __name__ == '__main__':
++++++++++++++--
++++++++++++++--	#################
++++++++++++++--	## latex
++++++++++++++--	#################
++++++++++++++--	matplotlib.rc('font', **{'family': 'serif', 'serif': ['Computer Modern']})
++++++++++++++--	matplotlib.rc('text', usetex=True)
++++++++++++++--	matplotlib.rcParams['text.latex.preamble']=[r"\usepackage{amsmath}"]
++++++++++++++--	#################
++++++++++++++--
++++++++++++++--	fig = plt.gcf()
++++++++++++++--	ax = plt.gca()
++++++++++++++--	fig.set_size_inches(7.5, 2.5)
++++++++++++++--
++++++++++++++--	means = {k: get_mean(v, exclude='ant') for k, v in means.items()}
++++++++++++++--	print(means)
++++++++++++++--
++++++++++++++--	algs = ['BC', 'MBOP', 'BRAC', 'CQL', 'Decision\nTransformer', 'Trajectory\nTransformer']
++++++++++++++--	vals = [means[alg] for alg in algs]
++++++++++++++--
++++++++++++++--	colors = [
++++++++++++++--		Colors.grey, Colors.gold,
++++++++++++++--		Colors.red, Colors.red, Colors.blue, Colors.blue
++++++++++++++--	]
++++++++++++++--
++++++++++++++--	labels = [LABELS.get(alg, alg) for alg in algs]
++++++++++++++--	plt.bar(labels, vals, color=colors, edgecolor=Colors.gold, lw=0)
++++++++++++++--	plt.ylabel('Average normalized return', labelpad=15)
++++++++++++++--	# plt.title('Offline RL Results')
++++++++++++++--
++++++++++++++--	legend_labels = ['Behavior Cloning', 'Trajectory Optimization', 'Temporal Difference', 'Sequence Modeling']
++++++++++++++--	colors = [Colors.grey, Colors.gold, Colors.red, Colors.blue]
++++++++++++++--	handles = [plt.Rectangle((0,0),1,1, color=color) for label, color in zip(legend_labels, colors)]
++++++++++++++--	plt.legend(handles, legend_labels, ncol=4,
++++++++++++++--		bbox_to_anchor=(1.07, -.18), fancybox=False, framealpha=0, shadow=False, columnspacing=1.5, handlelength=1.5)
++++++++++++++--
++++++++++++++--	matplotlib.rcParams['hatch.linewidth'] = 7.5
++++++++++++++--	# ax.patches[-1].set_hatch('/')
++++++++++++++--
++++++++++++++--	ax.spines['right'].set_visible(False)
++++++++++++++--	ax.spines['top'].set_visible(False)
++++++++++++++--
++++++++++++++--	# plt.savefig('plotting/bar.pdf', bbox_inches='tight')
++++++++++++++--	plt.savefig('plotting/bar.png', bbox_inches='tight', dpi=500)
++++++++++++++-diff --git a/halfcheetah/plotting/read_results.py b/halfcheetah/plotting/read_results.py
++++++++++++++-deleted file mode 100644
++++++++++++++-index 5a5fb62..0000000
++++++++++++++---- a/halfcheetah/plotting/read_results.py
+++++++++++++++index e81007c..0000000
+++++++++++++++--- a/halfcheetah/environment.yml
++++++++++++++ +++ /dev/null
++++++++++++++-@@ -1,70 +0,0 @@
++++++++++++++--import os
++++++++++++++--import glob
++++++++++++++--import numpy as np
++++++++++++++--import json
++++++++++++++--import pdb
++++++++++++++--
++++++++++++++--import trajectory.utils as utils
++++++++++++++--
++++++++++++++--DATASETS = [
++++++++++++++--	f'{env}-{buffer}'
++++++++++++++--	for env in ['hopper', 'walker2d', 'halfcheetah', 'ant']
++++++++++++++--	for buffer in ['medium-expert-v2', 'medium-v2', 'medium-replay-v2']
++++++++++++++--]
++++++++++++++--
++++++++++++++--LOGBASE = 'logs'
++++++++++++++--TRIAL = '*'
++++++++++++++--EXP_NAME = 'plans/pretrained'
++++++++++++++--
++++++++++++++--def load_results(paths):
++++++++++++++--	'''
++++++++++++++--		paths : path to directory containing experiment trials
++++++++++++++--	'''
++++++++++++++--	scores = []
++++++++++++++--	for i, path in enumerate(sorted(paths)):
++++++++++++++--		score = load_result(path)
++++++++++++++--		if score is None:
++++++++++++++--			print(f'Skipping {path}')
++++++++++++++--			continue
++++++++++++++--		scores.append(score)
++++++++++++++--
++++++++++++++--		suffix = path.split('/')[-1]
++++++++++++++--
++++++++++++++--	mean = np.mean(scores)
++++++++++++++--	err = np.std(scores) / np.sqrt(len(scores))
++++++++++++++--	return mean, err, scores
++++++++++++++--
++++++++++++++--def load_result(path):
++++++++++++++--	'''
++++++++++++++--		path : path to experiment directory; expects `rollout.json` to be in directory
++++++++++++++--	'''
++++++++++++++--	fullpath = os.path.join(path, 'rollout.json')
++++++++++++++--	suffix = path.split('/')[-1]
++++++++++++++--
++++++++++++++--	if not os.path.exists(fullpath):
++++++++++++++--		return None
++++++++++++++--
++++++++++++++--	results = json.load(open(fullpath, 'rb'))
++++++++++++++--	score = results['score']
++++++++++++++--	return score * 100
++++++++++++++--
++++++++++++++--#######################
++++++++++++++--######## setup ########
++++++++++++++--#######################
++++++++++++++--
++++++++++++++--if __name__ == '__main__':
++++++++++++++--
++++++++++++++--	class Parser(utils.Parser):
++++++++++++++--	    dataset: str = None
++++++++++++++--
++++++++++++++--	args = Parser().parse_args()
++++++++++++++--
++++++++++++++--	for dataset in ([args.dataset] if args.dataset else DATASETS):
++++++++++++++--		subdirs = glob.glob(os.path.join(LOGBASE, dataset, EXP_NAME))
++++++++++++++--
++++++++++++++--		for subdir in subdirs:
++++++++++++++--			reldir = subdir.split('/')[-1]
++++++++++++++--			paths = glob.glob(os.path.join(subdir, TRIAL))
++++++++++++++--
++++++++++++++--			mean, err, scores = load_results(paths)
++++++++++++++--			print(f'{dataset.ljust(30)} | {subdir.ljust(50)} | {len(scores)} scores \n    {mean:.2f} +/- {err:.2f}\n')
++++++++++++++-diff --git a/halfcheetah/plotting/scores.py b/halfcheetah/plotting/scores.py
++++++++++++++-deleted file mode 100644
++++++++++++++-index f1917f7..0000000
++++++++++++++---- a/halfcheetah/plotting/scores.py
++++++++++++++-+++ /dev/null
++++++++++++++-@@ -1,123 +0,0 @@
++++++++++++++--means = {
++++++++++++++--	'Trajectory\nTransformer': {
++++++++++++++--		##
++++++++++++++--		'halfcheetah-medium-expert-v2': 95.0,
++++++++++++++--		'hopper-medium-expert-v2': 110.0,
++++++++++++++--		'walker2d-medium-expert-v2': 101.9,
++++++++++++++--		'ant-medium-expert-v2': 116.1,
++++++++++++++--		##
++++++++++++++--		'halfcheetah-medium-v2': 46.9,
++++++++++++++--		'hopper-medium-v2': 61.1,
++++++++++++++--		'walker2d-medium-v2': 79.0,
++++++++++++++--		'ant-medium-v2': 83.1,
++++++++++++++--		##
++++++++++++++--		'halfcheetah-medium-replay-v2': 41.9,
++++++++++++++--		'hopper-medium-replay-v2': 91.5,
++++++++++++++--		'walker2d-medium-replay-v2': 82.6,
++++++++++++++--		'ant-medium-replay-v2': 77.0,
++++++++++++++--	},
++++++++++++++--	'Decision\nTransformer': {
++++++++++++++--		##
++++++++++++++--		'halfcheetah-medium-expert-v2': 86.8,
++++++++++++++--		'hopper-medium-expert-v2': 107.6,
++++++++++++++--		'walker2d-medium-expert-v2': 108.1,
++++++++++++++--		##
++++++++++++++--		'halfcheetah-medium-v2': 42.6,
++++++++++++++--		'hopper-medium-v2': 67.6,
++++++++++++++--		'walker2d-medium-v2': 74.0,
++++++++++++++--		##
++++++++++++++--		'halfcheetah-medium-replay-v2': 36.6,
++++++++++++++--		'hopper-medium-replay-v2': 82.7,
++++++++++++++--		'walker2d-medium-replay-v2': 66.6,
++++++++++++++--	},
++++++++++++++--	'CQL': {
++++++++++++++--		##
++++++++++++++--		'halfcheetah-medium-expert-v2': 91.6,
++++++++++++++--		'hopper-medium-expert-v2': 105.4,
++++++++++++++--		'walker2d-medium-expert-v2': 108.8,
++++++++++++++--		##
++++++++++++++--		'halfcheetah-medium-v2': 44.0,
++++++++++++++--		'hopper-medium-v2': 58.5,
++++++++++++++--		'walker2d-medium-v2': 72.5,
++++++++++++++--		##
++++++++++++++--		'halfcheetah-medium-replay-v2': 45.5,
++++++++++++++--		'hopper-medium-replay-v2': 95.0,
++++++++++++++--		'walker2d-medium-replay-v2': 77.2,
++++++++++++++--	},
++++++++++++++--	'MOPO': {
++++++++++++++--		##
++++++++++++++--		'halfcheetah-medium-expert-v2': 63.3,
++++++++++++++--		'hopper-medium-expert-v2': 23.7,
++++++++++++++--		'walker2d-medium-expert-v2': 44.6,
++++++++++++++--		##
++++++++++++++--		'halfcheetah-medium-v2': 42.3,
++++++++++++++--		'hopper-medium-v2': 28.0,
++++++++++++++--		'walker2d-medium-v2': 17.8,
++++++++++++++--		##
++++++++++++++--		'halfcheetah-medium-replay-v2': 53.1,
++++++++++++++--		'hopper-medium-replay-v2': 67.5,
++++++++++++++--		'walker2d-medium-replay-v2':39.0,
++++++++++++++--	},
++++++++++++++--	'MBOP': {
++++++++++++++--		##
++++++++++++++--		'halfcheetah-medium-expert-v2': 105.9,
++++++++++++++--		'hopper-medium-expert-v2': 55.1,
++++++++++++++--		'walker2d-medium-expert-v2': 70.2,
++++++++++++++--		##
++++++++++++++--		'halfcheetah-medium-v2': 44.6,
++++++++++++++--		'hopper-medium-v2': 48.8,
++++++++++++++--		'walker2d-medium-v2': 41.0,
++++++++++++++--		##
++++++++++++++--		'halfcheetah-medium-replay-v2': 42.3,
++++++++++++++--		'hopper-medium-replay-v2': 12.4,
++++++++++++++--		'walker2d-medium-replay-v2': 9.7,
++++++++++++++--	},
++++++++++++++--	'BRAC': {
++++++++++++++--		##
++++++++++++++--		'halfcheetah-medium-expert-v2': 41.9,
++++++++++++++--		'hopper-medium-expert-v2': 0.9,
++++++++++++++--		'walker2d-medium-expert-v2': 81.6,
++++++++++++++--		##
++++++++++++++--		'halfcheetah-medium-v2': 46.3,
++++++++++++++--		'hopper-medium-v2': 31.3,
++++++++++++++--		'walker2d-medium-v2': 81.1,
++++++++++++++--		##
++++++++++++++--		'halfcheetah-medium-replay-v2': 47.7,
++++++++++++++--		'hopper-medium-replay-v2': 0.6,
++++++++++++++--		'walker2d-medium-replay-v2': 0.9,
++++++++++++++--	},
++++++++++++++--	'BC': {
++++++++++++++--		##
++++++++++++++--		'halfcheetah-medium-expert-v2': 59.9,
++++++++++++++--		'hopper-medium-expert-v2': 79.6,
++++++++++++++--		'walker2d-medium-expert-v2': 36.6,
++++++++++++++--		##
++++++++++++++--		'halfcheetah-medium-v2': 43.1,
++++++++++++++--		'hopper-medium-v2': 63.9,
++++++++++++++--		'walker2d-medium-v2': 77.3,
++++++++++++++--		##
++++++++++++++--		'halfcheetah-medium-replay-v2': 4.3,
++++++++++++++--		'hopper-medium-replay-v2': 27.6,
++++++++++++++--		'walker2d-medium-replay-v2': 36.9,
++++++++++++++--	},
++++++++++++++--}
++++++++++++++--
++++++++++++++--errors = {
++++++++++++++--	'Trajectory\nTransformer': {
++++++++++++++--		##
++++++++++++++--		'halfcheetah-medium-expert-v2': 0.2,
++++++++++++++--		'hopper-medium-expert-v2': 2.7,
++++++++++++++--		'walker2d-medium-expert-v2': 6.8,
++++++++++++++--		'ant-medium-expert-v2': 9.0,
++++++++++++++--		##
++++++++++++++--		'halfcheetah-medium-v2': 0.4,
++++++++++++++--		'hopper-medium-v2': 3.6,
++++++++++++++--		'walker2d-medium-v2': 2.8,
++++++++++++++--		'ant-medium-v2': 7.3,
++++++++++++++--		##
++++++++++++++--		'halfcheetah-medium-replay-v2': 2.5,
++++++++++++++--		'hopper-medium-replay-v2': 3.6,
++++++++++++++--		'walker2d-medium-replay-v2': 6.9,
++++++++++++++--		'ant-medium-replay-v2': 6.8,
++++++++++++++--	},
++++++++++++++--}
++++++++++++++-\ No newline at end of file
++++++++++++++-diff --git a/halfcheetah/plotting/table.py b/halfcheetah/plotting/table.py
++++++++++++++-deleted file mode 100644
++++++++++++++-index eae74e6..0000000
++++++++++++++---- a/halfcheetah/plotting/table.py
++++++++++++++-+++ /dev/null
++++++++++++++-@@ -1,127 +0,0 @@
++++++++++++++--import numpy as np
++++++++++++++--import pdb
++++++++++++++--
++++++++++++++--from plotting.plot import get_mean
++++++++++++++--from plotting.scores import (
++++++++++++++--	means as MEANS,
++++++++++++++--	errors as ERRORS,
++++++++++++++--)
++++++++++++++--
++++++++++++++--ALGORITHM_STRINGS = {
++++++++++++++--	'Trajectory\nTransformer': 'TT (Ours)',
++++++++++++++--	'Decision\nTransformer': 'DT',	
++++++++++++++--}
++++++++++++++--
++++++++++++++--BUFFER_STRINGS = {
++++++++++++++--	'medium-expert': 'Medium-Expert',
++++++++++++++--	'medium': 'Medium',
++++++++++++++--	'medium-replay': 'Medium-Replay',	
++++++++++++++--}
++++++++++++++--
++++++++++++++--ENVIRONMENT_STRINGS = {
++++++++++++++--	'halfcheetah': 'HalfCheetah',
++++++++++++++--	'hopper': 'Hopper',
++++++++++++++--	'walker2d': 'Walker2d',
++++++++++++++--	'ant': 'Ant',
++++++++++++++--}
++++++++++++++--
++++++++++++++--SHOW_ERRORS = ['Trajectory\nTransformer']
++++++++++++++--
++++++++++++++--def get_result(algorithm, buffer, environment, version='v2'):
++++++++++++++--	key = f'{environment}-{buffer}-{version}'
++++++++++++++--	mean = MEANS[algorithm].get(key, '-')
++++++++++++++--	if algorithm in SHOW_ERRORS:
++++++++++++++--		error = ERRORS[algorithm].get(key)
++++++++++++++--		return (mean, error)
++++++++++++++--	else:
++++++++++++++--		return mean
++++++++++++++--
++++++++++++++--def format_result(result):
++++++++++++++--	if type(result) == tuple:
++++++++++++++--		mean, std = result
++++++++++++++--		return f'${mean}$ \\scriptsize{{\\raisebox{{1pt}}{{$\\pm {std}$}}}}'
++++++++++++++--	else:
++++++++++++++--		return f'${result}$'
++++++++++++++--
++++++++++++++--def format_row(buffer, environment, results):
++++++++++++++--	buffer_str = BUFFER_STRINGS[buffer]
++++++++++++++--	environment_str = ENVIRONMENT_STRINGS[environment]
++++++++++++++--	results_str = ' & '.join(format_result(result) for result in results)
++++++++++++++--	row = f'{buffer_str} & {environment_str} & {results_str} \\\\ \n'
++++++++++++++--	return row
++++++++++++++--
++++++++++++++--def format_buffer_block(algorithms, buffer, environments):
++++++++++++++--	block_str = '\\midrule\n'
++++++++++++++--	for environment in environments:
++++++++++++++--		results = [get_result(alg, buffer, environment) for alg in algorithms]
++++++++++++++--		row_str = format_row(buffer, environment, results)
++++++++++++++--		block_str += row_str
++++++++++++++--	return block_str
++++++++++++++--
++++++++++++++--def format_algorithm(algorithm):
++++++++++++++--	algorithm_str = ALGORITHM_STRINGS.get(algorithm, algorithm)
++++++++++++++--	return f'\multicolumn{{1}}{{c}}{{\\bf {algorithm_str}}}'
++++++++++++++--
++++++++++++++--def format_algorithms(algorithms):
++++++++++++++--	return ' & '.join(format_algorithm(algorithm) for algorithm in algorithms)
++++++++++++++--
++++++++++++++--def format_averages(means, label):
++++++++++++++--	prefix = f'\\multicolumn{{2}}{{c}}{{\\bf Average ({label})}} & '
++++++++++++++--	formatted = ' & '.join(str(mean) for mean in means)
++++++++++++++--	return prefix + formatted
++++++++++++++--
++++++++++++++--def format_averages_block(algorithms):
++++++++++++++--	means_filtered = [np.round(get_mean(MEANS[algorithm], exclude='ant'), 1) for algorithm in algorithms]
++++++++++++++--	means_all = [np.round(get_mean(MEANS[algorithm], exclude=None), 1) for algorithm in algorithms]
++++++++++++++--
++++++++++++++--	means_all = [
++++++++++++++--		means
++++++++++++++--		if 'ant-medium-expert-v2' in MEANS[algorithm]
++++++++++++++--		else '$-$'
++++++++++++++--		for algorithm, means in zip(algorithms, means_all)
++++++++++++++--	]
++++++++++++++--
++++++++++++++--	formatted_filtered = format_averages(means_filtered, 'without Ant')
++++++++++++++--	formatted_all = format_averages(means_all, 'all settings')
++++++++++++++--
++++++++++++++--	formatted_block = (
++++++++++++++--		f'{formatted_filtered} \\hspace{{.6cm}} \\\\ \n'
++++++++++++++--		f'{formatted_all} \\hspace{{.6cm}} \\\\ \n'
++++++++++++++--	)
++++++++++++++--	return formatted_block
++++++++++++++--
++++++++++++++--def format_table(algorithms, buffers, environments):
++++++++++++++--	justify_str = 'll' + 'r' * len(algorithms)
++++++++++++++--	algorithm_str = format_algorithms(['Dataset', 'Environment'] + algorithms)
++++++++++++++--	averages_str = format_averages_block(algorithms)
++++++++++++++--	table_prefix = (
++++++++++++++--		'\\begin{table*}[h]\n'
++++++++++++++--		'\\centering\n'
++++++++++++++--		'\\small\n'
++++++++++++++--		f'\\begin{{tabular}}{{{justify_str}}}\n'
++++++++++++++--		'\\toprule\n'
++++++++++++++--		f'{algorithm_str} \\\\ \n'
++++++++++++++--	)
++++++++++++++--	table_suffix = (
++++++++++++++--		'\\midrule\n'
++++++++++++++--		f'{averages_str}'
++++++++++++++--		'\\bottomrule\n'
++++++++++++++--		'\\end{tabular}\n'
++++++++++++++--		'\\label{table:d4rl}\n'
++++++++++++++--		'\\end{table*}'
++++++++++++++--	)
++++++++++++++--	blocks = ''.join(format_buffer_block(algorithms, buffer, environments) for buffer in buffers)
++++++++++++++--	table = (
++++++++++++++--		f'{table_prefix}'
++++++++++++++--		f'{blocks}'
++++++++++++++--		f'{table_suffix}'
++++++++++++++--	)
++++++++++++++--	return table
++++++++++++++--
++++++++++++++--
++++++++++++++--algorithms =['BC', 'MBOP', 'BRAC', 'CQL',  'Decision\nTransformer', 'Trajectory\nTransformer']
++++++++++++++--buffers = ['medium-expert', 'medium', 'medium-replay']
++++++++++++++--environments = ['halfcheetah', 'hopper', 'walker2d', 'ant']
++++++++++++++--
++++++++++++++--table = format_table(algorithms, buffers, environments)
++++++++++++++--print(table)
++++++++++++++-diff --git a/halfcheetah/scripts/plan.py b/halfcheetah/scripts/plan.py
++++++++++++++-deleted file mode 100644
++++++++++++++-index f13d4cc..0000000
++++++++++++++---- a/halfcheetah/scripts/plan.py
++++++++++++++-+++ /dev/null
++++++++++++++-@@ -1,124 +0,0 @@
++++++++++++++--import json
++++++++++++++--import pdb
++++++++++++++--from os.path import join
++++++++++++++--
++++++++++++++--import trajectory.utils as utils
++++++++++++++--import trajectory.datasets as datasets
++++++++++++++--from trajectory.search import (
++++++++++++++--    beam_plan,
++++++++++++++--    make_prefix,
++++++++++++++--    extract_actions,
++++++++++++++--    update_context,
++++++++++++++--)
++++++++++++++--
++++++++++++++--class Parser(utils.Parser):
++++++++++++++--    dataset: str = 'halfcheetah-medium-expert-v2'
++++++++++++++--    config: str = 'config.offline'
++++++++++++++--
++++++++++++++--#######################
++++++++++++++--######## setup ########
++++++++++++++--#######################
++++++++++++++--
++++++++++++++--args = Parser().parse_args('plan')
++++++++++++++--
++++++++++++++--#######################
++++++++++++++--####### models ########
++++++++++++++--#######################
++++++++++++++--
++++++++++++++--dataset = utils.load_from_config(args.logbase, args.dataset, args.gpt_loadpath,
++++++++++++++--        'data_config.pkl')
++++++++++++++--
++++++++++++++--gpt, gpt_epoch = utils.load_model(args.logbase, args.dataset, args.gpt_loadpath,
++++++++++++++--        epoch=args.gpt_epoch, device=args.device)
++++++++++++++--
++++++++++++++--#######################
++++++++++++++--####### dataset #######
++++++++++++++--#######################
++++++++++++++--
++++++++++++++--env = datasets.load_environment(args.dataset)
++++++++++++++--print('yo')
++++++++++++++--renderer = utils.make_renderer(args)
++++++++++++++--timer = utils.timer.Timer()
++++++++++++++--
++++++++++++++--discretizer = dataset.discretizer
++++++++++++++--discount = dataset.discount
++++++++++++++--observation_dim = dataset.observation_dim
++++++++++++++--action_dim = dataset.action_dim
++++++++++++++--
++++++++++++++--value_fn = lambda x: discretizer.value_fn(x, args.percentile)
++++++++++++++--preprocess_fn = datasets.get_preprocess_fn(env.name)
++++++++++++++--
++++++++++++++--print('yo2')
++++++++++++++--
++++++++++++++--#######################
++++++++++++++--###### main loop ######
++++++++++++++--#######################
++++++++++++++--
++++++++++++++--observation = env.reset()
++++++++++++++--total_reward = 0
++++++++++++++--
++++++++++++++--## observations for rendering
++++++++++++++--rollout = [observation.copy()]
++++++++++++++--
++++++++++++++--## previous (tokenized) transitions for conditioning transformer
++++++++++++++--context = []
++++++++++++++--
++++++++++++++--T = env.max_episode_steps
++++++++++++++--for t in range(T):
++++++++++++++--
++++++++++++++--    observation = preprocess_fn(observation)
++++++++++++++--
++++++++++++++--    if t % args.plan_freq == 0:
++++++++++++++--        ## concatenate previous transitions and current observations to input to model
++++++++++++++--        prefix = make_prefix(discretizer, context, observation, args.prefix_context)
++++++++++++++--
++++++++++++++--        ## sample sequence from model beginning with `prefix`
++++++++++++++--        sequence = beam_plan(
++++++++++++++--            gpt, value_fn, prefix,
++++++++++++++--            args.horizon, args.beam_width, args.n_expand, observation_dim, action_dim,
++++++++++++++--            discount, args.max_context_transitions, verbose=args.verbose,
++++++++++++++--            k_obs=args.k_obs, k_act=args.k_act, cdf_obs=args.cdf_obs, cdf_act=args.cdf_act,
++++++++++++++--        )
++++++++++++++--
++++++++++++++--    else:
++++++++++++++--        sequence = sequence[1:]
++++++++++++++--
++++++++++++++--    ## [ horizon x transition_dim ] convert sampled tokens to continuous trajectory
++++++++++++++--    sequence_recon = discretizer.reconstruct(sequence)
++++++++++++++--
++++++++++++++--    ## [ action_dim ] index into sampled trajectory to grab first action
++++++++++++++--    action = extract_actions(sequence_recon, observation_dim, action_dim, t=0)
++++++++++++++--
++++++++++++++--    ## execute action in environment
++++++++++++++--    next_observation, reward, terminal, _ = env.step(action)
++++++++++++++--
++++++++++++++--    ## update return
++++++++++++++--    total_reward += reward
++++++++++++++--    score = env.get_normalized_score(total_reward)
++++++++++++++--
++++++++++++++--    ## update rollout observations and context transitions
++++++++++++++--    rollout.append(next_observation.copy())
++++++++++++++--    context = update_context(context, discretizer, observation, action, reward, args.max_context_transitions)
++++++++++++++--
++++++++++++++--    print(
++++++++++++++--        f'[ plan ] t: {t} / {T} | r: {reward:.2f} | R: {total_reward:.2f} | score: {score:.4f} | '
++++++++++++++--        f'time: {timer():.2f} | {args.dataset} | {args.exp_name} | {args.suffix}\n'
++++++++++++++--    )
++++++++++++++--
++++++++++++++--    ## visualization
++++++++++++++--    if t % args.vis_freq == 0 or terminal or t == T:
++++++++++++++--
++++++++++++++--        ## save current plan
++++++++++++++--        renderer.render_plan(join(args.savepath, f'{t}_plan.mp4'), sequence_recon, env.state_vector())
++++++++++++++--
++++++++++++++--        ## save rollout thus far
++++++++++++++--        renderer.render_rollout(join(args.savepath, f'rollout.mp4'), rollout, fps=80)
++++++++++++++--
++++++++++++++--    if terminal: break
++++++++++++++--
++++++++++++++--    observation = next_observation
++++++++++++++--
++++++++++++++--## save result as a json file
++++++++++++++--json_path = join(args.savepath, 'rollout.json')
++++++++++++++--json_data = {'score': score, 'step': t, 'return': total_reward, 'term': terminal, 'gpt_epoch': gpt_epoch}
++++++++++++++--json.dump(json_data, open(json_path, 'w'), indent=2, sort_keys=True)
++++++++++++++-diff --git a/halfcheetah/scripts/train.py b/halfcheetah/scripts/train.py
++++++++++++++-deleted file mode 100644
++++++++++++++-index 04af8d7..0000000
++++++++++++++---- a/halfcheetah/scripts/train.py
++++++++++++++-+++ /dev/null
++++++++++++++-@@ -1,122 +0,0 @@
++++++++++++++--import os
++++++++++++++--import numpy as np
++++++++++++++--import torch
++++++++++++++--import pdb
++++++++++++++--
++++++++++++++--import trajectory.utils as utils
++++++++++++++--import trajectory.datasets as datasets
++++++++++++++--from trajectory.models.transformers import GPT
++++++++++++++--
++++++++++++++--
++++++++++++++--class Parser(utils.Parser):
++++++++++++++--    dataset: str = 'halfcheetah-medium-expert-v2'
++++++++++++++--    config: str = 'config.offline'
++++++++++++++--
++++++++++++++--#######################
++++++++++++++--######## setup ########
++++++++++++++--#######################
++++++++++++++--
++++++++++++++--args = Parser().parse_args('train')
++++++++++++++--
++++++++++++++--#######################
++++++++++++++--####### dataset #######
++++++++++++++--#######################
++++++++++++++--
++++++++++++++--env = datasets.load_environment(args.dataset)
++++++++++++++--
++++++++++++++--sequence_length = args.subsampled_sequence_length * args.step
++++++++++++++--
++++++++++++++--dataset_config = utils.Config(
++++++++++++++--    datasets.DiscretizedDataset,
++++++++++++++--    savepath=(args.savepath, 'data_config.pkl'),
++++++++++++++--    env=args.dataset,
++++++++++++++--    N=args.N,
++++++++++++++--    penalty=args.termination_penalty,
++++++++++++++--    sequence_length=sequence_length,
++++++++++++++--    step=args.step,
++++++++++++++--    discount=args.discount,
++++++++++++++--    discretizer=args.discretizer,
++++++++++++++--)
++++++++++++++--
++++++++++++++--dataset = dataset_config()
++++++++++++++--obs_dim = dataset.observation_dim
++++++++++++++--act_dim = dataset.action_dim
++++++++++++++--transition_dim = dataset.joined_dim
++++++++++++++--
++++++++++++++--#######################
++++++++++++++--######## model ########
++++++++++++++--#######################
++++++++++++++--
++++++++++++++--block_size = args.subsampled_sequence_length * transition_dim - 1
++++++++++++++--print(
++++++++++++++--    f'Dataset size: {len(dataset)} | '
++++++++++++++--    f'Joined dim: {transition_dim} '
++++++++++++++--    f'(observation: {obs_dim}, action: {act_dim}) | Block size: {block_size}'
++++++++++++++--)
++++++++++++++--
++++++++++++++--model_config = utils.Config(
++++++++++++++--    GPT,
++++++++++++++--    savepath=(args.savepath, 'model_config.pkl'),
++++++++++++++--    ## discretization
++++++++++++++--    vocab_size=args.N, block_size=block_size,
++++++++++++++--    ## architecture
++++++++++++++--    n_layer=args.n_layer, n_head=args.n_head, n_embd=args.n_embd*args.n_head,
++++++++++++++--    ## dimensions
++++++++++++++--    observation_dim=obs_dim, action_dim=act_dim, transition_dim=transition_dim,
++++++++++++++--    ## loss weighting
++++++++++++++--    action_weight=args.action_weight, reward_weight=args.reward_weight, value_weight=args.value_weight,
++++++++++++++--    ## dropout probabilities
++++++++++++++--    embd_pdrop=args.embd_pdrop, resid_pdrop=args.resid_pdrop, attn_pdrop=args.attn_pdrop,
++++++++++++++--)
++++++++++++++--
++++++++++++++--model = model_config()
++++++++++++++--model.to(args.device)
++++++++++++++--
++++++++++++++--#######################
++++++++++++++--####### trainer #######
++++++++++++++--#######################
++++++++++++++--
++++++++++++++--warmup_tokens = len(dataset) * block_size ## number of tokens seen per epoch
++++++++++++++--final_tokens = 20 * warmup_tokens
++++++++++++++--
++++++++++++++--trainer_config = utils.Config(
++++++++++++++--    utils.Trainer,
++++++++++++++--    savepath=(args.savepath, 'trainer_config.pkl'),
++++++++++++++--    # optimization parameters
++++++++++++++--    batch_size=args.batch_size,
++++++++++++++--    learning_rate=args.learning_rate,
++++++++++++++--    betas=(0.9, 0.95),
++++++++++++++--    grad_norm_clip=1.0,
++++++++++++++--    weight_decay=0.1, # only applied on matmul weights
++++++++++++++--    # learning rate decay: linear warmup followed by cosine decay to 10% of original
++++++++++++++--    lr_decay=args.lr_decay,
++++++++++++++--    warmup_tokens=warmup_tokens,
++++++++++++++--    final_tokens=final_tokens,
++++++++++++++--    ## dataloader
++++++++++++++--    num_workers=0,
++++++++++++++--    device=args.device,
++++++++++++++--)
++++++++++++++--
++++++++++++++--trainer = trainer_config()
++++++++++++++--
++++++++++++++--#######################
++++++++++++++--###### main loop ######
++++++++++++++--#######################
++++++++++++++--
++++++++++++++--## scale number of epochs to keep number of updates constant
++++++++++++++--n_epochs = int(1e6 / len(dataset) * args.n_epochs_ref)
++++++++++++++--save_freq = int(n_epochs // args.n_saves)
++++++++++++++--
++++++++++++++--for epoch in range(n_epochs):
++++++++++++++--    print(f'\nEpoch: {epoch} / {n_epochs} | {args.dataset} | {args.exp_name}')
++++++++++++++--
++++++++++++++--    trainer.train(model, dataset)
++++++++++++++--
++++++++++++++--    ## get greatest multiple of `save_freq` less than or equal to `save_epoch`
++++++++++++++--    save_epoch = (epoch + 1) // save_freq * save_freq
++++++++++++++--    statepath = os.path.join(args.savepath, f'state_{save_epoch}.pt')
++++++++++++++--    print(f'Saving model to {statepath}')
++++++++++++++--
++++++++++++++--    ## save state to disk
++++++++++++++--    state = model.state_dict()
++++++++++++++--    torch.save(state, statepath)
++++++++++++++-diff --git a/halfcheetah/scripts/xrl.py b/halfcheetah/scripts/xrl.py
++++++++++++++-deleted file mode 100644
++++++++++++++-index 134232a..0000000
++++++++++++++---- a/halfcheetah/scripts/xrl.py
++++++++++++++-+++ /dev/null
++++++++++++++-@@ -1,372 +0,0 @@
++++++++++++++--import json
++++++++++++++--import pdb
++++++++++++++--from os.path import join
++++++++++++++--
++++++++++++++--import trajectory.utils as utils
++++++++++++++--import trajectory.datasets as datasets
++++++++++++++--from trajectory.search import (
++++++++++++++--    make_prefix,
++++++++++++++--    update_context,
++++++++++++++--)
++++++++++++++--from trajectory.search.sampling import forward
++++++++++++++--
++++++++++++++--import gym
++++++++++++++--import d4rl # Import required to register environments, you may need to also import the submodule
++++++++++++++--import numpy as np
++++++++++++++--import d3rlpy
++++++++++++++--import math as mt
++++++++++++++--from sklearn.cluster import KMeans
++++++++++++++--from sklearn import datasets as skdatasets
++++++++++++++--from sklearn.decomposition import PCA
++++++++++++++--
++++++++++++++--from pyclustering.cluster.xmeans import xmeans
++++++++++++++--from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer
++++++++++++++--
++++++++++++++--from scipy.stats import wasserstein_distance
++++++++++++++--
++++++++++++++--class Parser(utils.Parser):
++++++++++++++--    dataset: str = 'halfcheetah-medium-expert-v2'
++++++++++++++--    config: str = 'config.offline'
++++++++++++++--
+++++++++++++++@@ -1,27 +0,0 @@
+++++++++++++++-name: trajectory
+++++++++++++++-channels:
+++++++++++++++-- defaults
+++++++++++++++-- conda-forge
+++++++++++++++-dependencies:
+++++++++++++++-- python=3.8
+++++++++++++++-- pip
+++++++++++++++-- patchelf
+++++++++++++++-- pip:
+++++++++++++++-    - -f https://download.pytorch.org/whl/torch_stable.html
+++++++++++++++-    - numpy
+++++++++++++++-    - wheel==0.38.4
+++++++++++++++-    - setuptools==65.5.0
+++++++++++++++-    - gym==0.20.0
+++++++++++++++-    - matplotlib==3.3.4
+++++++++++++++-    - torch==1.9.1+cu111
+++++++++++++++-    - typed-argument-parser
+++++++++++++++-    # - git+https://github.com/Farama-Foundation/d4rl@f2a05c0d66722499bf8031b094d9af3aea7c372b#egg=d4rl
+++++++++++++++-    - scikit-image==0.17.2
+++++++++++++++-    - scikit-video==1.1.11
+++++++++++++++-    - gitpython
+++++++++++++++-    - os
+++++++++++++++-    - d3rlpy
+++++++++++++++-    - pyclustering
+++++++++++++++-    - moviepy
+++++++++++++++-    - scipy
+++++++++++++++-    - scikit-learn
+++++++++++++++diff --git a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json
+++++++++++++++index 18993c1..81db9d7 100644
+++++++++++++++--- a/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json
++++++++++++++++++ b/halfcheetah/logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0/args.json
+++++++++++++++@@ -1,23 +1,23 @@
+++++++++++++++ {
+++++++++++++++     "add_extras": {
+++++++++++++++         "_type": "python_object (type = method)",
+++++++++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgfhpRSlC4="
++++++++++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMBW1rZGlylGgCaAZoCIaUUpSMB3ZlcmJvc2WUiIwJcGxhbl9mcmVxlEsBjAVrX29ic5RLAYwMZ3B0X2xvYWRwYXRolIwOZ3B0L3ByZXRyYWluZWSUjAZwcmVmaXiUjA9wbGFucy9kZWZhdWx0cy+UjAZjb25maWeUjA5jb25maWcub2ZmbGluZZSMDnByZWZpeF9jb250ZXh0lIiMCmFkZF9leHRyYXOUaAJoBmgVhpRSlIwHZGF0YXNldJSMFWhhbGZjaGVldGFoLW1lZGl1bS12MpSMCmdldF9jb21taXSUaAJoBmgahpRSlIwHY2RmX2FjdJRHP+MzMzMzMzOMCGV4cF9uYW1llIwecGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMylIwIbl9leHBhbmSUSwKMCHJlbmRlcmVylIwIUmVuZGVyZXKUjAVrX2FjdJROjAtyZWFkX2NvbmZpZ5RoAmgGaCSGlFKUjAh2aXNfZnJlcZRLMowRZ2VuZXJhdGVfZXhwX25hbWWUaAJoBmgohpRSlIwGY29tbWl0lIwtZjc3YTE5NDk5Njg3Mjg3OWJkMDQxNGZjYjNkYmMxNWIzZGEyOWNiOCBtYWlulIwKcGVyY2VudGlsZZSMBG1lYW6UjAhzZXRfc2VlZJRoAmgGaC+GlFKUjAdob3Jpem9ulEsFjBdtYXhfY29udGV4dF90cmFuc2l0aW9uc5RLBYwKYmVhbV93aWR0aJRLIIwHY2RmX29ic5ROjAdsb2diYXNllIwFbG9ncy+UjAlncHRfZXBvY2iUjAZsYXRlc3SUjAZkZXZpY2WUjANjcHWUjAlzYXZlX2RpZmaUaAJoBmg8hpRSlIwGc3VmZml4lIwBMJSMCHNhdmVwYXRolIw7bG9ncy9oYWxmY2hlZXRhaC1tZWRpdW0tdjIvcGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMyLzCUdWJoFYaUUpQu"
+++++++++++++++     },
+++++++++++++++     "beam_width": 32,
+++++++++++++++     "cdf_act": 0.6,
+++++++++++++++     "cdf_obs": null,
+++++++++++++++-    "commit": "cca8d898e10f9f6102a8c33dac7758e2993dfc60 halfcheetah-xrl",
++++++++++++++++    "commit": "f77a194996872879bd0414fcb3dbc15b3da29cb8 main",
+++++++++++++++     "config": "config.offline",
+++++++++++++++     "dataset": "halfcheetah-medium-v2",
+++++++++++++++     "device": "cpu",
+++++++++++++++     "exp_name": "plans/defaults/freq1_H5_beam32",
+++++++++++++++     "generate_exp_name": {
+++++++++++++++         "_type": "python_object (type = method)",
+++++++++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgxhpRSlC4="
++++++++++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMBW1rZGlylGgCaAZoCIaUUpSMB3ZlcmJvc2WUiIwJcGxhbl9mcmVxlEsBjAVrX29ic5RLAYwMZ3B0X2xvYWRwYXRolIwOZ3B0L3ByZXRyYWluZWSUjAZwcmVmaXiUjA9wbGFucy9kZWZhdWx0cy+UjAZjb25maWeUjA5jb25maWcub2ZmbGluZZSMDnByZWZpeF9jb250ZXh0lIiMCmFkZF9leHRyYXOUaAJoBmgVhpRSlIwHZGF0YXNldJSMFWhhbGZjaGVldGFoLW1lZGl1bS12MpSMCmdldF9jb21taXSUaAJoBmgahpRSlIwHY2RmX2FjdJRHP+MzMzMzMzOMCGV4cF9uYW1llIwecGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMylIwIbl9leHBhbmSUSwKMCHJlbmRlcmVylIwIUmVuZGVyZXKUjAVrX2FjdJROjAtyZWFkX2NvbmZpZ5RoAmgGaCSGlFKUjAh2aXNfZnJlcZRLMowRZ2VuZXJhdGVfZXhwX25hbWWUaAJoBmgohpRSlIwGY29tbWl0lIwtZjc3YTE5NDk5Njg3Mjg3OWJkMDQxNGZjYjNkYmMxNWIzZGEyOWNiOCBtYWlulIwKcGVyY2VudGlsZZSMBG1lYW6UjAhzZXRfc2VlZJRoAmgGaC+GlFKUjAdob3Jpem9ulEsFjBdtYXhfY29udGV4dF90cmFuc2l0aW9uc5RLBYwKYmVhbV93aWR0aJRLIIwHY2RmX29ic5ROjAdsb2diYXNllIwFbG9ncy+UjAlncHRfZXBvY2iUjAZsYXRlc3SUjAZkZXZpY2WUjANjcHWUjAlzYXZlX2RpZmaUaAJoBmg8hpRSlIwGc3VmZml4lIwBMJSMCHNhdmVwYXRolIw7bG9ncy9oYWxmY2hlZXRhaC1tZWRpdW0tdjIvcGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMyLzCUdWJoKIaUUpQu"
+++++++++++++++     },
+++++++++++++++     "get_commit": {
+++++++++++++++         "_type": "python_object (type = method)",
+++++++++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmglhpRSlC4="
++++++++++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMBW1rZGlylGgCaAZoCIaUUpSMB3ZlcmJvc2WUiIwJcGxhbl9mcmVxlEsBjAVrX29ic5RLAYwMZ3B0X2xvYWRwYXRolIwOZ3B0L3ByZXRyYWluZWSUjAZwcmVmaXiUjA9wbGFucy9kZWZhdWx0cy+UjAZjb25maWeUjA5jb25maWcub2ZmbGluZZSMDnByZWZpeF9jb250ZXh0lIiMCmFkZF9leHRyYXOUaAJoBmgVhpRSlIwHZGF0YXNldJSMFWhhbGZjaGVldGFoLW1lZGl1bS12MpSMCmdldF9jb21taXSUaAJoBmgahpRSlIwHY2RmX2FjdJRHP+MzMzMzMzOMCGV4cF9uYW1llIwecGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMylIwIbl9leHBhbmSUSwKMCHJlbmRlcmVylIwIUmVuZGVyZXKUjAVrX2FjdJROjAtyZWFkX2NvbmZpZ5RoAmgGaCSGlFKUjAh2aXNfZnJlcZRLMowRZ2VuZXJhdGVfZXhwX25hbWWUaAJoBmgohpRSlIwGY29tbWl0lIwtZjc3YTE5NDk5Njg3Mjg3OWJkMDQxNGZjYjNkYmMxNWIzZGEyOWNiOCBtYWlulIwKcGVyY2VudGlsZZSMBG1lYW6UjAhzZXRfc2VlZJRoAmgGaC+GlFKUjAdob3Jpem9ulEsFjBdtYXhfY29udGV4dF90cmFuc2l0aW9uc5RLBYwKYmVhbV93aWR0aJRLIIwHY2RmX29ic5ROjAdsb2diYXNllIwFbG9ncy+UjAlncHRfZXBvY2iUjAZsYXRlc3SUjAZkZXZpY2WUjANjcHWUjAlzYXZlX2RpZmaUaAJoBmg8hpRSlIwGc3VmZml4lIwBMJSMCHNhdmVwYXRolIw7bG9ncy9oYWxmY2hlZXRhaC1tZWRpdW0tdjIvcGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMyLzCUdWJoGoaUUpQu"
+++++++++++++++     },
+++++++++++++++     "gpt_epoch": "latest",
+++++++++++++++     "gpt_loadpath": "gpt/pretrained",
+++++++++++++++@@ -28,7 +28,7 @@
+++++++++++++++     "max_context_transitions": 5,
+++++++++++++++     "mkdir": {
+++++++++++++++         "_type": "python_object (type = method)",
+++++++++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgihpRSlC4="
++++++++++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMBW1rZGlylGgCaAZoCIaUUpSMB3ZlcmJvc2WUiIwJcGxhbl9mcmVxlEsBjAVrX29ic5RLAYwMZ3B0X2xvYWRwYXRolIwOZ3B0L3ByZXRyYWluZWSUjAZwcmVmaXiUjA9wbGFucy9kZWZhdWx0cy+UjAZjb25maWeUjA5jb25maWcub2ZmbGluZZSMDnByZWZpeF9jb250ZXh0lIiMCmFkZF9leHRyYXOUaAJoBmgVhpRSlIwHZGF0YXNldJSMFWhhbGZjaGVldGFoLW1lZGl1bS12MpSMCmdldF9jb21taXSUaAJoBmgahpRSlIwHY2RmX2FjdJRHP+MzMzMzMzOMCGV4cF9uYW1llIwecGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMylIwIbl9leHBhbmSUSwKMCHJlbmRlcmVylIwIUmVuZGVyZXKUjAVrX2FjdJROjAtyZWFkX2NvbmZpZ5RoAmgGaCSGlFKUjAh2aXNfZnJlcZRLMowRZ2VuZXJhdGVfZXhwX25hbWWUaAJoBmgohpRSlIwGY29tbWl0lIwtZjc3YTE5NDk5Njg3Mjg3OWJkMDQxNGZjYjNkYmMxNWIzZGEyOWNiOCBtYWlulIwKcGVyY2VudGlsZZSMBG1lYW6UjAhzZXRfc2VlZJRoAmgGaC+GlFKUjAdob3Jpem9ulEsFjBdtYXhfY29udGV4dF90cmFuc2l0aW9uc5RLBYwKYmVhbV93aWR0aJRLIIwHY2RmX29ic5ROjAdsb2diYXNllIwFbG9ncy+UjAlncHRfZXBvY2iUjAZsYXRlc3SUjAZkZXZpY2WUjANjcHWUjAlzYXZlX2RpZmaUaAJoBmg8hpRSlIwGc3VmZml4lIwBMJSMCHNhdmVwYXRolIw7bG9ncy9oYWxmY2hlZXRhaC1tZWRpdW0tdjIvcGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMyLzCUdWJoCIaUUpQu"
+++++++++++++++     },
+++++++++++++++     "n_expand": 2,
+++++++++++++++     "percentile": "mean",
+++++++++++++++@@ -37,24 +37,24 @@
+++++++++++++++     "prefix_context": true,
+++++++++++++++     "read_config": {
+++++++++++++++         "_type": "python_object (type = method)",
+++++++++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgZhpRSlC4="
++++++++++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMBW1rZGlylGgCaAZoCIaUUpSMB3ZlcmJvc2WUiIwJcGxhbl9mcmVxlEsBjAVrX29ic5RLAYwMZ3B0X2xvYWRwYXRolIwOZ3B0L3ByZXRyYWluZWSUjAZwcmVmaXiUjA9wbGFucy9kZWZhdWx0cy+UjAZjb25maWeUjA5jb25maWcub2ZmbGluZZSMDnByZWZpeF9jb250ZXh0lIiMCmFkZF9leHRyYXOUaAJoBmgVhpRSlIwHZGF0YXNldJSMFWhhbGZjaGVldGFoLW1lZGl1bS12MpSMCmdldF9jb21taXSUaAJoBmgahpRSlIwHY2RmX2FjdJRHP+MzMzMzMzOMCGV4cF9uYW1llIwecGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMylIwIbl9leHBhbmSUSwKMCHJlbmRlcmVylIwIUmVuZGVyZXKUjAVrX2FjdJROjAtyZWFkX2NvbmZpZ5RoAmgGaCSGlFKUjAh2aXNfZnJlcZRLMowRZ2VuZXJhdGVfZXhwX25hbWWUaAJoBmgohpRSlIwGY29tbWl0lIwtZjc3YTE5NDk5Njg3Mjg3OWJkMDQxNGZjYjNkYmMxNWIzZGEyOWNiOCBtYWlulIwKcGVyY2VudGlsZZSMBG1lYW6UjAhzZXRfc2VlZJRoAmgGaC+GlFKUjAdob3Jpem9ulEsFjBdtYXhfY29udGV4dF90cmFuc2l0aW9uc5RLBYwKYmVhbV93aWR0aJRLIIwHY2RmX29ic5ROjAdsb2diYXNllIwFbG9ncy+UjAlncHRfZXBvY2iUjAZsYXRlc3SUjAZkZXZpY2WUjANjcHWUjAlzYXZlX2RpZmaUaAJoBmg8hpRSlIwGc3VmZml4lIwBMJSMCHNhdmVwYXRolIw7bG9ncy9oYWxmY2hlZXRhaC1tZWRpdW0tdjIvcGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMyLzCUdWJoJIaUUpQu"
+++++++++++++++     },
+++++++++++++++     "renderer": "Renderer",
+++++++++++++++     "reproducibility": {
+++++++++++++++         "command_line": "python scripts/xrl_v2.py --dataset halfcheetah-medium-v2 --gpt_loadpath gpt/pretrained",
+++++++++++++++         "git_has_uncommitted_changes": true,
+++++++++++++++         "git_root": "/home/colin/Desktop/FACT/FACT_assignment",
+++++++++++++++-        "git_url": "https://github.com/fclio/FACT_assignment/tree/cca8d898e10f9f6102a8c33dac7758e2993dfc60",
+++++++++++++++-        "time": "Fri Feb  2 11:39:41 2024"
++++++++++++++++        "git_url": "https://github.com/fclio/FACT_assignment/tree/f77a194996872879bd0414fcb3dbc15b3da29cb8",
++++++++++++++++        "time": "Sat Feb  3 20:36:44 2024"
+++++++++++++++     },
+++++++++++++++     "save_diff": {
+++++++++++++++         "_type": "python_object (type = method)",
+++++++++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1Ymg0hpRSlC4="
++++++++++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMBW1rZGlylGgCaAZoCIaUUpSMB3ZlcmJvc2WUiIwJcGxhbl9mcmVxlEsBjAVrX29ic5RLAYwMZ3B0X2xvYWRwYXRolIwOZ3B0L3ByZXRyYWluZWSUjAZwcmVmaXiUjA9wbGFucy9kZWZhdWx0cy+UjAZjb25maWeUjA5jb25maWcub2ZmbGluZZSMDnByZWZpeF9jb250ZXh0lIiMCmFkZF9leHRyYXOUaAJoBmgVhpRSlIwHZGF0YXNldJSMFWhhbGZjaGVldGFoLW1lZGl1bS12MpSMCmdldF9jb21taXSUaAJoBmgahpRSlIwHY2RmX2FjdJRHP+MzMzMzMzOMCGV4cF9uYW1llIwecGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMylIwIbl9leHBhbmSUSwKMCHJlbmRlcmVylIwIUmVuZGVyZXKUjAVrX2FjdJROjAtyZWFkX2NvbmZpZ5RoAmgGaCSGlFKUjAh2aXNfZnJlcZRLMowRZ2VuZXJhdGVfZXhwX25hbWWUaAJoBmgohpRSlIwGY29tbWl0lIwtZjc3YTE5NDk5Njg3Mjg3OWJkMDQxNGZjYjNkYmMxNWIzZGEyOWNiOCBtYWlulIwKcGVyY2VudGlsZZSMBG1lYW6UjAhzZXRfc2VlZJRoAmgGaC+GlFKUjAdob3Jpem9ulEsFjBdtYXhfY29udGV4dF90cmFuc2l0aW9uc5RLBYwKYmVhbV93aWR0aJRLIIwHY2RmX29ic5ROjAdsb2diYXNllIwFbG9ncy+UjAlncHRfZXBvY2iUjAZsYXRlc3SUjAZkZXZpY2WUjANjcHWUjAlzYXZlX2RpZmaUaAJoBmg8hpRSlIwGc3VmZml4lIwBMJSMCHNhdmVwYXRolIw7bG9ncy9oYWxmY2hlZXRhaC1tZWRpdW0tdjIvcGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMyLzCUdWJoPIaUUpQu"
+++++++++++++++     },
+++++++++++++++     "savepath": "logs/halfcheetah-medium-v2/plans/defaults/freq1_H5_beam32/0",
+++++++++++++++     "set_seed": {
+++++++++++++++         "_type": "python_object (type = method)",
+++++++++++++++-        "_value": "gASVJwMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMF21heF9jb250ZXh0X3RyYW5zaXRpb25zlEsFjA5wcmVmaXhfY29udGV4dJSIjAhyZW5kZXJlcpSMCFJlbmRlcmVylIwHbG9nYmFzZZSMBWxvZ3MvlIwIc2F2ZXBhdGiUjDtsb2dzL2hhbGZjaGVldGFoLW1lZGl1bS12Mi9wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzIvMJSMBWtfYWN0lE6MCWdwdF9lcG9jaJSMBmxhdGVzdJSMBnN1ZmZpeJSMATCUjAlwbGFuX2ZyZXGUSwGMBWtfb2JzlEsBjAZkZXZpY2WUjANjcHWUjAtyZWFkX2NvbmZpZ5RoAmgGaBmGlFKUjAd2ZXJib3NllIiMB2RhdGFzZXSUjBVoYWxmY2hlZXRhaC1tZWRpdW0tdjKUjAphZGRfZXh0cmFzlGgCaAZoH4aUUpSMBW1rZGlylGgCaAZoIoaUUpSMCmdldF9jb21taXSUaAJoBmglhpRSlIwGcHJlZml4lIwPcGxhbnMvZGVmYXVsdHMvlIwIc2V0X3NlZWSUaAJoBmgqhpRSlIwGY29uZmlnlIwOY29uZmlnLm9mZmxpbmWUjAdjZGZfb2JzlE6MB2NkZl9hY3SURz/jMzMzMzMzjBFnZW5lcmF0ZV9leHBfbmFtZZRoAmgGaDGGlFKUjAlzYXZlX2RpZmaUaAJoBmg0hpRSlIwIdmlzX2ZyZXGUSzKMCmJlYW1fd2lkdGiUSyCMCG5fZXhwYW5klEsCjApwZXJjZW50aWxllIwEbWVhbpSMDGdwdF9sb2FkcGF0aJSMDmdwdC9wcmV0cmFpbmVklIwIZXhwX25hbWWUjB5wbGFucy9kZWZhdWx0cy9mcmVxMV9INV9iZWFtMzKUjAZjb21taXSUjDhjY2E4ZDg5OGUxMGY5ZjYxMDJhOGMzM2RhYzc3NThlMjk5M2RmYzYwIGhhbGZjaGVldGFoLXhybJSMB2hvcml6b26USwV1YmgqhpRSlC4="
++++++++++++++++        "_value": "gASVHAMAAAAAAACMCGJ1aWx0aW5zlIwHZ2V0YXR0cpSTlIwIX19tYWluX1+UjAZQYXJzZXKUk5QpgZR9lCiMBW1rZGlylGgCaAZoCIaUUpSMB3ZlcmJvc2WUiIwJcGxhbl9mcmVxlEsBjAVrX29ic5RLAYwMZ3B0X2xvYWRwYXRolIwOZ3B0L3ByZXRyYWluZWSUjAZwcmVmaXiUjA9wbGFucy9kZWZhdWx0cy+UjAZjb25maWeUjA5jb25maWcub2ZmbGluZZSMDnByZWZpeF9jb250ZXh0lIiMCmFkZF9leHRyYXOUaAJoBmgVhpRSlIwHZGF0YXNldJSMFWhhbGZjaGVldGFoLW1lZGl1bS12MpSMCmdldF9jb21taXSUaAJoBmgahpRSlIwHY2RmX2FjdJRHP+MzMzMzMzOMCGV4cF9uYW1llIwecGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMylIwIbl9leHBhbmSUSwKMCHJlbmRlcmVylIwIUmVuZGVyZXKUjAVrX2FjdJROjAtyZWFkX2NvbmZpZ5RoAmgGaCSGlFKUjAh2aXNfZnJlcZRLMowRZ2VuZXJhdGVfZXhwX25hbWWUaAJoBmgohpRSlIwGY29tbWl0lIwtZjc3YTE5NDk5Njg3Mjg3OWJkMDQxNGZjYjNkYmMxNWIzZGEyOWNiOCBtYWlulIwKcGVyY2VudGlsZZSMBG1lYW6UjAhzZXRfc2VlZJRoAmgGaC+GlFKUjAdob3Jpem9ulEsFjBdtYXhfY29udGV4dF90cmFuc2l0aW9uc5RLBYwKYmVhbV93aWR0aJRLIIwHY2RmX29ic5ROjAdsb2diYXNllIwFbG9ncy+UjAlncHRfZXBvY2iUjAZsYXRlc3SUjAZkZXZpY2WUjANjcHWUjAlzYXZlX2RpZmaUaAJoBmg8hpRSlIwGc3VmZml4lIwBMJSMCHNhdmVwYXRolIw7bG9ncy9oYWxmY2hlZXRhaC1tZWRpdW0tdjIvcGxhbnMvZGVmYXVsdHMvZnJlcTFfSDVfYmVhbTMyLzCUdWJoL4aUUpQu"
+++++++++++++++     },
+++++++++++++++     "suffix": "0",
+++++++++++++++     "verbose": true,
+++++++++++++++diff --git a/halfcheetah/scripts/xrl_v2.py b/halfcheetah/scripts/xrl_v2.py
+++++++++++++++index 62a3d4d..489d927 100644
+++++++++++++++--- a/halfcheetah/scripts/xrl_v2.py
++++++++++++++++++ b/halfcheetah/scripts/xrl_v2.py
+++++++++++++++@@ -21,42 +21,64 @@ from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer
+++++++++++++++ from scipy.stats import wasserstein_distance
+++++++++++++++ from moviepy.editor import VideoFileClip
+++++++++++++++ 
++++++++++++++++
+++++++++++++++ class Parser(utils.Parser):
+++++++++++++++     dataset: str = 'halfcheetah-medium-v2'
+++++++++++++++     config: str = 'config.offline'
+++++++++++++++ 
++++++++++++++ -# utils
++++++++++++++--    
++++++++++++++--class XMeans:
++++++++++++++--    def loglikelihood(self, r, rn, var, m, k):
++++++++++++++--        l1 = - rn / 2.0 * mt.log(2 * mt.pi)
++++++++++++++--        l2 = - rn * m / 2.0 * mt.log(var)
++++++++++++++--        l3 = - (rn - k) / 2.0
++++++++++++++--        l4 = rn * mt.log(rn)
++++++++++++++--        l5 = - rn * mt.log(r)
++++++++++++++--
++++++++++++++--        return l1 + l2 + l3 + l4 + l5
++++++++++++++--
++++++++++++++--    def __init__(self, X, kmax = 20):
++++++++++++++--        self.X = X
++++++++++++++--        self.num = np.size(self.X, axis=0)
++++++++++++++--        self.dim = np.size(X, axis=1)
++++++++++++++--        self.KMax = kmax
++++++++++++++--
++++++++++++++--    def fit(self):
++++++++++++++--        k = 1
++++++++++++++--        X = self.X
++++++++++++++--        M = self.dim
++++++++++++++--        num = self.num
++++++++++++++--
++++++++++++++--        while(1):
++++++++++++++--            ok = k
++++++++++++++--
++++++++++++++--            #Improve Params
++++++++++++++--            kmeans = KMeans(n_clusters=k).fit(X)
++++++++++++++--            labels = kmeans.labels_
++++++++++++++--            m = kmeans.cluster_centers_
++++++++++++++--
++++++++++++++--            #Improve Structure
++++++++++++++--            #Calculate BIC
++++++++++++++--            p = M + 1
++++++++++++++--
++++++++++++++--            obic = np.zeros(k)
++++++++++++++--
++++++++++++++--            for i in range(k):
++++++++++++++--                rn = np.size(np.where(labels == i))
++++++++++++++--                var = np.sum((X[labels == i] - m[i])**2)/float(rn - 1)
++++++++++++++--                obic[i] = self.loglikelihood(rn, rn, var, M, 1) - p/2.0*mt.log(rn)
++++++++++++++--
++++++++++++++--            #Split each cluster into two subclusters and calculate BIC of each splitted cluster
++++++++++++++--            sk = 2 #The number of subclusters
++++++++++++++--            nbic = np.zeros(k)
++++++++++++++--            addk = 0
++++++++++++++--
++++++++++++++--            for i in range(k):
++++++++++++++--                ci = X[labels == i]
++++++++++++++--                r = np.size(np.where(labels == i))
++++++++++++++--
++++++++++++++--                kmeans = KMeans(n_clusters=sk).fit(ci)
++++++++++++++--                ci_labels = kmeans.labels_
++++++++++++++--                sm = kmeans.cluster_centers_
++++++++++++++--
++++++++++++++--                for l in range(sk):
++++++++++++++--                    rn = np.size(np.where(ci_labels == l))
++++++++++++++--                    var = np.sum((ci[ci_labels == l] - sm[l])**2)/float(rn - sk)
++++++++++++++--                    nbic[i] += self.loglikelihood(r, rn, var, M, sk)
++++++++++++++--
++++++++++++++--                p = sk * (M + 1)
++++++++++++++--                nbic[i] -= p/2.0*mt.log(r)
++++++++++++++--
++++++++++++++--                if obic[i] < nbic[i]:
++++++++++++++--                    addk += 1
++++++++++++++--
++++++++++++++--            k += addk
++++++++++++++--
++++++++++++++--            if ok == k or k >= self.KMax:
++++++++++++++--                break
++++++++++++++--
++++++++++++++--
++++++++++++++--        #Calculate labels and centroids
++++++++++++++--        kmeans = KMeans(n_clusters=k).fit(X)
++++++++++++++--        self.labels = kmeans.labels_
++++++++++++++--        self.k = k
++++++++++++++--        self.m = kmeans.cluster_centers_
++++++++++++++--
++++++++++++++--
++++++++++++++--def cluster_trajectories(trajectories):
++++++++++++++--    xmeans_instance = XMeans(trajectories, kmax=10)
++++++++++++++--    xmeans_instance.fit()
++++++++++++++--
++++++++++++++--    clusters = xmeans_instance.labels
++++++++++++++--    return clusters
++++++++++++++--
++++++++++++++--def cluster_trajectories_2(trajectories):
+++++++++++++++ 
+++++++++++++++ def cluster_trajectories(trajectories, n_clusters=10):
+++++++++++++++-    """TODO"""
++++++++++++++++    """
++++++++++++++++    Cluster trajectories using X-means.
++++++++++++++++    
++++++++++++++++    Args:
++++++++++++++++    - trajectories: np.array, shape (n_trajectories, encoding_dim)
++++++++++++++++    - n_clusters: int, max number of clusters
++++++++++++++++    
++++++++++++++++    Returns:
++++++++++++++++    - idxs_per_cluster: list, trajectory idxs per cluster idxs
++++++++++++++++    - clusters: np.array, shape (n_trajectories), cluster idxs per trajectory idx
++++++++++++++++    """ 
+++++++++++++++ 
++++++++++++++ -    # Prepare initial centers - amount of initial centers defines amount of clusters from which X-Means will
++++++++++++++ -    # start analysis.
++++++++++++++--    amount_initial_centers = 2
++++++++++++++--    initial_centers = kmeans_plusplus_initializer(trajectories, amount_initial_centers).initialize()
++++++++++++++--    
++++++++++++++++    # Set 2 initial cluster centers
+++++++++++++++     amount_initial_centers = 2
+++++++++++++++     initial_centers = kmeans_plusplus_initializer(trajectories, amount_initial_centers).initialize()
+++++++++++++++     
++++++++++++++ -    # Create instance of X-Means algorithm. The algorithm will start analysis from 2 clusters, the maximum
++++++++++++++ -    # number of clusters that can be allocated is 10.
++++++++++++++--    xmeans_instance = xmeans(trajectories, initial_centers, 10)
++++++++++++++--    xmeans_instance.process()
++++++++++++++--    
++++++++++++++--    # Extract clustering results: clusters
++++++++++++++--    idxs_per_cluster = xmeans_instance.get_clusters()
++++++++++++++--
++++++++++++++--    clusters = []
++++++++++++++--    for i in range(len(trajectories)):
++++++++++++++--        for j in range(len(idxs_per_cluster)):
++++++++++++++--            if i in idxs_per_cluster[j]: clusters.append(j)
++++++++++++++--
++++++++++++++--    return idxs_per_cluster, np.array(clusters)
++++++++++++++++    # Run X-means
+++++++++++++++     xmeans_instance = xmeans(trajectories, initial_centers, n_clusters)
+++++++++++++++     xmeans_instance.process()
+++++++++++++++     
+++++++++++++++     # Extract clustering results: clusters
+++++++++++++++     idxs_per_cluster = xmeans_instance.get_clusters()
+++++++++++++++ 
++++++++++++++++    # 
+++++++++++++++     clusters = []
+++++++++++++++     for i in range(len(trajectories)):
+++++++++++++++         for j in range(len(idxs_per_cluster)):
+++++++++++++++             if i in idxs_per_cluster[j]: clusters.append(j)
+++++++++++++++ 
+++++++++++++++     return idxs_per_cluster, np.array(clusters)
++++++++++++++ - 
++++++++++++++--# https://github.com/sascha-kirch/ML_Notebooks/blob/main/Softmax_Temperature.ipynb
++++++++++++++--def softmax(x, temp):
++++++++++++++--    """Compute softmax values for each sets of scores in x."""
++++++++++++++--    return np.exp(np.divide(x,temp)) / np.sum(np.exp(np.divide(x,temp)))
++++++++++++++--
++++++++++++++--def generate_data_embedding(trajectory_embeddings, normalizing_factor=1, temperature=1):
++++++++++++++--    embedding = np.sum(trajectory_embeddings, axis=0) / normalizing_factor
++++++++++++++--    embedding = softmax(embedding, temperature)
++++++++++++++--    return embedding
++++++++++++++--
++++++++++++++--def embed_trajectory(gpt, discretizer, observations, actions, rewards, preprocess_fn):
++++++++++++++--    context = []
++++++++++++++--
++++++++++++++--    for i in range(len(observations)):
++++++++++++++--        observation = observations[i]
++++++++++++++--        action = actions[i]
++++++++++++++--        reward = rewards[i]
++++++++++++++--
++++++++++++++--        observation = preprocess_fn(observation)
++++++++++++++--
++++++++++++++--        # print(observation)
++++++++++++++--        prefix = make_prefix(discretizer, context, observation, True)
++++++++++++++--        # print("prefix", prefix.shape)
++++++++++++++--
++++++++++++++--        out = forward(gpt, prefix)
++++++++++++++--        # print("out", out.shape)
++++++++++++++--        context = update_context(context, discretizer, observation, action, reward, len(observations))
++++++++++++++--        # print("cotext", context)
++++++++++++++--    
++++++++++++++--    emb = []
++++++++++++++--    for context_step in context:
++++++++++++++--        emb.append(context_step.numpy())
++++++++++++++--    emb = np.array(emb)
++++++++++++++--    emb = np.mean(emb, axis=0)[0]
++++++++++++++--
++++++++++++++--    return emb
++++++++++++++--
++++++++++++++--
++++++++++++++--def create_complementary_dataset(dataset, idxs, trajectory_length=10):
++++++++++++++--    observations = []
++++++++++++++--    actions = []
++++++++++++++--    rewards = []
++++++++++++++--    terminals = []
++++++++++++++--    for i in range(1000):
++++++++++++++--        if i not in idxs:
++++++++++++++--            observations += list(dataset.observations[1000*i:1000*i+trajectory_length])
++++++++++++++--            actions += list(dataset.actions[1000*i:1000*i+trajectory_length])
++++++++++++++--            rewards += list(dataset.rewards[1000*i:1000*i+trajectory_length])
++++++++++++++--            terminals += list(dataset.terminals[1000*i:1000*i+trajectory_length])
++++++++++++++--
++++++++++++++--    new_dataset = d3rlpy.dataset.MDPDataset(
++++++++++++++--        observations=np.array(observations),
++++++++++++++--        actions=np.array(actions),
++++++++++++++--        rewards=np.array(rewards),
++++++++++++++--        terminals=np.array(terminals)
++++++++++++++--    )
++++++++++++++--    return new_dataset
++++++++++++++--    
++++++++++++++--
++++++++++++++--
++++++++++++++--
++++++++++++++--def main():
++++++++++++++--    # args = Parser().parse_args('plan')
++++++++++++++--
++++++++++++++--    #######################
++++++++++++++--    ####### models ########
++++++++++++++--    #######################
++++++++++++++--
++++++++++++++--
++++++++++++++--
++++++++++++++--
++++++++++++++--
++++++++++++++--    # print(args.dataset)
++++++++++++++--
++++++++++++++--    # dataset = utils.load_from_config(args.logbase, args.dataset, args.gpt_loadpath,
++++++++++++++--    #         'data_config.pkl')
++++++++++++++--
++++++++++++++--
++++++++++++++--    # gpt, gpt_epoch = utils.load_model(args.logbase, args.dataset, args.gpt_loadpath,
++++++++++++++--    #         epoch=args.gpt_epoch, device=args.device)
++++++++++++++--
++++++++++++++--    # env = datasets.load_environment(args.dataset)
++++++++++++++--
++++++++++++++--    # discretizer = dataset.discretizer
++++++++++++++--
++++++++++++++--    # preprocess_fn = datasets.get_preprocess_fn(env.name)
++++++++++++++--
++++++++++++++--    # #######################
++++++++++++++--    # ####### dataset #######
++++++++++++++--    # #######################
++++++++++++++--
++++++++++++++--    # # env = datasets.load_environment(args.dataset)
++++++++++++++--    # discretizer = dataset.discretizer
++++++++++++++--    # preprocess_fn = datasets.get_preprocess_fn(env.name)
++++++++++++++--
++++++++++++++--    # # dataset
++++++++++++++--    dataset_d3, env = d3rlpy.datasets.get_dataset("halfcheetah-medium-v2")
++++++++++++++--
++++++++++++++--    # env = gym.make('halfcheetah-medium-v2')
++++++++++++++--    # dataset_d4 = d4rl.qlearning_dataset(env)
++++++++++++++--
++++++++++++++--    # # checks to see if d3rl & d4rl datasets are equal
++++++++++++++--    # print(np.allclose(dataset_d3.actions[100], dataset_d4['actions'][100]))
++++++++++++++--
++++++++++++++--    # # dr4rl has same trajectories, just cut off 1 element before the end
++++++++++++++--    # for j in range(1000):
++++++++++++++--    #     for i in range(999):
++++++++++++++--    #         if dataset_d4['rewards'][j * 999 + i] != dataset_d3.rewards[j * 1000 + i]: print("yo", i)
++++++++++++++--
++++++++++++++--    # #######################
++++++++++++++--    # ###### main loop ######
++++++++++++++--    # #######################
++++++++++++++--
++++++++++++++--    trajectory_length = 10 # 10 = max
++++++++++++++--
++++++++++++++--    # embeddings = []
++++++++++++++--    # for i in range(1000):
++++++++++++++--    #     observations = dataset_d3.observations[1000*i:1000*i+trajectory_length]
++++++++++++++--    #     actions = dataset_d3.actions[1000*i:1000*i+trajectory_length]
++++++++++++++--    #     rewards = dataset_d3.rewards[1000*i:1000*i+trajectory_length]
++++++++++++++--    #     terminals = dataset_d3.terminals[1000*i:1000*i+trajectory_length]
++++++++++++++--    #     emb = embed_trajectory(gpt, discretizer, observations, actions, rewards, preprocess_fn)
++++++++++++++--    #     embeddings.append(emb)
++++++++++++++--    # embeddings = np.array(embeddings)
++++++++++++++--    # np.save("embeddings.npy", embeddings)
++++++++++++++--    # print(embeddings)
++++++++++++++--
++++++++++++++--    embeddings = np.load("embeddings.npy")
++++++++++++++--
++++++++++++++--    pca = PCA(n_components=2)
++++++++++++++--    pca = PCA(n_components=2)
++++++++++++++--    pca_embeddings = pca.fit_transform(embeddings)
++++++++++++++--    np.save("pca.py", pca_embeddings)
++++++++++++++--
++++++++++++++--    idxs_per_cluster, clusters = cluster_trajectories_2(embeddings)
++++++++++++++--    # print(clusters)
++++++++++++++--    # return
++++++++++++++--    np.save("clusters.npy", clusters)
++++++++++++++--
++++++++++++++--    import matplotlib.pyplot as plt
++++++++++++++--
++++++++++++++--    d_orig = generate_data_embedding(embeddings)
++++++++++++++--    unique_clusters = np.unique(clusters)
++++++++++++++--    
++++++++++++++--    d_j = []
++++++++++++++--    complementary_datasets = []
++++++++++++++--    for j in np.sort(unique_clusters):
++++++++++++++--        print(j)
++++++++++++++--        d_j.append(generate_data_embedding(embeddings[clusters != j]))
++++++++++++++--        plt.scatter(pca_embeddings[clusters == j][:,0], pca_embeddings[clusters == j][:,1], label=j)
++++++++++++++--        complementary_datasets.append(create_complementary_dataset(dataset_d3, idxs_per_cluster[j], trajectory_length))
++++++++++++++--    
++++++++++++++--    original_dataset = create_complementary_dataset(dataset_d3, [], trajectory_length)
++++++++++++++--
++++++++++++++--    print(complementary_datasets, original_dataset)
++++++++++++++--
++++++++++++++--    plt.legend()
++++++++++++++--    plt.show()
++++++++++++++--
++++++++++++++--    agent_orig = d3rlpy.algos.SAC(
++++++++++++++--        actor_learning_rate=3e-4,
++++++++++++++--        critic_learning_rate=3e-4,
++++++++++++++--        temp_learning_rate=3e-4,
++++++++++++++--        batch_size=256)
++++++++++++++--
++++++++++++++--    print(agent_orig)
++++++++++++++--
++++++++++++++--    training_steps = 1000
++++++++++++++--
++++++++++++++--    agent_orig.fit(original_dataset, n_steps=training_steps)
++++++++++++++--
++++++++++++++--    agents_compl = []
++++++++++++++--
++++++++++++++--    for dset in complementary_datasets:
++++++++++++++--        agent = d3rlpy.algos.SAC(
++++++++++++++--            actor_learning_rate=3e-4,
++++++++++++++--            critic_learning_rate=3e-4,
++++++++++++++--            temp_learning_rate=3e-4,
++++++++++++++--            batch_size=256)
++++++++++++++--        agent.fit(dset, n_steps=training_steps)
++++++++++++++--        agents_compl.append(agent)
++++++++++++++--
++++++++++++++--    action_orig = agent_orig.predict(dataset_d3.observations[0])
++++++++++++++--
++++++++++++++--    actions_compl = []
++++++++++++++--    for agent in agents_compl:
++++++++++++++--        actions_compl.append(agent.predict(dataset_d3.observations[0]))
++++++++++++++--    
++++++++++++++--    action_dists = []
++++++++++++++--    for action in actions_compl:
++++++++++++++--        action_dists.append(np.linalg.norm(action_orig-action))
++++++++++++++--
++++++++++++++--    k = 3
++++++++++++++--    topk = np.argpartition(action_dists, -k)[-k:]
++++++++++++++--
++++++++++++++--    d_w = {}
++++++++++++++--    for idx in topk:
++++++++++++++--        d_w[idx] = wasserstein_distance(d_j[idx], d_orig)
++++++++++++++--
++++++++++++++--    cluster_assignment = min(d_w, key=d_w.get)
++++++++++++++--    print("explanation assigned to cluster", cluster_assignment)
++++++++++++++--
++++++++++++++--    
++++++++++++++--def assignment_test():
++++++++++++++--    action_orig = np.random.rand(10)
++++++++++++++--    d_orig = np.random.rand(5)
++++++++++++++--
++++++++++++++--    actions_compl = np.random.rand(6,10)
++++++++++++++--    d_j = np.random.rand(6,5)
++++++++++++++--
++++++++++++++--    action_dists = []
++++++++++++++--    for action in actions_compl:
++++++++++++++--        action_dists.append(np.linalg.norm(action_orig-action))
++++++++++++++--
++++++++++++++--    print(action_dists)
++++++++++++++--
++++++++++++++--    k = 3
++++++++++++++--    topk = np.argpartition(action_dists, -k)[-k:]
++++++++++++++--
++++++++++++++--    print(topk)
++++++++++++++--
++++++++++++++--    d_w = {}
++++++++++++++--    for idx in topk:
++++++++++++++--        d_w[idx] = wasserstein_distance(d_j[idx], d_orig)
++++++++++++++--
++++++++++++++--    print(d_w)
++++++++++++++--
++++++++++++++--    cluster_assignment = min(d_w, key=d_w.get)
++++++++++++++--    print("explanation assigned to cluster", cluster_assignment)
++++++++++++++--
++++++++++++++--
++++++++++++++--if __name__ == "__main__":
++++++++++++++--    # main()
++++++++++++++--    assignment_test()
++++++++++++++-diff --git a/halfcheetah/trajectory.egg-info/PKG-INFO b/halfcheetah/trajectory.egg-info/PKG-INFO
++++++++++++++-index 452c6cb..2603850 100644
++++++++++++++---- a/halfcheetah/trajectory.egg-info/PKG-INFO
++++++++++++++-+++ b/halfcheetah/trajectory.egg-info/PKG-INFO
++++++++++++++-@@ -1,4 +1,11 @@
++++++++++++++- Metadata-Version: 2.1
++++++++++++++- Name: trajectory
++++++++++++++- Version: 0.0.0
++++++++++++++-+Summary: UNKNOWN
++++++++++++++-+Home-page: UNKNOWN
++++++++++++++-+License: UNKNOWN
++++++++++++++-+Platform: UNKNOWN
++++++++++++++- License-File: LICENSE
++++++++++++++ +
++++++++++++++-+UNKNOWN
++++++++++++++ +
+++++++++++++++ def softmax(x, temp):
+++++++++++++++-    """TODO"""
++++++++++++++++    """
++++++++++++++++    Softmax with temperature using max-trick.
++++++++++++++++    
++++++++++++++++    Args:
++++++++++++++++    - x: np.array, shape (n_data, dim_data)
++++++++++++++++    - temp: int, softmax temperature
++++++++++++++++    
++++++++++++++++    Returns:
++++++++++++++++    - embedding: np.array: shape (dim_data)
++++++++++++++++    """ 
+++++++++++++++     max_x = np.max(x)
+++++++++++++++     return np.exp(np.divide(x-max_x,temp)) / np.sum(np.exp(np.divide(x-max_x,temp)))
+++++++++++++++ 
++++++++++++++++
+++++++++++++++ def generate_data_embedding(trajectory_embeddings, temperature=10000):
+++++++++++++++-    """TODO"""
++++++++++++++++    """
++++++++++++++++    TODO
++++++++++++++++    """ 
+++++++++++++++ 
+++++++++++++++     embedding = np.sum(trajectory_embeddings, axis=0)
+++++++++++++++     embedding = softmax(embedding, temperature)
++++++++++++++ diff --git a/halfcheetah/trajectory.egg-info/SOURCES.txt b/halfcheetah/trajectory.egg-info/SOURCES.txt
++++++++++++++-index 4474d85..84e8e3a 100644
+++++++++++++++index 84e8e3a..4474d85 100644
++++++++++++++ --- a/halfcheetah/trajectory.egg-info/SOURCES.txt
++++++++++++++ +++ b/halfcheetah/trajectory.egg-info/SOURCES.txt
++++++++++++++-@@ -30,4 +30,5 @@ trajectory/utils/serialization.py
+++++++++++++++@@ -30,5 +30,4 @@ trajectory/utils/serialization.py
++++++++++++++  trajectory/utils/setup.py
++++++++++++++  trajectory/utils/timer.py
++++++++++++++  trajectory/utils/training.py
++++++++++++++ -trajectory/utils/video.py
+++++++++++++++-trajectory_aaa/__init__.py
++++++++++++++ \ No newline at end of file
++++++++++++++ +trajectory/utils/video.py
++++++++++++++-+trajectory_aaa/__init__.py
++++++++++++++ \ No newline at end of file
++++++++++++++ diff --git a/halfcheetah/trajectory.egg-info/top_level.txt b/halfcheetah/trajectory.egg-info/top_level.txt
++++++++++++++-index ce65198..1d5271f 100644
+++++++++++++++index 1d5271f..ce65198 100644
++++++++++++++ --- a/halfcheetah/trajectory.egg-info/top_level.txt
++++++++++++++ +++ b/halfcheetah/trajectory.egg-info/top_level.txt
++++++++++++++-@@ -1 +1,2 @@
+++++++++++++++@@ -1,2 +1 @@
++++++++++++++  trajectory
++++++++++++++-+trajectory_aaa
+++++++++++++ \ No newline at end of file
+++++++++++++-diff --git a/halfcheetah/pca.py.npy b/halfcheetah/pca.py.npy
+++++++++++++-deleted file mode 100644
+++++++++++++-index bb19150..0000000
+++++++++++++-Binary files a/halfcheetah/pca.py.npy and /dev/null differ
+++++++++++++-diff --git a/halfcheetah/plotting/bar.png b/halfcheetah/plotting/bar.png
+++++++++++++-deleted file mode 100644
+++++++++++++-index 3679667..0000000
+++++++++++++-Binary files a/halfcheetah/plotting/bar.png and /dev/null differ
+++++++++++++-diff --git a/halfcheetah/plotting/plot.py b/halfcheetah/plotting/plot.py
+++++++++++++-deleted file mode 100644
+++++++++++++-index 163d0e4..0000000
+++++++++++++---- a/halfcheetah/plotting/plot.py
+++++++++++++-+++ /dev/null
+++++++++++++-@@ -1,74 +0,0 @@
+++++++++++++--import numpy as np
+++++++++++++--import matplotlib
+++++++++++++--import matplotlib.pyplot as plt
+++++++++++++--import pdb
+++++++++++++--
+++++++++++++--from plotting.scores import means
+++++++++++++--
+++++++++++++--class Colors:
+++++++++++++--	grey = '#B4B4B4'
+++++++++++++--	gold = '#F6C781'
+++++++++++++--	red = '#EC7C7D'
+++++++++++++--	blue = '#70ABCC'
+++++++++++++--
+++++++++++++--LABELS = {
+++++++++++++--	# 'BC': 'Behavior\nCloning',
+++++++++++++--	# 'MBOP': 'Model-Based\nOffline Planning',
+++++++++++++--	# 'BRAC': 'Behavior-Reg.\nActor-Critic',
+++++++++++++--	# 'CQL': 'Conservative\nQ-Learning',
+++++++++++++--}
+++++++++++++--
+++++++++++++--def get_mean(results, exclude=None):
+++++++++++++--	'''
+++++++++++++--		results : { environment: score, ... }
+++++++++++++--	'''
+++++++++++++--	filtered = {
+++++++++++++--		k: v for k, v in results.items()
+++++++++++++--		if (not exclude) or (exclude and exclude not in k)
+++++++++++++--	}
+++++++++++++--	return np.mean(list(filtered.values()))
+++++++++++++--
+++++++++++++--if __name__ == '__main__':
+++++++++++++--
+++++++++++++--	#################
+++++++++++++--	## latex
+++++++++++++--	#################
+++++++++++++--	matplotlib.rc('font', **{'family': 'serif', 'serif': ['Computer Modern']})
+++++++++++++--	matplotlib.rc('text', usetex=True)
+++++++++++++--	matplotlib.rcParams['text.latex.preamble']=[r"\usepackage{amsmath}"]
+++++++++++++--	#################
+++++++++++++--
+++++++++++++--	fig = plt.gcf()
+++++++++++++--	ax = plt.gca()
+++++++++++++--	fig.set_size_inches(7.5, 2.5)
+++++++++++++--
+++++++++++++--	means = {k: get_mean(v, exclude='ant') for k, v in means.items()}
+++++++++++++--	print(means)
+++++++++++++--
+++++++++++++--	algs = ['BC', 'MBOP', 'BRAC', 'CQL', 'Decision\nTransformer', 'Trajectory\nTransformer']
+++++++++++++--	vals = [means[alg] for alg in algs]
+++++++++++++--
+++++++++++++--	colors = [
+++++++++++++--		Colors.grey, Colors.gold,
+++++++++++++--		Colors.red, Colors.red, Colors.blue, Colors.blue
+++++++++++++--	]
+++++++++++++--
+++++++++++++--	labels = [LABELS.get(alg, alg) for alg in algs]
+++++++++++++--	plt.bar(labels, vals, color=colors, edgecolor=Colors.gold, lw=0)
+++++++++++++--	plt.ylabel('Average normalized return', labelpad=15)
+++++++++++++--	# plt.title('Offline RL Results')
+++++++++++++--
+++++++++++++--	legend_labels = ['Behavior Cloning', 'Trajectory Optimization', 'Temporal Difference', 'Sequence Modeling']
+++++++++++++--	colors = [Colors.grey, Colors.gold, Colors.red, Colors.blue]
+++++++++++++--	handles = [plt.Rectangle((0,0),1,1, color=color) for label, color in zip(legend_labels, colors)]
+++++++++++++--	plt.legend(handles, legend_labels, ncol=4,
+++++++++++++--		bbox_to_anchor=(1.07, -.18), fancybox=False, framealpha=0, shadow=False, columnspacing=1.5, handlelength=1.5)
+++++++++++++--
+++++++++++++--	matplotlib.rcParams['hatch.linewidth'] = 7.5
+++++++++++++--	# ax.patches[-1].set_hatch('/')
+++++++++++++--
+++++++++++++--	ax.spines['right'].set_visible(False)
+++++++++++++--	ax.spines['top'].set_visible(False)
+++++++++++++--
+++++++++++++--	# plt.savefig('plotting/bar.pdf', bbox_inches='tight')
+++++++++++++--	plt.savefig('plotting/bar.png', bbox_inches='tight', dpi=500)
+++++++++++++-diff --git a/halfcheetah/plotting/read_results.py b/halfcheetah/plotting/read_results.py
+++++++++++++-deleted file mode 100644
+++++++++++++-index 5a5fb62..0000000
+++++++++++++---- a/halfcheetah/plotting/read_results.py
+++++++++++++-+++ /dev/null
+++++++++++++-@@ -1,70 +0,0 @@
+++++++++++++--import os
+++++++++++++--import glob
+++++++++++++--import numpy as np
+++++++++++++--import json
+++++++++++++--import pdb
+++++++++++++--
+++++++++++++--import trajectory.utils as utils
+++++++++++++--
+++++++++++++--DATASETS = [
+++++++++++++--	f'{env}-{buffer}'
+++++++++++++--	for env in ['hopper', 'walker2d', 'halfcheetah', 'ant']
+++++++++++++--	for buffer in ['medium-expert-v2', 'medium-v2', 'medium-replay-v2']
+++++++++++++--]
+++++++++++++--
+++++++++++++--LOGBASE = 'logs'
+++++++++++++--TRIAL = '*'
+++++++++++++--EXP_NAME = 'plans/pretrained'
+++++++++++++--
+++++++++++++--def load_results(paths):
+++++++++++++--	'''
+++++++++++++--		paths : path to directory containing experiment trials
+++++++++++++--	'''
+++++++++++++--	scores = []
+++++++++++++--	for i, path in enumerate(sorted(paths)):
+++++++++++++--		score = load_result(path)
+++++++++++++--		if score is None:
+++++++++++++--			print(f'Skipping {path}')
+++++++++++++--			continue
+++++++++++++--		scores.append(score)
+++++++++++++--
+++++++++++++--		suffix = path.split('/')[-1]
+++++++++++++--
+++++++++++++--	mean = np.mean(scores)
+++++++++++++--	err = np.std(scores) / np.sqrt(len(scores))
+++++++++++++--	return mean, err, scores
+++++++++++++--
+++++++++++++--def load_result(path):
+++++++++++++--	'''
+++++++++++++--		path : path to experiment directory; expects `rollout.json` to be in directory
+++++++++++++--	'''
+++++++++++++--	fullpath = os.path.join(path, 'rollout.json')
+++++++++++++--	suffix = path.split('/')[-1]
+++++++++++++--
+++++++++++++--	if not os.path.exists(fullpath):
+++++++++++++--		return None
+++++++++++++--
+++++++++++++--	results = json.load(open(fullpath, 'rb'))
+++++++++++++--	score = results['score']
+++++++++++++--	return score * 100
+++++++++++++--
+++++++++++++--#######################
+++++++++++++--######## setup ########
+++++++++++++--#######################
+++++++++++++--
+++++++++++++--if __name__ == '__main__':
+++++++++++++--
+++++++++++++--	class Parser(utils.Parser):
+++++++++++++--	    dataset: str = None
+++++++++++++--
+++++++++++++--	args = Parser().parse_args()
+++++++++++++--
+++++++++++++--	for dataset in ([args.dataset] if args.dataset else DATASETS):
+++++++++++++--		subdirs = glob.glob(os.path.join(LOGBASE, dataset, EXP_NAME))
+++++++++++++--
+++++++++++++--		for subdir in subdirs:
+++++++++++++--			reldir = subdir.split('/')[-1]
+++++++++++++--			paths = glob.glob(os.path.join(subdir, TRIAL))
+++++++++++++--
+++++++++++++--			mean, err, scores = load_results(paths)
+++++++++++++--			print(f'{dataset.ljust(30)} | {subdir.ljust(50)} | {len(scores)} scores \n    {mean:.2f} +/- {err:.2f}\n')
+++++++++++++-diff --git a/halfcheetah/plotting/scores.py b/halfcheetah/plotting/scores.py
+++++++++++++-deleted file mode 100644
+++++++++++++-index f1917f7..0000000
+++++++++++++---- a/halfcheetah/plotting/scores.py
+++++++++++++-+++ /dev/null
+++++++++++++-@@ -1,123 +0,0 @@
+++++++++++++--means = {
+++++++++++++--	'Trajectory\nTransformer': {
+++++++++++++--		##
+++++++++++++--		'halfcheetah-medium-expert-v2': 95.0,
+++++++++++++--		'hopper-medium-expert-v2': 110.0,
+++++++++++++--		'walker2d-medium-expert-v2': 101.9,
+++++++++++++--		'ant-medium-expert-v2': 116.1,
+++++++++++++--		##
+++++++++++++--		'halfcheetah-medium-v2': 46.9,
+++++++++++++--		'hopper-medium-v2': 61.1,
+++++++++++++--		'walker2d-medium-v2': 79.0,
+++++++++++++--		'ant-medium-v2': 83.1,
+++++++++++++--		##
+++++++++++++--		'halfcheetah-medium-replay-v2': 41.9,
+++++++++++++--		'hopper-medium-replay-v2': 91.5,
+++++++++++++--		'walker2d-medium-replay-v2': 82.6,
+++++++++++++--		'ant-medium-replay-v2': 77.0,
+++++++++++++--	},
+++++++++++++--	'Decision\nTransformer': {
+++++++++++++--		##
+++++++++++++--		'halfcheetah-medium-expert-v2': 86.8,
+++++++++++++--		'hopper-medium-expert-v2': 107.6,
+++++++++++++--		'walker2d-medium-expert-v2': 108.1,
+++++++++++++--		##
+++++++++++++--		'halfcheetah-medium-v2': 42.6,
+++++++++++++--		'hopper-medium-v2': 67.6,
+++++++++++++--		'walker2d-medium-v2': 74.0,
+++++++++++++--		##
+++++++++++++--		'halfcheetah-medium-replay-v2': 36.6,
+++++++++++++--		'hopper-medium-replay-v2': 82.7,
+++++++++++++--		'walker2d-medium-replay-v2': 66.6,
+++++++++++++--	},
+++++++++++++--	'CQL': {
+++++++++++++--		##
+++++++++++++--		'halfcheetah-medium-expert-v2': 91.6,
+++++++++++++--		'hopper-medium-expert-v2': 105.4,
+++++++++++++--		'walker2d-medium-expert-v2': 108.8,
+++++++++++++--		##
+++++++++++++--		'halfcheetah-medium-v2': 44.0,
+++++++++++++--		'hopper-medium-v2': 58.5,
+++++++++++++--		'walker2d-medium-v2': 72.5,
+++++++++++++--		##
+++++++++++++--		'halfcheetah-medium-replay-v2': 45.5,
+++++++++++++--		'hopper-medium-replay-v2': 95.0,
+++++++++++++--		'walker2d-medium-replay-v2': 77.2,
+++++++++++++--	},
+++++++++++++--	'MOPO': {
+++++++++++++--		##
+++++++++++++--		'halfcheetah-medium-expert-v2': 63.3,
+++++++++++++--		'hopper-medium-expert-v2': 23.7,
+++++++++++++--		'walker2d-medium-expert-v2': 44.6,
+++++++++++++--		##
+++++++++++++--		'halfcheetah-medium-v2': 42.3,
+++++++++++++--		'hopper-medium-v2': 28.0,
+++++++++++++--		'walker2d-medium-v2': 17.8,
+++++++++++++--		##
+++++++++++++--		'halfcheetah-medium-replay-v2': 53.1,
+++++++++++++--		'hopper-medium-replay-v2': 67.5,
+++++++++++++--		'walker2d-medium-replay-v2':39.0,
+++++++++++++--	},
+++++++++++++--	'MBOP': {
+++++++++++++--		##
+++++++++++++--		'halfcheetah-medium-expert-v2': 105.9,
+++++++++++++--		'hopper-medium-expert-v2': 55.1,
+++++++++++++--		'walker2d-medium-expert-v2': 70.2,
+++++++++++++--		##
+++++++++++++--		'halfcheetah-medium-v2': 44.6,
+++++++++++++--		'hopper-medium-v2': 48.8,
+++++++++++++--		'walker2d-medium-v2': 41.0,
+++++++++++++--		##
+++++++++++++--		'halfcheetah-medium-replay-v2': 42.3,
+++++++++++++--		'hopper-medium-replay-v2': 12.4,
+++++++++++++--		'walker2d-medium-replay-v2': 9.7,
+++++++++++++--	},
+++++++++++++--	'BRAC': {
+++++++++++++--		##
+++++++++++++--		'halfcheetah-medium-expert-v2': 41.9,
+++++++++++++--		'hopper-medium-expert-v2': 0.9,
+++++++++++++--		'walker2d-medium-expert-v2': 81.6,
+++++++++++++--		##
+++++++++++++--		'halfcheetah-medium-v2': 46.3,
+++++++++++++--		'hopper-medium-v2': 31.3,
+++++++++++++--		'walker2d-medium-v2': 81.1,
+++++++++++++--		##
+++++++++++++--		'halfcheetah-medium-replay-v2': 47.7,
+++++++++++++--		'hopper-medium-replay-v2': 0.6,
+++++++++++++--		'walker2d-medium-replay-v2': 0.9,
+++++++++++++--	},
+++++++++++++--	'BC': {
+++++++++++++--		##
+++++++++++++--		'halfcheetah-medium-expert-v2': 59.9,
+++++++++++++--		'hopper-medium-expert-v2': 79.6,
+++++++++++++--		'walker2d-medium-expert-v2': 36.6,
+++++++++++++--		##
+++++++++++++--		'halfcheetah-medium-v2': 43.1,
+++++++++++++--		'hopper-medium-v2': 63.9,
+++++++++++++--		'walker2d-medium-v2': 77.3,
+++++++++++++--		##
+++++++++++++--		'halfcheetah-medium-replay-v2': 4.3,
+++++++++++++--		'hopper-medium-replay-v2': 27.6,
+++++++++++++--		'walker2d-medium-replay-v2': 36.9,
+++++++++++++--	},
+++++++++++++--}
+++++++++++++--
+++++++++++++--errors = {
+++++++++++++--	'Trajectory\nTransformer': {
+++++++++++++--		##
+++++++++++++--		'halfcheetah-medium-expert-v2': 0.2,
+++++++++++++--		'hopper-medium-expert-v2': 2.7,
+++++++++++++--		'walker2d-medium-expert-v2': 6.8,
+++++++++++++--		'ant-medium-expert-v2': 9.0,
+++++++++++++--		##
+++++++++++++--		'halfcheetah-medium-v2': 0.4,
+++++++++++++--		'hopper-medium-v2': 3.6,
+++++++++++++--		'walker2d-medium-v2': 2.8,
+++++++++++++--		'ant-medium-v2': 7.3,
+++++++++++++--		##
+++++++++++++--		'halfcheetah-medium-replay-v2': 2.5,
+++++++++++++--		'hopper-medium-replay-v2': 3.6,
+++++++++++++--		'walker2d-medium-replay-v2': 6.9,
+++++++++++++--		'ant-medium-replay-v2': 6.8,
+++++++++++++--	},
+++++++++++++--}
+++++++++++++++-trajectory_aaa
+++++++++++++++diff --git a/seaquest/readme.md b/seaquest/readme.md
+++++++++++++++index 84e53f8..53561f9 100644
+++++++++++++++--- a/seaquest/readme.md
++++++++++++++++++ b/seaquest/readme.md
+++++++++++++++@@ -10,4 +10,4 @@ pip install git+https://github.com/takuseno/d4rl-atari
+++++++++++++++ pip install "gym[atari, accept-rom-license]"
+++++++++++++++ pip install pyclustering
+++++++++++++++ pip install seaborn
+++++++++++++++-pip install d3rlpy==1.1.1
+++++++++++++++\ No newline at end of file
++++++++++++++++pip install d3rlpy==1.1.1
+++++++++++++ \ No newline at end of file
+++++++++++++-diff --git a/halfcheetah/plotting/table.py b/halfcheetah/plotting/table.py
+++++++++++++-deleted file mode 100644
+++++++++++++-index eae74e6..0000000
+++++++++++++---- a/halfcheetah/plotting/table.py
+++++++++++++-+++ /dev/null
+++++++++++++-@@ -1,127 +0,0 @@
+++++++++++++--import numpy as np
+++++++++++++--import pdb
+++++++++++++--
+++++++++++++--from plotting.plot import get_mean
+++++++++++++--from plotting.scores import (
+++++++++++++--	means as MEANS,
+++++++++++++--	errors as ERRORS,
+++++++++++++--)
+++++++++++++--
+++++++++++++--ALGORITHM_STRINGS = {
+++++++++++++--	'Trajectory\nTransformer': 'TT (Ours)',
+++++++++++++--	'Decision\nTransformer': 'DT',	
+++++++++++++--}
+++++++++++++--
+++++++++++++--BUFFER_STRINGS = {
+++++++++++++--	'medium-expert': 'Medium-Expert',
+++++++++++++--	'medium': 'Medium',
+++++++++++++--	'medium-replay': 'Medium-Replay',	
+++++++++++++--}
+++++++++++++--
+++++++++++++--ENVIRONMENT_STRINGS = {
+++++++++++++--	'halfcheetah': 'HalfCheetah',
+++++++++++++--	'hopper': 'Hopper',
+++++++++++++--	'walker2d': 'Walker2d',
+++++++++++++--	'ant': 'Ant',
+++++++++++++--}
+++++++++++++--
+++++++++++++--SHOW_ERRORS = ['Trajectory\nTransformer']
+++++++++++++--
+++++++++++++--def get_result(algorithm, buffer, environment, version='v2'):
+++++++++++++--	key = f'{environment}-{buffer}-{version}'
+++++++++++++--	mean = MEANS[algorithm].get(key, '-')
+++++++++++++--	if algorithm in SHOW_ERRORS:
+++++++++++++--		error = ERRORS[algorithm].get(key)
+++++++++++++--		return (mean, error)
+++++++++++++--	else:
+++++++++++++--		return mean
+++++++++++++--
+++++++++++++--def format_result(result):
+++++++++++++--	if type(result) == tuple:
+++++++++++++--		mean, std = result
+++++++++++++--		return f'${mean}$ \\scriptsize{{\\raisebox{{1pt}}{{$\\pm {std}$}}}}'
+++++++++++++--	else:
+++++++++++++--		return f'${result}$'
+++++++++++++--
+++++++++++++--def format_row(buffer, environment, results):
+++++++++++++--	buffer_str = BUFFER_STRINGS[buffer]
+++++++++++++--	environment_str = ENVIRONMENT_STRINGS[environment]
+++++++++++++--	results_str = ' & '.join(format_result(result) for result in results)
+++++++++++++--	row = f'{buffer_str} & {environment_str} & {results_str} \\\\ \n'
+++++++++++++--	return row
+++++++++++++--
+++++++++++++--def format_buffer_block(algorithms, buffer, environments):
+++++++++++++--	block_str = '\\midrule\n'
+++++++++++++--	for environment in environments:
+++++++++++++--		results = [get_result(alg, buffer, environment) for alg in algorithms]
+++++++++++++--		row_str = format_row(buffer, environment, results)
+++++++++++++--		block_str += row_str
+++++++++++++--	return block_str
+++++++++++++--
+++++++++++++--def format_algorithm(algorithm):
+++++++++++++--	algorithm_str = ALGORITHM_STRINGS.get(algorithm, algorithm)
+++++++++++++--	return f'\multicolumn{{1}}{{c}}{{\\bf {algorithm_str}}}'
+++++++++++++--
+++++++++++++--def format_algorithms(algorithms):
+++++++++++++--	return ' & '.join(format_algorithm(algorithm) for algorithm in algorithms)
+++++++++++++--
+++++++++++++--def format_averages(means, label):
+++++++++++++--	prefix = f'\\multicolumn{{2}}{{c}}{{\\bf Average ({label})}} & '
+++++++++++++--	formatted = ' & '.join(str(mean) for mean in means)
+++++++++++++--	return prefix + formatted
+++++++++++++--
+++++++++++++--def format_averages_block(algorithms):
+++++++++++++--	means_filtered = [np.round(get_mean(MEANS[algorithm], exclude='ant'), 1) for algorithm in algorithms]
+++++++++++++--	means_all = [np.round(get_mean(MEANS[algorithm], exclude=None), 1) for algorithm in algorithms]
+++++++++++++--
+++++++++++++--	means_all = [
+++++++++++++--		means
+++++++++++++--		if 'ant-medium-expert-v2' in MEANS[algorithm]
+++++++++++++--		else '$-$'
+++++++++++++--		for algorithm, means in zip(algorithms, means_all)
+++++++++++++--	]
+++++++++++++--
+++++++++++++--	formatted_filtered = format_averages(means_filtered, 'without Ant')
+++++++++++++--	formatted_all = format_averages(means_all, 'all settings')
+++++++++++++--
+++++++++++++--	formatted_block = (
+++++++++++++--		f'{formatted_filtered} \\hspace{{.6cm}} \\\\ \n'
+++++++++++++--		f'{formatted_all} \\hspace{{.6cm}} \\\\ \n'
+++++++++++++--	)
+++++++++++++--	return formatted_block
+++++++++++++--
+++++++++++++--def format_table(algorithms, buffers, environments):
+++++++++++++--	justify_str = 'll' + 'r' * len(algorithms)
+++++++++++++--	algorithm_str = format_algorithms(['Dataset', 'Environment'] + algorithms)
+++++++++++++--	averages_str = format_averages_block(algorithms)
+++++++++++++--	table_prefix = (
+++++++++++++--		'\\begin{table*}[h]\n'
+++++++++++++--		'\\centering\n'
+++++++++++++--		'\\small\n'
+++++++++++++--		f'\\begin{{tabular}}{{{justify_str}}}\n'
+++++++++++++--		'\\toprule\n'
+++++++++++++--		f'{algorithm_str} \\\\ \n'
+++++++++++++--	)
+++++++++++++--	table_suffix = (
+++++++++++++--		'\\midrule\n'
+++++++++++++--		f'{averages_str}'
+++++++++++++--		'\\bottomrule\n'
+++++++++++++--		'\\end{tabular}\n'
+++++++++++++--		'\\label{table:d4rl}\n'
+++++++++++++--		'\\end{table*}'
+++++++++++++--	)
+++++++++++++--	blocks = ''.join(format_buffer_block(algorithms, buffer, environments) for buffer in buffers)
+++++++++++++--	table = (
+++++++++++++--		f'{table_prefix}'
+++++++++++++--		f'{blocks}'
+++++++++++++--		f'{table_suffix}'
+++++++++++++--	)
+++++++++++++--	return table
+++++++++++++--
+++++++++++++--
+++++++++++++--algorithms =['BC', 'MBOP', 'BRAC', 'CQL',  'Decision\nTransformer', 'Trajectory\nTransformer']
+++++++++++++--buffers = ['medium-expert', 'medium', 'medium-replay']
+++++++++++++--environments = ['halfcheetah', 'hopper', 'walker2d', 'ant']
+++++++++++++--
+++++++++++++--table = format_table(algorithms, buffers, environments)
+++++++++++++--print(table)
+++++++++++++-diff --git a/halfcheetah/scripts/plan.py b/halfcheetah/scripts/plan.py
+++++++++++++-deleted file mode 100644
+++++++++++++-index f13d4cc..0000000
+++++++++++++---- a/halfcheetah/scripts/plan.py
+++++++++++++-+++ /dev/null
+++++++++++++-@@ -1,124 +0,0 @@
+++++++++++++--import json
+++++++++++++--import pdb
+++++++++++++--from os.path import join
+++++++++++++--
+++++++++++++--import trajectory.utils as utils
+++++++++++++--import trajectory.datasets as datasets
+++++++++++++--from trajectory.search import (
+++++++++++++--    beam_plan,
+++++++++++++--    make_prefix,
+++++++++++++--    extract_actions,
+++++++++++++--    update_context,
+++++++++++++--)
+++++++++++++--
+++++++++++++--class Parser(utils.Parser):
+++++++++++++--    dataset: str = 'halfcheetah-medium-expert-v2'
+++++++++++++--    config: str = 'config.offline'
+++++++++++++--
+++++++++++++--#######################
+++++++++++++--######## setup ########
+++++++++++++--#######################
+++++++++++++--
+++++++++++++--args = Parser().parse_args('plan')
+++++++++++++--
+++++++++++++--#######################
+++++++++++++--####### models ########
+++++++++++++--#######################
+++++++++++++--
+++++++++++++--dataset = utils.load_from_config(args.logbase, args.dataset, args.gpt_loadpath,
+++++++++++++--        'data_config.pkl')
+++++++++++++--
+++++++++++++--gpt, gpt_epoch = utils.load_model(args.logbase, args.dataset, args.gpt_loadpath,
+++++++++++++--        epoch=args.gpt_epoch, device=args.device)
+++++++++++++--
+++++++++++++--#######################
+++++++++++++--####### dataset #######
+++++++++++++--#######################
+++++++++++++--
+++++++++++++--env = datasets.load_environment(args.dataset)
+++++++++++++--print('yo')
+++++++++++++--renderer = utils.make_renderer(args)
+++++++++++++--timer = utils.timer.Timer()
+++++++++++++--
+++++++++++++--discretizer = dataset.discretizer
+++++++++++++--discount = dataset.discount
+++++++++++++--observation_dim = dataset.observation_dim
+++++++++++++--action_dim = dataset.action_dim
+++++++++++++--
+++++++++++++--value_fn = lambda x: discretizer.value_fn(x, args.percentile)
+++++++++++++--preprocess_fn = datasets.get_preprocess_fn(env.name)
+++++++++++++--
+++++++++++++--print('yo2')
+++++++++++++--
+++++++++++++--#######################
+++++++++++++--###### main loop ######
+++++++++++++--#######################
+++++++++++++--
+++++++++++++--observation = env.reset()
+++++++++++++--total_reward = 0
+++++++++++++--
+++++++++++++--## observations for rendering
+++++++++++++--rollout = [observation.copy()]
+++++++++++++--
+++++++++++++--## previous (tokenized) transitions for conditioning transformer
+++++++++++++--context = []
+++++++++++++--
+++++++++++++--T = env.max_episode_steps
+++++++++++++--for t in range(T):
+++++++++++++--
+++++++++++++--    observation = preprocess_fn(observation)
+++++++++++++--
+++++++++++++--    if t % args.plan_freq == 0:
+++++++++++++--        ## concatenate previous transitions and current observations to input to model
+++++++++++++--        prefix = make_prefix(discretizer, context, observation, args.prefix_context)
+++++++++++++--
+++++++++++++--        ## sample sequence from model beginning with `prefix`
+++++++++++++--        sequence = beam_plan(
+++++++++++++--            gpt, value_fn, prefix,
+++++++++++++--            args.horizon, args.beam_width, args.n_expand, observation_dim, action_dim,
+++++++++++++--            discount, args.max_context_transitions, verbose=args.verbose,
+++++++++++++--            k_obs=args.k_obs, k_act=args.k_act, cdf_obs=args.cdf_obs, cdf_act=args.cdf_act,
+++++++++++++--        )
+++++++++++++--
+++++++++++++--    else:
+++++++++++++--        sequence = sequence[1:]
+++++++++++++--
+++++++++++++--    ## [ horizon x transition_dim ] convert sampled tokens to continuous trajectory
+++++++++++++--    sequence_recon = discretizer.reconstruct(sequence)
+++++++++++++--
+++++++++++++--    ## [ action_dim ] index into sampled trajectory to grab first action
+++++++++++++--    action = extract_actions(sequence_recon, observation_dim, action_dim, t=0)
+++++++++++++--
+++++++++++++--    ## execute action in environment
+++++++++++++--    next_observation, reward, terminal, _ = env.step(action)
+++++++++++++--
+++++++++++++--    ## update return
+++++++++++++--    total_reward += reward
+++++++++++++--    score = env.get_normalized_score(total_reward)
+++++++++++++--
+++++++++++++--    ## update rollout observations and context transitions
+++++++++++++--    rollout.append(next_observation.copy())
+++++++++++++--    context = update_context(context, discretizer, observation, action, reward, args.max_context_transitions)
+++++++++++++--
+++++++++++++--    print(
+++++++++++++--        f'[ plan ] t: {t} / {T} | r: {reward:.2f} | R: {total_reward:.2f} | score: {score:.4f} | '
+++++++++++++--        f'time: {timer():.2f} | {args.dataset} | {args.exp_name} | {args.suffix}\n'
+++++++++++++--    )
+++++++++++++--
+++++++++++++--    ## visualization
+++++++++++++--    if t % args.vis_freq == 0 or terminal or t == T:
+++++++++++++--
+++++++++++++--        ## save current plan
+++++++++++++--        renderer.render_plan(join(args.savepath, f'{t}_plan.mp4'), sequence_recon, env.state_vector())
+++++++++++++--
+++++++++++++--        ## save rollout thus far
+++++++++++++--        renderer.render_rollout(join(args.savepath, f'rollout.mp4'), rollout, fps=80)
+++++++++++++--
+++++++++++++--    if terminal: break
+++++++++++++--
+++++++++++++--    observation = next_observation
+++++++++++++--
+++++++++++++--## save result as a json file
+++++++++++++--json_path = join(args.savepath, 'rollout.json')
+++++++++++++--json_data = {'score': score, 'step': t, 'return': total_reward, 'term': terminal, 'gpt_epoch': gpt_epoch}
+++++++++++++--json.dump(json_data, open(json_path, 'w'), indent=2, sort_keys=True)
+++++++++++++-diff --git a/halfcheetah/scripts/train.py b/halfcheetah/scripts/train.py
+++++++++++++-deleted file mode 100644
+++++++++++++-index 04af8d7..0000000
+++++++++++++---- a/halfcheetah/scripts/train.py
+++++++++++++-+++ /dev/null
+++++++++++++-@@ -1,122 +0,0 @@
+++++++++++++--import os
+++++++++++++--import numpy as np
+++++++++++++--import torch
+++++++++++++--import pdb
+++++++++++++--
+++++++++++++--import trajectory.utils as utils
+++++++++++++--import trajectory.datasets as datasets
+++++++++++++--from trajectory.models.transformers import GPT
+++++++++++++--
+++++++++++++--
+++++++++++++--class Parser(utils.Parser):
+++++++++++++--    dataset: str = 'halfcheetah-medium-expert-v2'
+++++++++++++--    config: str = 'config.offline'
+++++++++++++--
+++++++++++++--#######################
+++++++++++++--######## setup ########
+++++++++++++--#######################
+++++++++++++--
+++++++++++++--args = Parser().parse_args('train')
+++++++++++++--
+++++++++++++--#######################
+++++++++++++--####### dataset #######
+++++++++++++--#######################
+++++++++++++--
+++++++++++++--env = datasets.load_environment(args.dataset)
+++++++++++++--
+++++++++++++--sequence_length = args.subsampled_sequence_length * args.step
+++++++++++++--
+++++++++++++--dataset_config = utils.Config(
+++++++++++++--    datasets.DiscretizedDataset,
+++++++++++++--    savepath=(args.savepath, 'data_config.pkl'),
+++++++++++++--    env=args.dataset,
+++++++++++++--    N=args.N,
+++++++++++++--    penalty=args.termination_penalty,
+++++++++++++--    sequence_length=sequence_length,
+++++++++++++--    step=args.step,
+++++++++++++--    discount=args.discount,
+++++++++++++--    discretizer=args.discretizer,
+++++++++++++--)
+++++++++++++--
+++++++++++++--dataset = dataset_config()
+++++++++++++--obs_dim = dataset.observation_dim
+++++++++++++--act_dim = dataset.action_dim
+++++++++++++--transition_dim = dataset.joined_dim
+++++++++++++--
+++++++++++++--#######################
+++++++++++++--######## model ########
+++++++++++++--#######################
+++++++++++++--
+++++++++++++--block_size = args.subsampled_sequence_length * transition_dim - 1
+++++++++++++--print(
+++++++++++++--    f'Dataset size: {len(dataset)} | '
+++++++++++++--    f'Joined dim: {transition_dim} '
+++++++++++++--    f'(observation: {obs_dim}, action: {act_dim}) | Block size: {block_size}'
+++++++++++++--)
+++++++++++++--
+++++++++++++--model_config = utils.Config(
+++++++++++++--    GPT,
+++++++++++++--    savepath=(args.savepath, 'model_config.pkl'),
+++++++++++++--    ## discretization
+++++++++++++--    vocab_size=args.N, block_size=block_size,
+++++++++++++--    ## architecture
+++++++++++++--    n_layer=args.n_layer, n_head=args.n_head, n_embd=args.n_embd*args.n_head,
+++++++++++++--    ## dimensions
+++++++++++++--    observation_dim=obs_dim, action_dim=act_dim, transition_dim=transition_dim,
+++++++++++++--    ## loss weighting
+++++++++++++--    action_weight=args.action_weight, reward_weight=args.reward_weight, value_weight=args.value_weight,
+++++++++++++--    ## dropout probabilities
+++++++++++++--    embd_pdrop=args.embd_pdrop, resid_pdrop=args.resid_pdrop, attn_pdrop=args.attn_pdrop,
+++++++++++++--)
+++++++++++++--
+++++++++++++--model = model_config()
+++++++++++++--model.to(args.device)
+++++++++++++--
+++++++++++++--#######################
+++++++++++++--####### trainer #######
+++++++++++++--#######################
+++++++++++++--
+++++++++++++--warmup_tokens = len(dataset) * block_size ## number of tokens seen per epoch
+++++++++++++--final_tokens = 20 * warmup_tokens
+++++++++++++--
+++++++++++++--trainer_config = utils.Config(
+++++++++++++--    utils.Trainer,
+++++++++++++--    savepath=(args.savepath, 'trainer_config.pkl'),
+++++++++++++--    # optimization parameters
+++++++++++++--    batch_size=args.batch_size,
+++++++++++++--    learning_rate=args.learning_rate,
+++++++++++++--    betas=(0.9, 0.95),
+++++++++++++--    grad_norm_clip=1.0,
+++++++++++++--    weight_decay=0.1, # only applied on matmul weights
+++++++++++++--    # learning rate decay: linear warmup followed by cosine decay to 10% of original
+++++++++++++--    lr_decay=args.lr_decay,
+++++++++++++--    warmup_tokens=warmup_tokens,
+++++++++++++--    final_tokens=final_tokens,
+++++++++++++--    ## dataloader
+++++++++++++--    num_workers=0,
+++++++++++++--    device=args.device,
+++++++++++++--)
+++++++++++++--
+++++++++++++--trainer = trainer_config()
+++++++++++++--
+++++++++++++--#######################
+++++++++++++--###### main loop ######
+++++++++++++--#######################
+++++++++++++--
+++++++++++++--## scale number of epochs to keep number of updates constant
+++++++++++++--n_epochs = int(1e6 / len(dataset) * args.n_epochs_ref)
+++++++++++++--save_freq = int(n_epochs // args.n_saves)
+++++++++++++--
+++++++++++++--for epoch in range(n_epochs):
+++++++++++++--    print(f'\nEpoch: {epoch} / {n_epochs} | {args.dataset} | {args.exp_name}')
+++++++++++++--
+++++++++++++--    trainer.train(model, dataset)
+++++++++++++--
+++++++++++++--    ## get greatest multiple of `save_freq` less than or equal to `save_epoch`
+++++++++++++--    save_epoch = (epoch + 1) // save_freq * save_freq
+++++++++++++--    statepath = os.path.join(args.savepath, f'state_{save_epoch}.pt')
+++++++++++++--    print(f'Saving model to {statepath}')
+++++++++++++--
+++++++++++++--    ## save state to disk
+++++++++++++--    state = model.state_dict()
+++++++++++++--    torch.save(state, statepath)
+++++++++++++-diff --git a/halfcheetah/scripts/xrl.py b/halfcheetah/scripts/xrl.py
+++++++++++++-deleted file mode 100644
+++++++++++++-index 134232a..0000000
+++++++++++++---- a/halfcheetah/scripts/xrl.py
+++++++++++++-+++ /dev/null
+++++++++++++-@@ -1,372 +0,0 @@
+++++++++++++--import json
+++++++++++++--import pdb
+++++++++++++--from os.path import join
+++++++++++++--
+++++++++++++--import trajectory.utils as utils
+++++++++++++--import trajectory.datasets as datasets
+++++++++++++--from trajectory.search import (
+++++++++++++--    make_prefix,
+++++++++++++--    update_context,
+++++++++++++--)
+++++++++++++--from trajectory.search.sampling import forward
+++++++++++++--
+++++++++++++--import gym
+++++++++++++--import d4rl # Import required to register environments, you may need to also import the submodule
+++++++++++++--import numpy as np
+++++++++++++--import d3rlpy
+++++++++++++--import math as mt
+++++++++++++--from sklearn.cluster import KMeans
+++++++++++++--from sklearn import datasets as skdatasets
+++++++++++++--from sklearn.decomposition import PCA
+++++++++++++--
+++++++++++++--from pyclustering.cluster.xmeans import xmeans
+++++++++++++--from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer
+++++++++++++--
+++++++++++++--from scipy.stats import wasserstein_distance
+++++++++++++--
+++++++++++++--class Parser(utils.Parser):
+++++++++++++--    dataset: str = 'halfcheetah-medium-expert-v2'
+++++++++++++--    config: str = 'config.offline'
+++++++++++++--
++++++++++++++diff --git a/halfcheetah/scripts/xrl_v2.py b/halfcheetah/scripts/xrl_v2.py
++++++++++++++index 62a3d4d..e6f9de0 100644
++++++++++++++--- a/halfcheetah/scripts/xrl_v2.py
+++++++++++++++++ b/halfcheetah/scripts/xrl_v2.py
++++++++++++++@@ -21,42 +21,64 @@ from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer
++++++++++++++ from scipy.stats import wasserstein_distance
++++++++++++++ from moviepy.editor import VideoFileClip
++++++++++++++ 
+++++++++++++++
++++++++++++++ class Parser(utils.Parser):
++++++++++++++     dataset: str = 'halfcheetah-medium-v2'
++++++++++++++     config: str = 'config.offline'
++++++++++++++ 
+++++++++++++ -# utils
+++++++++++++--    
+++++++++++++--class XMeans:
+++++++++++++--    def loglikelihood(self, r, rn, var, m, k):
+++++++++++++--        l1 = - rn / 2.0 * mt.log(2 * mt.pi)
+++++++++++++--        l2 = - rn * m / 2.0 * mt.log(var)
+++++++++++++--        l3 = - (rn - k) / 2.0
+++++++++++++--        l4 = rn * mt.log(rn)
+++++++++++++--        l5 = - rn * mt.log(r)
+++++++++++++--
+++++++++++++--        return l1 + l2 + l3 + l4 + l5
+++++++++++++--
+++++++++++++--    def __init__(self, X, kmax = 20):
+++++++++++++--        self.X = X
+++++++++++++--        self.num = np.size(self.X, axis=0)
+++++++++++++--        self.dim = np.size(X, axis=1)
+++++++++++++--        self.KMax = kmax
+++++++++++++--
+++++++++++++--    def fit(self):
+++++++++++++--        k = 1
+++++++++++++--        X = self.X
+++++++++++++--        M = self.dim
+++++++++++++--        num = self.num
+++++++++++++--
+++++++++++++--        while(1):
+++++++++++++--            ok = k
+++++++++++++--
+++++++++++++--            #Improve Params
+++++++++++++--            kmeans = KMeans(n_clusters=k).fit(X)
+++++++++++++--            labels = kmeans.labels_
+++++++++++++--            m = kmeans.cluster_centers_
+++++++++++++--
+++++++++++++--            #Improve Structure
+++++++++++++--            #Calculate BIC
+++++++++++++--            p = M + 1
+++++++++++++--
+++++++++++++--            obic = np.zeros(k)
+++++++++++++--
+++++++++++++--            for i in range(k):
+++++++++++++--                rn = np.size(np.where(labels == i))
+++++++++++++--                var = np.sum((X[labels == i] - m[i])**2)/float(rn - 1)
+++++++++++++--                obic[i] = self.loglikelihood(rn, rn, var, M, 1) - p/2.0*mt.log(rn)
+++++++++++++--
+++++++++++++--            #Split each cluster into two subclusters and calculate BIC of each splitted cluster
+++++++++++++--            sk = 2 #The number of subclusters
+++++++++++++--            nbic = np.zeros(k)
+++++++++++++--            addk = 0
+++++++++++++--
+++++++++++++--            for i in range(k):
+++++++++++++--                ci = X[labels == i]
+++++++++++++--                r = np.size(np.where(labels == i))
+++++++++++++--
+++++++++++++--                kmeans = KMeans(n_clusters=sk).fit(ci)
+++++++++++++--                ci_labels = kmeans.labels_
+++++++++++++--                sm = kmeans.cluster_centers_
+++++++++++++--
+++++++++++++--                for l in range(sk):
+++++++++++++--                    rn = np.size(np.where(ci_labels == l))
+++++++++++++--                    var = np.sum((ci[ci_labels == l] - sm[l])**2)/float(rn - sk)
+++++++++++++--                    nbic[i] += self.loglikelihood(r, rn, var, M, sk)
+++++++++++++--
+++++++++++++--                p = sk * (M + 1)
+++++++++++++--                nbic[i] -= p/2.0*mt.log(r)
+++++++++++++--
+++++++++++++--                if obic[i] < nbic[i]:
+++++++++++++--                    addk += 1
+++++++++++++--
+++++++++++++--            k += addk
+++++++++++++--
+++++++++++++--            if ok == k or k >= self.KMax:
+++++++++++++--                break
+++++++++++++--
+++++++++++++--
+++++++++++++--        #Calculate labels and centroids
+++++++++++++--        kmeans = KMeans(n_clusters=k).fit(X)
+++++++++++++--        self.labels = kmeans.labels_
+++++++++++++--        self.k = k
+++++++++++++--        self.m = kmeans.cluster_centers_
+++++++++++++--
+++++++++++++--
+++++++++++++--def cluster_trajectories(trajectories):
+++++++++++++--    xmeans_instance = XMeans(trajectories, kmax=10)
+++++++++++++--    xmeans_instance.fit()
+++++++++++++--
+++++++++++++--    clusters = xmeans_instance.labels
+++++++++++++--    return clusters
+++++++++++++--
+++++++++++++--def cluster_trajectories_2(trajectories):
++++++++++++++ 
++++++++++++++ def cluster_trajectories(trajectories, n_clusters=10):
++++++++++++++-    """TODO"""
+++++++++++++++    """
+++++++++++++++    Cluster trajectories using X-means.
+++++++++++++++    
+++++++++++++++    Args:
+++++++++++++++    - trajectories: np.array, shape (n_trajectories, encoding_dim)
+++++++++++++++    - n_clusters: int, max number of clusters
+++++++++++++++    
+++++++++++++++    Returns:
+++++++++++++++    - idxs_per_cluster: list, trajectory idxs per cluster idxs
+++++++++++++++    - clusters: np.array, shape (n_trajectories), cluster idxs per trajectory idx
+++++++++++++++    """ 
++++++++++++++ 
+++++++++++++ -    # Prepare initial centers - amount of initial centers defines amount of clusters from which X-Means will
+++++++++++++ -    # start analysis.
+++++++++++++--    amount_initial_centers = 2
+++++++++++++--    initial_centers = kmeans_plusplus_initializer(trajectories, amount_initial_centers).initialize()
+++++++++++++--    
+++++++++++++++    # Set 2 initial cluster centers
++++++++++++++     amount_initial_centers = 2
++++++++++++++     initial_centers = kmeans_plusplus_initializer(trajectories, amount_initial_centers).initialize()
++++++++++++++     
+++++++++++++ -    # Create instance of X-Means algorithm. The algorithm will start analysis from 2 clusters, the maximum
+++++++++++++ -    # number of clusters that can be allocated is 10.
+++++++++++++--    xmeans_instance = xmeans(trajectories, initial_centers, 10)
+++++++++++++--    xmeans_instance.process()
+++++++++++++--    
+++++++++++++--    # Extract clustering results: clusters
+++++++++++++--    idxs_per_cluster = xmeans_instance.get_clusters()
+++++++++++++--
+++++++++++++--    clusters = []
+++++++++++++--    for i in range(len(trajectories)):
+++++++++++++--        for j in range(len(idxs_per_cluster)):
+++++++++++++--            if i in idxs_per_cluster[j]: clusters.append(j)
+++++++++++++--
+++++++++++++--    return idxs_per_cluster, np.array(clusters)
+++++++++++++++    # Run X-means
++++++++++++++     xmeans_instance = xmeans(trajectories, initial_centers, n_clusters)
++++++++++++++     xmeans_instance.process()
++++++++++++++     
++++++++++++++     # Extract clustering results: clusters
++++++++++++++     idxs_per_cluster = xmeans_instance.get_clusters()
++++++++++++++ 
+++++++++++++++    # 
++++++++++++++     clusters = []
++++++++++++++     for i in range(len(trajectories)):
++++++++++++++         for j in range(len(idxs_per_cluster)):
++++++++++++++             if i in idxs_per_cluster[j]: clusters.append(j)
++++++++++++++ 
++++++++++++++     return idxs_per_cluster, np.array(clusters)
+++++++++++++ - 
+++++++++++++--# https://github.com/sascha-kirch/ML_Notebooks/blob/main/Softmax_Temperature.ipynb
+++++++++++++--def softmax(x, temp):
+++++++++++++--    """Compute softmax values for each sets of scores in x."""
+++++++++++++--    return np.exp(np.divide(x,temp)) / np.sum(np.exp(np.divide(x,temp)))
+++++++++++++--
+++++++++++++--def generate_data_embedding(trajectory_embeddings, normalizing_factor=1, temperature=1):
+++++++++++++--    embedding = np.sum(trajectory_embeddings, axis=0) / normalizing_factor
+++++++++++++--    embedding = softmax(embedding, temperature)
+++++++++++++--    return embedding
+++++++++++++--
+++++++++++++--def embed_trajectory(gpt, discretizer, observations, actions, rewards, preprocess_fn):
+++++++++++++--    context = []
+++++++++++++--
+++++++++++++--    for i in range(len(observations)):
+++++++++++++--        observation = observations[i]
+++++++++++++--        action = actions[i]
+++++++++++++--        reward = rewards[i]
+++++++++++++--
+++++++++++++--        observation = preprocess_fn(observation)
+++++++++++++--
+++++++++++++--        # print(observation)
+++++++++++++--        prefix = make_prefix(discretizer, context, observation, True)
+++++++++++++--        # print("prefix", prefix.shape)
+++++++++++++--
+++++++++++++--        out = forward(gpt, prefix)
+++++++++++++--        # print("out", out.shape)
+++++++++++++--        context = update_context(context, discretizer, observation, action, reward, len(observations))
+++++++++++++--        # print("cotext", context)
+++++++++++++--    
+++++++++++++--    emb = []
+++++++++++++--    for context_step in context:
+++++++++++++--        emb.append(context_step.numpy())
+++++++++++++--    emb = np.array(emb)
+++++++++++++--    emb = np.mean(emb, axis=0)[0]
+++++++++++++--
+++++++++++++--    return emb
+++++++++++++--
+++++++++++++--
+++++++++++++--def create_complementary_dataset(dataset, idxs, trajectory_length=10):
+++++++++++++--    observations = []
+++++++++++++--    actions = []
+++++++++++++--    rewards = []
+++++++++++++--    terminals = []
+++++++++++++--    for i in range(1000):
+++++++++++++--        if i not in idxs:
+++++++++++++--            observations += list(dataset.observations[1000*i:1000*i+trajectory_length])
+++++++++++++--            actions += list(dataset.actions[1000*i:1000*i+trajectory_length])
+++++++++++++--            rewards += list(dataset.rewards[1000*i:1000*i+trajectory_length])
+++++++++++++--            terminals += list(dataset.terminals[1000*i:1000*i+trajectory_length])
+++++++++++++--
+++++++++++++--    new_dataset = d3rlpy.dataset.MDPDataset(
+++++++++++++--        observations=np.array(observations),
+++++++++++++--        actions=np.array(actions),
+++++++++++++--        rewards=np.array(rewards),
+++++++++++++--        terminals=np.array(terminals)
+++++++++++++--    )
+++++++++++++--    return new_dataset
+++++++++++++--    
+++++++++++++--
+++++++++++++--
+++++++++++++--
+++++++++++++--def main():
+++++++++++++--    # args = Parser().parse_args('plan')
+++++++++++++--
+++++++++++++--    #######################
+++++++++++++--    ####### models ########
+++++++++++++--    #######################
+++++++++++++--
+++++++++++++--
+++++++++++++--
+++++++++++++--
+++++++++++++--
+++++++++++++--    # print(args.dataset)
+++++++++++++--
+++++++++++++--    # dataset = utils.load_from_config(args.logbase, args.dataset, args.gpt_loadpath,
+++++++++++++--    #         'data_config.pkl')
+++++++++++++--
+++++++++++++--
+++++++++++++--    # gpt, gpt_epoch = utils.load_model(args.logbase, args.dataset, args.gpt_loadpath,
+++++++++++++--    #         epoch=args.gpt_epoch, device=args.device)
+++++++++++++--
+++++++++++++--    # env = datasets.load_environment(args.dataset)
+++++++++++++--
+++++++++++++--    # discretizer = dataset.discretizer
+++++++++++++--
+++++++++++++--    # preprocess_fn = datasets.get_preprocess_fn(env.name)
+++++++++++++--
+++++++++++++--    # #######################
+++++++++++++--    # ####### dataset #######
+++++++++++++--    # #######################
+++++++++++++--
+++++++++++++--    # # env = datasets.load_environment(args.dataset)
+++++++++++++--    # discretizer = dataset.discretizer
+++++++++++++--    # preprocess_fn = datasets.get_preprocess_fn(env.name)
+++++++++++++--
+++++++++++++--    # # dataset
+++++++++++++--    dataset_d3, env = d3rlpy.datasets.get_dataset("halfcheetah-medium-v2")
+++++++++++++--
+++++++++++++--    # env = gym.make('halfcheetah-medium-v2')
+++++++++++++--    # dataset_d4 = d4rl.qlearning_dataset(env)
+++++++++++++--
+++++++++++++--    # # checks to see if d3rl & d4rl datasets are equal
+++++++++++++--    # print(np.allclose(dataset_d3.actions[100], dataset_d4['actions'][100]))
+++++++++++++--
+++++++++++++--    # # dr4rl has same trajectories, just cut off 1 element before the end
+++++++++++++--    # for j in range(1000):
+++++++++++++--    #     for i in range(999):
+++++++++++++--    #         if dataset_d4['rewards'][j * 999 + i] != dataset_d3.rewards[j * 1000 + i]: print("yo", i)
+++++++++++++--
+++++++++++++--    # #######################
+++++++++++++--    # ###### main loop ######
+++++++++++++--    # #######################
+++++++++++++--
+++++++++++++--    trajectory_length = 10 # 10 = max
+++++++++++++--
+++++++++++++--    # embeddings = []
+++++++++++++--    # for i in range(1000):
+++++++++++++--    #     observations = dataset_d3.observations[1000*i:1000*i+trajectory_length]
+++++++++++++--    #     actions = dataset_d3.actions[1000*i:1000*i+trajectory_length]
+++++++++++++--    #     rewards = dataset_d3.rewards[1000*i:1000*i+trajectory_length]
+++++++++++++--    #     terminals = dataset_d3.terminals[1000*i:1000*i+trajectory_length]
+++++++++++++--    #     emb = embed_trajectory(gpt, discretizer, observations, actions, rewards, preprocess_fn)
+++++++++++++--    #     embeddings.append(emb)
+++++++++++++--    # embeddings = np.array(embeddings)
+++++++++++++--    # np.save("embeddings.npy", embeddings)
+++++++++++++--    # print(embeddings)
+++++++++++++--
+++++++++++++--    embeddings = np.load("embeddings.npy")
+++++++++++++--
+++++++++++++--    pca = PCA(n_components=2)
+++++++++++++--    pca = PCA(n_components=2)
+++++++++++++--    pca_embeddings = pca.fit_transform(embeddings)
+++++++++++++--    np.save("pca.py", pca_embeddings)
+++++++++++++--
+++++++++++++--    idxs_per_cluster, clusters = cluster_trajectories_2(embeddings)
+++++++++++++--    # print(clusters)
+++++++++++++--    # return
+++++++++++++--    np.save("clusters.npy", clusters)
+++++++++++++--
+++++++++++++--    import matplotlib.pyplot as plt
+++++++++++++--
+++++++++++++--    d_orig = generate_data_embedding(embeddings)
+++++++++++++--    unique_clusters = np.unique(clusters)
+++++++++++++--    
+++++++++++++--    d_j = []
+++++++++++++--    complementary_datasets = []
+++++++++++++--    for j in np.sort(unique_clusters):
+++++++++++++--        print(j)
+++++++++++++--        d_j.append(generate_data_embedding(embeddings[clusters != j]))
+++++++++++++--        plt.scatter(pca_embeddings[clusters == j][:,0], pca_embeddings[clusters == j][:,1], label=j)
+++++++++++++--        complementary_datasets.append(create_complementary_dataset(dataset_d3, idxs_per_cluster[j], trajectory_length))
+++++++++++++--    
+++++++++++++--    original_dataset = create_complementary_dataset(dataset_d3, [], trajectory_length)
+++++++++++++--
+++++++++++++--    print(complementary_datasets, original_dataset)
+++++++++++++--
+++++++++++++--    plt.legend()
+++++++++++++--    plt.show()
+++++++++++++--
+++++++++++++--    agent_orig = d3rlpy.algos.SAC(
+++++++++++++--        actor_learning_rate=3e-4,
+++++++++++++--        critic_learning_rate=3e-4,
+++++++++++++--        temp_learning_rate=3e-4,
+++++++++++++--        batch_size=256)
+++++++++++++--
+++++++++++++--    print(agent_orig)
+++++++++++++--
+++++++++++++--    training_steps = 1000
+++++++++++++--
+++++++++++++--    agent_orig.fit(original_dataset, n_steps=training_steps)
+++++++++++++--
+++++++++++++--    agents_compl = []
+++++++++++++--
+++++++++++++--    for dset in complementary_datasets:
+++++++++++++--        agent = d3rlpy.algos.SAC(
+++++++++++++--            actor_learning_rate=3e-4,
+++++++++++++--            critic_learning_rate=3e-4,
+++++++++++++--            temp_learning_rate=3e-4,
+++++++++++++--            batch_size=256)
+++++++++++++--        agent.fit(dset, n_steps=training_steps)
+++++++++++++--        agents_compl.append(agent)
+++++++++++++--
+++++++++++++--    action_orig = agent_orig.predict(dataset_d3.observations[0])
+++++++++++++--
+++++++++++++--    actions_compl = []
+++++++++++++--    for agent in agents_compl:
+++++++++++++--        actions_compl.append(agent.predict(dataset_d3.observations[0]))
+++++++++++++--    
+++++++++++++--    action_dists = []
+++++++++++++--    for action in actions_compl:
+++++++++++++--        action_dists.append(np.linalg.norm(action_orig-action))
+++++++++++++--
+++++++++++++--    k = 3
+++++++++++++--    topk = np.argpartition(action_dists, -k)[-k:]
+++++++++++++--
+++++++++++++--    d_w = {}
+++++++++++++--    for idx in topk:
+++++++++++++--        d_w[idx] = wasserstein_distance(d_j[idx], d_orig)
+++++++++++++--
+++++++++++++--    cluster_assignment = min(d_w, key=d_w.get)
+++++++++++++--    print("explanation assigned to cluster", cluster_assignment)
+++++++++++++--
+++++++++++++--    
+++++++++++++--def assignment_test():
+++++++++++++--    action_orig = np.random.rand(10)
+++++++++++++--    d_orig = np.random.rand(5)
+++++++++++++--
+++++++++++++--    actions_compl = np.random.rand(6,10)
+++++++++++++--    d_j = np.random.rand(6,5)
+++++++++++++--
+++++++++++++--    action_dists = []
+++++++++++++--    for action in actions_compl:
+++++++++++++--        action_dists.append(np.linalg.norm(action_orig-action))
+++++++++++++--
+++++++++++++--    print(action_dists)
+++++++++++++--
+++++++++++++--    k = 3
+++++++++++++--    topk = np.argpartition(action_dists, -k)[-k:]
+++++++++++++--
+++++++++++++--    print(topk)
+++++++++++++--
+++++++++++++--    d_w = {}
+++++++++++++--    for idx in topk:
+++++++++++++--        d_w[idx] = wasserstein_distance(d_j[idx], d_orig)
+++++++++++++--
+++++++++++++--    print(d_w)
+++++++++++++--
+++++++++++++--    cluster_assignment = min(d_w, key=d_w.get)
+++++++++++++--    print("explanation assigned to cluster", cluster_assignment)
+++++++++++++--
+++++++++++++--
+++++++++++++--if __name__ == "__main__":
+++++++++++++--    # main()
+++++++++++++--    assignment_test()
+++++++++++++-diff --git a/halfcheetah/trajectory.egg-info/PKG-INFO b/halfcheetah/trajectory.egg-info/PKG-INFO
+++++++++++++-index 452c6cb..2603850 100644
+++++++++++++---- a/halfcheetah/trajectory.egg-info/PKG-INFO
+++++++++++++-+++ b/halfcheetah/trajectory.egg-info/PKG-INFO
+++++++++++++-@@ -1,4 +1,11 @@
+++++++++++++- Metadata-Version: 2.1
+++++++++++++- Name: trajectory
+++++++++++++- Version: 0.0.0
+++++++++++++-+Summary: UNKNOWN
+++++++++++++-+Home-page: UNKNOWN
+++++++++++++-+License: UNKNOWN
+++++++++++++-+Platform: UNKNOWN
+++++++++++++- License-File: LICENSE
+++++++++++++ +
+++++++++++++-+UNKNOWN
+++++++++++++ +
++++++++++++++ def softmax(x, temp):
++++++++++++++-    """TODO"""
+++++++++++++++    """
+++++++++++++++    Softmax with temperature using max-trick.
+++++++++++++++    
+++++++++++++++    Args:
+++++++++++++++    - x: np.array, shape (n_data, dim_data)
+++++++++++++++    - temp: int, softmax temperature
+++++++++++++++    
+++++++++++++++    Returns:
+++++++++++++++    - embedding: np.array: shape (dim_data)
+++++++++++++++    """ 
++++++++++++++     max_x = np.max(x)
++++++++++++++     return np.exp(np.divide(x-max_x,temp)) / np.sum(np.exp(np.divide(x-max_x,temp)))
++++++++++++++ 
+++++++++++++++
++++++++++++++ def generate_data_embedding(trajectory_embeddings, temperature=10000):
++++++++++++++-    """TODO"""
+++++++++++++++    """
+++++++++++++++    TODO
+++++++++++++++    """ 
++++++++++++++ 
++++++++++++++     embedding = np.sum(trajectory_embeddings, axis=0)
++++++++++++++     embedding = softmax(embedding, temperature)
++++++++++++++@@ -142,7 +164,7 @@ def main():
++++++++++++++ 
++++++++++++++     ### IMPORTANT DEFINITIONS XRL SCRIPT ###
++++++++++++++ 
++++++++++++++-    load_embeddings = False
+++++++++++++++    load_embeddings = True
++++++++++++++     load_clusters = True
++++++++++++++     load_agents = True
++++++++++++++     generate_human_study = False
+++++++++++++ diff --git a/halfcheetah/trajectory.egg-info/SOURCES.txt b/halfcheetah/trajectory.egg-info/SOURCES.txt
+++++++++++++-index 4474d85..84e8e3a 100644
++++++++++++++index 84e8e3a..4474d85 100644
+++++++++++++ --- a/halfcheetah/trajectory.egg-info/SOURCES.txt
+++++++++++++ +++ b/halfcheetah/trajectory.egg-info/SOURCES.txt
+++++++++++++-@@ -30,4 +30,5 @@ trajectory/utils/serialization.py
++++++++++++++@@ -30,5 +30,4 @@ trajectory/utils/serialization.py
+++++++++++++  trajectory/utils/setup.py
+++++++++++++  trajectory/utils/timer.py
+++++++++++++  trajectory/utils/training.py
+++++++++++++ -trajectory/utils/video.py
++++++++++++++-trajectory_aaa/__init__.py
+++++++++++++ \ No newline at end of file
+++++++++++++ +trajectory/utils/video.py
+++++++++++++-+trajectory_aaa/__init__.py
+++++++++++++ \ No newline at end of file
+++++++++++++ diff --git a/halfcheetah/trajectory.egg-info/top_level.txt b/halfcheetah/trajectory.egg-info/top_level.txt
+++++++++++++-index ce65198..1d5271f 100644
++++++++++++++index 1d5271f..ce65198 100644
+++++++++++++ --- a/halfcheetah/trajectory.egg-info/top_level.txt
+++++++++++++ +++ b/halfcheetah/trajectory.egg-info/top_level.txt
+++++++++++++-@@ -1 +1,2 @@
++++++++++++++@@ -1,2 +1 @@
+++++++++++++  trajectory
+++++++++++++-+trajectory_aaa
++++++++++++ \ No newline at end of file
++++++++++++-diff --git a/halfcheetah/pca.py.npy b/halfcheetah/pca.py.npy
++++++++++++-deleted file mode 100644
++++++++++++-index bb19150..0000000
++++++++++++-Binary files a/halfcheetah/pca.py.npy and /dev/null differ
++++++++++++-diff --git a/halfcheetah/plotting/bar.png b/halfcheetah/plotting/bar.png
++++++++++++-deleted file mode 100644
++++++++++++-index 3679667..0000000
++++++++++++-Binary files a/halfcheetah/plotting/bar.png and /dev/null differ
++++++++++++-diff --git a/halfcheetah/plotting/plot.py b/halfcheetah/plotting/plot.py
++++++++++++-deleted file mode 100644
++++++++++++-index 163d0e4..0000000
++++++++++++---- a/halfcheetah/plotting/plot.py
++++++++++++-+++ /dev/null
++++++++++++-@@ -1,74 +0,0 @@
++++++++++++--import numpy as np
++++++++++++--import matplotlib
++++++++++++--import matplotlib.pyplot as plt
++++++++++++--import pdb
++++++++++++--
++++++++++++--from plotting.scores import means
++++++++++++--
++++++++++++--class Colors:
++++++++++++--	grey = '#B4B4B4'
++++++++++++--	gold = '#F6C781'
++++++++++++--	red = '#EC7C7D'
++++++++++++--	blue = '#70ABCC'
++++++++++++--
++++++++++++--LABELS = {
++++++++++++--	# 'BC': 'Behavior\nCloning',
++++++++++++--	# 'MBOP': 'Model-Based\nOffline Planning',
++++++++++++--	# 'BRAC': 'Behavior-Reg.\nActor-Critic',
++++++++++++--	# 'CQL': 'Conservative\nQ-Learning',
++++++++++++--}
++++++++++++--
++++++++++++--def get_mean(results, exclude=None):
++++++++++++--	'''
++++++++++++--		results : { environment: score, ... }
++++++++++++--	'''
++++++++++++--	filtered = {
++++++++++++--		k: v for k, v in results.items()
++++++++++++--		if (not exclude) or (exclude and exclude not in k)
++++++++++++--	}
++++++++++++--	return np.mean(list(filtered.values()))
++++++++++++--
++++++++++++--if __name__ == '__main__':
++++++++++++--
++++++++++++--	#################
++++++++++++--	## latex
++++++++++++--	#################
++++++++++++--	matplotlib.rc('font', **{'family': 'serif', 'serif': ['Computer Modern']})
++++++++++++--	matplotlib.rc('text', usetex=True)
++++++++++++--	matplotlib.rcParams['text.latex.preamble']=[r"\usepackage{amsmath}"]
++++++++++++--	#################
++++++++++++--
++++++++++++--	fig = plt.gcf()
++++++++++++--	ax = plt.gca()
++++++++++++--	fig.set_size_inches(7.5, 2.5)
++++++++++++--
++++++++++++--	means = {k: get_mean(v, exclude='ant') for k, v in means.items()}
++++++++++++--	print(means)
++++++++++++--
++++++++++++--	algs = ['BC', 'MBOP', 'BRAC', 'CQL', 'Decision\nTransformer', 'Trajectory\nTransformer']
++++++++++++--	vals = [means[alg] for alg in algs]
++++++++++++--
++++++++++++--	colors = [
++++++++++++--		Colors.grey, Colors.gold,
++++++++++++--		Colors.red, Colors.red, Colors.blue, Colors.blue
++++++++++++--	]
++++++++++++--
++++++++++++--	labels = [LABELS.get(alg, alg) for alg in algs]
++++++++++++--	plt.bar(labels, vals, color=colors, edgecolor=Colors.gold, lw=0)
++++++++++++--	plt.ylabel('Average normalized return', labelpad=15)
++++++++++++--	# plt.title('Offline RL Results')
++++++++++++--
++++++++++++--	legend_labels = ['Behavior Cloning', 'Trajectory Optimization', 'Temporal Difference', 'Sequence Modeling']
++++++++++++--	colors = [Colors.grey, Colors.gold, Colors.red, Colors.blue]
++++++++++++--	handles = [plt.Rectangle((0,0),1,1, color=color) for label, color in zip(legend_labels, colors)]
++++++++++++--	plt.legend(handles, legend_labels, ncol=4,
++++++++++++--		bbox_to_anchor=(1.07, -.18), fancybox=False, framealpha=0, shadow=False, columnspacing=1.5, handlelength=1.5)
++++++++++++--
++++++++++++--	matplotlib.rcParams['hatch.linewidth'] = 7.5
++++++++++++--	# ax.patches[-1].set_hatch('/')
++++++++++++--
++++++++++++--	ax.spines['right'].set_visible(False)
++++++++++++--	ax.spines['top'].set_visible(False)
++++++++++++--
++++++++++++--	# plt.savefig('plotting/bar.pdf', bbox_inches='tight')
++++++++++++--	plt.savefig('plotting/bar.png', bbox_inches='tight', dpi=500)
++++++++++++-diff --git a/halfcheetah/plotting/read_results.py b/halfcheetah/plotting/read_results.py
++++++++++++-deleted file mode 100644
++++++++++++-index 5a5fb62..0000000
++++++++++++---- a/halfcheetah/plotting/read_results.py
++++++++++++-+++ /dev/null
++++++++++++-@@ -1,70 +0,0 @@
++++++++++++--import os
++++++++++++--import glob
++++++++++++--import numpy as np
++++++++++++--import json
++++++++++++--import pdb
++++++++++++--
++++++++++++--import trajectory.utils as utils
++++++++++++--
++++++++++++--DATASETS = [
++++++++++++--	f'{env}-{buffer}'
++++++++++++--	for env in ['hopper', 'walker2d', 'halfcheetah', 'ant']
++++++++++++--	for buffer in ['medium-expert-v2', 'medium-v2', 'medium-replay-v2']
++++++++++++--]
++++++++++++--
++++++++++++--LOGBASE = 'logs'
++++++++++++--TRIAL = '*'
++++++++++++--EXP_NAME = 'plans/pretrained'
++++++++++++--
++++++++++++--def load_results(paths):
++++++++++++--	'''
++++++++++++--		paths : path to directory containing experiment trials
++++++++++++--	'''
++++++++++++--	scores = []
++++++++++++--	for i, path in enumerate(sorted(paths)):
++++++++++++--		score = load_result(path)
++++++++++++--		if score is None:
++++++++++++--			print(f'Skipping {path}')
++++++++++++--			continue
++++++++++++--		scores.append(score)
++++++++++++--
++++++++++++--		suffix = path.split('/')[-1]
++++++++++++--
++++++++++++--	mean = np.mean(scores)
++++++++++++--	err = np.std(scores) / np.sqrt(len(scores))
++++++++++++--	return mean, err, scores
++++++++++++--
++++++++++++--def load_result(path):
++++++++++++--	'''
++++++++++++--		path : path to experiment directory; expects `rollout.json` to be in directory
++++++++++++--	'''
++++++++++++--	fullpath = os.path.join(path, 'rollout.json')
++++++++++++--	suffix = path.split('/')[-1]
++++++++++++--
++++++++++++--	if not os.path.exists(fullpath):
++++++++++++--		return None
++++++++++++--
++++++++++++--	results = json.load(open(fullpath, 'rb'))
++++++++++++--	score = results['score']
++++++++++++--	return score * 100
++++++++++++--
++++++++++++--#######################
++++++++++++--######## setup ########
++++++++++++--#######################
++++++++++++--
++++++++++++--if __name__ == '__main__':
++++++++++++--
++++++++++++--	class Parser(utils.Parser):
++++++++++++--	    dataset: str = None
++++++++++++--
++++++++++++--	args = Parser().parse_args()
++++++++++++--
++++++++++++--	for dataset in ([args.dataset] if args.dataset else DATASETS):
++++++++++++--		subdirs = glob.glob(os.path.join(LOGBASE, dataset, EXP_NAME))
++++++++++++--
++++++++++++--		for subdir in subdirs:
++++++++++++--			reldir = subdir.split('/')[-1]
++++++++++++--			paths = glob.glob(os.path.join(subdir, TRIAL))
++++++++++++--
++++++++++++--			mean, err, scores = load_results(paths)
++++++++++++--			print(f'{dataset.ljust(30)} | {subdir.ljust(50)} | {len(scores)} scores \n    {mean:.2f} +/- {err:.2f}\n')
++++++++++++-diff --git a/halfcheetah/plotting/scores.py b/halfcheetah/plotting/scores.py
++++++++++++-deleted file mode 100644
++++++++++++-index f1917f7..0000000
++++++++++++---- a/halfcheetah/plotting/scores.py
++++++++++++-+++ /dev/null
++++++++++++-@@ -1,123 +0,0 @@
++++++++++++--means = {
++++++++++++--	'Trajectory\nTransformer': {
++++++++++++--		##
++++++++++++--		'halfcheetah-medium-expert-v2': 95.0,
++++++++++++--		'hopper-medium-expert-v2': 110.0,
++++++++++++--		'walker2d-medium-expert-v2': 101.9,
++++++++++++--		'ant-medium-expert-v2': 116.1,
++++++++++++--		##
++++++++++++--		'halfcheetah-medium-v2': 46.9,
++++++++++++--		'hopper-medium-v2': 61.1,
++++++++++++--		'walker2d-medium-v2': 79.0,
++++++++++++--		'ant-medium-v2': 83.1,
++++++++++++--		##
++++++++++++--		'halfcheetah-medium-replay-v2': 41.9,
++++++++++++--		'hopper-medium-replay-v2': 91.5,
++++++++++++--		'walker2d-medium-replay-v2': 82.6,
++++++++++++--		'ant-medium-replay-v2': 77.0,
++++++++++++--	},
++++++++++++--	'Decision\nTransformer': {
++++++++++++--		##
++++++++++++--		'halfcheetah-medium-expert-v2': 86.8,
++++++++++++--		'hopper-medium-expert-v2': 107.6,
++++++++++++--		'walker2d-medium-expert-v2': 108.1,
++++++++++++--		##
++++++++++++--		'halfcheetah-medium-v2': 42.6,
++++++++++++--		'hopper-medium-v2': 67.6,
++++++++++++--		'walker2d-medium-v2': 74.0,
++++++++++++--		##
++++++++++++--		'halfcheetah-medium-replay-v2': 36.6,
++++++++++++--		'hopper-medium-replay-v2': 82.7,
++++++++++++--		'walker2d-medium-replay-v2': 66.6,
++++++++++++--	},
++++++++++++--	'CQL': {
++++++++++++--		##
++++++++++++--		'halfcheetah-medium-expert-v2': 91.6,
++++++++++++--		'hopper-medium-expert-v2': 105.4,
++++++++++++--		'walker2d-medium-expert-v2': 108.8,
++++++++++++--		##
++++++++++++--		'halfcheetah-medium-v2': 44.0,
++++++++++++--		'hopper-medium-v2': 58.5,
++++++++++++--		'walker2d-medium-v2': 72.5,
++++++++++++--		##
++++++++++++--		'halfcheetah-medium-replay-v2': 45.5,
++++++++++++--		'hopper-medium-replay-v2': 95.0,
++++++++++++--		'walker2d-medium-replay-v2': 77.2,
++++++++++++--	},
++++++++++++--	'MOPO': {
++++++++++++--		##
++++++++++++--		'halfcheetah-medium-expert-v2': 63.3,
++++++++++++--		'hopper-medium-expert-v2': 23.7,
++++++++++++--		'walker2d-medium-expert-v2': 44.6,
++++++++++++--		##
++++++++++++--		'halfcheetah-medium-v2': 42.3,
++++++++++++--		'hopper-medium-v2': 28.0,
++++++++++++--		'walker2d-medium-v2': 17.8,
++++++++++++--		##
++++++++++++--		'halfcheetah-medium-replay-v2': 53.1,
++++++++++++--		'hopper-medium-replay-v2': 67.5,
++++++++++++--		'walker2d-medium-replay-v2':39.0,
++++++++++++--	},
++++++++++++--	'MBOP': {
++++++++++++--		##
++++++++++++--		'halfcheetah-medium-expert-v2': 105.9,
++++++++++++--		'hopper-medium-expert-v2': 55.1,
++++++++++++--		'walker2d-medium-expert-v2': 70.2,
++++++++++++--		##
++++++++++++--		'halfcheetah-medium-v2': 44.6,
++++++++++++--		'hopper-medium-v2': 48.8,
++++++++++++--		'walker2d-medium-v2': 41.0,
++++++++++++--		##
++++++++++++--		'halfcheetah-medium-replay-v2': 42.3,
++++++++++++--		'hopper-medium-replay-v2': 12.4,
++++++++++++--		'walker2d-medium-replay-v2': 9.7,
++++++++++++--	},
++++++++++++--	'BRAC': {
++++++++++++--		##
++++++++++++--		'halfcheetah-medium-expert-v2': 41.9,
++++++++++++--		'hopper-medium-expert-v2': 0.9,
++++++++++++--		'walker2d-medium-expert-v2': 81.6,
++++++++++++--		##
++++++++++++--		'halfcheetah-medium-v2': 46.3,
++++++++++++--		'hopper-medium-v2': 31.3,
++++++++++++--		'walker2d-medium-v2': 81.1,
++++++++++++--		##
++++++++++++--		'halfcheetah-medium-replay-v2': 47.7,
++++++++++++--		'hopper-medium-replay-v2': 0.6,
++++++++++++--		'walker2d-medium-replay-v2': 0.9,
++++++++++++--	},
++++++++++++--	'BC': {
++++++++++++--		##
++++++++++++--		'halfcheetah-medium-expert-v2': 59.9,
++++++++++++--		'hopper-medium-expert-v2': 79.6,
++++++++++++--		'walker2d-medium-expert-v2': 36.6,
++++++++++++--		##
++++++++++++--		'halfcheetah-medium-v2': 43.1,
++++++++++++--		'hopper-medium-v2': 63.9,
++++++++++++--		'walker2d-medium-v2': 77.3,
++++++++++++--		##
++++++++++++--		'halfcheetah-medium-replay-v2': 4.3,
++++++++++++--		'hopper-medium-replay-v2': 27.6,
++++++++++++--		'walker2d-medium-replay-v2': 36.9,
++++++++++++--	},
++++++++++++--}
++++++++++++--
++++++++++++--errors = {
++++++++++++--	'Trajectory\nTransformer': {
++++++++++++--		##
++++++++++++--		'halfcheetah-medium-expert-v2': 0.2,
++++++++++++--		'hopper-medium-expert-v2': 2.7,
++++++++++++--		'walker2d-medium-expert-v2': 6.8,
++++++++++++--		'ant-medium-expert-v2': 9.0,
++++++++++++--		##
++++++++++++--		'halfcheetah-medium-v2': 0.4,
++++++++++++--		'hopper-medium-v2': 3.6,
++++++++++++--		'walker2d-medium-v2': 2.8,
++++++++++++--		'ant-medium-v2': 7.3,
++++++++++++--		##
++++++++++++--		'halfcheetah-medium-replay-v2': 2.5,
++++++++++++--		'hopper-medium-replay-v2': 3.6,
++++++++++++--		'walker2d-medium-replay-v2': 6.9,
++++++++++++--		'ant-medium-replay-v2': 6.8,
++++++++++++--	},
++++++++++++--}
++++++++++++++-trajectory_aaa
++++++++++++++diff --git a/seaquest/readme.md b/seaquest/readme.md
++++++++++++++index 84e53f8..53561f9 100644
++++++++++++++--- a/seaquest/readme.md
+++++++++++++++++ b/seaquest/readme.md
++++++++++++++@@ -10,4 +10,4 @@ pip install git+https://github.com/takuseno/d4rl-atari
++++++++++++++ pip install "gym[atari, accept-rom-license]"
++++++++++++++ pip install pyclustering
++++++++++++++ pip install seaborn
++++++++++++++-pip install d3rlpy==1.1.1
++++++++++++++\ No newline at end of file
+++++++++++++++pip install d3rlpy==1.1.1
++++++++++++ \ No newline at end of file
++++++++++++-diff --git a/halfcheetah/plotting/table.py b/halfcheetah/plotting/table.py
++++++++++++-deleted file mode 100644
++++++++++++-index eae74e6..0000000
++++++++++++---- a/halfcheetah/plotting/table.py
++++++++++++-+++ /dev/null
++++++++++++-@@ -1,127 +0,0 @@
++++++++++++--import numpy as np
++++++++++++--import pdb
++++++++++++--
++++++++++++--from plotting.plot import get_mean
++++++++++++--from plotting.scores import (
++++++++++++--	means as MEANS,
++++++++++++--	errors as ERRORS,
++++++++++++--)
++++++++++++--
++++++++++++--ALGORITHM_STRINGS = {
++++++++++++--	'Trajectory\nTransformer': 'TT (Ours)',
++++++++++++--	'Decision\nTransformer': 'DT',	
++++++++++++--}
++++++++++++--
++++++++++++--BUFFER_STRINGS = {
++++++++++++--	'medium-expert': 'Medium-Expert',
++++++++++++--	'medium': 'Medium',
++++++++++++--	'medium-replay': 'Medium-Replay',	
++++++++++++--}
++++++++++++--
++++++++++++--ENVIRONMENT_STRINGS = {
++++++++++++--	'halfcheetah': 'HalfCheetah',
++++++++++++--	'hopper': 'Hopper',
++++++++++++--	'walker2d': 'Walker2d',
++++++++++++--	'ant': 'Ant',
++++++++++++--}
++++++++++++--
++++++++++++--SHOW_ERRORS = ['Trajectory\nTransformer']
++++++++++++--
++++++++++++--def get_result(algorithm, buffer, environment, version='v2'):
++++++++++++--	key = f'{environment}-{buffer}-{version}'
++++++++++++--	mean = MEANS[algorithm].get(key, '-')
++++++++++++--	if algorithm in SHOW_ERRORS:
++++++++++++--		error = ERRORS[algorithm].get(key)
++++++++++++--		return (mean, error)
++++++++++++--	else:
++++++++++++--		return mean
++++++++++++--
++++++++++++--def format_result(result):
++++++++++++--	if type(result) == tuple:
++++++++++++--		mean, std = result
++++++++++++--		return f'${mean}$ \\scriptsize{{\\raisebox{{1pt}}{{$\\pm {std}$}}}}'
++++++++++++--	else:
++++++++++++--		return f'${result}$'
++++++++++++--
++++++++++++--def format_row(buffer, environment, results):
++++++++++++--	buffer_str = BUFFER_STRINGS[buffer]
++++++++++++--	environment_str = ENVIRONMENT_STRINGS[environment]
++++++++++++--	results_str = ' & '.join(format_result(result) for result in results)
++++++++++++--	row = f'{buffer_str} & {environment_str} & {results_str} \\\\ \n'
++++++++++++--	return row
++++++++++++--
++++++++++++--def format_buffer_block(algorithms, buffer, environments):
++++++++++++--	block_str = '\\midrule\n'
++++++++++++--	for environment in environments:
++++++++++++--		results = [get_result(alg, buffer, environment) for alg in algorithms]
++++++++++++--		row_str = format_row(buffer, environment, results)
++++++++++++--		block_str += row_str
++++++++++++--	return block_str
++++++++++++--
++++++++++++--def format_algorithm(algorithm):
++++++++++++--	algorithm_str = ALGORITHM_STRINGS.get(algorithm, algorithm)
++++++++++++--	return f'\multicolumn{{1}}{{c}}{{\\bf {algorithm_str}}}'
++++++++++++--
++++++++++++--def format_algorithms(algorithms):
++++++++++++--	return ' & '.join(format_algorithm(algorithm) for algorithm in algorithms)
++++++++++++--
++++++++++++--def format_averages(means, label):
++++++++++++--	prefix = f'\\multicolumn{{2}}{{c}}{{\\bf Average ({label})}} & '
++++++++++++--	formatted = ' & '.join(str(mean) for mean in means)
++++++++++++--	return prefix + formatted
++++++++++++--
++++++++++++--def format_averages_block(algorithms):
++++++++++++--	means_filtered = [np.round(get_mean(MEANS[algorithm], exclude='ant'), 1) for algorithm in algorithms]
++++++++++++--	means_all = [np.round(get_mean(MEANS[algorithm], exclude=None), 1) for algorithm in algorithms]
++++++++++++--
++++++++++++--	means_all = [
++++++++++++--		means
++++++++++++--		if 'ant-medium-expert-v2' in MEANS[algorithm]
++++++++++++--		else '$-$'
++++++++++++--		for algorithm, means in zip(algorithms, means_all)
++++++++++++--	]
++++++++++++--
++++++++++++--	formatted_filtered = format_averages(means_filtered, 'without Ant')
++++++++++++--	formatted_all = format_averages(means_all, 'all settings')
++++++++++++--
++++++++++++--	formatted_block = (
++++++++++++--		f'{formatted_filtered} \\hspace{{.6cm}} \\\\ \n'
++++++++++++--		f'{formatted_all} \\hspace{{.6cm}} \\\\ \n'
++++++++++++--	)
++++++++++++--	return formatted_block
++++++++++++--
++++++++++++--def format_table(algorithms, buffers, environments):
++++++++++++--	justify_str = 'll' + 'r' * len(algorithms)
++++++++++++--	algorithm_str = format_algorithms(['Dataset', 'Environment'] + algorithms)
++++++++++++--	averages_str = format_averages_block(algorithms)
++++++++++++--	table_prefix = (
++++++++++++--		'\\begin{table*}[h]\n'
++++++++++++--		'\\centering\n'
++++++++++++--		'\\small\n'
++++++++++++--		f'\\begin{{tabular}}{{{justify_str}}}\n'
++++++++++++--		'\\toprule\n'
++++++++++++--		f'{algorithm_str} \\\\ \n'
++++++++++++--	)
++++++++++++--	table_suffix = (
++++++++++++--		'\\midrule\n'
++++++++++++--		f'{averages_str}'
++++++++++++--		'\\bottomrule\n'
++++++++++++--		'\\end{tabular}\n'
++++++++++++--		'\\label{table:d4rl}\n'
++++++++++++--		'\\end{table*}'
++++++++++++--	)
++++++++++++--	blocks = ''.join(format_buffer_block(algorithms, buffer, environments) for buffer in buffers)
++++++++++++--	table = (
++++++++++++--		f'{table_prefix}'
++++++++++++--		f'{blocks}'
++++++++++++--		f'{table_suffix}'
++++++++++++--	)
++++++++++++--	return table
++++++++++++--
++++++++++++--
++++++++++++--algorithms =['BC', 'MBOP', 'BRAC', 'CQL',  'Decision\nTransformer', 'Trajectory\nTransformer']
++++++++++++--buffers = ['medium-expert', 'medium', 'medium-replay']
++++++++++++--environments = ['halfcheetah', 'hopper', 'walker2d', 'ant']
++++++++++++--
++++++++++++--table = format_table(algorithms, buffers, environments)
++++++++++++--print(table)
++++++++++++-diff --git a/halfcheetah/scripts/plan.py b/halfcheetah/scripts/plan.py
++++++++++++-deleted file mode 100644
++++++++++++-index f13d4cc..0000000
++++++++++++---- a/halfcheetah/scripts/plan.py
++++++++++++-+++ /dev/null
++++++++++++-@@ -1,124 +0,0 @@
++++++++++++--import json
++++++++++++--import pdb
++++++++++++--from os.path import join
++++++++++++--
++++++++++++--import trajectory.utils as utils
++++++++++++--import trajectory.datasets as datasets
++++++++++++--from trajectory.search import (
++++++++++++--    beam_plan,
++++++++++++--    make_prefix,
++++++++++++--    extract_actions,
++++++++++++--    update_context,
++++++++++++--)
++++++++++++--
++++++++++++--class Parser(utils.Parser):
++++++++++++--    dataset: str = 'halfcheetah-medium-expert-v2'
++++++++++++--    config: str = 'config.offline'
++++++++++++--
++++++++++++--#######################
++++++++++++--######## setup ########
++++++++++++--#######################
++++++++++++--
++++++++++++--args = Parser().parse_args('plan')
++++++++++++--
++++++++++++--#######################
++++++++++++--####### models ########
++++++++++++--#######################
++++++++++++--
++++++++++++--dataset = utils.load_from_config(args.logbase, args.dataset, args.gpt_loadpath,
++++++++++++--        'data_config.pkl')
++++++++++++--
++++++++++++--gpt, gpt_epoch = utils.load_model(args.logbase, args.dataset, args.gpt_loadpath,
++++++++++++--        epoch=args.gpt_epoch, device=args.device)
++++++++++++--
++++++++++++--#######################
++++++++++++--####### dataset #######
++++++++++++--#######################
++++++++++++--
++++++++++++--env = datasets.load_environment(args.dataset)
++++++++++++--print('yo')
++++++++++++--renderer = utils.make_renderer(args)
++++++++++++--timer = utils.timer.Timer()
++++++++++++--
++++++++++++--discretizer = dataset.discretizer
++++++++++++--discount = dataset.discount
++++++++++++--observation_dim = dataset.observation_dim
++++++++++++--action_dim = dataset.action_dim
++++++++++++--
++++++++++++--value_fn = lambda x: discretizer.value_fn(x, args.percentile)
++++++++++++--preprocess_fn = datasets.get_preprocess_fn(env.name)
++++++++++++--
++++++++++++--print('yo2')
++++++++++++--
++++++++++++--#######################
++++++++++++--###### main loop ######
++++++++++++--#######################
++++++++++++--
++++++++++++--observation = env.reset()
++++++++++++--total_reward = 0
++++++++++++--
++++++++++++--## observations for rendering
++++++++++++--rollout = [observation.copy()]
++++++++++++--
++++++++++++--## previous (tokenized) transitions for conditioning transformer
++++++++++++--context = []
++++++++++++--
++++++++++++--T = env.max_episode_steps
++++++++++++--for t in range(T):
++++++++++++--
++++++++++++--    observation = preprocess_fn(observation)
++++++++++++--
++++++++++++--    if t % args.plan_freq == 0:
++++++++++++--        ## concatenate previous transitions and current observations to input to model
++++++++++++--        prefix = make_prefix(discretizer, context, observation, args.prefix_context)
++++++++++++--
++++++++++++--        ## sample sequence from model beginning with `prefix`
++++++++++++--        sequence = beam_plan(
++++++++++++--            gpt, value_fn, prefix,
++++++++++++--            args.horizon, args.beam_width, args.n_expand, observation_dim, action_dim,
++++++++++++--            discount, args.max_context_transitions, verbose=args.verbose,
++++++++++++--            k_obs=args.k_obs, k_act=args.k_act, cdf_obs=args.cdf_obs, cdf_act=args.cdf_act,
++++++++++++--        )
++++++++++++--
++++++++++++--    else:
++++++++++++--        sequence = sequence[1:]
++++++++++++--
++++++++++++--    ## [ horizon x transition_dim ] convert sampled tokens to continuous trajectory
++++++++++++--    sequence_recon = discretizer.reconstruct(sequence)
++++++++++++--
++++++++++++--    ## [ action_dim ] index into sampled trajectory to grab first action
++++++++++++--    action = extract_actions(sequence_recon, observation_dim, action_dim, t=0)
++++++++++++--
++++++++++++--    ## execute action in environment
++++++++++++--    next_observation, reward, terminal, _ = env.step(action)
++++++++++++--
++++++++++++--    ## update return
++++++++++++--    total_reward += reward
++++++++++++--    score = env.get_normalized_score(total_reward)
++++++++++++--
++++++++++++--    ## update rollout observations and context transitions
++++++++++++--    rollout.append(next_observation.copy())
++++++++++++--    context = update_context(context, discretizer, observation, action, reward, args.max_context_transitions)
++++++++++++--
++++++++++++--    print(
++++++++++++--        f'[ plan ] t: {t} / {T} | r: {reward:.2f} | R: {total_reward:.2f} | score: {score:.4f} | '
++++++++++++--        f'time: {timer():.2f} | {args.dataset} | {args.exp_name} | {args.suffix}\n'
++++++++++++--    )
++++++++++++--
++++++++++++--    ## visualization
++++++++++++--    if t % args.vis_freq == 0 or terminal or t == T:
++++++++++++--
++++++++++++--        ## save current plan
++++++++++++--        renderer.render_plan(join(args.savepath, f'{t}_plan.mp4'), sequence_recon, env.state_vector())
++++++++++++--
++++++++++++--        ## save rollout thus far
++++++++++++--        renderer.render_rollout(join(args.savepath, f'rollout.mp4'), rollout, fps=80)
++++++++++++--
++++++++++++--    if terminal: break
++++++++++++--
++++++++++++--    observation = next_observation
++++++++++++--
++++++++++++--## save result as a json file
++++++++++++--json_path = join(args.savepath, 'rollout.json')
++++++++++++--json_data = {'score': score, 'step': t, 'return': total_reward, 'term': terminal, 'gpt_epoch': gpt_epoch}
++++++++++++--json.dump(json_data, open(json_path, 'w'), indent=2, sort_keys=True)
++++++++++++-diff --git a/halfcheetah/scripts/train.py b/halfcheetah/scripts/train.py
++++++++++++-deleted file mode 100644
++++++++++++-index 04af8d7..0000000
++++++++++++---- a/halfcheetah/scripts/train.py
++++++++++++-+++ /dev/null
++++++++++++-@@ -1,122 +0,0 @@
++++++++++++--import os
++++++++++++--import numpy as np
++++++++++++--import torch
++++++++++++--import pdb
++++++++++++--
++++++++++++--import trajectory.utils as utils
++++++++++++--import trajectory.datasets as datasets
++++++++++++--from trajectory.models.transformers import GPT
++++++++++++--
++++++++++++--
++++++++++++--class Parser(utils.Parser):
++++++++++++--    dataset: str = 'halfcheetah-medium-expert-v2'
++++++++++++--    config: str = 'config.offline'
++++++++++++--
++++++++++++--#######################
++++++++++++--######## setup ########
++++++++++++--#######################
++++++++++++--
++++++++++++--args = Parser().parse_args('train')
++++++++++++--
++++++++++++--#######################
++++++++++++--####### dataset #######
++++++++++++--#######################
++++++++++++--
++++++++++++--env = datasets.load_environment(args.dataset)
++++++++++++--
++++++++++++--sequence_length = args.subsampled_sequence_length * args.step
++++++++++++--
++++++++++++--dataset_config = utils.Config(
++++++++++++--    datasets.DiscretizedDataset,
++++++++++++--    savepath=(args.savepath, 'data_config.pkl'),
++++++++++++--    env=args.dataset,
++++++++++++--    N=args.N,
++++++++++++--    penalty=args.termination_penalty,
++++++++++++--    sequence_length=sequence_length,
++++++++++++--    step=args.step,
++++++++++++--    discount=args.discount,
++++++++++++--    discretizer=args.discretizer,
++++++++++++--)
++++++++++++--
++++++++++++--dataset = dataset_config()
++++++++++++--obs_dim = dataset.observation_dim
++++++++++++--act_dim = dataset.action_dim
++++++++++++--transition_dim = dataset.joined_dim
++++++++++++--
++++++++++++--#######################
++++++++++++--######## model ########
++++++++++++--#######################
++++++++++++--
++++++++++++--block_size = args.subsampled_sequence_length * transition_dim - 1
++++++++++++--print(
++++++++++++--    f'Dataset size: {len(dataset)} | '
++++++++++++--    f'Joined dim: {transition_dim} '
++++++++++++--    f'(observation: {obs_dim}, action: {act_dim}) | Block size: {block_size}'
++++++++++++--)
++++++++++++--
++++++++++++--model_config = utils.Config(
++++++++++++--    GPT,
++++++++++++--    savepath=(args.savepath, 'model_config.pkl'),
++++++++++++--    ## discretization
++++++++++++--    vocab_size=args.N, block_size=block_size,
++++++++++++--    ## architecture
++++++++++++--    n_layer=args.n_layer, n_head=args.n_head, n_embd=args.n_embd*args.n_head,
++++++++++++--    ## dimensions
++++++++++++--    observation_dim=obs_dim, action_dim=act_dim, transition_dim=transition_dim,
++++++++++++--    ## loss weighting
++++++++++++--    action_weight=args.action_weight, reward_weight=args.reward_weight, value_weight=args.value_weight,
++++++++++++--    ## dropout probabilities
++++++++++++--    embd_pdrop=args.embd_pdrop, resid_pdrop=args.resid_pdrop, attn_pdrop=args.attn_pdrop,
++++++++++++--)
++++++++++++--
++++++++++++--model = model_config()
++++++++++++--model.to(args.device)
++++++++++++--
++++++++++++--#######################
++++++++++++--####### trainer #######
++++++++++++--#######################
++++++++++++--
++++++++++++--warmup_tokens = len(dataset) * block_size ## number of tokens seen per epoch
++++++++++++--final_tokens = 20 * warmup_tokens
++++++++++++--
++++++++++++--trainer_config = utils.Config(
++++++++++++--    utils.Trainer,
++++++++++++--    savepath=(args.savepath, 'trainer_config.pkl'),
++++++++++++--    # optimization parameters
++++++++++++--    batch_size=args.batch_size,
++++++++++++--    learning_rate=args.learning_rate,
++++++++++++--    betas=(0.9, 0.95),
++++++++++++--    grad_norm_clip=1.0,
++++++++++++--    weight_decay=0.1, # only applied on matmul weights
++++++++++++--    # learning rate decay: linear warmup followed by cosine decay to 10% of original
++++++++++++--    lr_decay=args.lr_decay,
++++++++++++--    warmup_tokens=warmup_tokens,
++++++++++++--    final_tokens=final_tokens,
++++++++++++--    ## dataloader
++++++++++++--    num_workers=0,
++++++++++++--    device=args.device,
++++++++++++--)
++++++++++++--
++++++++++++--trainer = trainer_config()
++++++++++++--
++++++++++++--#######################
++++++++++++--###### main loop ######
++++++++++++--#######################
++++++++++++--
++++++++++++--## scale number of epochs to keep number of updates constant
++++++++++++--n_epochs = int(1e6 / len(dataset) * args.n_epochs_ref)
++++++++++++--save_freq = int(n_epochs // args.n_saves)
++++++++++++--
++++++++++++--for epoch in range(n_epochs):
++++++++++++--    print(f'\nEpoch: {epoch} / {n_epochs} | {args.dataset} | {args.exp_name}')
++++++++++++--
++++++++++++--    trainer.train(model, dataset)
++++++++++++--
++++++++++++--    ## get greatest multiple of `save_freq` less than or equal to `save_epoch`
++++++++++++--    save_epoch = (epoch + 1) // save_freq * save_freq
++++++++++++--    statepath = os.path.join(args.savepath, f'state_{save_epoch}.pt')
++++++++++++--    print(f'Saving model to {statepath}')
++++++++++++--
++++++++++++--    ## save state to disk
++++++++++++--    state = model.state_dict()
++++++++++++--    torch.save(state, statepath)
++++++++++++-diff --git a/halfcheetah/scripts/xrl.py b/halfcheetah/scripts/xrl.py
++++++++++++-deleted file mode 100644
++++++++++++-index 134232a..0000000
++++++++++++---- a/halfcheetah/scripts/xrl.py
++++++++++++-+++ /dev/null
++++++++++++-@@ -1,372 +0,0 @@
++++++++++++--import json
++++++++++++--import pdb
++++++++++++--from os.path import join
++++++++++++--
++++++++++++--import trajectory.utils as utils
++++++++++++--import trajectory.datasets as datasets
++++++++++++--from trajectory.search import (
++++++++++++--    make_prefix,
++++++++++++--    update_context,
++++++++++++--)
++++++++++++--from trajectory.search.sampling import forward
++++++++++++--
++++++++++++--import gym
++++++++++++--import d4rl # Import required to register environments, you may need to also import the submodule
++++++++++++--import numpy as np
++++++++++++--import d3rlpy
++++++++++++--import math as mt
++++++++++++--from sklearn.cluster import KMeans
++++++++++++--from sklearn import datasets as skdatasets
++++++++++++--from sklearn.decomposition import PCA
++++++++++++--
++++++++++++--from pyclustering.cluster.xmeans import xmeans
++++++++++++--from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer
++++++++++++--
++++++++++++--from scipy.stats import wasserstein_distance
++++++++++++--
++++++++++++--class Parser(utils.Parser):
++++++++++++--    dataset: str = 'halfcheetah-medium-expert-v2'
++++++++++++--    config: str = 'config.offline'
++++++++++++--
+++++++++++++diff --git a/halfcheetah/scripts/xrl_v2.py b/halfcheetah/scripts/xrl_v2.py
+++++++++++++index 62a3d4d..1b48203 100644
+++++++++++++--- a/halfcheetah/scripts/xrl_v2.py
++++++++++++++++ b/halfcheetah/scripts/xrl_v2.py
+++++++++++++@@ -21,54 +21,95 @@ from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer
+++++++++++++ from scipy.stats import wasserstein_distance
+++++++++++++ from moviepy.editor import VideoFileClip
+++++++++++++ 
++++++++++++++
+++++++++++++ class Parser(utils.Parser):
+++++++++++++     dataset: str = 'halfcheetah-medium-v2'
+++++++++++++     config: str = 'config.offline'
+++++++++++++ 
++++++++++++ -# utils
++++++++++++--    
++++++++++++--class XMeans:
++++++++++++--    def loglikelihood(self, r, rn, var, m, k):
++++++++++++--        l1 = - rn / 2.0 * mt.log(2 * mt.pi)
++++++++++++--        l2 = - rn * m / 2.0 * mt.log(var)
++++++++++++--        l3 = - (rn - k) / 2.0
++++++++++++--        l4 = rn * mt.log(rn)
++++++++++++--        l5 = - rn * mt.log(r)
++++++++++++--
++++++++++++--        return l1 + l2 + l3 + l4 + l5
++++++++++++--
++++++++++++--    def __init__(self, X, kmax = 20):
++++++++++++--        self.X = X
++++++++++++--        self.num = np.size(self.X, axis=0)
++++++++++++--        self.dim = np.size(X, axis=1)
++++++++++++--        self.KMax = kmax
++++++++++++--
++++++++++++--    def fit(self):
++++++++++++--        k = 1
++++++++++++--        X = self.X
++++++++++++--        M = self.dim
++++++++++++--        num = self.num
++++++++++++--
++++++++++++--        while(1):
++++++++++++--            ok = k
++++++++++++--
++++++++++++--            #Improve Params
++++++++++++--            kmeans = KMeans(n_clusters=k).fit(X)
++++++++++++--            labels = kmeans.labels_
++++++++++++--            m = kmeans.cluster_centers_
++++++++++++--
++++++++++++--            #Improve Structure
++++++++++++--            #Calculate BIC
++++++++++++--            p = M + 1
++++++++++++--
++++++++++++--            obic = np.zeros(k)
++++++++++++--
++++++++++++--            for i in range(k):
++++++++++++--                rn = np.size(np.where(labels == i))
++++++++++++--                var = np.sum((X[labels == i] - m[i])**2)/float(rn - 1)
++++++++++++--                obic[i] = self.loglikelihood(rn, rn, var, M, 1) - p/2.0*mt.log(rn)
++++++++++++--
++++++++++++--            #Split each cluster into two subclusters and calculate BIC of each splitted cluster
++++++++++++--            sk = 2 #The number of subclusters
++++++++++++--            nbic = np.zeros(k)
++++++++++++--            addk = 0
++++++++++++--
++++++++++++--            for i in range(k):
++++++++++++--                ci = X[labels == i]
++++++++++++--                r = np.size(np.where(labels == i))
++++++++++++--
++++++++++++--                kmeans = KMeans(n_clusters=sk).fit(ci)
++++++++++++--                ci_labels = kmeans.labels_
++++++++++++--                sm = kmeans.cluster_centers_
++++++++++++--
++++++++++++--                for l in range(sk):
++++++++++++--                    rn = np.size(np.where(ci_labels == l))
++++++++++++--                    var = np.sum((ci[ci_labels == l] - sm[l])**2)/float(rn - sk)
++++++++++++--                    nbic[i] += self.loglikelihood(r, rn, var, M, sk)
++++++++++++--
++++++++++++--                p = sk * (M + 1)
++++++++++++--                nbic[i] -= p/2.0*mt.log(r)
++++++++++++--
++++++++++++--                if obic[i] < nbic[i]:
++++++++++++--                    addk += 1
++++++++++++--
++++++++++++--            k += addk
++++++++++++--
++++++++++++--            if ok == k or k >= self.KMax:
++++++++++++--                break
++++++++++++--
++++++++++++--
++++++++++++--        #Calculate labels and centroids
++++++++++++--        kmeans = KMeans(n_clusters=k).fit(X)
++++++++++++--        self.labels = kmeans.labels_
++++++++++++--        self.k = k
++++++++++++--        self.m = kmeans.cluster_centers_
++++++++++++--
++++++++++++--
++++++++++++--def cluster_trajectories(trajectories):
++++++++++++--    xmeans_instance = XMeans(trajectories, kmax=10)
++++++++++++--    xmeans_instance.fit()
++++++++++++--
++++++++++++--    clusters = xmeans_instance.labels
++++++++++++--    return clusters
++++++++++++--
++++++++++++--def cluster_trajectories_2(trajectories):
+++++++++++++ 
+++++++++++++ def cluster_trajectories(trajectories, n_clusters=10):
+++++++++++++-    """TODO"""
++++++++++++++    """
++++++++++++++    Cluster trajectories using X-means.
++++++++++++++    
++++++++++++++    Args:
++++++++++++++    - trajectories: np.array, shape (n_trajectories, encoding_dim)
++++++++++++++    - n_clusters: int, max number of clusters
++++++++++++++    
++++++++++++++    Returns:
++++++++++++++    - idxs_per_cluster: list, trajectory idxs per cluster idxs
++++++++++++++    - clusters: np.array, shape (n_trajectories), cluster idxs per trajectory idx
++++++++++++++    """ 
+++++++++++++ 
++++++++++++ -    # Prepare initial centers - amount of initial centers defines amount of clusters from which X-Means will
++++++++++++ -    # start analysis.
++++++++++++--    amount_initial_centers = 2
++++++++++++--    initial_centers = kmeans_plusplus_initializer(trajectories, amount_initial_centers).initialize()
++++++++++++--    
++++++++++++++    # Set 2 initial cluster centers
+++++++++++++     amount_initial_centers = 2
+++++++++++++     initial_centers = kmeans_plusplus_initializer(trajectories, amount_initial_centers).initialize()
+++++++++++++     
++++++++++++ -    # Create instance of X-Means algorithm. The algorithm will start analysis from 2 clusters, the maximum
++++++++++++ -    # number of clusters that can be allocated is 10.
++++++++++++--    xmeans_instance = xmeans(trajectories, initial_centers, 10)
++++++++++++--    xmeans_instance.process()
++++++++++++--    
++++++++++++--    # Extract clustering results: clusters
++++++++++++--    idxs_per_cluster = xmeans_instance.get_clusters()
++++++++++++--
++++++++++++--    clusters = []
++++++++++++--    for i in range(len(trajectories)):
++++++++++++--        for j in range(len(idxs_per_cluster)):
++++++++++++--            if i in idxs_per_cluster[j]: clusters.append(j)
++++++++++++--
++++++++++++--    return idxs_per_cluster, np.array(clusters)
++++++++++++++    # Run X-means
+++++++++++++     xmeans_instance = xmeans(trajectories, initial_centers, n_clusters)
+++++++++++++     xmeans_instance.process()
+++++++++++++     
+++++++++++++     # Extract clustering results: clusters
+++++++++++++     idxs_per_cluster = xmeans_instance.get_clusters()
+++++++++++++ 
++++++++++++++    # Turn list of trajectory idxs per cluster to array of cluster idx per trajectory idx
+++++++++++++     clusters = []
+++++++++++++     for i in range(len(trajectories)):
+++++++++++++         for j in range(len(idxs_per_cluster)):
+++++++++++++             if i in idxs_per_cluster[j]: clusters.append(j)
+++++++++++++ 
+++++++++++++     return idxs_per_cluster, np.array(clusters)
++++++++++++ - 
++++++++++++--# https://github.com/sascha-kirch/ML_Notebooks/blob/main/Softmax_Temperature.ipynb
++++++++++++--def softmax(x, temp):
++++++++++++--    """Compute softmax values for each sets of scores in x."""
++++++++++++--    return np.exp(np.divide(x,temp)) / np.sum(np.exp(np.divide(x,temp)))
++++++++++++--
++++++++++++--def generate_data_embedding(trajectory_embeddings, normalizing_factor=1, temperature=1):
++++++++++++--    embedding = np.sum(trajectory_embeddings, axis=0) / normalizing_factor
++++++++++++--    embedding = softmax(embedding, temperature)
++++++++++++--    return embedding
++++++++++++--
++++++++++++--def embed_trajectory(gpt, discretizer, observations, actions, rewards, preprocess_fn):
++++++++++++--    context = []
++++++++++++--
++++++++++++--    for i in range(len(observations)):
++++++++++++--        observation = observations[i]
++++++++++++--        action = actions[i]
++++++++++++--        reward = rewards[i]
++++++++++++--
++++++++++++--        observation = preprocess_fn(observation)
++++++++++++--
++++++++++++--        # print(observation)
++++++++++++--        prefix = make_prefix(discretizer, context, observation, True)
++++++++++++--        # print("prefix", prefix.shape)
++++++++++++--
++++++++++++--        out = forward(gpt, prefix)
++++++++++++--        # print("out", out.shape)
++++++++++++--        context = update_context(context, discretizer, observation, action, reward, len(observations))
++++++++++++--        # print("cotext", context)
++++++++++++--    
++++++++++++--    emb = []
++++++++++++--    for context_step in context:
++++++++++++--        emb.append(context_step.numpy())
++++++++++++--    emb = np.array(emb)
++++++++++++--    emb = np.mean(emb, axis=0)[0]
++++++++++++--
++++++++++++--    return emb
++++++++++++--
++++++++++++--
++++++++++++--def create_complementary_dataset(dataset, idxs, trajectory_length=10):
++++++++++++--    observations = []
++++++++++++--    actions = []
++++++++++++--    rewards = []
++++++++++++--    terminals = []
++++++++++++--    for i in range(1000):
++++++++++++--        if i not in idxs:
++++++++++++--            observations += list(dataset.observations[1000*i:1000*i+trajectory_length])
++++++++++++--            actions += list(dataset.actions[1000*i:1000*i+trajectory_length])
++++++++++++--            rewards += list(dataset.rewards[1000*i:1000*i+trajectory_length])
++++++++++++--            terminals += list(dataset.terminals[1000*i:1000*i+trajectory_length])
++++++++++++--
++++++++++++--    new_dataset = d3rlpy.dataset.MDPDataset(
++++++++++++--        observations=np.array(observations),
++++++++++++--        actions=np.array(actions),
++++++++++++--        rewards=np.array(rewards),
++++++++++++--        terminals=np.array(terminals)
++++++++++++--    )
++++++++++++--    return new_dataset
++++++++++++--    
++++++++++++--
++++++++++++--
++++++++++++--
++++++++++++--def main():
++++++++++++--    # args = Parser().parse_args('plan')
++++++++++++--
++++++++++++--    #######################
++++++++++++--    ####### models ########
++++++++++++--    #######################
++++++++++++--
++++++++++++--
++++++++++++--
++++++++++++--
++++++++++++--
++++++++++++--    # print(args.dataset)
++++++++++++--
++++++++++++--    # dataset = utils.load_from_config(args.logbase, args.dataset, args.gpt_loadpath,
++++++++++++--    #         'data_config.pkl')
++++++++++++--
++++++++++++--
++++++++++++--    # gpt, gpt_epoch = utils.load_model(args.logbase, args.dataset, args.gpt_loadpath,
++++++++++++--    #         epoch=args.gpt_epoch, device=args.device)
++++++++++++--
++++++++++++--    # env = datasets.load_environment(args.dataset)
++++++++++++--
++++++++++++--    # discretizer = dataset.discretizer
++++++++++++--
++++++++++++--    # preprocess_fn = datasets.get_preprocess_fn(env.name)
++++++++++++--
++++++++++++--    # #######################
++++++++++++--    # ####### dataset #######
++++++++++++--    # #######################
++++++++++++--
++++++++++++--    # # env = datasets.load_environment(args.dataset)
++++++++++++--    # discretizer = dataset.discretizer
++++++++++++--    # preprocess_fn = datasets.get_preprocess_fn(env.name)
++++++++++++--
++++++++++++--    # # dataset
++++++++++++--    dataset_d3, env = d3rlpy.datasets.get_dataset("halfcheetah-medium-v2")
++++++++++++--
++++++++++++--    # env = gym.make('halfcheetah-medium-v2')
++++++++++++--    # dataset_d4 = d4rl.qlearning_dataset(env)
++++++++++++--
++++++++++++--    # # checks to see if d3rl & d4rl datasets are equal
++++++++++++--    # print(np.allclose(dataset_d3.actions[100], dataset_d4['actions'][100]))
++++++++++++--
++++++++++++--    # # dr4rl has same trajectories, just cut off 1 element before the end
++++++++++++--    # for j in range(1000):
++++++++++++--    #     for i in range(999):
++++++++++++--    #         if dataset_d4['rewards'][j * 999 + i] != dataset_d3.rewards[j * 1000 + i]: print("yo", i)
++++++++++++--
++++++++++++--    # #######################
++++++++++++--    # ###### main loop ######
++++++++++++--    # #######################
++++++++++++--
++++++++++++--    trajectory_length = 10 # 10 = max
++++++++++++--
++++++++++++--    # embeddings = []
++++++++++++--    # for i in range(1000):
++++++++++++--    #     observations = dataset_d3.observations[1000*i:1000*i+trajectory_length]
++++++++++++--    #     actions = dataset_d3.actions[1000*i:1000*i+trajectory_length]
++++++++++++--    #     rewards = dataset_d3.rewards[1000*i:1000*i+trajectory_length]
++++++++++++--    #     terminals = dataset_d3.terminals[1000*i:1000*i+trajectory_length]
++++++++++++--    #     emb = embed_trajectory(gpt, discretizer, observations, actions, rewards, preprocess_fn)
++++++++++++--    #     embeddings.append(emb)
++++++++++++--    # embeddings = np.array(embeddings)
++++++++++++--    # np.save("embeddings.npy", embeddings)
++++++++++++--    # print(embeddings)
++++++++++++--
++++++++++++--    embeddings = np.load("embeddings.npy")
++++++++++++--
++++++++++++--    pca = PCA(n_components=2)
++++++++++++--    pca = PCA(n_components=2)
++++++++++++--    pca_embeddings = pca.fit_transform(embeddings)
++++++++++++--    np.save("pca.py", pca_embeddings)
++++++++++++--
++++++++++++--    idxs_per_cluster, clusters = cluster_trajectories_2(embeddings)
++++++++++++--    # print(clusters)
++++++++++++--    # return
++++++++++++--    np.save("clusters.npy", clusters)
++++++++++++--
++++++++++++--    import matplotlib.pyplot as plt
++++++++++++--
++++++++++++--    d_orig = generate_data_embedding(embeddings)
++++++++++++--    unique_clusters = np.unique(clusters)
++++++++++++--    
++++++++++++--    d_j = []
++++++++++++--    complementary_datasets = []
++++++++++++--    for j in np.sort(unique_clusters):
++++++++++++--        print(j)
++++++++++++--        d_j.append(generate_data_embedding(embeddings[clusters != j]))
++++++++++++--        plt.scatter(pca_embeddings[clusters == j][:,0], pca_embeddings[clusters == j][:,1], label=j)
++++++++++++--        complementary_datasets.append(create_complementary_dataset(dataset_d3, idxs_per_cluster[j], trajectory_length))
++++++++++++--    
++++++++++++--    original_dataset = create_complementary_dataset(dataset_d3, [], trajectory_length)
++++++++++++--
++++++++++++--    print(complementary_datasets, original_dataset)
++++++++++++--
++++++++++++--    plt.legend()
++++++++++++--    plt.show()
++++++++++++--
++++++++++++--    agent_orig = d3rlpy.algos.SAC(
++++++++++++--        actor_learning_rate=3e-4,
++++++++++++--        critic_learning_rate=3e-4,
++++++++++++--        temp_learning_rate=3e-4,
++++++++++++--        batch_size=256)
++++++++++++--
++++++++++++--    print(agent_orig)
++++++++++++--
++++++++++++--    training_steps = 1000
++++++++++++--
++++++++++++--    agent_orig.fit(original_dataset, n_steps=training_steps)
++++++++++++--
++++++++++++--    agents_compl = []
++++++++++++--
++++++++++++--    for dset in complementary_datasets:
++++++++++++--        agent = d3rlpy.algos.SAC(
++++++++++++--            actor_learning_rate=3e-4,
++++++++++++--            critic_learning_rate=3e-4,
++++++++++++--            temp_learning_rate=3e-4,
++++++++++++--            batch_size=256)
++++++++++++--        agent.fit(dset, n_steps=training_steps)
++++++++++++--        agents_compl.append(agent)
++++++++++++--
++++++++++++--    action_orig = agent_orig.predict(dataset_d3.observations[0])
++++++++++++--
++++++++++++--    actions_compl = []
++++++++++++--    for agent in agents_compl:
++++++++++++--        actions_compl.append(agent.predict(dataset_d3.observations[0]))
++++++++++++--    
++++++++++++--    action_dists = []
++++++++++++--    for action in actions_compl:
++++++++++++--        action_dists.append(np.linalg.norm(action_orig-action))
++++++++++++--
++++++++++++--    k = 3
++++++++++++--    topk = np.argpartition(action_dists, -k)[-k:]
++++++++++++--
++++++++++++--    d_w = {}
++++++++++++--    for idx in topk:
++++++++++++--        d_w[idx] = wasserstein_distance(d_j[idx], d_orig)
++++++++++++--
++++++++++++--    cluster_assignment = min(d_w, key=d_w.get)
++++++++++++--    print("explanation assigned to cluster", cluster_assignment)
++++++++++++--
++++++++++++--    
++++++++++++--def assignment_test():
++++++++++++--    action_orig = np.random.rand(10)
++++++++++++--    d_orig = np.random.rand(5)
++++++++++++--
++++++++++++--    actions_compl = np.random.rand(6,10)
++++++++++++--    d_j = np.random.rand(6,5)
++++++++++++--
++++++++++++--    action_dists = []
++++++++++++--    for action in actions_compl:
++++++++++++--        action_dists.append(np.linalg.norm(action_orig-action))
++++++++++++--
++++++++++++--    print(action_dists)
++++++++++++--
++++++++++++--    k = 3
++++++++++++--    topk = np.argpartition(action_dists, -k)[-k:]
++++++++++++--
++++++++++++--    print(topk)
++++++++++++--
++++++++++++--    d_w = {}
++++++++++++--    for idx in topk:
++++++++++++--        d_w[idx] = wasserstein_distance(d_j[idx], d_orig)
++++++++++++--
++++++++++++--    print(d_w)
++++++++++++--
++++++++++++--    cluster_assignment = min(d_w, key=d_w.get)
++++++++++++--    print("explanation assigned to cluster", cluster_assignment)
++++++++++++--
++++++++++++--
++++++++++++--if __name__ == "__main__":
++++++++++++--    # main()
++++++++++++--    assignment_test()
++++++++++++-diff --git a/halfcheetah/trajectory.egg-info/PKG-INFO b/halfcheetah/trajectory.egg-info/PKG-INFO
++++++++++++-index 452c6cb..2603850 100644
++++++++++++---- a/halfcheetah/trajectory.egg-info/PKG-INFO
++++++++++++-+++ b/halfcheetah/trajectory.egg-info/PKG-INFO
++++++++++++-@@ -1,4 +1,11 @@
++++++++++++- Metadata-Version: 2.1
++++++++++++- Name: trajectory
++++++++++++- Version: 0.0.0
++++++++++++-+Summary: UNKNOWN
++++++++++++-+Home-page: UNKNOWN
++++++++++++-+License: UNKNOWN
++++++++++++-+Platform: UNKNOWN
++++++++++++- License-File: LICENSE
++++++++++++ +
++++++++++++-+UNKNOWN
++++++++++++ +
+++++++++++++ def softmax(x, temp):
+++++++++++++-    """TODO"""
++++++++++++++    """
++++++++++++++    Softmax with temperature using max-trick.
++++++++++++++    
++++++++++++++    Args:
++++++++++++++    - x: np.array, shape (n_data, dim_data)
++++++++++++++    - temp: int, softmax temperature
++++++++++++++    
++++++++++++++    Returns:
++++++++++++++    - softmax_x: np.array: shape (dim_data)
++++++++++++++    """ 
+++++++++++++     max_x = np.max(x)
+++++++++++++-    return np.exp(np.divide(x-max_x,temp)) / np.sum(np.exp(np.divide(x-max_x,temp)))
++++++++++++++    softmax_x = np.exp(np.divide(x-max_x,temp)) / np.sum(np.exp(np.divide(x-max_x,temp)))
++++++++++++++    return softmax_x
++++++++++++++
+++++++++++++ 
+++++++++++++ def generate_data_embedding(trajectory_embeddings, temperature=10000):
+++++++++++++-    """TODO"""
++++++++++++++    """
++++++++++++++    Generate data embedding (sum+softmax) for set of encoded trajectories.
++++++++++++++    
++++++++++++++    Args:
++++++++++++++    - trajectory_embeddings: np.array, shape (n_data, dim_data)
++++++++++++++    - temperature: int, softmax temperature
++++++++++++++    
++++++++++++++    Returns:
++++++++++++++    - embedding: np.array, shape (dim_data)
++++++++++++++    """ 
+++++++++++++ 
+++++++++++++     embedding = np.sum(trajectory_embeddings, axis=0)
+++++++++++++     embedding = softmax(embedding, temperature)
+++++++++++++     
+++++++++++++-
+++++++++++++     return embedding
+++++++++++++ 
+++++++++++++ def embed_trajectory(gpt, discretizer, observations, actions, rewards, preprocess_fn):
+++++++++++++-    """TODO"""
++++++++++++++    """
++++++++++++++    Encode trajectory using trajectory transformer with sliding window.
++++++++++++++    
++++++++++++++    Args:
++++++++++++++    - gpt: trajectory transformer
++++++++++++++    - discretizer: environment discretizer
++++++++++++++    - observations: trajectory observations
++++++++++++++    - actions: trajectory actions
++++++++++++++    - rewards: trajectory rewards
++++++++++++++    - preprocess_fn: observations preprocessing functions
++++++++++++++    
++++++++++++++    Returns:
++++++++++++++    - embedding: np.array, shape (hidden_dim), encoded trajectory
++++++++++++++    """ 
+++++++++++++ 
+++++++++++++     context = []
+++++++++++++-
+++++++++++++     output = []
+++++++++++++ 
+++++++++++++     for i in range(len(observations)):
+++++++++++++@@ -91,8 +132,8 @@ def embed_trajectory(gpt, discretizer, observations, actions, rewards, preproces
+++++++++++++ 
+++++++++++++         context = update_context(context, discretizer, observation, action, reward, len(observations))
+++++++++++++ 
+++++++++++++-    emb = np.mean(output, axis=0)
+++++++++++++-    return emb
++++++++++++++    embedding = np.mean(output, axis=0)
++++++++++++++    return embedding
+++++++++++++ 
+++++++++++++ def create_complementary_dataset(dataset, idxs, trajectory_length=10, inverse=False):
+++++++++++++     """TODO"""
+++++++++++++@@ -142,7 +183,7 @@ def main():
+++++++++++++ 
+++++++++++++     ### IMPORTANT DEFINITIONS XRL SCRIPT ###
+++++++++++++ 
+++++++++++++-    load_embeddings = False
++++++++++++++    load_embeddings = True
+++++++++++++     load_clusters = True
+++++++++++++     load_agents = True
+++++++++++++     generate_human_study = False
++++++++++++ diff --git a/halfcheetah/trajectory.egg-info/SOURCES.txt b/halfcheetah/trajectory.egg-info/SOURCES.txt
++++++++++++-index 4474d85..84e8e3a 100644
+++++++++++++index 84e8e3a..4474d85 100644
++++++++++++ --- a/halfcheetah/trajectory.egg-info/SOURCES.txt
++++++++++++ +++ b/halfcheetah/trajectory.egg-info/SOURCES.txt
++++++++++++-@@ -30,4 +30,5 @@ trajectory/utils/serialization.py
+++++++++++++@@ -30,5 +30,4 @@ trajectory/utils/serialization.py
++++++++++++  trajectory/utils/setup.py
++++++++++++  trajectory/utils/timer.py
++++++++++++  trajectory/utils/training.py
++++++++++++ -trajectory/utils/video.py
+++++++++++++-trajectory_aaa/__init__.py
++++++++++++ \ No newline at end of file
++++++++++++ +trajectory/utils/video.py
++++++++++++-+trajectory_aaa/__init__.py
++++++++++++ \ No newline at end of file
++++++++++++ diff --git a/halfcheetah/trajectory.egg-info/top_level.txt b/halfcheetah/trajectory.egg-info/top_level.txt
++++++++++++-index ce65198..1d5271f 100644
+++++++++++++index 1d5271f..ce65198 100644
++++++++++++ --- a/halfcheetah/trajectory.egg-info/top_level.txt
++++++++++++ +++ b/halfcheetah/trajectory.egg-info/top_level.txt
++++++++++++-@@ -1 +1,2 @@
+++++++++++++@@ -1,2 +1 @@
++++++++++++  trajectory
++++++++++++-+trajectory_aaa
+++++++++++ \ No newline at end of file
+++++++++++-diff --git a/halfcheetah/pca.py.npy b/halfcheetah/pca.py.npy
+++++++++++-deleted file mode 100644
+++++++++++-index bb19150..0000000
+++++++++++-Binary files a/halfcheetah/pca.py.npy and /dev/null differ
+++++++++++-diff --git a/halfcheetah/plotting/bar.png b/halfcheetah/plotting/bar.png
+++++++++++-deleted file mode 100644
+++++++++++-index 3679667..0000000
+++++++++++-Binary files a/halfcheetah/plotting/bar.png and /dev/null differ
+++++++++++-diff --git a/halfcheetah/plotting/plot.py b/halfcheetah/plotting/plot.py
+++++++++++-deleted file mode 100644
+++++++++++-index 163d0e4..0000000
+++++++++++---- a/halfcheetah/plotting/plot.py
+++++++++++-+++ /dev/null
+++++++++++-@@ -1,74 +0,0 @@
+++++++++++--import numpy as np
+++++++++++--import matplotlib
+++++++++++--import matplotlib.pyplot as plt
+++++++++++--import pdb
+++++++++++--
+++++++++++--from plotting.scores import means
+++++++++++--
+++++++++++--class Colors:
+++++++++++--	grey = '#B4B4B4'
+++++++++++--	gold = '#F6C781'
+++++++++++--	red = '#EC7C7D'
+++++++++++--	blue = '#70ABCC'
+++++++++++--
+++++++++++--LABELS = {
+++++++++++--	# 'BC': 'Behavior\nCloning',
+++++++++++--	# 'MBOP': 'Model-Based\nOffline Planning',
+++++++++++--	# 'BRAC': 'Behavior-Reg.\nActor-Critic',
+++++++++++--	# 'CQL': 'Conservative\nQ-Learning',
+++++++++++--}
+++++++++++--
+++++++++++--def get_mean(results, exclude=None):
+++++++++++--	'''
+++++++++++--		results : { environment: score, ... }
+++++++++++--	'''
+++++++++++--	filtered = {
+++++++++++--		k: v for k, v in results.items()
+++++++++++--		if (not exclude) or (exclude and exclude not in k)
+++++++++++--	}
+++++++++++--	return np.mean(list(filtered.values()))
+++++++++++--
+++++++++++--if __name__ == '__main__':
+++++++++++--
+++++++++++--	#################
+++++++++++--	## latex
+++++++++++--	#################
+++++++++++--	matplotlib.rc('font', **{'family': 'serif', 'serif': ['Computer Modern']})
+++++++++++--	matplotlib.rc('text', usetex=True)
+++++++++++--	matplotlib.rcParams['text.latex.preamble']=[r"\usepackage{amsmath}"]
+++++++++++--	#################
+++++++++++--
+++++++++++--	fig = plt.gcf()
+++++++++++--	ax = plt.gca()
+++++++++++--	fig.set_size_inches(7.5, 2.5)
+++++++++++--
+++++++++++--	means = {k: get_mean(v, exclude='ant') for k, v in means.items()}
+++++++++++--	print(means)
+++++++++++--
+++++++++++--	algs = ['BC', 'MBOP', 'BRAC', 'CQL', 'Decision\nTransformer', 'Trajectory\nTransformer']
+++++++++++--	vals = [means[alg] for alg in algs]
+++++++++++--
+++++++++++--	colors = [
+++++++++++--		Colors.grey, Colors.gold,
+++++++++++--		Colors.red, Colors.red, Colors.blue, Colors.blue
+++++++++++--	]
+++++++++++--
+++++++++++--	labels = [LABELS.get(alg, alg) for alg in algs]
+++++++++++--	plt.bar(labels, vals, color=colors, edgecolor=Colors.gold, lw=0)
+++++++++++--	plt.ylabel('Average normalized return', labelpad=15)
+++++++++++--	# plt.title('Offline RL Results')
+++++++++++--
+++++++++++--	legend_labels = ['Behavior Cloning', 'Trajectory Optimization', 'Temporal Difference', 'Sequence Modeling']
+++++++++++--	colors = [Colors.grey, Colors.gold, Colors.red, Colors.blue]
+++++++++++--	handles = [plt.Rectangle((0,0),1,1, color=color) for label, color in zip(legend_labels, colors)]
+++++++++++--	plt.legend(handles, legend_labels, ncol=4,
+++++++++++--		bbox_to_anchor=(1.07, -.18), fancybox=False, framealpha=0, shadow=False, columnspacing=1.5, handlelength=1.5)
+++++++++++--
+++++++++++--	matplotlib.rcParams['hatch.linewidth'] = 7.5
+++++++++++--	# ax.patches[-1].set_hatch('/')
+++++++++++--
+++++++++++--	ax.spines['right'].set_visible(False)
+++++++++++--	ax.spines['top'].set_visible(False)
+++++++++++--
+++++++++++--	# plt.savefig('plotting/bar.pdf', bbox_inches='tight')
+++++++++++--	plt.savefig('plotting/bar.png', bbox_inches='tight', dpi=500)
+++++++++++-diff --git a/halfcheetah/plotting/read_results.py b/halfcheetah/plotting/read_results.py
+++++++++++-deleted file mode 100644
+++++++++++-index 5a5fb62..0000000
+++++++++++---- a/halfcheetah/plotting/read_results.py
+++++++++++-+++ /dev/null
+++++++++++-@@ -1,70 +0,0 @@
+++++++++++--import os
+++++++++++--import glob
+++++++++++--import numpy as np
+++++++++++--import json
+++++++++++--import pdb
+++++++++++--
+++++++++++--import trajectory.utils as utils
+++++++++++--
+++++++++++--DATASETS = [
+++++++++++--	f'{env}-{buffer}'
+++++++++++--	for env in ['hopper', 'walker2d', 'halfcheetah', 'ant']
+++++++++++--	for buffer in ['medium-expert-v2', 'medium-v2', 'medium-replay-v2']
+++++++++++--]
+++++++++++--
+++++++++++--LOGBASE = 'logs'
+++++++++++--TRIAL = '*'
+++++++++++--EXP_NAME = 'plans/pretrained'
+++++++++++--
+++++++++++--def load_results(paths):
+++++++++++--	'''
+++++++++++--		paths : path to directory containing experiment trials
+++++++++++--	'''
+++++++++++--	scores = []
+++++++++++--	for i, path in enumerate(sorted(paths)):
+++++++++++--		score = load_result(path)
+++++++++++--		if score is None:
+++++++++++--			print(f'Skipping {path}')
+++++++++++--			continue
+++++++++++--		scores.append(score)
+++++++++++--
+++++++++++--		suffix = path.split('/')[-1]
+++++++++++--
+++++++++++--	mean = np.mean(scores)
+++++++++++--	err = np.std(scores) / np.sqrt(len(scores))
+++++++++++--	return mean, err, scores
+++++++++++--
+++++++++++--def load_result(path):
+++++++++++--	'''
+++++++++++--		path : path to experiment directory; expects `rollout.json` to be in directory
+++++++++++--	'''
+++++++++++--	fullpath = os.path.join(path, 'rollout.json')
+++++++++++--	suffix = path.split('/')[-1]
+++++++++++--
+++++++++++--	if not os.path.exists(fullpath):
+++++++++++--		return None
+++++++++++--
+++++++++++--	results = json.load(open(fullpath, 'rb'))
+++++++++++--	score = results['score']
+++++++++++--	return score * 100
+++++++++++--
+++++++++++--#######################
+++++++++++--######## setup ########
+++++++++++--#######################
+++++++++++--
+++++++++++--if __name__ == '__main__':
+++++++++++--
+++++++++++--	class Parser(utils.Parser):
+++++++++++--	    dataset: str = None
+++++++++++--
+++++++++++--	args = Parser().parse_args()
+++++++++++--
+++++++++++--	for dataset in ([args.dataset] if args.dataset else DATASETS):
+++++++++++--		subdirs = glob.glob(os.path.join(LOGBASE, dataset, EXP_NAME))
+++++++++++--
+++++++++++--		for subdir in subdirs:
+++++++++++--			reldir = subdir.split('/')[-1]
+++++++++++--			paths = glob.glob(os.path.join(subdir, TRIAL))
+++++++++++--
+++++++++++--			mean, err, scores = load_results(paths)
+++++++++++--			print(f'{dataset.ljust(30)} | {subdir.ljust(50)} | {len(scores)} scores \n    {mean:.2f} +/- {err:.2f}\n')
+++++++++++-diff --git a/halfcheetah/plotting/scores.py b/halfcheetah/plotting/scores.py
+++++++++++-deleted file mode 100644
+++++++++++-index f1917f7..0000000
+++++++++++---- a/halfcheetah/plotting/scores.py
+++++++++++-+++ /dev/null
+++++++++++-@@ -1,123 +0,0 @@
+++++++++++--means = {
+++++++++++--	'Trajectory\nTransformer': {
+++++++++++--		##
+++++++++++--		'halfcheetah-medium-expert-v2': 95.0,
+++++++++++--		'hopper-medium-expert-v2': 110.0,
+++++++++++--		'walker2d-medium-expert-v2': 101.9,
+++++++++++--		'ant-medium-expert-v2': 116.1,
+++++++++++--		##
+++++++++++--		'halfcheetah-medium-v2': 46.9,
+++++++++++--		'hopper-medium-v2': 61.1,
+++++++++++--		'walker2d-medium-v2': 79.0,
+++++++++++--		'ant-medium-v2': 83.1,
+++++++++++--		##
+++++++++++--		'halfcheetah-medium-replay-v2': 41.9,
+++++++++++--		'hopper-medium-replay-v2': 91.5,
+++++++++++--		'walker2d-medium-replay-v2': 82.6,
+++++++++++--		'ant-medium-replay-v2': 77.0,
+++++++++++--	},
+++++++++++--	'Decision\nTransformer': {
+++++++++++--		##
+++++++++++--		'halfcheetah-medium-expert-v2': 86.8,
+++++++++++--		'hopper-medium-expert-v2': 107.6,
+++++++++++--		'walker2d-medium-expert-v2': 108.1,
+++++++++++--		##
+++++++++++--		'halfcheetah-medium-v2': 42.6,
+++++++++++--		'hopper-medium-v2': 67.6,
+++++++++++--		'walker2d-medium-v2': 74.0,
+++++++++++--		##
+++++++++++--		'halfcheetah-medium-replay-v2': 36.6,
+++++++++++--		'hopper-medium-replay-v2': 82.7,
+++++++++++--		'walker2d-medium-replay-v2': 66.6,
+++++++++++--	},
+++++++++++--	'CQL': {
+++++++++++--		##
+++++++++++--		'halfcheetah-medium-expert-v2': 91.6,
+++++++++++--		'hopper-medium-expert-v2': 105.4,
+++++++++++--		'walker2d-medium-expert-v2': 108.8,
+++++++++++--		##
+++++++++++--		'halfcheetah-medium-v2': 44.0,
+++++++++++--		'hopper-medium-v2': 58.5,
+++++++++++--		'walker2d-medium-v2': 72.5,
+++++++++++--		##
+++++++++++--		'halfcheetah-medium-replay-v2': 45.5,
+++++++++++--		'hopper-medium-replay-v2': 95.0,
+++++++++++--		'walker2d-medium-replay-v2': 77.2,
+++++++++++--	},
+++++++++++--	'MOPO': {
+++++++++++--		##
+++++++++++--		'halfcheetah-medium-expert-v2': 63.3,
+++++++++++--		'hopper-medium-expert-v2': 23.7,
+++++++++++--		'walker2d-medium-expert-v2': 44.6,
+++++++++++--		##
+++++++++++--		'halfcheetah-medium-v2': 42.3,
+++++++++++--		'hopper-medium-v2': 28.0,
+++++++++++--		'walker2d-medium-v2': 17.8,
+++++++++++--		##
+++++++++++--		'halfcheetah-medium-replay-v2': 53.1,
+++++++++++--		'hopper-medium-replay-v2': 67.5,
+++++++++++--		'walker2d-medium-replay-v2':39.0,
+++++++++++--	},
+++++++++++--	'MBOP': {
+++++++++++--		##
+++++++++++--		'halfcheetah-medium-expert-v2': 105.9,
+++++++++++--		'hopper-medium-expert-v2': 55.1,
+++++++++++--		'walker2d-medium-expert-v2': 70.2,
+++++++++++--		##
+++++++++++--		'halfcheetah-medium-v2': 44.6,
+++++++++++--		'hopper-medium-v2': 48.8,
+++++++++++--		'walker2d-medium-v2': 41.0,
+++++++++++--		##
+++++++++++--		'halfcheetah-medium-replay-v2': 42.3,
+++++++++++--		'hopper-medium-replay-v2': 12.4,
+++++++++++--		'walker2d-medium-replay-v2': 9.7,
+++++++++++--	},
+++++++++++--	'BRAC': {
+++++++++++--		##
+++++++++++--		'halfcheetah-medium-expert-v2': 41.9,
+++++++++++--		'hopper-medium-expert-v2': 0.9,
+++++++++++--		'walker2d-medium-expert-v2': 81.6,
+++++++++++--		##
+++++++++++--		'halfcheetah-medium-v2': 46.3,
+++++++++++--		'hopper-medium-v2': 31.3,
+++++++++++--		'walker2d-medium-v2': 81.1,
+++++++++++--		##
+++++++++++--		'halfcheetah-medium-replay-v2': 47.7,
+++++++++++--		'hopper-medium-replay-v2': 0.6,
+++++++++++--		'walker2d-medium-replay-v2': 0.9,
+++++++++++--	},
+++++++++++--	'BC': {
+++++++++++--		##
+++++++++++--		'halfcheetah-medium-expert-v2': 59.9,
+++++++++++--		'hopper-medium-expert-v2': 79.6,
+++++++++++--		'walker2d-medium-expert-v2': 36.6,
+++++++++++--		##
+++++++++++--		'halfcheetah-medium-v2': 43.1,
+++++++++++--		'hopper-medium-v2': 63.9,
+++++++++++--		'walker2d-medium-v2': 77.3,
+++++++++++--		##
+++++++++++--		'halfcheetah-medium-replay-v2': 4.3,
+++++++++++--		'hopper-medium-replay-v2': 27.6,
+++++++++++--		'walker2d-medium-replay-v2': 36.9,
+++++++++++--	},
+++++++++++--}
+++++++++++--
+++++++++++--errors = {
+++++++++++--	'Trajectory\nTransformer': {
+++++++++++--		##
+++++++++++--		'halfcheetah-medium-expert-v2': 0.2,
+++++++++++--		'hopper-medium-expert-v2': 2.7,
+++++++++++--		'walker2d-medium-expert-v2': 6.8,
+++++++++++--		'ant-medium-expert-v2': 9.0,
+++++++++++--		##
+++++++++++--		'halfcheetah-medium-v2': 0.4,
+++++++++++--		'hopper-medium-v2': 3.6,
+++++++++++--		'walker2d-medium-v2': 2.8,
+++++++++++--		'ant-medium-v2': 7.3,
+++++++++++--		##
+++++++++++--		'halfcheetah-medium-replay-v2': 2.5,
+++++++++++--		'hopper-medium-replay-v2': 3.6,
+++++++++++--		'walker2d-medium-replay-v2': 6.9,
+++++++++++--		'ant-medium-replay-v2': 6.8,
+++++++++++--	},
+++++++++++--}
+++++++++++++-trajectory_aaa
+++++++++++++diff --git a/seaquest/readme.md b/seaquest/readme.md
+++++++++++++index 84e53f8..53561f9 100644
+++++++++++++--- a/seaquest/readme.md
++++++++++++++++ b/seaquest/readme.md
+++++++++++++@@ -10,4 +10,4 @@ pip install git+https://github.com/takuseno/d4rl-atari
+++++++++++++ pip install "gym[atari, accept-rom-license]"
+++++++++++++ pip install pyclustering
+++++++++++++ pip install seaborn
+++++++++++++-pip install d3rlpy==1.1.1
+++++++++++++\ No newline at end of file
++++++++++++++pip install d3rlpy==1.1.1
+++++++++++ \ No newline at end of file
+++++++++++-diff --git a/halfcheetah/plotting/table.py b/halfcheetah/plotting/table.py
+++++++++++-deleted file mode 100644
+++++++++++-index eae74e6..0000000
+++++++++++---- a/halfcheetah/plotting/table.py
+++++++++++-+++ /dev/null
+++++++++++-@@ -1,127 +0,0 @@
+++++++++++--import numpy as np
+++++++++++--import pdb
+++++++++++--
+++++++++++--from plotting.plot import get_mean
+++++++++++--from plotting.scores import (
+++++++++++--	means as MEANS,
+++++++++++--	errors as ERRORS,
+++++++++++--)
+++++++++++--
+++++++++++--ALGORITHM_STRINGS = {
+++++++++++--	'Trajectory\nTransformer': 'TT (Ours)',
+++++++++++--	'Decision\nTransformer': 'DT',	
+++++++++++--}
+++++++++++--
+++++++++++--BUFFER_STRINGS = {
+++++++++++--	'medium-expert': 'Medium-Expert',
+++++++++++--	'medium': 'Medium',
+++++++++++--	'medium-replay': 'Medium-Replay',	
+++++++++++--}
+++++++++++--
+++++++++++--ENVIRONMENT_STRINGS = {
+++++++++++--	'halfcheetah': 'HalfCheetah',
+++++++++++--	'hopper': 'Hopper',
+++++++++++--	'walker2d': 'Walker2d',
+++++++++++--	'ant': 'Ant',
+++++++++++--}
+++++++++++--
+++++++++++--SHOW_ERRORS = ['Trajectory\nTransformer']
+++++++++++--
+++++++++++--def get_result(algorithm, buffer, environment, version='v2'):
+++++++++++--	key = f'{environment}-{buffer}-{version}'
+++++++++++--	mean = MEANS[algorithm].get(key, '-')
+++++++++++--	if algorithm in SHOW_ERRORS:
+++++++++++--		error = ERRORS[algorithm].get(key)
+++++++++++--		return (mean, error)
+++++++++++--	else:
+++++++++++--		return mean
+++++++++++--
+++++++++++--def format_result(result):
+++++++++++--	if type(result) == tuple:
+++++++++++--		mean, std = result
+++++++++++--		return f'${mean}$ \\scriptsize{{\\raisebox{{1pt}}{{$\\pm {std}$}}}}'
+++++++++++--	else:
+++++++++++--		return f'${result}$'
+++++++++++--
+++++++++++--def format_row(buffer, environment, results):
+++++++++++--	buffer_str = BUFFER_STRINGS[buffer]
+++++++++++--	environment_str = ENVIRONMENT_STRINGS[environment]
+++++++++++--	results_str = ' & '.join(format_result(result) for result in results)
+++++++++++--	row = f'{buffer_str} & {environment_str} & {results_str} \\\\ \n'
+++++++++++--	return row
+++++++++++--
+++++++++++--def format_buffer_block(algorithms, buffer, environments):
+++++++++++--	block_str = '\\midrule\n'
+++++++++++--	for environment in environments:
+++++++++++--		results = [get_result(alg, buffer, environment) for alg in algorithms]
+++++++++++--		row_str = format_row(buffer, environment, results)
+++++++++++--		block_str += row_str
+++++++++++--	return block_str
+++++++++++--
+++++++++++--def format_algorithm(algorithm):
+++++++++++--	algorithm_str = ALGORITHM_STRINGS.get(algorithm, algorithm)
+++++++++++--	return f'\multicolumn{{1}}{{c}}{{\\bf {algorithm_str}}}'
+++++++++++--
+++++++++++--def format_algorithms(algorithms):
+++++++++++--	return ' & '.join(format_algorithm(algorithm) for algorithm in algorithms)
+++++++++++--
+++++++++++--def format_averages(means, label):
+++++++++++--	prefix = f'\\multicolumn{{2}}{{c}}{{\\bf Average ({label})}} & '
+++++++++++--	formatted = ' & '.join(str(mean) for mean in means)
+++++++++++--	return prefix + formatted
+++++++++++--
+++++++++++--def format_averages_block(algorithms):
+++++++++++--	means_filtered = [np.round(get_mean(MEANS[algorithm], exclude='ant'), 1) for algorithm in algorithms]
+++++++++++--	means_all = [np.round(get_mean(MEANS[algorithm], exclude=None), 1) for algorithm in algorithms]
+++++++++++--
+++++++++++--	means_all = [
+++++++++++--		means
+++++++++++--		if 'ant-medium-expert-v2' in MEANS[algorithm]
+++++++++++--		else '$-$'
+++++++++++--		for algorithm, means in zip(algorithms, means_all)
+++++++++++--	]
+++++++++++--
+++++++++++--	formatted_filtered = format_averages(means_filtered, 'without Ant')
+++++++++++--	formatted_all = format_averages(means_all, 'all settings')
+++++++++++--
+++++++++++--	formatted_block = (
+++++++++++--		f'{formatted_filtered} \\hspace{{.6cm}} \\\\ \n'
+++++++++++--		f'{formatted_all} \\hspace{{.6cm}} \\\\ \n'
+++++++++++--	)
+++++++++++--	return formatted_block
+++++++++++--
+++++++++++--def format_table(algorithms, buffers, environments):
+++++++++++--	justify_str = 'll' + 'r' * len(algorithms)
+++++++++++--	algorithm_str = format_algorithms(['Dataset', 'Environment'] + algorithms)
+++++++++++--	averages_str = format_averages_block(algorithms)
+++++++++++--	table_prefix = (
+++++++++++--		'\\begin{table*}[h]\n'
+++++++++++--		'\\centering\n'
+++++++++++--		'\\small\n'
+++++++++++--		f'\\begin{{tabular}}{{{justify_str}}}\n'
+++++++++++--		'\\toprule\n'
+++++++++++--		f'{algorithm_str} \\\\ \n'
+++++++++++--	)
+++++++++++--	table_suffix = (
+++++++++++--		'\\midrule\n'
+++++++++++--		f'{averages_str}'
+++++++++++--		'\\bottomrule\n'
+++++++++++--		'\\end{tabular}\n'
+++++++++++--		'\\label{table:d4rl}\n'
+++++++++++--		'\\end{table*}'
+++++++++++--	)
+++++++++++--	blocks = ''.join(format_buffer_block(algorithms, buffer, environments) for buffer in buffers)
+++++++++++--	table = (
+++++++++++--		f'{table_prefix}'
+++++++++++--		f'{blocks}'
+++++++++++--		f'{table_suffix}'
+++++++++++--	)
+++++++++++--	return table
+++++++++++--
+++++++++++--
+++++++++++--algorithms =['BC', 'MBOP', 'BRAC', 'CQL',  'Decision\nTransformer', 'Trajectory\nTransformer']
+++++++++++--buffers = ['medium-expert', 'medium', 'medium-replay']
+++++++++++--environments = ['halfcheetah', 'hopper', 'walker2d', 'ant']
+++++++++++--
+++++++++++--table = format_table(algorithms, buffers, environments)
+++++++++++--print(table)
+++++++++++-diff --git a/halfcheetah/scripts/plan.py b/halfcheetah/scripts/plan.py
+++++++++++-deleted file mode 100644
+++++++++++-index f13d4cc..0000000
+++++++++++---- a/halfcheetah/scripts/plan.py
+++++++++++-+++ /dev/null
+++++++++++-@@ -1,124 +0,0 @@
+++++++++++--import json
+++++++++++--import pdb
+++++++++++--from os.path import join
+++++++++++--
+++++++++++--import trajectory.utils as utils
+++++++++++--import trajectory.datasets as datasets
+++++++++++--from trajectory.search import (
+++++++++++--    beam_plan,
+++++++++++--    make_prefix,
+++++++++++--    extract_actions,
+++++++++++--    update_context,
+++++++++++--)
+++++++++++--
+++++++++++--class Parser(utils.Parser):
+++++++++++--    dataset: str = 'halfcheetah-medium-expert-v2'
+++++++++++--    config: str = 'config.offline'
+++++++++++--
+++++++++++--#######################
+++++++++++--######## setup ########
+++++++++++--#######################
+++++++++++--
+++++++++++--args = Parser().parse_args('plan')
+++++++++++--
+++++++++++--#######################
+++++++++++--####### models ########
+++++++++++--#######################
+++++++++++--
+++++++++++--dataset = utils.load_from_config(args.logbase, args.dataset, args.gpt_loadpath,
+++++++++++--        'data_config.pkl')
+++++++++++--
+++++++++++--gpt, gpt_epoch = utils.load_model(args.logbase, args.dataset, args.gpt_loadpath,
+++++++++++--        epoch=args.gpt_epoch, device=args.device)
+++++++++++--
+++++++++++--#######################
+++++++++++--####### dataset #######
+++++++++++--#######################
+++++++++++--
+++++++++++--env = datasets.load_environment(args.dataset)
+++++++++++--print('yo')
+++++++++++--renderer = utils.make_renderer(args)
+++++++++++--timer = utils.timer.Timer()
+++++++++++--
+++++++++++--discretizer = dataset.discretizer
+++++++++++--discount = dataset.discount
+++++++++++--observation_dim = dataset.observation_dim
+++++++++++--action_dim = dataset.action_dim
+++++++++++--
+++++++++++--value_fn = lambda x: discretizer.value_fn(x, args.percentile)
+++++++++++--preprocess_fn = datasets.get_preprocess_fn(env.name)
+++++++++++--
+++++++++++--print('yo2')
+++++++++++--
+++++++++++--#######################
+++++++++++--###### main loop ######
+++++++++++--#######################
+++++++++++--
+++++++++++--observation = env.reset()
+++++++++++--total_reward = 0
+++++++++++--
+++++++++++--## observations for rendering
+++++++++++--rollout = [observation.copy()]
+++++++++++--
+++++++++++--## previous (tokenized) transitions for conditioning transformer
+++++++++++--context = []
+++++++++++--
+++++++++++--T = env.max_episode_steps
+++++++++++--for t in range(T):
+++++++++++--
+++++++++++--    observation = preprocess_fn(observation)
+++++++++++--
+++++++++++--    if t % args.plan_freq == 0:
+++++++++++--        ## concatenate previous transitions and current observations to input to model
+++++++++++--        prefix = make_prefix(discretizer, context, observation, args.prefix_context)
+++++++++++--
+++++++++++--        ## sample sequence from model beginning with `prefix`
+++++++++++--        sequence = beam_plan(
+++++++++++--            gpt, value_fn, prefix,
+++++++++++--            args.horizon, args.beam_width, args.n_expand, observation_dim, action_dim,
+++++++++++--            discount, args.max_context_transitions, verbose=args.verbose,
+++++++++++--            k_obs=args.k_obs, k_act=args.k_act, cdf_obs=args.cdf_obs, cdf_act=args.cdf_act,
+++++++++++--        )
+++++++++++--
+++++++++++--    else:
+++++++++++--        sequence = sequence[1:]
+++++++++++--
+++++++++++--    ## [ horizon x transition_dim ] convert sampled tokens to continuous trajectory
+++++++++++--    sequence_recon = discretizer.reconstruct(sequence)
+++++++++++--
+++++++++++--    ## [ action_dim ] index into sampled trajectory to grab first action
+++++++++++--    action = extract_actions(sequence_recon, observation_dim, action_dim, t=0)
+++++++++++--
+++++++++++--    ## execute action in environment
+++++++++++--    next_observation, reward, terminal, _ = env.step(action)
+++++++++++--
+++++++++++--    ## update return
+++++++++++--    total_reward += reward
+++++++++++--    score = env.get_normalized_score(total_reward)
+++++++++++--
+++++++++++--    ## update rollout observations and context transitions
+++++++++++--    rollout.append(next_observation.copy())
+++++++++++--    context = update_context(context, discretizer, observation, action, reward, args.max_context_transitions)
+++++++++++--
+++++++++++--    print(
+++++++++++--        f'[ plan ] t: {t} / {T} | r: {reward:.2f} | R: {total_reward:.2f} | score: {score:.4f} | '
+++++++++++--        f'time: {timer():.2f} | {args.dataset} | {args.exp_name} | {args.suffix}\n'
+++++++++++--    )
+++++++++++--
+++++++++++--    ## visualization
+++++++++++--    if t % args.vis_freq == 0 or terminal or t == T:
+++++++++++--
+++++++++++--        ## save current plan
+++++++++++--        renderer.render_plan(join(args.savepath, f'{t}_plan.mp4'), sequence_recon, env.state_vector())
+++++++++++--
+++++++++++--        ## save rollout thus far
+++++++++++--        renderer.render_rollout(join(args.savepath, f'rollout.mp4'), rollout, fps=80)
+++++++++++--
+++++++++++--    if terminal: break
+++++++++++--
+++++++++++--    observation = next_observation
+++++++++++--
+++++++++++--## save result as a json file
+++++++++++--json_path = join(args.savepath, 'rollout.json')
+++++++++++--json_data = {'score': score, 'step': t, 'return': total_reward, 'term': terminal, 'gpt_epoch': gpt_epoch}
+++++++++++--json.dump(json_data, open(json_path, 'w'), indent=2, sort_keys=True)
+++++++++++-diff --git a/halfcheetah/scripts/train.py b/halfcheetah/scripts/train.py
+++++++++++-deleted file mode 100644
+++++++++++-index 04af8d7..0000000
+++++++++++---- a/halfcheetah/scripts/train.py
+++++++++++-+++ /dev/null
+++++++++++-@@ -1,122 +0,0 @@
+++++++++++--import os
+++++++++++--import numpy as np
+++++++++++--import torch
+++++++++++--import pdb
+++++++++++--
+++++++++++--import trajectory.utils as utils
+++++++++++--import trajectory.datasets as datasets
+++++++++++--from trajectory.models.transformers import GPT
+++++++++++--
+++++++++++--
+++++++++++--class Parser(utils.Parser):
+++++++++++--    dataset: str = 'halfcheetah-medium-expert-v2'
+++++++++++--    config: str = 'config.offline'
+++++++++++--
+++++++++++--#######################
+++++++++++--######## setup ########
+++++++++++--#######################
+++++++++++--
+++++++++++--args = Parser().parse_args('train')
+++++++++++--
+++++++++++--#######################
+++++++++++--####### dataset #######
+++++++++++--#######################
+++++++++++--
+++++++++++--env = datasets.load_environment(args.dataset)
+++++++++++--
+++++++++++--sequence_length = args.subsampled_sequence_length * args.step
+++++++++++--
+++++++++++--dataset_config = utils.Config(
+++++++++++--    datasets.DiscretizedDataset,
+++++++++++--    savepath=(args.savepath, 'data_config.pkl'),
+++++++++++--    env=args.dataset,
+++++++++++--    N=args.N,
+++++++++++--    penalty=args.termination_penalty,
+++++++++++--    sequence_length=sequence_length,
+++++++++++--    step=args.step,
+++++++++++--    discount=args.discount,
+++++++++++--    discretizer=args.discretizer,
+++++++++++--)
+++++++++++--
+++++++++++--dataset = dataset_config()
+++++++++++--obs_dim = dataset.observation_dim
+++++++++++--act_dim = dataset.action_dim
+++++++++++--transition_dim = dataset.joined_dim
+++++++++++--
+++++++++++--#######################
+++++++++++--######## model ########
+++++++++++--#######################
+++++++++++--
+++++++++++--block_size = args.subsampled_sequence_length * transition_dim - 1
+++++++++++--print(
+++++++++++--    f'Dataset size: {len(dataset)} | '
+++++++++++--    f'Joined dim: {transition_dim} '
+++++++++++--    f'(observation: {obs_dim}, action: {act_dim}) | Block size: {block_size}'
+++++++++++--)
+++++++++++--
+++++++++++--model_config = utils.Config(
+++++++++++--    GPT,
+++++++++++--    savepath=(args.savepath, 'model_config.pkl'),
+++++++++++--    ## discretization
+++++++++++--    vocab_size=args.N, block_size=block_size,
+++++++++++--    ## architecture
+++++++++++--    n_layer=args.n_layer, n_head=args.n_head, n_embd=args.n_embd*args.n_head,
+++++++++++--    ## dimensions
+++++++++++--    observation_dim=obs_dim, action_dim=act_dim, transition_dim=transition_dim,
+++++++++++--    ## loss weighting
+++++++++++--    action_weight=args.action_weight, reward_weight=args.reward_weight, value_weight=args.value_weight,
+++++++++++--    ## dropout probabilities
+++++++++++--    embd_pdrop=args.embd_pdrop, resid_pdrop=args.resid_pdrop, attn_pdrop=args.attn_pdrop,
+++++++++++--)
+++++++++++--
+++++++++++--model = model_config()
+++++++++++--model.to(args.device)
+++++++++++--
+++++++++++--#######################
+++++++++++--####### trainer #######
+++++++++++--#######################
+++++++++++--
+++++++++++--warmup_tokens = len(dataset) * block_size ## number of tokens seen per epoch
+++++++++++--final_tokens = 20 * warmup_tokens
+++++++++++--
+++++++++++--trainer_config = utils.Config(
+++++++++++--    utils.Trainer,
+++++++++++--    savepath=(args.savepath, 'trainer_config.pkl'),
+++++++++++--    # optimization parameters
+++++++++++--    batch_size=args.batch_size,
+++++++++++--    learning_rate=args.learning_rate,
+++++++++++--    betas=(0.9, 0.95),
+++++++++++--    grad_norm_clip=1.0,
+++++++++++--    weight_decay=0.1, # only applied on matmul weights
+++++++++++--    # learning rate decay: linear warmup followed by cosine decay to 10% of original
+++++++++++--    lr_decay=args.lr_decay,
+++++++++++--    warmup_tokens=warmup_tokens,
+++++++++++--    final_tokens=final_tokens,
+++++++++++--    ## dataloader
+++++++++++--    num_workers=0,
+++++++++++--    device=args.device,
+++++++++++--)
+++++++++++--
+++++++++++--trainer = trainer_config()
+++++++++++--
+++++++++++--#######################
+++++++++++--###### main loop ######
+++++++++++--#######################
+++++++++++--
+++++++++++--## scale number of epochs to keep number of updates constant
+++++++++++--n_epochs = int(1e6 / len(dataset) * args.n_epochs_ref)
+++++++++++--save_freq = int(n_epochs // args.n_saves)
+++++++++++--
+++++++++++--for epoch in range(n_epochs):
+++++++++++--    print(f'\nEpoch: {epoch} / {n_epochs} | {args.dataset} | {args.exp_name}')
+++++++++++--
+++++++++++--    trainer.train(model, dataset)
+++++++++++--
+++++++++++--    ## get greatest multiple of `save_freq` less than or equal to `save_epoch`
+++++++++++--    save_epoch = (epoch + 1) // save_freq * save_freq
+++++++++++--    statepath = os.path.join(args.savepath, f'state_{save_epoch}.pt')
+++++++++++--    print(f'Saving model to {statepath}')
+++++++++++--
+++++++++++--    ## save state to disk
+++++++++++--    state = model.state_dict()
+++++++++++--    torch.save(state, statepath)
+++++++++++-diff --git a/halfcheetah/scripts/xrl.py b/halfcheetah/scripts/xrl.py
+++++++++++-deleted file mode 100644
+++++++++++-index 134232a..0000000
+++++++++++---- a/halfcheetah/scripts/xrl.py
+++++++++++-+++ /dev/null
+++++++++++-@@ -1,372 +0,0 @@
+++++++++++--import json
+++++++++++--import pdb
+++++++++++--from os.path import join
+++++++++++--
+++++++++++--import trajectory.utils as utils
+++++++++++--import trajectory.datasets as datasets
+++++++++++--from trajectory.search import (
+++++++++++--    make_prefix,
+++++++++++--    update_context,
+++++++++++--)
+++++++++++--from trajectory.search.sampling import forward
+++++++++++--
+++++++++++--import gym
+++++++++++--import d4rl # Import required to register environments, you may need to also import the submodule
+++++++++++--import numpy as np
+++++++++++--import d3rlpy
+++++++++++--import math as mt
+++++++++++--from sklearn.cluster import KMeans
+++++++++++--from sklearn import datasets as skdatasets
+++++++++++--from sklearn.decomposition import PCA
+++++++++++--
+++++++++++--from pyclustering.cluster.xmeans import xmeans
+++++++++++--from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer
+++++++++++--
+++++++++++--from scipy.stats import wasserstein_distance
+++++++++++--
+++++++++++--class Parser(utils.Parser):
+++++++++++--    dataset: str = 'halfcheetah-medium-expert-v2'
+++++++++++--    config: str = 'config.offline'
+++++++++++--
++++++++++++diff --git a/halfcheetah/scripts/xrl_v2.py b/halfcheetah/scripts/xrl_v2.py
++++++++++++index 62a3d4d..d0e65fa 100644
++++++++++++--- a/halfcheetah/scripts/xrl_v2.py
+++++++++++++++ b/halfcheetah/scripts/xrl_v2.py
++++++++++++@@ -21,54 +21,96 @@ from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer
++++++++++++ from scipy.stats import wasserstein_distance
++++++++++++ from moviepy.editor import VideoFileClip
++++++++++++ 
+++++++++++++
++++++++++++ class Parser(utils.Parser):
++++++++++++     dataset: str = 'halfcheetah-medium-v2'
++++++++++++     config: str = 'config.offline'
++++++++++++ 
+++++++++++ -# utils
+++++++++++--    
+++++++++++--class XMeans:
+++++++++++--    def loglikelihood(self, r, rn, var, m, k):
+++++++++++--        l1 = - rn / 2.0 * mt.log(2 * mt.pi)
+++++++++++--        l2 = - rn * m / 2.0 * mt.log(var)
+++++++++++--        l3 = - (rn - k) / 2.0
+++++++++++--        l4 = rn * mt.log(rn)
+++++++++++--        l5 = - rn * mt.log(r)
+++++++++++--
+++++++++++--        return l1 + l2 + l3 + l4 + l5
+++++++++++--
+++++++++++--    def __init__(self, X, kmax = 20):
+++++++++++--        self.X = X
+++++++++++--        self.num = np.size(self.X, axis=0)
+++++++++++--        self.dim = np.size(X, axis=1)
+++++++++++--        self.KMax = kmax
+++++++++++--
+++++++++++--    def fit(self):
+++++++++++--        k = 1
+++++++++++--        X = self.X
+++++++++++--        M = self.dim
+++++++++++--        num = self.num
+++++++++++--
+++++++++++--        while(1):
+++++++++++--            ok = k
+++++++++++--
+++++++++++--            #Improve Params
+++++++++++--            kmeans = KMeans(n_clusters=k).fit(X)
+++++++++++--            labels = kmeans.labels_
+++++++++++--            m = kmeans.cluster_centers_
+++++++++++--
+++++++++++--            #Improve Structure
+++++++++++--            #Calculate BIC
+++++++++++--            p = M + 1
+++++++++++--
+++++++++++--            obic = np.zeros(k)
+++++++++++--
+++++++++++--            for i in range(k):
+++++++++++--                rn = np.size(np.where(labels == i))
+++++++++++--                var = np.sum((X[labels == i] - m[i])**2)/float(rn - 1)
+++++++++++--                obic[i] = self.loglikelihood(rn, rn, var, M, 1) - p/2.0*mt.log(rn)
+++++++++++--
+++++++++++--            #Split each cluster into two subclusters and calculate BIC of each splitted cluster
+++++++++++--            sk = 2 #The number of subclusters
+++++++++++--            nbic = np.zeros(k)
+++++++++++--            addk = 0
+++++++++++--
+++++++++++--            for i in range(k):
+++++++++++--                ci = X[labels == i]
+++++++++++--                r = np.size(np.where(labels == i))
+++++++++++--
+++++++++++--                kmeans = KMeans(n_clusters=sk).fit(ci)
+++++++++++--                ci_labels = kmeans.labels_
+++++++++++--                sm = kmeans.cluster_centers_
+++++++++++--
+++++++++++--                for l in range(sk):
+++++++++++--                    rn = np.size(np.where(ci_labels == l))
+++++++++++--                    var = np.sum((ci[ci_labels == l] - sm[l])**2)/float(rn - sk)
+++++++++++--                    nbic[i] += self.loglikelihood(r, rn, var, M, sk)
+++++++++++--
+++++++++++--                p = sk * (M + 1)
+++++++++++--                nbic[i] -= p/2.0*mt.log(r)
+++++++++++--
+++++++++++--                if obic[i] < nbic[i]:
+++++++++++--                    addk += 1
+++++++++++--
+++++++++++--            k += addk
+++++++++++--
+++++++++++--            if ok == k or k >= self.KMax:
+++++++++++--                break
+++++++++++--
+++++++++++--
+++++++++++--        #Calculate labels and centroids
+++++++++++--        kmeans = KMeans(n_clusters=k).fit(X)
+++++++++++--        self.labels = kmeans.labels_
+++++++++++--        self.k = k
+++++++++++--        self.m = kmeans.cluster_centers_
+++++++++++--
+++++++++++--
+++++++++++--def cluster_trajectories(trajectories):
+++++++++++--    xmeans_instance = XMeans(trajectories, kmax=10)
+++++++++++--    xmeans_instance.fit()
+++++++++++--
+++++++++++--    clusters = xmeans_instance.labels
+++++++++++--    return clusters
+++++++++++--
+++++++++++--def cluster_trajectories_2(trajectories):
++++++++++++ 
++++++++++++ def cluster_trajectories(trajectories, n_clusters=10):
++++++++++++-    """TODO"""
+++++++++++++    """
+++++++++++++    Cluster trajectories using X-means.
+++++++++++++    
+++++++++++++    Args:
+++++++++++++    - trajectories: np.array, shape (n_trajectories, encoding_dim)
+++++++++++++    - n_clusters: int, max number of clusters
+++++++++++++    
+++++++++++++    Returns:
+++++++++++++    - idxs_per_cluster: list, trajectory idxs per cluster idxs
+++++++++++++    - clusters: np.array, shape (n_trajectories), cluster idxs per trajectory idx
+++++++++++++    """ 
++++++++++++ 
+++++++++++ -    # Prepare initial centers - amount of initial centers defines amount of clusters from which X-Means will
+++++++++++ -    # start analysis.
+++++++++++--    amount_initial_centers = 2
+++++++++++--    initial_centers = kmeans_plusplus_initializer(trajectories, amount_initial_centers).initialize()
+++++++++++--    
+++++++++++++    # Set 2 initial cluster centers
++++++++++++     amount_initial_centers = 2
++++++++++++     initial_centers = kmeans_plusplus_initializer(trajectories, amount_initial_centers).initialize()
++++++++++++     
+++++++++++ -    # Create instance of X-Means algorithm. The algorithm will start analysis from 2 clusters, the maximum
+++++++++++ -    # number of clusters that can be allocated is 10.
+++++++++++--    xmeans_instance = xmeans(trajectories, initial_centers, 10)
+++++++++++--    xmeans_instance.process()
+++++++++++--    
+++++++++++--    # Extract clustering results: clusters
+++++++++++--    idxs_per_cluster = xmeans_instance.get_clusters()
+++++++++++--
+++++++++++--    clusters = []
+++++++++++--    for i in range(len(trajectories)):
+++++++++++--        for j in range(len(idxs_per_cluster)):
+++++++++++--            if i in idxs_per_cluster[j]: clusters.append(j)
+++++++++++--
+++++++++++--    return idxs_per_cluster, np.array(clusters)
+++++++++++++    # Run X-means
++++++++++++     xmeans_instance = xmeans(trajectories, initial_centers, n_clusters)
++++++++++++     xmeans_instance.process()
++++++++++++     
++++++++++++     # Extract clustering results: clusters
++++++++++++     idxs_per_cluster = xmeans_instance.get_clusters()
++++++++++++ 
+++++++++++++    # Turn list of trajectory idxs per cluster to array of cluster idx per trajectory idx
++++++++++++     clusters = []
++++++++++++     for i in range(len(trajectories)):
++++++++++++         for j in range(len(idxs_per_cluster)):
++++++++++++             if i in idxs_per_cluster[j]: clusters.append(j)
++++++++++++ 
++++++++++++     return idxs_per_cluster, np.array(clusters)
+++++++++++ - 
+++++++++++--# https://github.com/sascha-kirch/ML_Notebooks/blob/main/Softmax_Temperature.ipynb
+++++++++++--def softmax(x, temp):
+++++++++++--    """Compute softmax values for each sets of scores in x."""
+++++++++++--    return np.exp(np.divide(x,temp)) / np.sum(np.exp(np.divide(x,temp)))
+++++++++++--
+++++++++++--def generate_data_embedding(trajectory_embeddings, normalizing_factor=1, temperature=1):
+++++++++++--    embedding = np.sum(trajectory_embeddings, axis=0) / normalizing_factor
+++++++++++--    embedding = softmax(embedding, temperature)
+++++++++++--    return embedding
+++++++++++--
+++++++++++--def embed_trajectory(gpt, discretizer, observations, actions, rewards, preprocess_fn):
+++++++++++--    context = []
+++++++++++--
+++++++++++--    for i in range(len(observations)):
+++++++++++--        observation = observations[i]
+++++++++++--        action = actions[i]
+++++++++++--        reward = rewards[i]
+++++++++++--
+++++++++++--        observation = preprocess_fn(observation)
+++++++++++--
+++++++++++--        # print(observation)
+++++++++++--        prefix = make_prefix(discretizer, context, observation, True)
+++++++++++--        # print("prefix", prefix.shape)
+++++++++++--
+++++++++++--        out = forward(gpt, prefix)
+++++++++++--        # print("out", out.shape)
+++++++++++--        context = update_context(context, discretizer, observation, action, reward, len(observations))
+++++++++++--        # print("cotext", context)
+++++++++++--    
+++++++++++--    emb = []
+++++++++++--    for context_step in context:
+++++++++++--        emb.append(context_step.numpy())
+++++++++++--    emb = np.array(emb)
+++++++++++--    emb = np.mean(emb, axis=0)[0]
+++++++++++--
+++++++++++--    return emb
+++++++++++--
+++++++++++--
+++++++++++--def create_complementary_dataset(dataset, idxs, trajectory_length=10):
+++++++++++--    observations = []
+++++++++++--    actions = []
+++++++++++--    rewards = []
+++++++++++--    terminals = []
+++++++++++--    for i in range(1000):
+++++++++++--        if i not in idxs:
+++++++++++--            observations += list(dataset.observations[1000*i:1000*i+trajectory_length])
+++++++++++--            actions += list(dataset.actions[1000*i:1000*i+trajectory_length])
+++++++++++--            rewards += list(dataset.rewards[1000*i:1000*i+trajectory_length])
+++++++++++--            terminals += list(dataset.terminals[1000*i:1000*i+trajectory_length])
+++++++++++--
+++++++++++--    new_dataset = d3rlpy.dataset.MDPDataset(
+++++++++++--        observations=np.array(observations),
+++++++++++--        actions=np.array(actions),
+++++++++++--        rewards=np.array(rewards),
+++++++++++--        terminals=np.array(terminals)
+++++++++++--    )
+++++++++++--    return new_dataset
+++++++++++--    
+++++++++++--
+++++++++++--
+++++++++++--
+++++++++++--def main():
+++++++++++--    # args = Parser().parse_args('plan')
+++++++++++--
+++++++++++--    #######################
+++++++++++--    ####### models ########
+++++++++++--    #######################
+++++++++++--
+++++++++++--
+++++++++++--
+++++++++++--
+++++++++++--
+++++++++++--    # print(args.dataset)
+++++++++++--
+++++++++++--    # dataset = utils.load_from_config(args.logbase, args.dataset, args.gpt_loadpath,
+++++++++++--    #         'data_config.pkl')
+++++++++++--
+++++++++++--
+++++++++++--    # gpt, gpt_epoch = utils.load_model(args.logbase, args.dataset, args.gpt_loadpath,
+++++++++++--    #         epoch=args.gpt_epoch, device=args.device)
+++++++++++--
+++++++++++--    # env = datasets.load_environment(args.dataset)
+++++++++++--
+++++++++++--    # discretizer = dataset.discretizer
+++++++++++--
+++++++++++--    # preprocess_fn = datasets.get_preprocess_fn(env.name)
+++++++++++--
+++++++++++--    # #######################
+++++++++++--    # ####### dataset #######
+++++++++++--    # #######################
+++++++++++--
+++++++++++--    # # env = datasets.load_environment(args.dataset)
+++++++++++--    # discretizer = dataset.discretizer
+++++++++++--    # preprocess_fn = datasets.get_preprocess_fn(env.name)
+++++++++++--
+++++++++++--    # # dataset
+++++++++++--    dataset_d3, env = d3rlpy.datasets.get_dataset("halfcheetah-medium-v2")
+++++++++++--
+++++++++++--    # env = gym.make('halfcheetah-medium-v2')
+++++++++++--    # dataset_d4 = d4rl.qlearning_dataset(env)
+++++++++++--
+++++++++++--    # # checks to see if d3rl & d4rl datasets are equal
+++++++++++--    # print(np.allclose(dataset_d3.actions[100], dataset_d4['actions'][100]))
+++++++++++--
+++++++++++--    # # dr4rl has same trajectories, just cut off 1 element before the end
+++++++++++--    # for j in range(1000):
+++++++++++--    #     for i in range(999):
+++++++++++--    #         if dataset_d4['rewards'][j * 999 + i] != dataset_d3.rewards[j * 1000 + i]: print("yo", i)
+++++++++++--
+++++++++++--    # #######################
+++++++++++--    # ###### main loop ######
+++++++++++--    # #######################
+++++++++++--
+++++++++++--    trajectory_length = 10 # 10 = max
+++++++++++--
+++++++++++--    # embeddings = []
+++++++++++--    # for i in range(1000):
+++++++++++--    #     observations = dataset_d3.observations[1000*i:1000*i+trajectory_length]
+++++++++++--    #     actions = dataset_d3.actions[1000*i:1000*i+trajectory_length]
+++++++++++--    #     rewards = dataset_d3.rewards[1000*i:1000*i+trajectory_length]
+++++++++++--    #     terminals = dataset_d3.terminals[1000*i:1000*i+trajectory_length]
+++++++++++--    #     emb = embed_trajectory(gpt, discretizer, observations, actions, rewards, preprocess_fn)
+++++++++++--    #     embeddings.append(emb)
+++++++++++--    # embeddings = np.array(embeddings)
+++++++++++--    # np.save("embeddings.npy", embeddings)
+++++++++++--    # print(embeddings)
+++++++++++--
+++++++++++--    embeddings = np.load("embeddings.npy")
+++++++++++--
+++++++++++--    pca = PCA(n_components=2)
+++++++++++--    pca = PCA(n_components=2)
+++++++++++--    pca_embeddings = pca.fit_transform(embeddings)
+++++++++++--    np.save("pca.py", pca_embeddings)
+++++++++++--
+++++++++++--    idxs_per_cluster, clusters = cluster_trajectories_2(embeddings)
+++++++++++--    # print(clusters)
+++++++++++--    # return
+++++++++++--    np.save("clusters.npy", clusters)
+++++++++++--
+++++++++++--    import matplotlib.pyplot as plt
+++++++++++--
+++++++++++--    d_orig = generate_data_embedding(embeddings)
+++++++++++--    unique_clusters = np.unique(clusters)
+++++++++++--    
+++++++++++--    d_j = []
+++++++++++--    complementary_datasets = []
+++++++++++--    for j in np.sort(unique_clusters):
+++++++++++--        print(j)
+++++++++++--        d_j.append(generate_data_embedding(embeddings[clusters != j]))
+++++++++++--        plt.scatter(pca_embeddings[clusters == j][:,0], pca_embeddings[clusters == j][:,1], label=j)
+++++++++++--        complementary_datasets.append(create_complementary_dataset(dataset_d3, idxs_per_cluster[j], trajectory_length))
+++++++++++--    
+++++++++++--    original_dataset = create_complementary_dataset(dataset_d3, [], trajectory_length)
+++++++++++--
+++++++++++--    print(complementary_datasets, original_dataset)
+++++++++++--
+++++++++++--    plt.legend()
+++++++++++--    plt.show()
+++++++++++--
+++++++++++--    agent_orig = d3rlpy.algos.SAC(
+++++++++++--        actor_learning_rate=3e-4,
+++++++++++--        critic_learning_rate=3e-4,
+++++++++++--        temp_learning_rate=3e-4,
+++++++++++--        batch_size=256)
+++++++++++--
+++++++++++--    print(agent_orig)
+++++++++++--
+++++++++++--    training_steps = 1000
+++++++++++--
+++++++++++--    agent_orig.fit(original_dataset, n_steps=training_steps)
+++++++++++--
+++++++++++--    agents_compl = []
+++++++++++--
+++++++++++--    for dset in complementary_datasets:
+++++++++++--        agent = d3rlpy.algos.SAC(
+++++++++++--            actor_learning_rate=3e-4,
+++++++++++--            critic_learning_rate=3e-4,
+++++++++++--            temp_learning_rate=3e-4,
+++++++++++--            batch_size=256)
+++++++++++--        agent.fit(dset, n_steps=training_steps)
+++++++++++--        agents_compl.append(agent)
+++++++++++--
+++++++++++--    action_orig = agent_orig.predict(dataset_d3.observations[0])
+++++++++++--
+++++++++++--    actions_compl = []
+++++++++++--    for agent in agents_compl:
+++++++++++--        actions_compl.append(agent.predict(dataset_d3.observations[0]))
+++++++++++--    
+++++++++++--    action_dists = []
+++++++++++--    for action in actions_compl:
+++++++++++--        action_dists.append(np.linalg.norm(action_orig-action))
+++++++++++--
+++++++++++--    k = 3
+++++++++++--    topk = np.argpartition(action_dists, -k)[-k:]
+++++++++++--
+++++++++++--    d_w = {}
+++++++++++--    for idx in topk:
+++++++++++--        d_w[idx] = wasserstein_distance(d_j[idx], d_orig)
+++++++++++--
+++++++++++--    cluster_assignment = min(d_w, key=d_w.get)
+++++++++++--    print("explanation assigned to cluster", cluster_assignment)
+++++++++++--
+++++++++++--    
+++++++++++--def assignment_test():
+++++++++++--    action_orig = np.random.rand(10)
+++++++++++--    d_orig = np.random.rand(5)
+++++++++++--
+++++++++++--    actions_compl = np.random.rand(6,10)
+++++++++++--    d_j = np.random.rand(6,5)
+++++++++++--
+++++++++++--    action_dists = []
+++++++++++--    for action in actions_compl:
+++++++++++--        action_dists.append(np.linalg.norm(action_orig-action))
+++++++++++--
+++++++++++--    print(action_dists)
+++++++++++--
+++++++++++--    k = 3
+++++++++++--    topk = np.argpartition(action_dists, -k)[-k:]
+++++++++++--
+++++++++++--    print(topk)
+++++++++++--
+++++++++++--    d_w = {}
+++++++++++--    for idx in topk:
+++++++++++--        d_w[idx] = wasserstein_distance(d_j[idx], d_orig)
+++++++++++--
+++++++++++--    print(d_w)
+++++++++++--
+++++++++++--    cluster_assignment = min(d_w, key=d_w.get)
+++++++++++--    print("explanation assigned to cluster", cluster_assignment)
+++++++++++--
+++++++++++--
+++++++++++--if __name__ == "__main__":
+++++++++++--    # main()
+++++++++++--    assignment_test()
+++++++++++-diff --git a/halfcheetah/trajectory.egg-info/PKG-INFO b/halfcheetah/trajectory.egg-info/PKG-INFO
+++++++++++-index 452c6cb..2603850 100644
+++++++++++---- a/halfcheetah/trajectory.egg-info/PKG-INFO
+++++++++++-+++ b/halfcheetah/trajectory.egg-info/PKG-INFO
+++++++++++-@@ -1,4 +1,11 @@
+++++++++++- Metadata-Version: 2.1
+++++++++++- Name: trajectory
+++++++++++- Version: 0.0.0
+++++++++++-+Summary: UNKNOWN
+++++++++++-+Home-page: UNKNOWN
+++++++++++-+License: UNKNOWN
+++++++++++-+Platform: UNKNOWN
+++++++++++- License-File: LICENSE
+++++++++++ +
+++++++++++-+UNKNOWN
+++++++++++ +
++++++++++++ def softmax(x, temp):
++++++++++++-    """TODO"""
+++++++++++++    """
+++++++++++++    Softmax with temperature using max-trick.
+++++++++++++    
+++++++++++++    Args:
+++++++++++++    - x: np.array, shape (n_data, dim_data)
+++++++++++++    - temp: int, softmax temperature
+++++++++++++    
+++++++++++++    Returns:
+++++++++++++    - softmax_x: np.array: shape (dim_data)
+++++++++++++    """ 
++++++++++++     max_x = np.max(x)
++++++++++++-    return np.exp(np.divide(x-max_x,temp)) / np.sum(np.exp(np.divide(x-max_x,temp)))
+++++++++++++    softmax_x = np.exp(np.divide(x-max_x,temp)) / np.sum(np.exp(np.divide(x-max_x,temp)))
+++++++++++++    return softmax_x
+++++++++++++
++++++++++++ 
++++++++++++ def generate_data_embedding(trajectory_embeddings, temperature=10000):
++++++++++++-    """TODO"""
+++++++++++++    """
+++++++++++++    Generate data embedding (sum+softmax) for set of encoded trajectories.
+++++++++++++    
+++++++++++++    Args:
+++++++++++++    - trajectory_embeddings: np.array, shape (n_data, dim_data)
+++++++++++++    - temperature: int, softmax temperature
+++++++++++++    
+++++++++++++    Returns:
+++++++++++++    - embedding: np.array, shape (dim_data)
+++++++++++++    """ 
++++++++++++ 
++++++++++++     embedding = np.sum(trajectory_embeddings, axis=0)
++++++++++++     embedding = softmax(embedding, temperature)
++++++++++++     
++++++++++++-
++++++++++++     return embedding
++++++++++++ 
+++++++++++++
++++++++++++ def embed_trajectory(gpt, discretizer, observations, actions, rewards, preprocess_fn):
++++++++++++-    """TODO"""
+++++++++++++    """
+++++++++++++    Encode trajectory using a trajectory transformer with a sliding window.
+++++++++++++    
+++++++++++++    Args:
+++++++++++++    - gpt: trajectory transformer
+++++++++++++    - discretizer: environment discretizer
+++++++++++++    - observations: trajectory observations
+++++++++++++    - actions: trajectory actions
+++++++++++++    - rewards: trajectory rewards
+++++++++++++    - preprocess_fn: observations preprocessing functions
+++++++++++++    
+++++++++++++    Returns:
+++++++++++++    - embedding: np.array, shape (hidden_dim), encoded trajectory
+++++++++++++    """ 
++++++++++++ 
++++++++++++     context = []
++++++++++++-
++++++++++++     output = []
++++++++++++ 
++++++++++++     for i in range(len(observations)):
++++++++++++@@ -76,12 +118,12 @@ def embed_trajectory(gpt, discretizer, observations, actions, rewards, preproces
++++++++++++         action = actions[i]
++++++++++++         reward = rewards[i]
++++++++++++ 
+++++++++++++        # Preprocess, discretize & forward through trajectory transformer
++++++++++++         observation = preprocess_fn(observation)
++++++++++++-
++++++++++++         prefix = make_prefix(discretizer, context, observation, True)
++++++++++++-
++++++++++++         out = forward(gpt, prefix)
++++++++++++ 
+++++++++++++        # Sliding window
++++++++++++         if len(context) >= 9:
++++++++++++             context.pop(0)
++++++++++++             if len(output) == 0:
++++++++++++@@ -91,8 +133,10 @@ def embed_trajectory(gpt, discretizer, observations, actions, rewards, preproces
++++++++++++ 
++++++++++++         context = update_context(context, discretizer, observation, action, reward, len(observations))
++++++++++++ 
++++++++++++-    emb = np.mean(output, axis=0)
++++++++++++-    return emb
+++++++++++++    # Embedding is the average of encoded states
+++++++++++++    embedding = np.mean(output, axis=0)
+++++++++++++    return embedding
+++++++++++++
++++++++++++ 
++++++++++++ def create_complementary_dataset(dataset, idxs, trajectory_length=10, inverse=False):
++++++++++++     """TODO"""
++++++++++++@@ -142,7 +186,7 @@ def main():
++++++++++++ 
++++++++++++     ### IMPORTANT DEFINITIONS XRL SCRIPT ###
++++++++++++ 
++++++++++++-    load_embeddings = False
+++++++++++++    load_embeddings = True
++++++++++++     load_clusters = True
++++++++++++     load_agents = True
++++++++++++     generate_human_study = False
+++++++++++ diff --git a/halfcheetah/trajectory.egg-info/SOURCES.txt b/halfcheetah/trajectory.egg-info/SOURCES.txt
+++++++++++-index 4474d85..84e8e3a 100644
++++++++++++index 84e8e3a..4474d85 100644
+++++++++++ --- a/halfcheetah/trajectory.egg-info/SOURCES.txt
+++++++++++ +++ b/halfcheetah/trajectory.egg-info/SOURCES.txt
+++++++++++-@@ -30,4 +30,5 @@ trajectory/utils/serialization.py
++++++++++++@@ -30,5 +30,4 @@ trajectory/utils/serialization.py
+++++++++++  trajectory/utils/setup.py
+++++++++++  trajectory/utils/timer.py
+++++++++++  trajectory/utils/training.py
+++++++++++ -trajectory/utils/video.py
++++++++++++-trajectory_aaa/__init__.py
+++++++++++ \ No newline at end of file
+++++++++++ +trajectory/utils/video.py
+++++++++++-+trajectory_aaa/__init__.py
+++++++++++ \ No newline at end of file
+++++++++++ diff --git a/halfcheetah/trajectory.egg-info/top_level.txt b/halfcheetah/trajectory.egg-info/top_level.txt
+++++++++++-index ce65198..1d5271f 100644
++++++++++++index 1d5271f..ce65198 100644
+++++++++++ --- a/halfcheetah/trajectory.egg-info/top_level.txt
+++++++++++ +++ b/halfcheetah/trajectory.egg-info/top_level.txt
+++++++++++-@@ -1 +1,2 @@
++++++++++++@@ -1,2 +1 @@
+++++++++++  trajectory
+++++++++++-+trajectory_aaa
++++++++++ \ No newline at end of file
++++++++++-diff --git a/halfcheetah/pca.py.npy b/halfcheetah/pca.py.npy
++++++++++-deleted file mode 100644
++++++++++-index bb19150..0000000
++++++++++-Binary files a/halfcheetah/pca.py.npy and /dev/null differ
++++++++++-diff --git a/halfcheetah/plotting/bar.png b/halfcheetah/plotting/bar.png
++++++++++-deleted file mode 100644
++++++++++-index 3679667..0000000
++++++++++-Binary files a/halfcheetah/plotting/bar.png and /dev/null differ
++++++++++-diff --git a/halfcheetah/plotting/plot.py b/halfcheetah/plotting/plot.py
++++++++++-deleted file mode 100644
++++++++++-index 163d0e4..0000000
++++++++++---- a/halfcheetah/plotting/plot.py
++++++++++-+++ /dev/null
++++++++++-@@ -1,74 +0,0 @@
++++++++++--import numpy as np
++++++++++--import matplotlib
++++++++++--import matplotlib.pyplot as plt
++++++++++--import pdb
++++++++++--
++++++++++--from plotting.scores import means
++++++++++--
++++++++++--class Colors:
++++++++++--	grey = '#B4B4B4'
++++++++++--	gold = '#F6C781'
++++++++++--	red = '#EC7C7D'
++++++++++--	blue = '#70ABCC'
++++++++++--
++++++++++--LABELS = {
++++++++++--	# 'BC': 'Behavior\nCloning',
++++++++++--	# 'MBOP': 'Model-Based\nOffline Planning',
++++++++++--	# 'BRAC': 'Behavior-Reg.\nActor-Critic',
++++++++++--	# 'CQL': 'Conservative\nQ-Learning',
++++++++++--}
++++++++++--
++++++++++--def get_mean(results, exclude=None):
++++++++++--	'''
++++++++++--		results : { environment: score, ... }
++++++++++--	'''
++++++++++--	filtered = {
++++++++++--		k: v for k, v in results.items()
++++++++++--		if (not exclude) or (exclude and exclude not in k)
++++++++++--	}
++++++++++--	return np.mean(list(filtered.values()))
++++++++++--
++++++++++--if __name__ == '__main__':
++++++++++--
++++++++++--	#################
++++++++++--	## latex
++++++++++--	#################
++++++++++--	matplotlib.rc('font', **{'family': 'serif', 'serif': ['Computer Modern']})
++++++++++--	matplotlib.rc('text', usetex=True)
++++++++++--	matplotlib.rcParams['text.latex.preamble']=[r"\usepackage{amsmath}"]
++++++++++--	#################
++++++++++--
++++++++++--	fig = plt.gcf()
++++++++++--	ax = plt.gca()
++++++++++--	fig.set_size_inches(7.5, 2.5)
++++++++++--
++++++++++--	means = {k: get_mean(v, exclude='ant') for k, v in means.items()}
++++++++++--	print(means)
++++++++++--
++++++++++--	algs = ['BC', 'MBOP', 'BRAC', 'CQL', 'Decision\nTransformer', 'Trajectory\nTransformer']
++++++++++--	vals = [means[alg] for alg in algs]
++++++++++--
++++++++++--	colors = [
++++++++++--		Colors.grey, Colors.gold,
++++++++++--		Colors.red, Colors.red, Colors.blue, Colors.blue
++++++++++--	]
++++++++++--
++++++++++--	labels = [LABELS.get(alg, alg) for alg in algs]
++++++++++--	plt.bar(labels, vals, color=colors, edgecolor=Colors.gold, lw=0)
++++++++++--	plt.ylabel('Average normalized return', labelpad=15)
++++++++++--	# plt.title('Offline RL Results')
++++++++++--
++++++++++--	legend_labels = ['Behavior Cloning', 'Trajectory Optimization', 'Temporal Difference', 'Sequence Modeling']
++++++++++--	colors = [Colors.grey, Colors.gold, Colors.red, Colors.blue]
++++++++++--	handles = [plt.Rectangle((0,0),1,1, color=color) for label, color in zip(legend_labels, colors)]
++++++++++--	plt.legend(handles, legend_labels, ncol=4,
++++++++++--		bbox_to_anchor=(1.07, -.18), fancybox=False, framealpha=0, shadow=False, columnspacing=1.5, handlelength=1.5)
++++++++++--
++++++++++--	matplotlib.rcParams['hatch.linewidth'] = 7.5
++++++++++--	# ax.patches[-1].set_hatch('/')
++++++++++--
++++++++++--	ax.spines['right'].set_visible(False)
++++++++++--	ax.spines['top'].set_visible(False)
++++++++++--
++++++++++--	# plt.savefig('plotting/bar.pdf', bbox_inches='tight')
++++++++++--	plt.savefig('plotting/bar.png', bbox_inches='tight', dpi=500)
++++++++++-diff --git a/halfcheetah/plotting/read_results.py b/halfcheetah/plotting/read_results.py
++++++++++-deleted file mode 100644
++++++++++-index 5a5fb62..0000000
++++++++++---- a/halfcheetah/plotting/read_results.py
++++++++++-+++ /dev/null
++++++++++-@@ -1,70 +0,0 @@
++++++++++--import os
++++++++++--import glob
++++++++++--import numpy as np
++++++++++--import json
++++++++++--import pdb
++++++++++--
++++++++++--import trajectory.utils as utils
++++++++++--
++++++++++--DATASETS = [
++++++++++--	f'{env}-{buffer}'
++++++++++--	for env in ['hopper', 'walker2d', 'halfcheetah', 'ant']
++++++++++--	for buffer in ['medium-expert-v2', 'medium-v2', 'medium-replay-v2']
++++++++++--]
++++++++++--
++++++++++--LOGBASE = 'logs'
++++++++++--TRIAL = '*'
++++++++++--EXP_NAME = 'plans/pretrained'
++++++++++--
++++++++++--def load_results(paths):
++++++++++--	'''
++++++++++--		paths : path to directory containing experiment trials
++++++++++--	'''
++++++++++--	scores = []
++++++++++--	for i, path in enumerate(sorted(paths)):
++++++++++--		score = load_result(path)
++++++++++--		if score is None:
++++++++++--			print(f'Skipping {path}')
++++++++++--			continue
++++++++++--		scores.append(score)
++++++++++--
++++++++++--		suffix = path.split('/')[-1]
++++++++++--
++++++++++--	mean = np.mean(scores)
++++++++++--	err = np.std(scores) / np.sqrt(len(scores))
++++++++++--	return mean, err, scores
++++++++++--
++++++++++--def load_result(path):
++++++++++--	'''
++++++++++--		path : path to experiment directory; expects `rollout.json` to be in directory
++++++++++--	'''
++++++++++--	fullpath = os.path.join(path, 'rollout.json')
++++++++++--	suffix = path.split('/')[-1]
++++++++++--
++++++++++--	if not os.path.exists(fullpath):
++++++++++--		return None
++++++++++--
++++++++++--	results = json.load(open(fullpath, 'rb'))
++++++++++--	score = results['score']
++++++++++--	return score * 100
++++++++++--
++++++++++--#######################
++++++++++--######## setup ########
++++++++++--#######################
++++++++++--
++++++++++--if __name__ == '__main__':
++++++++++--
++++++++++--	class Parser(utils.Parser):
++++++++++--	    dataset: str = None
++++++++++--
++++++++++--	args = Parser().parse_args()
++++++++++--
++++++++++--	for dataset in ([args.dataset] if args.dataset else DATASETS):
++++++++++--		subdirs = glob.glob(os.path.join(LOGBASE, dataset, EXP_NAME))
++++++++++--
++++++++++--		for subdir in subdirs:
++++++++++--			reldir = subdir.split('/')[-1]
++++++++++--			paths = glob.glob(os.path.join(subdir, TRIAL))
++++++++++--
++++++++++--			mean, err, scores = load_results(paths)
++++++++++--			print(f'{dataset.ljust(30)} | {subdir.ljust(50)} | {len(scores)} scores \n    {mean:.2f} +/- {err:.2f}\n')
++++++++++-diff --git a/halfcheetah/plotting/scores.py b/halfcheetah/plotting/scores.py
++++++++++-deleted file mode 100644
++++++++++-index f1917f7..0000000
++++++++++---- a/halfcheetah/plotting/scores.py
++++++++++-+++ /dev/null
++++++++++-@@ -1,123 +0,0 @@
++++++++++--means = {
++++++++++--	'Trajectory\nTransformer': {
++++++++++--		##
++++++++++--		'halfcheetah-medium-expert-v2': 95.0,
++++++++++--		'hopper-medium-expert-v2': 110.0,
++++++++++--		'walker2d-medium-expert-v2': 101.9,
++++++++++--		'ant-medium-expert-v2': 116.1,
++++++++++--		##
++++++++++--		'halfcheetah-medium-v2': 46.9,
++++++++++--		'hopper-medium-v2': 61.1,
++++++++++--		'walker2d-medium-v2': 79.0,
++++++++++--		'ant-medium-v2': 83.1,
++++++++++--		##
++++++++++--		'halfcheetah-medium-replay-v2': 41.9,
++++++++++--		'hopper-medium-replay-v2': 91.5,
++++++++++--		'walker2d-medium-replay-v2': 82.6,
++++++++++--		'ant-medium-replay-v2': 77.0,
++++++++++--	},
++++++++++--	'Decision\nTransformer': {
++++++++++--		##
++++++++++--		'halfcheetah-medium-expert-v2': 86.8,
++++++++++--		'hopper-medium-expert-v2': 107.6,
++++++++++--		'walker2d-medium-expert-v2': 108.1,
++++++++++--		##
++++++++++--		'halfcheetah-medium-v2': 42.6,
++++++++++--		'hopper-medium-v2': 67.6,
++++++++++--		'walker2d-medium-v2': 74.0,
++++++++++--		##
++++++++++--		'halfcheetah-medium-replay-v2': 36.6,
++++++++++--		'hopper-medium-replay-v2': 82.7,
++++++++++--		'walker2d-medium-replay-v2': 66.6,
++++++++++--	},
++++++++++--	'CQL': {
++++++++++--		##
++++++++++--		'halfcheetah-medium-expert-v2': 91.6,
++++++++++--		'hopper-medium-expert-v2': 105.4,
++++++++++--		'walker2d-medium-expert-v2': 108.8,
++++++++++--		##
++++++++++--		'halfcheetah-medium-v2': 44.0,
++++++++++--		'hopper-medium-v2': 58.5,
++++++++++--		'walker2d-medium-v2': 72.5,
++++++++++--		##
++++++++++--		'halfcheetah-medium-replay-v2': 45.5,
++++++++++--		'hopper-medium-replay-v2': 95.0,
++++++++++--		'walker2d-medium-replay-v2': 77.2,
++++++++++--	},
++++++++++--	'MOPO': {
++++++++++--		##
++++++++++--		'halfcheetah-medium-expert-v2': 63.3,
++++++++++--		'hopper-medium-expert-v2': 23.7,
++++++++++--		'walker2d-medium-expert-v2': 44.6,
++++++++++--		##
++++++++++--		'halfcheetah-medium-v2': 42.3,
++++++++++--		'hopper-medium-v2': 28.0,
++++++++++--		'walker2d-medium-v2': 17.8,
++++++++++--		##
++++++++++--		'halfcheetah-medium-replay-v2': 53.1,
++++++++++--		'hopper-medium-replay-v2': 67.5,
++++++++++--		'walker2d-medium-replay-v2':39.0,
++++++++++--	},
++++++++++--	'MBOP': {
++++++++++--		##
++++++++++--		'halfcheetah-medium-expert-v2': 105.9,
++++++++++--		'hopper-medium-expert-v2': 55.1,
++++++++++--		'walker2d-medium-expert-v2': 70.2,
++++++++++--		##
++++++++++--		'halfcheetah-medium-v2': 44.6,
++++++++++--		'hopper-medium-v2': 48.8,
++++++++++--		'walker2d-medium-v2': 41.0,
++++++++++--		##
++++++++++--		'halfcheetah-medium-replay-v2': 42.3,
++++++++++--		'hopper-medium-replay-v2': 12.4,
++++++++++--		'walker2d-medium-replay-v2': 9.7,
++++++++++--	},
++++++++++--	'BRAC': {
++++++++++--		##
++++++++++--		'halfcheetah-medium-expert-v2': 41.9,
++++++++++--		'hopper-medium-expert-v2': 0.9,
++++++++++--		'walker2d-medium-expert-v2': 81.6,
++++++++++--		##
++++++++++--		'halfcheetah-medium-v2': 46.3,
++++++++++--		'hopper-medium-v2': 31.3,
++++++++++--		'walker2d-medium-v2': 81.1,
++++++++++--		##
++++++++++--		'halfcheetah-medium-replay-v2': 47.7,
++++++++++--		'hopper-medium-replay-v2': 0.6,
++++++++++--		'walker2d-medium-replay-v2': 0.9,
++++++++++--	},
++++++++++--	'BC': {
++++++++++--		##
++++++++++--		'halfcheetah-medium-expert-v2': 59.9,
++++++++++--		'hopper-medium-expert-v2': 79.6,
++++++++++--		'walker2d-medium-expert-v2': 36.6,
++++++++++--		##
++++++++++--		'halfcheetah-medium-v2': 43.1,
++++++++++--		'hopper-medium-v2': 63.9,
++++++++++--		'walker2d-medium-v2': 77.3,
++++++++++--		##
++++++++++--		'halfcheetah-medium-replay-v2': 4.3,
++++++++++--		'hopper-medium-replay-v2': 27.6,
++++++++++--		'walker2d-medium-replay-v2': 36.9,
++++++++++--	},
++++++++++--}
++++++++++--
++++++++++--errors = {
++++++++++--	'Trajectory\nTransformer': {
++++++++++--		##
++++++++++--		'halfcheetah-medium-expert-v2': 0.2,
++++++++++--		'hopper-medium-expert-v2': 2.7,
++++++++++--		'walker2d-medium-expert-v2': 6.8,
++++++++++--		'ant-medium-expert-v2': 9.0,
++++++++++--		##
++++++++++--		'halfcheetah-medium-v2': 0.4,
++++++++++--		'hopper-medium-v2': 3.6,
++++++++++--		'walker2d-medium-v2': 2.8,
++++++++++--		'ant-medium-v2': 7.3,
++++++++++--		##
++++++++++--		'halfcheetah-medium-replay-v2': 2.5,
++++++++++--		'hopper-medium-replay-v2': 3.6,
++++++++++--		'walker2d-medium-replay-v2': 6.9,
++++++++++--		'ant-medium-replay-v2': 6.8,
++++++++++--	},
++++++++++--}
++++++++++++-trajectory_aaa
++++++++++++diff --git a/seaquest/readme.md b/seaquest/readme.md
++++++++++++index 84e53f8..53561f9 100644
++++++++++++--- a/seaquest/readme.md
+++++++++++++++ b/seaquest/readme.md
++++++++++++@@ -10,4 +10,4 @@ pip install git+https://github.com/takuseno/d4rl-atari
++++++++++++ pip install "gym[atari, accept-rom-license]"
++++++++++++ pip install pyclustering
++++++++++++ pip install seaborn
++++++++++++-pip install d3rlpy==1.1.1
++++++++++++\ No newline at end of file
+++++++++++++pip install d3rlpy==1.1.1
++++++++++ \ No newline at end of file
++++++++++-diff --git a/halfcheetah/plotting/table.py b/halfcheetah/plotting/table.py
++++++++++-deleted file mode 100644
++++++++++-index eae74e6..0000000
++++++++++---- a/halfcheetah/plotting/table.py
++++++++++-+++ /dev/null
++++++++++-@@ -1,127 +0,0 @@
++++++++++--import numpy as np
++++++++++--import pdb
++++++++++--
++++++++++--from plotting.plot import get_mean
++++++++++--from plotting.scores import (
++++++++++--	means as MEANS,
++++++++++--	errors as ERRORS,
++++++++++--)
++++++++++--
++++++++++--ALGORITHM_STRINGS = {
++++++++++--	'Trajectory\nTransformer': 'TT (Ours)',
++++++++++--	'Decision\nTransformer': 'DT',	
++++++++++--}
++++++++++--
++++++++++--BUFFER_STRINGS = {
++++++++++--	'medium-expert': 'Medium-Expert',
++++++++++--	'medium': 'Medium',
++++++++++--	'medium-replay': 'Medium-Replay',	
++++++++++--}
++++++++++--
++++++++++--ENVIRONMENT_STRINGS = {
++++++++++--	'halfcheetah': 'HalfCheetah',
++++++++++--	'hopper': 'Hopper',
++++++++++--	'walker2d': 'Walker2d',
++++++++++--	'ant': 'Ant',
++++++++++--}
++++++++++--
++++++++++--SHOW_ERRORS = ['Trajectory\nTransformer']
++++++++++--
++++++++++--def get_result(algorithm, buffer, environment, version='v2'):
++++++++++--	key = f'{environment}-{buffer}-{version}'
++++++++++--	mean = MEANS[algorithm].get(key, '-')
++++++++++--	if algorithm in SHOW_ERRORS:
++++++++++--		error = ERRORS[algorithm].get(key)
++++++++++--		return (mean, error)
++++++++++--	else:
++++++++++--		return mean
++++++++++--
++++++++++--def format_result(result):
++++++++++--	if type(result) == tuple:
++++++++++--		mean, std = result
++++++++++--		return f'${mean}$ \\scriptsize{{\\raisebox{{1pt}}{{$\\pm {std}$}}}}'
++++++++++--	else:
++++++++++--		return f'${result}$'
++++++++++--
++++++++++--def format_row(buffer, environment, results):
++++++++++--	buffer_str = BUFFER_STRINGS[buffer]
++++++++++--	environment_str = ENVIRONMENT_STRINGS[environment]
++++++++++--	results_str = ' & '.join(format_result(result) for result in results)
++++++++++--	row = f'{buffer_str} & {environment_str} & {results_str} \\\\ \n'
++++++++++--	return row
++++++++++--
++++++++++--def format_buffer_block(algorithms, buffer, environments):
++++++++++--	block_str = '\\midrule\n'
++++++++++--	for environment in environments:
++++++++++--		results = [get_result(alg, buffer, environment) for alg in algorithms]
++++++++++--		row_str = format_row(buffer, environment, results)
++++++++++--		block_str += row_str
++++++++++--	return block_str
++++++++++--
++++++++++--def format_algorithm(algorithm):
++++++++++--	algorithm_str = ALGORITHM_STRINGS.get(algorithm, algorithm)
++++++++++--	return f'\multicolumn{{1}}{{c}}{{\\bf {algorithm_str}}}'
++++++++++--
++++++++++--def format_algorithms(algorithms):
++++++++++--	return ' & '.join(format_algorithm(algorithm) for algorithm in algorithms)
++++++++++--
++++++++++--def format_averages(means, label):
++++++++++--	prefix = f'\\multicolumn{{2}}{{c}}{{\\bf Average ({label})}} & '
++++++++++--	formatted = ' & '.join(str(mean) for mean in means)
++++++++++--	return prefix + formatted
++++++++++--
++++++++++--def format_averages_block(algorithms):
++++++++++--	means_filtered = [np.round(get_mean(MEANS[algorithm], exclude='ant'), 1) for algorithm in algorithms]
++++++++++--	means_all = [np.round(get_mean(MEANS[algorithm], exclude=None), 1) for algorithm in algorithms]
++++++++++--
++++++++++--	means_all = [
++++++++++--		means
++++++++++--		if 'ant-medium-expert-v2' in MEANS[algorithm]
++++++++++--		else '$-$'
++++++++++--		for algorithm, means in zip(algorithms, means_all)
++++++++++--	]
++++++++++--
++++++++++--	formatted_filtered = format_averages(means_filtered, 'without Ant')
++++++++++--	formatted_all = format_averages(means_all, 'all settings')
++++++++++--
++++++++++--	formatted_block = (
++++++++++--		f'{formatted_filtered} \\hspace{{.6cm}} \\\\ \n'
++++++++++--		f'{formatted_all} \\hspace{{.6cm}} \\\\ \n'
++++++++++--	)
++++++++++--	return formatted_block
++++++++++--
++++++++++--def format_table(algorithms, buffers, environments):
++++++++++--	justify_str = 'll' + 'r' * len(algorithms)
++++++++++--	algorithm_str = format_algorithms(['Dataset', 'Environment'] + algorithms)
++++++++++--	averages_str = format_averages_block(algorithms)
++++++++++--	table_prefix = (
++++++++++--		'\\begin{table*}[h]\n'
++++++++++--		'\\centering\n'
++++++++++--		'\\small\n'
++++++++++--		f'\\begin{{tabular}}{{{justify_str}}}\n'
++++++++++--		'\\toprule\n'
++++++++++--		f'{algorithm_str} \\\\ \n'
++++++++++--	)
++++++++++--	table_suffix = (
++++++++++--		'\\midrule\n'
++++++++++--		f'{averages_str}'
++++++++++--		'\\bottomrule\n'
++++++++++--		'\\end{tabular}\n'
++++++++++--		'\\label{table:d4rl}\n'
++++++++++--		'\\end{table*}'
++++++++++--	)
++++++++++--	blocks = ''.join(format_buffer_block(algorithms, buffer, environments) for buffer in buffers)
++++++++++--	table = (
++++++++++--		f'{table_prefix}'
++++++++++--		f'{blocks}'
++++++++++--		f'{table_suffix}'
++++++++++--	)
++++++++++--	return table
++++++++++--
++++++++++--
++++++++++--algorithms =['BC', 'MBOP', 'BRAC', 'CQL',  'Decision\nTransformer', 'Trajectory\nTransformer']
++++++++++--buffers = ['medium-expert', 'medium', 'medium-replay']
++++++++++--environments = ['halfcheetah', 'hopper', 'walker2d', 'ant']
++++++++++--
++++++++++--table = format_table(algorithms, buffers, environments)
++++++++++--print(table)
++++++++++-diff --git a/halfcheetah/scripts/plan.py b/halfcheetah/scripts/plan.py
++++++++++-deleted file mode 100644
++++++++++-index f13d4cc..0000000
++++++++++---- a/halfcheetah/scripts/plan.py
++++++++++-+++ /dev/null
++++++++++-@@ -1,124 +0,0 @@
++++++++++--import json
++++++++++--import pdb
++++++++++--from os.path import join
++++++++++--
++++++++++--import trajectory.utils as utils
++++++++++--import trajectory.datasets as datasets
++++++++++--from trajectory.search import (
++++++++++--    beam_plan,
++++++++++--    make_prefix,
++++++++++--    extract_actions,
++++++++++--    update_context,
++++++++++--)
++++++++++--
++++++++++--class Parser(utils.Parser):
++++++++++--    dataset: str = 'halfcheetah-medium-expert-v2'
++++++++++--    config: str = 'config.offline'
++++++++++--
++++++++++--#######################
++++++++++--######## setup ########
++++++++++--#######################
++++++++++--
++++++++++--args = Parser().parse_args('plan')
++++++++++--
++++++++++--#######################
++++++++++--####### models ########
++++++++++--#######################
++++++++++--
++++++++++--dataset = utils.load_from_config(args.logbase, args.dataset, args.gpt_loadpath,
++++++++++--        'data_config.pkl')
++++++++++--
++++++++++--gpt, gpt_epoch = utils.load_model(args.logbase, args.dataset, args.gpt_loadpath,
++++++++++--        epoch=args.gpt_epoch, device=args.device)
++++++++++--
++++++++++--#######################
++++++++++--####### dataset #######
++++++++++--#######################
++++++++++--
++++++++++--env = datasets.load_environment(args.dataset)
++++++++++--print('yo')
++++++++++--renderer = utils.make_renderer(args)
++++++++++--timer = utils.timer.Timer()
++++++++++--
++++++++++--discretizer = dataset.discretizer
++++++++++--discount = dataset.discount
++++++++++--observation_dim = dataset.observation_dim
++++++++++--action_dim = dataset.action_dim
++++++++++--
++++++++++--value_fn = lambda x: discretizer.value_fn(x, args.percentile)
++++++++++--preprocess_fn = datasets.get_preprocess_fn(env.name)
++++++++++--
++++++++++--print('yo2')
++++++++++--
++++++++++--#######################
++++++++++--###### main loop ######
++++++++++--#######################
++++++++++--
++++++++++--observation = env.reset()
++++++++++--total_reward = 0
++++++++++--
++++++++++--## observations for rendering
++++++++++--rollout = [observation.copy()]
++++++++++--
++++++++++--## previous (tokenized) transitions for conditioning transformer
++++++++++--context = []
++++++++++--
++++++++++--T = env.max_episode_steps
++++++++++--for t in range(T):
++++++++++--
++++++++++--    observation = preprocess_fn(observation)
++++++++++--
++++++++++--    if t % args.plan_freq == 0:
++++++++++--        ## concatenate previous transitions and current observations to input to model
++++++++++--        prefix = make_prefix(discretizer, context, observation, args.prefix_context)
++++++++++--
++++++++++--        ## sample sequence from model beginning with `prefix`
++++++++++--        sequence = beam_plan(
++++++++++--            gpt, value_fn, prefix,
++++++++++--            args.horizon, args.beam_width, args.n_expand, observation_dim, action_dim,
++++++++++--            discount, args.max_context_transitions, verbose=args.verbose,
++++++++++--            k_obs=args.k_obs, k_act=args.k_act, cdf_obs=args.cdf_obs, cdf_act=args.cdf_act,
++++++++++--        )
++++++++++--
++++++++++--    else:
++++++++++--        sequence = sequence[1:]
++++++++++--
++++++++++--    ## [ horizon x transition_dim ] convert sampled tokens to continuous trajectory
++++++++++--    sequence_recon = discretizer.reconstruct(sequence)
++++++++++--
++++++++++--    ## [ action_dim ] index into sampled trajectory to grab first action
++++++++++--    action = extract_actions(sequence_recon, observation_dim, action_dim, t=0)
++++++++++--
++++++++++--    ## execute action in environment
++++++++++--    next_observation, reward, terminal, _ = env.step(action)
++++++++++--
++++++++++--    ## update return
++++++++++--    total_reward += reward
++++++++++--    score = env.get_normalized_score(total_reward)
++++++++++--
++++++++++--    ## update rollout observations and context transitions
++++++++++--    rollout.append(next_observation.copy())
++++++++++--    context = update_context(context, discretizer, observation, action, reward, args.max_context_transitions)
++++++++++--
++++++++++--    print(
++++++++++--        f'[ plan ] t: {t} / {T} | r: {reward:.2f} | R: {total_reward:.2f} | score: {score:.4f} | '
++++++++++--        f'time: {timer():.2f} | {args.dataset} | {args.exp_name} | {args.suffix}\n'
++++++++++--    )
++++++++++--
++++++++++--    ## visualization
++++++++++--    if t % args.vis_freq == 0 or terminal or t == T:
++++++++++--
++++++++++--        ## save current plan
++++++++++--        renderer.render_plan(join(args.savepath, f'{t}_plan.mp4'), sequence_recon, env.state_vector())
++++++++++--
++++++++++--        ## save rollout thus far
++++++++++--        renderer.render_rollout(join(args.savepath, f'rollout.mp4'), rollout, fps=80)
++++++++++--
++++++++++--    if terminal: break
++++++++++--
++++++++++--    observation = next_observation
++++++++++--
++++++++++--## save result as a json file
++++++++++--json_path = join(args.savepath, 'rollout.json')
++++++++++--json_data = {'score': score, 'step': t, 'return': total_reward, 'term': terminal, 'gpt_epoch': gpt_epoch}
++++++++++--json.dump(json_data, open(json_path, 'w'), indent=2, sort_keys=True)
++++++++++-diff --git a/halfcheetah/scripts/train.py b/halfcheetah/scripts/train.py
++++++++++-deleted file mode 100644
++++++++++-index 04af8d7..0000000
++++++++++---- a/halfcheetah/scripts/train.py
++++++++++-+++ /dev/null
++++++++++-@@ -1,122 +0,0 @@
++++++++++--import os
++++++++++--import numpy as np
++++++++++--import torch
++++++++++--import pdb
++++++++++--
++++++++++--import trajectory.utils as utils
++++++++++--import trajectory.datasets as datasets
++++++++++--from trajectory.models.transformers import GPT
++++++++++--
++++++++++--
++++++++++--class Parser(utils.Parser):
++++++++++--    dataset: str = 'halfcheetah-medium-expert-v2'
++++++++++--    config: str = 'config.offline'
++++++++++--
++++++++++--#######################
++++++++++--######## setup ########
++++++++++--#######################
++++++++++--
++++++++++--args = Parser().parse_args('train')
++++++++++--
++++++++++--#######################
++++++++++--####### dataset #######
++++++++++--#######################
++++++++++--
++++++++++--env = datasets.load_environment(args.dataset)
++++++++++--
++++++++++--sequence_length = args.subsampled_sequence_length * args.step
++++++++++--
++++++++++--dataset_config = utils.Config(
++++++++++--    datasets.DiscretizedDataset,
++++++++++--    savepath=(args.savepath, 'data_config.pkl'),
++++++++++--    env=args.dataset,
++++++++++--    N=args.N,
++++++++++--    penalty=args.termination_penalty,
++++++++++--    sequence_length=sequence_length,
++++++++++--    step=args.step,
++++++++++--    discount=args.discount,
++++++++++--    discretizer=args.discretizer,
++++++++++--)
++++++++++--
++++++++++--dataset = dataset_config()
++++++++++--obs_dim = dataset.observation_dim
++++++++++--act_dim = dataset.action_dim
++++++++++--transition_dim = dataset.joined_dim
++++++++++--
++++++++++--#######################
++++++++++--######## model ########
++++++++++--#######################
++++++++++--
++++++++++--block_size = args.subsampled_sequence_length * transition_dim - 1
++++++++++--print(
++++++++++--    f'Dataset size: {len(dataset)} | '
++++++++++--    f'Joined dim: {transition_dim} '
++++++++++--    f'(observation: {obs_dim}, action: {act_dim}) | Block size: {block_size}'
++++++++++--)
++++++++++--
++++++++++--model_config = utils.Config(
++++++++++--    GPT,
++++++++++--    savepath=(args.savepath, 'model_config.pkl'),
++++++++++--    ## discretization
++++++++++--    vocab_size=args.N, block_size=block_size,
++++++++++--    ## architecture
++++++++++--    n_layer=args.n_layer, n_head=args.n_head, n_embd=args.n_embd*args.n_head,
++++++++++--    ## dimensions
++++++++++--    observation_dim=obs_dim, action_dim=act_dim, transition_dim=transition_dim,
++++++++++--    ## loss weighting
++++++++++--    action_weight=args.action_weight, reward_weight=args.reward_weight, value_weight=args.value_weight,
++++++++++--    ## dropout probabilities
++++++++++--    embd_pdrop=args.embd_pdrop, resid_pdrop=args.resid_pdrop, attn_pdrop=args.attn_pdrop,
++++++++++--)
++++++++++--
++++++++++--model = model_config()
++++++++++--model.to(args.device)
++++++++++--
++++++++++--#######################
++++++++++--####### trainer #######
++++++++++--#######################
++++++++++--
++++++++++--warmup_tokens = len(dataset) * block_size ## number of tokens seen per epoch
++++++++++--final_tokens = 20 * warmup_tokens
++++++++++--
++++++++++--trainer_config = utils.Config(
++++++++++--    utils.Trainer,
++++++++++--    savepath=(args.savepath, 'trainer_config.pkl'),
++++++++++--    # optimization parameters
++++++++++--    batch_size=args.batch_size,
++++++++++--    learning_rate=args.learning_rate,
++++++++++--    betas=(0.9, 0.95),
++++++++++--    grad_norm_clip=1.0,
++++++++++--    weight_decay=0.1, # only applied on matmul weights
++++++++++--    # learning rate decay: linear warmup followed by cosine decay to 10% of original
++++++++++--    lr_decay=args.lr_decay,
++++++++++--    warmup_tokens=warmup_tokens,
++++++++++--    final_tokens=final_tokens,
++++++++++--    ## dataloader
++++++++++--    num_workers=0,
++++++++++--    device=args.device,
++++++++++--)
++++++++++--
++++++++++--trainer = trainer_config()
++++++++++--
++++++++++--#######################
++++++++++--###### main loop ######
++++++++++--#######################
++++++++++--
++++++++++--## scale number of epochs to keep number of updates constant
++++++++++--n_epochs = int(1e6 / len(dataset) * args.n_epochs_ref)
++++++++++--save_freq = int(n_epochs // args.n_saves)
++++++++++--
++++++++++--for epoch in range(n_epochs):
++++++++++--    print(f'\nEpoch: {epoch} / {n_epochs} | {args.dataset} | {args.exp_name}')
++++++++++--
++++++++++--    trainer.train(model, dataset)
++++++++++--
++++++++++--    ## get greatest multiple of `save_freq` less than or equal to `save_epoch`
++++++++++--    save_epoch = (epoch + 1) // save_freq * save_freq
++++++++++--    statepath = os.path.join(args.savepath, f'state_{save_epoch}.pt')
++++++++++--    print(f'Saving model to {statepath}')
++++++++++--
++++++++++--    ## save state to disk
++++++++++--    state = model.state_dict()
++++++++++--    torch.save(state, statepath)
++++++++++-diff --git a/halfcheetah/scripts/xrl.py b/halfcheetah/scripts/xrl.py
++++++++++-deleted file mode 100644
++++++++++-index 134232a..0000000
++++++++++---- a/halfcheetah/scripts/xrl.py
++++++++++-+++ /dev/null
++++++++++-@@ -1,372 +0,0 @@
++++++++++--import json
++++++++++--import pdb
++++++++++--from os.path import join
++++++++++--
++++++++++--import trajectory.utils as utils
++++++++++--import trajectory.datasets as datasets
++++++++++--from trajectory.search import (
++++++++++--    make_prefix,
++++++++++--    update_context,
++++++++++--)
++++++++++--from trajectory.search.sampling import forward
++++++++++--
++++++++++--import gym
++++++++++--import d4rl # Import required to register environments, you may need to also import the submodule
++++++++++--import numpy as np
++++++++++--import d3rlpy
++++++++++--import math as mt
++++++++++--from sklearn.cluster import KMeans
++++++++++--from sklearn import datasets as skdatasets
++++++++++--from sklearn.decomposition import PCA
++++++++++--
++++++++++--from pyclustering.cluster.xmeans import xmeans
++++++++++--from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer
++++++++++--
++++++++++--from scipy.stats import wasserstein_distance
++++++++++--
++++++++++--class Parser(utils.Parser):
++++++++++--    dataset: str = 'halfcheetah-medium-expert-v2'
++++++++++--    config: str = 'config.offline'
++++++++++--
+++++++++++diff --git a/halfcheetah/scripts/xrl_v2.py b/halfcheetah/scripts/xrl_v2.py
+++++++++++index 62a3d4d..d0e65fa 100644
+++++++++++--- a/halfcheetah/scripts/xrl_v2.py
++++++++++++++ b/halfcheetah/scripts/xrl_v2.py
+++++++++++@@ -21,54 +21,96 @@ from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer
+++++++++++ from scipy.stats import wasserstein_distance
+++++++++++ from moviepy.editor import VideoFileClip
+++++++++++ 
++++++++++++
+++++++++++ class Parser(utils.Parser):
+++++++++++     dataset: str = 'halfcheetah-medium-v2'
+++++++++++     config: str = 'config.offline'
+++++++++++ 
++++++++++ -# utils
++++++++++--    
++++++++++--class XMeans:
++++++++++--    def loglikelihood(self, r, rn, var, m, k):
++++++++++--        l1 = - rn / 2.0 * mt.log(2 * mt.pi)
++++++++++--        l2 = - rn * m / 2.0 * mt.log(var)
++++++++++--        l3 = - (rn - k) / 2.0
++++++++++--        l4 = rn * mt.log(rn)
++++++++++--        l5 = - rn * mt.log(r)
++++++++++--
++++++++++--        return l1 + l2 + l3 + l4 + l5
++++++++++--
++++++++++--    def __init__(self, X, kmax = 20):
++++++++++--        self.X = X
++++++++++--        self.num = np.size(self.X, axis=0)
++++++++++--        self.dim = np.size(X, axis=1)
++++++++++--        self.KMax = kmax
++++++++++--
++++++++++--    def fit(self):
++++++++++--        k = 1
++++++++++--        X = self.X
++++++++++--        M = self.dim
++++++++++--        num = self.num
++++++++++--
++++++++++--        while(1):
++++++++++--            ok = k
++++++++++--
++++++++++--            #Improve Params
++++++++++--            kmeans = KMeans(n_clusters=k).fit(X)
++++++++++--            labels = kmeans.labels_
++++++++++--            m = kmeans.cluster_centers_
++++++++++--
++++++++++--            #Improve Structure
++++++++++--            #Calculate BIC
++++++++++--            p = M + 1
++++++++++--
++++++++++--            obic = np.zeros(k)
++++++++++--
++++++++++--            for i in range(k):
++++++++++--                rn = np.size(np.where(labels == i))
++++++++++--                var = np.sum((X[labels == i] - m[i])**2)/float(rn - 1)
++++++++++--                obic[i] = self.loglikelihood(rn, rn, var, M, 1) - p/2.0*mt.log(rn)
++++++++++--
++++++++++--            #Split each cluster into two subclusters and calculate BIC of each splitted cluster
++++++++++--            sk = 2 #The number of subclusters
++++++++++--            nbic = np.zeros(k)
++++++++++--            addk = 0
++++++++++--
++++++++++--            for i in range(k):
++++++++++--                ci = X[labels == i]
++++++++++--                r = np.size(np.where(labels == i))
++++++++++--
++++++++++--                kmeans = KMeans(n_clusters=sk).fit(ci)
++++++++++--                ci_labels = kmeans.labels_
++++++++++--                sm = kmeans.cluster_centers_
++++++++++--
++++++++++--                for l in range(sk):
++++++++++--                    rn = np.size(np.where(ci_labels == l))
++++++++++--                    var = np.sum((ci[ci_labels == l] - sm[l])**2)/float(rn - sk)
++++++++++--                    nbic[i] += self.loglikelihood(r, rn, var, M, sk)
++++++++++--
++++++++++--                p = sk * (M + 1)
++++++++++--                nbic[i] -= p/2.0*mt.log(r)
++++++++++--
++++++++++--                if obic[i] < nbic[i]:
++++++++++--                    addk += 1
++++++++++--
++++++++++--            k += addk
++++++++++--
++++++++++--            if ok == k or k >= self.KMax:
++++++++++--                break
++++++++++--
++++++++++--
++++++++++--        #Calculate labels and centroids
++++++++++--        kmeans = KMeans(n_clusters=k).fit(X)
++++++++++--        self.labels = kmeans.labels_
++++++++++--        self.k = k
++++++++++--        self.m = kmeans.cluster_centers_
++++++++++--
++++++++++--
++++++++++--def cluster_trajectories(trajectories):
++++++++++--    xmeans_instance = XMeans(trajectories, kmax=10)
++++++++++--    xmeans_instance.fit()
++++++++++--
++++++++++--    clusters = xmeans_instance.labels
++++++++++--    return clusters
++++++++++--
++++++++++--def cluster_trajectories_2(trajectories):
+++++++++++ 
+++++++++++ def cluster_trajectories(trajectories, n_clusters=10):
+++++++++++-    """TODO"""
++++++++++++    """
++++++++++++    Cluster trajectories using X-means.
++++++++++++    
++++++++++++    Args:
++++++++++++    - trajectories: np.array, shape (n_trajectories, encoding_dim)
++++++++++++    - n_clusters: int, max number of clusters
++++++++++++    
++++++++++++    Returns:
++++++++++++    - idxs_per_cluster: list, trajectory idxs per cluster idxs
++++++++++++    - clusters: np.array, shape (n_trajectories), cluster idxs per trajectory idx
++++++++++++    """ 
+++++++++++ 
++++++++++ -    # Prepare initial centers - amount of initial centers defines amount of clusters from which X-Means will
++++++++++ -    # start analysis.
++++++++++--    amount_initial_centers = 2
++++++++++--    initial_centers = kmeans_plusplus_initializer(trajectories, amount_initial_centers).initialize()
++++++++++--    
++++++++++++    # Set 2 initial cluster centers
+++++++++++     amount_initial_centers = 2
+++++++++++     initial_centers = kmeans_plusplus_initializer(trajectories, amount_initial_centers).initialize()
+++++++++++     
++++++++++ -    # Create instance of X-Means algorithm. The algorithm will start analysis from 2 clusters, the maximum
++++++++++ -    # number of clusters that can be allocated is 10.
++++++++++--    xmeans_instance = xmeans(trajectories, initial_centers, 10)
++++++++++--    xmeans_instance.process()
++++++++++--    
++++++++++--    # Extract clustering results: clusters
++++++++++--    idxs_per_cluster = xmeans_instance.get_clusters()
++++++++++--
++++++++++--    clusters = []
++++++++++--    for i in range(len(trajectories)):
++++++++++--        for j in range(len(idxs_per_cluster)):
++++++++++--            if i in idxs_per_cluster[j]: clusters.append(j)
++++++++++--
++++++++++--    return idxs_per_cluster, np.array(clusters)
++++++++++++    # Run X-means
+++++++++++     xmeans_instance = xmeans(trajectories, initial_centers, n_clusters)
+++++++++++     xmeans_instance.process()
+++++++++++     
+++++++++++     # Extract clustering results: clusters
+++++++++++     idxs_per_cluster = xmeans_instance.get_clusters()
+++++++++++ 
++++++++++++    # Turn list of trajectory idxs per cluster to array of cluster idx per trajectory idx
+++++++++++     clusters = []
+++++++++++     for i in range(len(trajectories)):
+++++++++++         for j in range(len(idxs_per_cluster)):
+++++++++++             if i in idxs_per_cluster[j]: clusters.append(j)
+++++++++++ 
+++++++++++     return idxs_per_cluster, np.array(clusters)
++++++++++ - 
++++++++++--# https://github.com/sascha-kirch/ML_Notebooks/blob/main/Softmax_Temperature.ipynb
++++++++++--def softmax(x, temp):
++++++++++--    """Compute softmax values for each sets of scores in x."""
++++++++++--    return np.exp(np.divide(x,temp)) / np.sum(np.exp(np.divide(x,temp)))
++++++++++--
++++++++++--def generate_data_embedding(trajectory_embeddings, normalizing_factor=1, temperature=1):
++++++++++--    embedding = np.sum(trajectory_embeddings, axis=0) / normalizing_factor
++++++++++--    embedding = softmax(embedding, temperature)
++++++++++--    return embedding
++++++++++--
++++++++++--def embed_trajectory(gpt, discretizer, observations, actions, rewards, preprocess_fn):
++++++++++--    context = []
++++++++++--
++++++++++--    for i in range(len(observations)):
++++++++++--        observation = observations[i]
++++++++++--        action = actions[i]
++++++++++--        reward = rewards[i]
++++++++++--
++++++++++--        observation = preprocess_fn(observation)
++++++++++--
++++++++++--        # print(observation)
++++++++++--        prefix = make_prefix(discretizer, context, observation, True)
++++++++++--        # print("prefix", prefix.shape)
++++++++++--
++++++++++--        out = forward(gpt, prefix)
++++++++++--        # print("out", out.shape)
++++++++++--        context = update_context(context, discretizer, observation, action, reward, len(observations))
++++++++++--        # print("cotext", context)
++++++++++--    
++++++++++--    emb = []
++++++++++--    for context_step in context:
++++++++++--        emb.append(context_step.numpy())
++++++++++--    emb = np.array(emb)
++++++++++--    emb = np.mean(emb, axis=0)[0]
++++++++++--
++++++++++--    return emb
++++++++++--
++++++++++--
++++++++++--def create_complementary_dataset(dataset, idxs, trajectory_length=10):
++++++++++--    observations = []
++++++++++--    actions = []
++++++++++--    rewards = []
++++++++++--    terminals = []
++++++++++--    for i in range(1000):
++++++++++--        if i not in idxs:
++++++++++--            observations += list(dataset.observations[1000*i:1000*i+trajectory_length])
++++++++++--            actions += list(dataset.actions[1000*i:1000*i+trajectory_length])
++++++++++--            rewards += list(dataset.rewards[1000*i:1000*i+trajectory_length])
++++++++++--            terminals += list(dataset.terminals[1000*i:1000*i+trajectory_length])
++++++++++--
++++++++++--    new_dataset = d3rlpy.dataset.MDPDataset(
++++++++++--        observations=np.array(observations),
++++++++++--        actions=np.array(actions),
++++++++++--        rewards=np.array(rewards),
++++++++++--        terminals=np.array(terminals)
++++++++++--    )
++++++++++--    return new_dataset
++++++++++--    
++++++++++--
++++++++++--
++++++++++--
++++++++++--def main():
++++++++++--    # args = Parser().parse_args('plan')
++++++++++--
++++++++++--    #######################
++++++++++--    ####### models ########
++++++++++--    #######################
++++++++++--
++++++++++--
++++++++++--
++++++++++--
++++++++++--
++++++++++--    # print(args.dataset)
++++++++++--
++++++++++--    # dataset = utils.load_from_config(args.logbase, args.dataset, args.gpt_loadpath,
++++++++++--    #         'data_config.pkl')
++++++++++--
++++++++++--
++++++++++--    # gpt, gpt_epoch = utils.load_model(args.logbase, args.dataset, args.gpt_loadpath,
++++++++++--    #         epoch=args.gpt_epoch, device=args.device)
++++++++++--
++++++++++--    # env = datasets.load_environment(args.dataset)
++++++++++--
++++++++++--    # discretizer = dataset.discretizer
++++++++++--
++++++++++--    # preprocess_fn = datasets.get_preprocess_fn(env.name)
++++++++++--
++++++++++--    # #######################
++++++++++--    # ####### dataset #######
++++++++++--    # #######################
++++++++++--
++++++++++--    # # env = datasets.load_environment(args.dataset)
++++++++++--    # discretizer = dataset.discretizer
++++++++++--    # preprocess_fn = datasets.get_preprocess_fn(env.name)
++++++++++--
++++++++++--    # # dataset
++++++++++--    dataset_d3, env = d3rlpy.datasets.get_dataset("halfcheetah-medium-v2")
++++++++++--
++++++++++--    # env = gym.make('halfcheetah-medium-v2')
++++++++++--    # dataset_d4 = d4rl.qlearning_dataset(env)
++++++++++--
++++++++++--    # # checks to see if d3rl & d4rl datasets are equal
++++++++++--    # print(np.allclose(dataset_d3.actions[100], dataset_d4['actions'][100]))
++++++++++--
++++++++++--    # # dr4rl has same trajectories, just cut off 1 element before the end
++++++++++--    # for j in range(1000):
++++++++++--    #     for i in range(999):
++++++++++--    #         if dataset_d4['rewards'][j * 999 + i] != dataset_d3.rewards[j * 1000 + i]: print("yo", i)
++++++++++--
++++++++++--    # #######################
++++++++++--    # ###### main loop ######
++++++++++--    # #######################
++++++++++--
++++++++++--    trajectory_length = 10 # 10 = max
++++++++++--
++++++++++--    # embeddings = []
++++++++++--    # for i in range(1000):
++++++++++--    #     observations = dataset_d3.observations[1000*i:1000*i+trajectory_length]
++++++++++--    #     actions = dataset_d3.actions[1000*i:1000*i+trajectory_length]
++++++++++--    #     rewards = dataset_d3.rewards[1000*i:1000*i+trajectory_length]
++++++++++--    #     terminals = dataset_d3.terminals[1000*i:1000*i+trajectory_length]
++++++++++--    #     emb = embed_trajectory(gpt, discretizer, observations, actions, rewards, preprocess_fn)
++++++++++--    #     embeddings.append(emb)
++++++++++--    # embeddings = np.array(embeddings)
++++++++++--    # np.save("embeddings.npy", embeddings)
++++++++++--    # print(embeddings)
++++++++++--
++++++++++--    embeddings = np.load("embeddings.npy")
++++++++++--
++++++++++--    pca = PCA(n_components=2)
++++++++++--    pca = PCA(n_components=2)
++++++++++--    pca_embeddings = pca.fit_transform(embeddings)
++++++++++--    np.save("pca.py", pca_embeddings)
++++++++++--
++++++++++--    idxs_per_cluster, clusters = cluster_trajectories_2(embeddings)
++++++++++--    # print(clusters)
++++++++++--    # return
++++++++++--    np.save("clusters.npy", clusters)
++++++++++--
++++++++++--    import matplotlib.pyplot as plt
++++++++++--
++++++++++--    d_orig = generate_data_embedding(embeddings)
++++++++++--    unique_clusters = np.unique(clusters)
++++++++++--    
++++++++++--    d_j = []
++++++++++--    complementary_datasets = []
++++++++++--    for j in np.sort(unique_clusters):
++++++++++--        print(j)
++++++++++--        d_j.append(generate_data_embedding(embeddings[clusters != j]))
++++++++++--        plt.scatter(pca_embeddings[clusters == j][:,0], pca_embeddings[clusters == j][:,1], label=j)
++++++++++--        complementary_datasets.append(create_complementary_dataset(dataset_d3, idxs_per_cluster[j], trajectory_length))
++++++++++--    
++++++++++--    original_dataset = create_complementary_dataset(dataset_d3, [], trajectory_length)
++++++++++--
++++++++++--    print(complementary_datasets, original_dataset)
++++++++++--
++++++++++--    plt.legend()
++++++++++--    plt.show()
++++++++++--
++++++++++--    agent_orig = d3rlpy.algos.SAC(
++++++++++--        actor_learning_rate=3e-4,
++++++++++--        critic_learning_rate=3e-4,
++++++++++--        temp_learning_rate=3e-4,
++++++++++--        batch_size=256)
++++++++++--
++++++++++--    print(agent_orig)
++++++++++--
++++++++++--    training_steps = 1000
++++++++++--
++++++++++--    agent_orig.fit(original_dataset, n_steps=training_steps)
++++++++++--
++++++++++--    agents_compl = []
++++++++++--
++++++++++--    for dset in complementary_datasets:
++++++++++--        agent = d3rlpy.algos.SAC(
++++++++++--            actor_learning_rate=3e-4,
++++++++++--            critic_learning_rate=3e-4,
++++++++++--            temp_learning_rate=3e-4,
++++++++++--            batch_size=256)
++++++++++--        agent.fit(dset, n_steps=training_steps)
++++++++++--        agents_compl.append(agent)
++++++++++--
++++++++++--    action_orig = agent_orig.predict(dataset_d3.observations[0])
++++++++++--
++++++++++--    actions_compl = []
++++++++++--    for agent in agents_compl:
++++++++++--        actions_compl.append(agent.predict(dataset_d3.observations[0]))
++++++++++--    
++++++++++--    action_dists = []
++++++++++--    for action in actions_compl:
++++++++++--        action_dists.append(np.linalg.norm(action_orig-action))
++++++++++--
++++++++++--    k = 3
++++++++++--    topk = np.argpartition(action_dists, -k)[-k:]
++++++++++--
++++++++++--    d_w = {}
++++++++++--    for idx in topk:
++++++++++--        d_w[idx] = wasserstein_distance(d_j[idx], d_orig)
++++++++++--
++++++++++--    cluster_assignment = min(d_w, key=d_w.get)
++++++++++--    print("explanation assigned to cluster", cluster_assignment)
++++++++++--
++++++++++--    
++++++++++--def assignment_test():
++++++++++--    action_orig = np.random.rand(10)
++++++++++--    d_orig = np.random.rand(5)
++++++++++--
++++++++++--    actions_compl = np.random.rand(6,10)
++++++++++--    d_j = np.random.rand(6,5)
++++++++++--
++++++++++--    action_dists = []
++++++++++--    for action in actions_compl:
++++++++++--        action_dists.append(np.linalg.norm(action_orig-action))
++++++++++--
++++++++++--    print(action_dists)
++++++++++--
++++++++++--    k = 3
++++++++++--    topk = np.argpartition(action_dists, -k)[-k:]
++++++++++--
++++++++++--    print(topk)
++++++++++--
++++++++++--    d_w = {}
++++++++++--    for idx in topk:
++++++++++--        d_w[idx] = wasserstein_distance(d_j[idx], d_orig)
++++++++++--
++++++++++--    print(d_w)
++++++++++--
++++++++++--    cluster_assignment = min(d_w, key=d_w.get)
++++++++++--    print("explanation assigned to cluster", cluster_assignment)
++++++++++--
++++++++++--
++++++++++--if __name__ == "__main__":
++++++++++--    # main()
++++++++++--    assignment_test()
++++++++++-diff --git a/halfcheetah/trajectory.egg-info/PKG-INFO b/halfcheetah/trajectory.egg-info/PKG-INFO
++++++++++-index 452c6cb..2603850 100644
++++++++++---- a/halfcheetah/trajectory.egg-info/PKG-INFO
++++++++++-+++ b/halfcheetah/trajectory.egg-info/PKG-INFO
++++++++++-@@ -1,4 +1,11 @@
++++++++++- Metadata-Version: 2.1
++++++++++- Name: trajectory
++++++++++- Version: 0.0.0
++++++++++-+Summary: UNKNOWN
++++++++++-+Home-page: UNKNOWN
++++++++++-+License: UNKNOWN
++++++++++-+Platform: UNKNOWN
++++++++++- License-File: LICENSE
++++++++++ +
++++++++++-+UNKNOWN
++++++++++ +
+++++++++++ def softmax(x, temp):
+++++++++++-    """TODO"""
++++++++++++    """
++++++++++++    Softmax with temperature using max-trick.
++++++++++++    
++++++++++++    Args:
++++++++++++    - x: np.array, shape (n_data, dim_data)
++++++++++++    - temp: int, softmax temperature
++++++++++++    
++++++++++++    Returns:
++++++++++++    - softmax_x: np.array: shape (dim_data)
++++++++++++    """ 
+++++++++++     max_x = np.max(x)
+++++++++++-    return np.exp(np.divide(x-max_x,temp)) / np.sum(np.exp(np.divide(x-max_x,temp)))
++++++++++++    softmax_x = np.exp(np.divide(x-max_x,temp)) / np.sum(np.exp(np.divide(x-max_x,temp)))
++++++++++++    return softmax_x
++++++++++++
+++++++++++ 
+++++++++++ def generate_data_embedding(trajectory_embeddings, temperature=10000):
+++++++++++-    """TODO"""
++++++++++++    """
++++++++++++    Generate data embedding (sum+softmax) for set of encoded trajectories.
++++++++++++    
++++++++++++    Args:
++++++++++++    - trajectory_embeddings: np.array, shape (n_data, dim_data)
++++++++++++    - temperature: int, softmax temperature
++++++++++++    
++++++++++++    Returns:
++++++++++++    - embedding: np.array, shape (dim_data)
++++++++++++    """ 
+++++++++++ 
+++++++++++     embedding = np.sum(trajectory_embeddings, axis=0)
+++++++++++     embedding = softmax(embedding, temperature)
+++++++++++     
+++++++++++-
+++++++++++     return embedding
+++++++++++ 
++++++++++++
+++++++++++ def embed_trajectory(gpt, discretizer, observations, actions, rewards, preprocess_fn):
+++++++++++-    """TODO"""
++++++++++++    """
++++++++++++    Encode trajectory using a trajectory transformer with a sliding window.
++++++++++++    
++++++++++++    Args:
++++++++++++    - gpt: trajectory transformer
++++++++++++    - discretizer: environment discretizer
++++++++++++    - observations: trajectory observations
++++++++++++    - actions: trajectory actions
++++++++++++    - rewards: trajectory rewards
++++++++++++    - preprocess_fn: observations preprocessing functions
++++++++++++    
++++++++++++    Returns:
++++++++++++    - embedding: np.array, shape (hidden_dim), encoded trajectory
++++++++++++    """ 
+++++++++++ 
+++++++++++     context = []
+++++++++++-
+++++++++++     output = []
+++++++++++ 
+++++++++++     for i in range(len(observations)):
+++++++++++@@ -76,12 +118,12 @@ def embed_trajectory(gpt, discretizer, observations, actions, rewards, preproces
+++++++++++         action = actions[i]
+++++++++++         reward = rewards[i]
+++++++++++ 
++++++++++++        # Preprocess, discretize & forward through trajectory transformer
+++++++++++         observation = preprocess_fn(observation)
+++++++++++-
+++++++++++         prefix = make_prefix(discretizer, context, observation, True)
+++++++++++-
+++++++++++         out = forward(gpt, prefix)
+++++++++++ 
++++++++++++        # Sliding window
+++++++++++         if len(context) >= 9:
+++++++++++             context.pop(0)
+++++++++++             if len(output) == 0:
+++++++++++@@ -91,8 +133,10 @@ def embed_trajectory(gpt, discretizer, observations, actions, rewards, preproces
+++++++++++ 
+++++++++++         context = update_context(context, discretizer, observation, action, reward, len(observations))
+++++++++++ 
+++++++++++-    emb = np.mean(output, axis=0)
+++++++++++-    return emb
++++++++++++    # Embedding is the average of encoded states
++++++++++++    embedding = np.mean(output, axis=0)
++++++++++++    return embedding
++++++++++++
+++++++++++ 
+++++++++++ def create_complementary_dataset(dataset, idxs, trajectory_length=10, inverse=False):
+++++++++++     """TODO"""
+++++++++++@@ -142,7 +186,7 @@ def main():
+++++++++++ 
+++++++++++     ### IMPORTANT DEFINITIONS XRL SCRIPT ###
+++++++++++ 
+++++++++++-    load_embeddings = False
++++++++++++    load_embeddings = True
+++++++++++     load_clusters = True
+++++++++++     load_agents = True
+++++++++++     generate_human_study = False
++++++++++ diff --git a/halfcheetah/trajectory.egg-info/SOURCES.txt b/halfcheetah/trajectory.egg-info/SOURCES.txt
++++++++++-index 4474d85..84e8e3a 100644
+++++++++++index 84e8e3a..4474d85 100644
++++++++++ --- a/halfcheetah/trajectory.egg-info/SOURCES.txt
++++++++++ +++ b/halfcheetah/trajectory.egg-info/SOURCES.txt
++++++++++-@@ -30,4 +30,5 @@ trajectory/utils/serialization.py
+++++++++++@@ -30,5 +30,4 @@ trajectory/utils/serialization.py
++++++++++  trajectory/utils/setup.py
++++++++++  trajectory/utils/timer.py
++++++++++  trajectory/utils/training.py
++++++++++ -trajectory/utils/video.py
+++++++++++-trajectory_aaa/__init__.py
++++++++++ \ No newline at end of file
++++++++++ +trajectory/utils/video.py
++++++++++-+trajectory_aaa/__init__.py
++++++++++ \ No newline at end of file
++++++++++ diff --git a/halfcheetah/trajectory.egg-info/top_level.txt b/halfcheetah/trajectory.egg-info/top_level.txt
++++++++++-index ce65198..1d5271f 100644
+++++++++++index 1d5271f..ce65198 100644
++++++++++ --- a/halfcheetah/trajectory.egg-info/top_level.txt
++++++++++ +++ b/halfcheetah/trajectory.egg-info/top_level.txt
++++++++++-@@ -1 +1,2 @@
+++++++++++@@ -1,2 +1 @@
++++++++++  trajectory
++++++++++-+trajectory_aaa
+++++++++ \ No newline at end of file
+++++++++-diff --git a/halfcheetah/pca.py.npy b/halfcheetah/pca.py.npy
+++++++++-deleted file mode 100644
+++++++++-index bb19150..0000000
+++++++++-Binary files a/halfcheetah/pca.py.npy and /dev/null differ
+++++++++-diff --git a/halfcheetah/plotting/bar.png b/halfcheetah/plotting/bar.png
+++++++++-deleted file mode 100644
+++++++++-index 3679667..0000000
+++++++++-Binary files a/halfcheetah/plotting/bar.png and /dev/null differ
+++++++++-diff --git a/halfcheetah/plotting/plot.py b/halfcheetah/plotting/plot.py
+++++++++-deleted file mode 100644
+++++++++-index 163d0e4..0000000
+++++++++---- a/halfcheetah/plotting/plot.py
+++++++++-+++ /dev/null
+++++++++-@@ -1,74 +0,0 @@
+++++++++--import numpy as np
+++++++++--import matplotlib
+++++++++--import matplotlib.pyplot as plt
+++++++++--import pdb
+++++++++--
+++++++++--from plotting.scores import means
+++++++++--
+++++++++--class Colors:
+++++++++--	grey = '#B4B4B4'
+++++++++--	gold = '#F6C781'
+++++++++--	red = '#EC7C7D'
+++++++++--	blue = '#70ABCC'
+++++++++--
+++++++++--LABELS = {
+++++++++--	# 'BC': 'Behavior\nCloning',
+++++++++--	# 'MBOP': 'Model-Based\nOffline Planning',
+++++++++--	# 'BRAC': 'Behavior-Reg.\nActor-Critic',
+++++++++--	# 'CQL': 'Conservative\nQ-Learning',
+++++++++--}
+++++++++--
+++++++++--def get_mean(results, exclude=None):
+++++++++--	'''
+++++++++--		results : { environment: score, ... }
+++++++++--	'''
+++++++++--	filtered = {
+++++++++--		k: v for k, v in results.items()
+++++++++--		if (not exclude) or (exclude and exclude not in k)
+++++++++--	}
+++++++++--	return np.mean(list(filtered.values()))
+++++++++--
+++++++++--if __name__ == '__main__':
+++++++++--
+++++++++--	#################
+++++++++--	## latex
+++++++++--	#################
+++++++++--	matplotlib.rc('font', **{'family': 'serif', 'serif': ['Computer Modern']})
+++++++++--	matplotlib.rc('text', usetex=True)
+++++++++--	matplotlib.rcParams['text.latex.preamble']=[r"\usepackage{amsmath}"]
+++++++++--	#################
+++++++++--
+++++++++--	fig = plt.gcf()
+++++++++--	ax = plt.gca()
+++++++++--	fig.set_size_inches(7.5, 2.5)
+++++++++--
+++++++++--	means = {k: get_mean(v, exclude='ant') for k, v in means.items()}
+++++++++--	print(means)
+++++++++--
+++++++++--	algs = ['BC', 'MBOP', 'BRAC', 'CQL', 'Decision\nTransformer', 'Trajectory\nTransformer']
+++++++++--	vals = [means[alg] for alg in algs]
+++++++++--
+++++++++--	colors = [
+++++++++--		Colors.grey, Colors.gold,
+++++++++--		Colors.red, Colors.red, Colors.blue, Colors.blue
+++++++++--	]
+++++++++--
+++++++++--	labels = [LABELS.get(alg, alg) for alg in algs]
+++++++++--	plt.bar(labels, vals, color=colors, edgecolor=Colors.gold, lw=0)
+++++++++--	plt.ylabel('Average normalized return', labelpad=15)
+++++++++--	# plt.title('Offline RL Results')
+++++++++--
+++++++++--	legend_labels = ['Behavior Cloning', 'Trajectory Optimization', 'Temporal Difference', 'Sequence Modeling']
+++++++++--	colors = [Colors.grey, Colors.gold, Colors.red, Colors.blue]
+++++++++--	handles = [plt.Rectangle((0,0),1,1, color=color) for label, color in zip(legend_labels, colors)]
+++++++++--	plt.legend(handles, legend_labels, ncol=4,
+++++++++--		bbox_to_anchor=(1.07, -.18), fancybox=False, framealpha=0, shadow=False, columnspacing=1.5, handlelength=1.5)
+++++++++--
+++++++++--	matplotlib.rcParams['hatch.linewidth'] = 7.5
+++++++++--	# ax.patches[-1].set_hatch('/')
+++++++++--
+++++++++--	ax.spines['right'].set_visible(False)
+++++++++--	ax.spines['top'].set_visible(False)
+++++++++--
+++++++++--	# plt.savefig('plotting/bar.pdf', bbox_inches='tight')
+++++++++--	plt.savefig('plotting/bar.png', bbox_inches='tight', dpi=500)
+++++++++-diff --git a/halfcheetah/plotting/read_results.py b/halfcheetah/plotting/read_results.py
+++++++++-deleted file mode 100644
+++++++++-index 5a5fb62..0000000
+++++++++---- a/halfcheetah/plotting/read_results.py
+++++++++-+++ /dev/null
+++++++++-@@ -1,70 +0,0 @@
+++++++++--import os
+++++++++--import glob
+++++++++--import numpy as np
+++++++++--import json
+++++++++--import pdb
+++++++++--
+++++++++--import trajectory.utils as utils
+++++++++--
+++++++++--DATASETS = [
+++++++++--	f'{env}-{buffer}'
+++++++++--	for env in ['hopper', 'walker2d', 'halfcheetah', 'ant']
+++++++++--	for buffer in ['medium-expert-v2', 'medium-v2', 'medium-replay-v2']
+++++++++--]
+++++++++--
+++++++++--LOGBASE = 'logs'
+++++++++--TRIAL = '*'
+++++++++--EXP_NAME = 'plans/pretrained'
+++++++++--
+++++++++--def load_results(paths):
+++++++++--	'''
+++++++++--		paths : path to directory containing experiment trials
+++++++++--	'''
+++++++++--	scores = []
+++++++++--	for i, path in enumerate(sorted(paths)):
+++++++++--		score = load_result(path)
+++++++++--		if score is None:
+++++++++--			print(f'Skipping {path}')
+++++++++--			continue
+++++++++--		scores.append(score)
+++++++++--
+++++++++--		suffix = path.split('/')[-1]
+++++++++--
+++++++++--	mean = np.mean(scores)
+++++++++--	err = np.std(scores) / np.sqrt(len(scores))
+++++++++--	return mean, err, scores
+++++++++--
+++++++++--def load_result(path):
+++++++++--	'''
+++++++++--		path : path to experiment directory; expects `rollout.json` to be in directory
+++++++++--	'''
+++++++++--	fullpath = os.path.join(path, 'rollout.json')
+++++++++--	suffix = path.split('/')[-1]
+++++++++--
+++++++++--	if not os.path.exists(fullpath):
+++++++++--		return None
+++++++++--
+++++++++--	results = json.load(open(fullpath, 'rb'))
+++++++++--	score = results['score']
+++++++++--	return score * 100
+++++++++--
+++++++++--#######################
+++++++++--######## setup ########
+++++++++--#######################
+++++++++--
+++++++++--if __name__ == '__main__':
+++++++++--
+++++++++--	class Parser(utils.Parser):
+++++++++--	    dataset: str = None
+++++++++--
+++++++++--	args = Parser().parse_args()
+++++++++--
+++++++++--	for dataset in ([args.dataset] if args.dataset else DATASETS):
+++++++++--		subdirs = glob.glob(os.path.join(LOGBASE, dataset, EXP_NAME))
+++++++++--
+++++++++--		for subdir in subdirs:
+++++++++--			reldir = subdir.split('/')[-1]
+++++++++--			paths = glob.glob(os.path.join(subdir, TRIAL))
+++++++++--
+++++++++--			mean, err, scores = load_results(paths)
+++++++++--			print(f'{dataset.ljust(30)} | {subdir.ljust(50)} | {len(scores)} scores \n    {mean:.2f} +/- {err:.2f}\n')
+++++++++-diff --git a/halfcheetah/plotting/scores.py b/halfcheetah/plotting/scores.py
+++++++++-deleted file mode 100644
+++++++++-index f1917f7..0000000
+++++++++---- a/halfcheetah/plotting/scores.py
+++++++++-+++ /dev/null
+++++++++-@@ -1,123 +0,0 @@
+++++++++--means = {
+++++++++--	'Trajectory\nTransformer': {
+++++++++--		##
+++++++++--		'halfcheetah-medium-expert-v2': 95.0,
+++++++++--		'hopper-medium-expert-v2': 110.0,
+++++++++--		'walker2d-medium-expert-v2': 101.9,
+++++++++--		'ant-medium-expert-v2': 116.1,
+++++++++--		##
+++++++++--		'halfcheetah-medium-v2': 46.9,
+++++++++--		'hopper-medium-v2': 61.1,
+++++++++--		'walker2d-medium-v2': 79.0,
+++++++++--		'ant-medium-v2': 83.1,
+++++++++--		##
+++++++++--		'halfcheetah-medium-replay-v2': 41.9,
+++++++++--		'hopper-medium-replay-v2': 91.5,
+++++++++--		'walker2d-medium-replay-v2': 82.6,
+++++++++--		'ant-medium-replay-v2': 77.0,
+++++++++--	},
+++++++++--	'Decision\nTransformer': {
+++++++++--		##
+++++++++--		'halfcheetah-medium-expert-v2': 86.8,
+++++++++--		'hopper-medium-expert-v2': 107.6,
+++++++++--		'walker2d-medium-expert-v2': 108.1,
+++++++++--		##
+++++++++--		'halfcheetah-medium-v2': 42.6,
+++++++++--		'hopper-medium-v2': 67.6,
+++++++++--		'walker2d-medium-v2': 74.0,
+++++++++--		##
+++++++++--		'halfcheetah-medium-replay-v2': 36.6,
+++++++++--		'hopper-medium-replay-v2': 82.7,
+++++++++--		'walker2d-medium-replay-v2': 66.6,
+++++++++--	},
+++++++++--	'CQL': {
+++++++++--		##
+++++++++--		'halfcheetah-medium-expert-v2': 91.6,
+++++++++--		'hopper-medium-expert-v2': 105.4,
+++++++++--		'walker2d-medium-expert-v2': 108.8,
+++++++++--		##
+++++++++--		'halfcheetah-medium-v2': 44.0,
+++++++++--		'hopper-medium-v2': 58.5,
+++++++++--		'walker2d-medium-v2': 72.5,
+++++++++--		##
+++++++++--		'halfcheetah-medium-replay-v2': 45.5,
+++++++++--		'hopper-medium-replay-v2': 95.0,
+++++++++--		'walker2d-medium-replay-v2': 77.2,
+++++++++--	},
+++++++++--	'MOPO': {
+++++++++--		##
+++++++++--		'halfcheetah-medium-expert-v2': 63.3,
+++++++++--		'hopper-medium-expert-v2': 23.7,
+++++++++--		'walker2d-medium-expert-v2': 44.6,
+++++++++--		##
+++++++++--		'halfcheetah-medium-v2': 42.3,
+++++++++--		'hopper-medium-v2': 28.0,
+++++++++--		'walker2d-medium-v2': 17.8,
+++++++++--		##
+++++++++--		'halfcheetah-medium-replay-v2': 53.1,
+++++++++--		'hopper-medium-replay-v2': 67.5,
+++++++++--		'walker2d-medium-replay-v2':39.0,
+++++++++--	},
+++++++++--	'MBOP': {
+++++++++--		##
+++++++++--		'halfcheetah-medium-expert-v2': 105.9,
+++++++++--		'hopper-medium-expert-v2': 55.1,
+++++++++--		'walker2d-medium-expert-v2': 70.2,
+++++++++--		##
+++++++++--		'halfcheetah-medium-v2': 44.6,
+++++++++--		'hopper-medium-v2': 48.8,
+++++++++--		'walker2d-medium-v2': 41.0,
+++++++++--		##
+++++++++--		'halfcheetah-medium-replay-v2': 42.3,
+++++++++--		'hopper-medium-replay-v2': 12.4,
+++++++++--		'walker2d-medium-replay-v2': 9.7,
+++++++++--	},
+++++++++--	'BRAC': {
+++++++++--		##
+++++++++--		'halfcheetah-medium-expert-v2': 41.9,
+++++++++--		'hopper-medium-expert-v2': 0.9,
+++++++++--		'walker2d-medium-expert-v2': 81.6,
+++++++++--		##
+++++++++--		'halfcheetah-medium-v2': 46.3,
+++++++++--		'hopper-medium-v2': 31.3,
+++++++++--		'walker2d-medium-v2': 81.1,
+++++++++--		##
+++++++++--		'halfcheetah-medium-replay-v2': 47.7,
+++++++++--		'hopper-medium-replay-v2': 0.6,
+++++++++--		'walker2d-medium-replay-v2': 0.9,
+++++++++--	},
+++++++++--	'BC': {
+++++++++--		##
+++++++++--		'halfcheetah-medium-expert-v2': 59.9,
+++++++++--		'hopper-medium-expert-v2': 79.6,
+++++++++--		'walker2d-medium-expert-v2': 36.6,
+++++++++--		##
+++++++++--		'halfcheetah-medium-v2': 43.1,
+++++++++--		'hopper-medium-v2': 63.9,
+++++++++--		'walker2d-medium-v2': 77.3,
+++++++++--		##
+++++++++--		'halfcheetah-medium-replay-v2': 4.3,
+++++++++--		'hopper-medium-replay-v2': 27.6,
+++++++++--		'walker2d-medium-replay-v2': 36.9,
+++++++++--	},
+++++++++--}
+++++++++--
+++++++++--errors = {
+++++++++--	'Trajectory\nTransformer': {
+++++++++--		##
+++++++++--		'halfcheetah-medium-expert-v2': 0.2,
+++++++++--		'hopper-medium-expert-v2': 2.7,
+++++++++--		'walker2d-medium-expert-v2': 6.8,
+++++++++--		'ant-medium-expert-v2': 9.0,
+++++++++--		##
+++++++++--		'halfcheetah-medium-v2': 0.4,
+++++++++--		'hopper-medium-v2': 3.6,
+++++++++--		'walker2d-medium-v2': 2.8,
+++++++++--		'ant-medium-v2': 7.3,
+++++++++--		##
+++++++++--		'halfcheetah-medium-replay-v2': 2.5,
+++++++++--		'hopper-medium-replay-v2': 3.6,
+++++++++--		'walker2d-medium-replay-v2': 6.9,
+++++++++--		'ant-medium-replay-v2': 6.8,
+++++++++--	},
+++++++++--}
+++++++++++-trajectory_aaa
+++++++++++diff --git a/seaquest/readme.md b/seaquest/readme.md
+++++++++++index 84e53f8..53561f9 100644
+++++++++++--- a/seaquest/readme.md
++++++++++++++ b/seaquest/readme.md
+++++++++++@@ -10,4 +10,4 @@ pip install git+https://github.com/takuseno/d4rl-atari
+++++++++++ pip install "gym[atari, accept-rom-license]"
+++++++++++ pip install pyclustering
+++++++++++ pip install seaborn
+++++++++++-pip install d3rlpy==1.1.1
+++++++++++\ No newline at end of file
++++++++++++pip install d3rlpy==1.1.1
+++++++++ \ No newline at end of file
+++++++++-diff --git a/halfcheetah/plotting/table.py b/halfcheetah/plotting/table.py
+++++++++-deleted file mode 100644
+++++++++-index eae74e6..0000000
+++++++++---- a/halfcheetah/plotting/table.py
+++++++++-+++ /dev/null
+++++++++-@@ -1,127 +0,0 @@
+++++++++--import numpy as np
+++++++++--import pdb
+++++++++--
+++++++++--from plotting.plot import get_mean
+++++++++--from plotting.scores import (
+++++++++--	means as MEANS,
+++++++++--	errors as ERRORS,
+++++++++--)
+++++++++--
+++++++++--ALGORITHM_STRINGS = {
+++++++++--	'Trajectory\nTransformer': 'TT (Ours)',
+++++++++--	'Decision\nTransformer': 'DT',	
+++++++++--}
+++++++++--
+++++++++--BUFFER_STRINGS = {
+++++++++--	'medium-expert': 'Medium-Expert',
+++++++++--	'medium': 'Medium',
+++++++++--	'medium-replay': 'Medium-Replay',	
+++++++++--}
+++++++++--
+++++++++--ENVIRONMENT_STRINGS = {
+++++++++--	'halfcheetah': 'HalfCheetah',
+++++++++--	'hopper': 'Hopper',
+++++++++--	'walker2d': 'Walker2d',
+++++++++--	'ant': 'Ant',
+++++++++--}
+++++++++--
+++++++++--SHOW_ERRORS = ['Trajectory\nTransformer']
+++++++++--
+++++++++--def get_result(algorithm, buffer, environment, version='v2'):
+++++++++--	key = f'{environment}-{buffer}-{version}'
+++++++++--	mean = MEANS[algorithm].get(key, '-')
+++++++++--	if algorithm in SHOW_ERRORS:
+++++++++--		error = ERRORS[algorithm].get(key)
+++++++++--		return (mean, error)
+++++++++--	else:
+++++++++--		return mean
+++++++++--
+++++++++--def format_result(result):
+++++++++--	if type(result) == tuple:
+++++++++--		mean, std = result
+++++++++--		return f'${mean}$ \\scriptsize{{\\raisebox{{1pt}}{{$\\pm {std}$}}}}'
+++++++++--	else:
+++++++++--		return f'${result}$'
+++++++++--
+++++++++--def format_row(buffer, environment, results):
+++++++++--	buffer_str = BUFFER_STRINGS[buffer]
+++++++++--	environment_str = ENVIRONMENT_STRINGS[environment]
+++++++++--	results_str = ' & '.join(format_result(result) for result in results)
+++++++++--	row = f'{buffer_str} & {environment_str} & {results_str} \\\\ \n'
+++++++++--	return row
+++++++++--
+++++++++--def format_buffer_block(algorithms, buffer, environments):
+++++++++--	block_str = '\\midrule\n'
+++++++++--	for environment in environments:
+++++++++--		results = [get_result(alg, buffer, environment) for alg in algorithms]
+++++++++--		row_str = format_row(buffer, environment, results)
+++++++++--		block_str += row_str
+++++++++--	return block_str
+++++++++--
+++++++++--def format_algorithm(algorithm):
+++++++++--	algorithm_str = ALGORITHM_STRINGS.get(algorithm, algorithm)
+++++++++--	return f'\multicolumn{{1}}{{c}}{{\\bf {algorithm_str}}}'
+++++++++--
+++++++++--def format_algorithms(algorithms):
+++++++++--	return ' & '.join(format_algorithm(algorithm) for algorithm in algorithms)
+++++++++--
+++++++++--def format_averages(means, label):
+++++++++--	prefix = f'\\multicolumn{{2}}{{c}}{{\\bf Average ({label})}} & '
+++++++++--	formatted = ' & '.join(str(mean) for mean in means)
+++++++++--	return prefix + formatted
+++++++++--
+++++++++--def format_averages_block(algorithms):
+++++++++--	means_filtered = [np.round(get_mean(MEANS[algorithm], exclude='ant'), 1) for algorithm in algorithms]
+++++++++--	means_all = [np.round(get_mean(MEANS[algorithm], exclude=None), 1) for algorithm in algorithms]
+++++++++--
+++++++++--	means_all = [
+++++++++--		means
+++++++++--		if 'ant-medium-expert-v2' in MEANS[algorithm]
+++++++++--		else '$-$'
+++++++++--		for algorithm, means in zip(algorithms, means_all)
+++++++++--	]
+++++++++--
+++++++++--	formatted_filtered = format_averages(means_filtered, 'without Ant')
+++++++++--	formatted_all = format_averages(means_all, 'all settings')
+++++++++--
+++++++++--	formatted_block = (
+++++++++--		f'{formatted_filtered} \\hspace{{.6cm}} \\\\ \n'
+++++++++--		f'{formatted_all} \\hspace{{.6cm}} \\\\ \n'
+++++++++--	)
+++++++++--	return formatted_block
+++++++++--
+++++++++--def format_table(algorithms, buffers, environments):
+++++++++--	justify_str = 'll' + 'r' * len(algorithms)
+++++++++--	algorithm_str = format_algorithms(['Dataset', 'Environment'] + algorithms)
+++++++++--	averages_str = format_averages_block(algorithms)
+++++++++--	table_prefix = (
+++++++++--		'\\begin{table*}[h]\n'
+++++++++--		'\\centering\n'
+++++++++--		'\\small\n'
+++++++++--		f'\\begin{{tabular}}{{{justify_str}}}\n'
+++++++++--		'\\toprule\n'
+++++++++--		f'{algorithm_str} \\\\ \n'
+++++++++--	)
+++++++++--	table_suffix = (
+++++++++--		'\\midrule\n'
+++++++++--		f'{averages_str}'
+++++++++--		'\\bottomrule\n'
+++++++++--		'\\end{tabular}\n'
+++++++++--		'\\label{table:d4rl}\n'
+++++++++--		'\\end{table*}'
+++++++++--	)
+++++++++--	blocks = ''.join(format_buffer_block(algorithms, buffer, environments) for buffer in buffers)
+++++++++--	table = (
+++++++++--		f'{table_prefix}'
+++++++++--		f'{blocks}'
+++++++++--		f'{table_suffix}'
+++++++++--	)
+++++++++--	return table
+++++++++--
+++++++++--
+++++++++--algorithms =['BC', 'MBOP', 'BRAC', 'CQL',  'Decision\nTransformer', 'Trajectory\nTransformer']
+++++++++--buffers = ['medium-expert', 'medium', 'medium-replay']
+++++++++--environments = ['halfcheetah', 'hopper', 'walker2d', 'ant']
+++++++++--
+++++++++--table = format_table(algorithms, buffers, environments)
+++++++++--print(table)
+++++++++-diff --git a/halfcheetah/scripts/plan.py b/halfcheetah/scripts/plan.py
+++++++++-deleted file mode 100644
+++++++++-index f13d4cc..0000000
+++++++++---- a/halfcheetah/scripts/plan.py
+++++++++-+++ /dev/null
+++++++++-@@ -1,124 +0,0 @@
+++++++++--import json
+++++++++--import pdb
+++++++++--from os.path import join
+++++++++--
+++++++++--import trajectory.utils as utils
+++++++++--import trajectory.datasets as datasets
+++++++++--from trajectory.search import (
+++++++++--    beam_plan,
+++++++++--    make_prefix,
+++++++++--    extract_actions,
+++++++++--    update_context,
+++++++++--)
+++++++++--
+++++++++--class Parser(utils.Parser):
+++++++++--    dataset: str = 'halfcheetah-medium-expert-v2'
+++++++++--    config: str = 'config.offline'
+++++++++--
+++++++++--#######################
+++++++++--######## setup ########
+++++++++--#######################
+++++++++--
+++++++++--args = Parser().parse_args('plan')
+++++++++--
+++++++++--#######################
+++++++++--####### models ########
+++++++++--#######################
+++++++++--
+++++++++--dataset = utils.load_from_config(args.logbase, args.dataset, args.gpt_loadpath,
+++++++++--        'data_config.pkl')
+++++++++--
+++++++++--gpt, gpt_epoch = utils.load_model(args.logbase, args.dataset, args.gpt_loadpath,
+++++++++--        epoch=args.gpt_epoch, device=args.device)
+++++++++--
+++++++++--#######################
+++++++++--####### dataset #######
+++++++++--#######################
+++++++++--
+++++++++--env = datasets.load_environment(args.dataset)
+++++++++--print('yo')
+++++++++--renderer = utils.make_renderer(args)
+++++++++--timer = utils.timer.Timer()
+++++++++--
+++++++++--discretizer = dataset.discretizer
+++++++++--discount = dataset.discount
+++++++++--observation_dim = dataset.observation_dim
+++++++++--action_dim = dataset.action_dim
+++++++++--
+++++++++--value_fn = lambda x: discretizer.value_fn(x, args.percentile)
+++++++++--preprocess_fn = datasets.get_preprocess_fn(env.name)
+++++++++--
+++++++++--print('yo2')
+++++++++--
+++++++++--#######################
+++++++++--###### main loop ######
+++++++++--#######################
+++++++++--
+++++++++--observation = env.reset()
+++++++++--total_reward = 0
+++++++++--
+++++++++--## observations for rendering
+++++++++--rollout = [observation.copy()]
+++++++++--
+++++++++--## previous (tokenized) transitions for conditioning transformer
+++++++++--context = []
+++++++++--
+++++++++--T = env.max_episode_steps
+++++++++--for t in range(T):
+++++++++--
+++++++++--    observation = preprocess_fn(observation)
+++++++++--
+++++++++--    if t % args.plan_freq == 0:
+++++++++--        ## concatenate previous transitions and current observations to input to model
+++++++++--        prefix = make_prefix(discretizer, context, observation, args.prefix_context)
+++++++++--
+++++++++--        ## sample sequence from model beginning with `prefix`
+++++++++--        sequence = beam_plan(
+++++++++--            gpt, value_fn, prefix,
+++++++++--            args.horizon, args.beam_width, args.n_expand, observation_dim, action_dim,
+++++++++--            discount, args.max_context_transitions, verbose=args.verbose,
+++++++++--            k_obs=args.k_obs, k_act=args.k_act, cdf_obs=args.cdf_obs, cdf_act=args.cdf_act,
+++++++++--        )
+++++++++--
+++++++++--    else:
+++++++++--        sequence = sequence[1:]
+++++++++--
+++++++++--    ## [ horizon x transition_dim ] convert sampled tokens to continuous trajectory
+++++++++--    sequence_recon = discretizer.reconstruct(sequence)
+++++++++--
+++++++++--    ## [ action_dim ] index into sampled trajectory to grab first action
+++++++++--    action = extract_actions(sequence_recon, observation_dim, action_dim, t=0)
+++++++++--
+++++++++--    ## execute action in environment
+++++++++--    next_observation, reward, terminal, _ = env.step(action)
+++++++++--
+++++++++--    ## update return
+++++++++--    total_reward += reward
+++++++++--    score = env.get_normalized_score(total_reward)
+++++++++--
+++++++++--    ## update rollout observations and context transitions
+++++++++--    rollout.append(next_observation.copy())
+++++++++--    context = update_context(context, discretizer, observation, action, reward, args.max_context_transitions)
+++++++++--
+++++++++--    print(
+++++++++--        f'[ plan ] t: {t} / {T} | r: {reward:.2f} | R: {total_reward:.2f} | score: {score:.4f} | '
+++++++++--        f'time: {timer():.2f} | {args.dataset} | {args.exp_name} | {args.suffix}\n'
+++++++++--    )
+++++++++--
+++++++++--    ## visualization
+++++++++--    if t % args.vis_freq == 0 or terminal or t == T:
+++++++++--
+++++++++--        ## save current plan
+++++++++--        renderer.render_plan(join(args.savepath, f'{t}_plan.mp4'), sequence_recon, env.state_vector())
+++++++++--
+++++++++--        ## save rollout thus far
+++++++++--        renderer.render_rollout(join(args.savepath, f'rollout.mp4'), rollout, fps=80)
+++++++++--
+++++++++--    if terminal: break
+++++++++--
+++++++++--    observation = next_observation
+++++++++--
+++++++++--## save result as a json file
+++++++++--json_path = join(args.savepath, 'rollout.json')
+++++++++--json_data = {'score': score, 'step': t, 'return': total_reward, 'term': terminal, 'gpt_epoch': gpt_epoch}
+++++++++--json.dump(json_data, open(json_path, 'w'), indent=2, sort_keys=True)
+++++++++-diff --git a/halfcheetah/scripts/train.py b/halfcheetah/scripts/train.py
+++++++++-deleted file mode 100644
+++++++++-index 04af8d7..0000000
+++++++++---- a/halfcheetah/scripts/train.py
+++++++++-+++ /dev/null
+++++++++-@@ -1,122 +0,0 @@
+++++++++--import os
+++++++++--import numpy as np
+++++++++--import torch
+++++++++--import pdb
+++++++++--
+++++++++--import trajectory.utils as utils
+++++++++--import trajectory.datasets as datasets
+++++++++--from trajectory.models.transformers import GPT
+++++++++--
+++++++++--
+++++++++--class Parser(utils.Parser):
+++++++++--    dataset: str = 'halfcheetah-medium-expert-v2'
+++++++++--    config: str = 'config.offline'
+++++++++--
+++++++++--#######################
+++++++++--######## setup ########
+++++++++--#######################
+++++++++--
+++++++++--args = Parser().parse_args('train')
+++++++++--
+++++++++--#######################
+++++++++--####### dataset #######
+++++++++--#######################
+++++++++--
+++++++++--env = datasets.load_environment(args.dataset)
+++++++++--
+++++++++--sequence_length = args.subsampled_sequence_length * args.step
+++++++++--
+++++++++--dataset_config = utils.Config(
+++++++++--    datasets.DiscretizedDataset,
+++++++++--    savepath=(args.savepath, 'data_config.pkl'),
+++++++++--    env=args.dataset,
+++++++++--    N=args.N,
+++++++++--    penalty=args.termination_penalty,
+++++++++--    sequence_length=sequence_length,
+++++++++--    step=args.step,
+++++++++--    discount=args.discount,
+++++++++--    discretizer=args.discretizer,
+++++++++--)
+++++++++--
+++++++++--dataset = dataset_config()
+++++++++--obs_dim = dataset.observation_dim
+++++++++--act_dim = dataset.action_dim
+++++++++--transition_dim = dataset.joined_dim
+++++++++--
+++++++++--#######################
+++++++++--######## model ########
+++++++++--#######################
+++++++++--
+++++++++--block_size = args.subsampled_sequence_length * transition_dim - 1
+++++++++--print(
+++++++++--    f'Dataset size: {len(dataset)} | '
+++++++++--    f'Joined dim: {transition_dim} '
+++++++++--    f'(observation: {obs_dim}, action: {act_dim}) | Block size: {block_size}'
+++++++++--)
+++++++++--
+++++++++--model_config = utils.Config(
+++++++++--    GPT,
+++++++++--    savepath=(args.savepath, 'model_config.pkl'),
+++++++++--    ## discretization
+++++++++--    vocab_size=args.N, block_size=block_size,
+++++++++--    ## architecture
+++++++++--    n_layer=args.n_layer, n_head=args.n_head, n_embd=args.n_embd*args.n_head,
+++++++++--    ## dimensions
+++++++++--    observation_dim=obs_dim, action_dim=act_dim, transition_dim=transition_dim,
+++++++++--    ## loss weighting
+++++++++--    action_weight=args.action_weight, reward_weight=args.reward_weight, value_weight=args.value_weight,
+++++++++--    ## dropout probabilities
+++++++++--    embd_pdrop=args.embd_pdrop, resid_pdrop=args.resid_pdrop, attn_pdrop=args.attn_pdrop,
+++++++++--)
+++++++++--
+++++++++--model = model_config()
+++++++++--model.to(args.device)
+++++++++--
+++++++++--#######################
+++++++++--####### trainer #######
+++++++++--#######################
+++++++++--
+++++++++--warmup_tokens = len(dataset) * block_size ## number of tokens seen per epoch
+++++++++--final_tokens = 20 * warmup_tokens
+++++++++--
+++++++++--trainer_config = utils.Config(
+++++++++--    utils.Trainer,
+++++++++--    savepath=(args.savepath, 'trainer_config.pkl'),
+++++++++--    # optimization parameters
+++++++++--    batch_size=args.batch_size,
+++++++++--    learning_rate=args.learning_rate,
+++++++++--    betas=(0.9, 0.95),
+++++++++--    grad_norm_clip=1.0,
+++++++++--    weight_decay=0.1, # only applied on matmul weights
+++++++++--    # learning rate decay: linear warmup followed by cosine decay to 10% of original
+++++++++--    lr_decay=args.lr_decay,
+++++++++--    warmup_tokens=warmup_tokens,
+++++++++--    final_tokens=final_tokens,
+++++++++--    ## dataloader
+++++++++--    num_workers=0,
+++++++++--    device=args.device,
+++++++++--)
+++++++++--
+++++++++--trainer = trainer_config()
+++++++++--
+++++++++--#######################
+++++++++--###### main loop ######
+++++++++--#######################
+++++++++--
+++++++++--## scale number of epochs to keep number of updates constant
+++++++++--n_epochs = int(1e6 / len(dataset) * args.n_epochs_ref)
+++++++++--save_freq = int(n_epochs // args.n_saves)
+++++++++--
+++++++++--for epoch in range(n_epochs):
+++++++++--    print(f'\nEpoch: {epoch} / {n_epochs} | {args.dataset} | {args.exp_name}')
+++++++++--
+++++++++--    trainer.train(model, dataset)
+++++++++--
+++++++++--    ## get greatest multiple of `save_freq` less than or equal to `save_epoch`
+++++++++--    save_epoch = (epoch + 1) // save_freq * save_freq
+++++++++--    statepath = os.path.join(args.savepath, f'state_{save_epoch}.pt')
+++++++++--    print(f'Saving model to {statepath}')
+++++++++--
+++++++++--    ## save state to disk
+++++++++--    state = model.state_dict()
+++++++++--    torch.save(state, statepath)
+++++++++-diff --git a/halfcheetah/scripts/xrl.py b/halfcheetah/scripts/xrl.py
+++++++++-deleted file mode 100644
+++++++++-index 134232a..0000000
+++++++++---- a/halfcheetah/scripts/xrl.py
+++++++++-+++ /dev/null
+++++++++-@@ -1,372 +0,0 @@
+++++++++--import json
+++++++++--import pdb
+++++++++--from os.path import join
+++++++++--
+++++++++--import trajectory.utils as utils
+++++++++--import trajectory.datasets as datasets
+++++++++--from trajectory.search import (
+++++++++--    make_prefix,
+++++++++--    update_context,
+++++++++--)
+++++++++--from trajectory.search.sampling import forward
+++++++++--
+++++++++--import gym
+++++++++--import d4rl # Import required to register environments, you may need to also import the submodule
+++++++++--import numpy as np
+++++++++--import d3rlpy
+++++++++--import math as mt
+++++++++--from sklearn.cluster import KMeans
+++++++++--from sklearn import datasets as skdatasets
+++++++++--from sklearn.decomposition import PCA
+++++++++--
+++++++++--from pyclustering.cluster.xmeans import xmeans
+++++++++--from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer
+++++++++--
+++++++++--from scipy.stats import wasserstein_distance
+++++++++--
+++++++++--class Parser(utils.Parser):
+++++++++--    dataset: str = 'halfcheetah-medium-expert-v2'
+++++++++--    config: str = 'config.offline'
+++++++++--
++++++++++diff --git a/halfcheetah/scripts/xrl_v2.py b/halfcheetah/scripts/xrl_v2.py
++++++++++index 62a3d4d..d0e65fa 100644
++++++++++--- a/halfcheetah/scripts/xrl_v2.py
+++++++++++++ b/halfcheetah/scripts/xrl_v2.py
++++++++++@@ -21,54 +21,96 @@ from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer
++++++++++ from scipy.stats import wasserstein_distance
++++++++++ from moviepy.editor import VideoFileClip
++++++++++ 
+++++++++++
++++++++++ class Parser(utils.Parser):
++++++++++     dataset: str = 'halfcheetah-medium-v2'
++++++++++     config: str = 'config.offline'
++++++++++ 
+++++++++ -# utils
+++++++++--    
+++++++++--class XMeans:
+++++++++--    def loglikelihood(self, r, rn, var, m, k):
+++++++++--        l1 = - rn / 2.0 * mt.log(2 * mt.pi)
+++++++++--        l2 = - rn * m / 2.0 * mt.log(var)
+++++++++--        l3 = - (rn - k) / 2.0
+++++++++--        l4 = rn * mt.log(rn)
+++++++++--        l5 = - rn * mt.log(r)
+++++++++--
+++++++++--        return l1 + l2 + l3 + l4 + l5
+++++++++--
+++++++++--    def __init__(self, X, kmax = 20):
+++++++++--        self.X = X
+++++++++--        self.num = np.size(self.X, axis=0)
+++++++++--        self.dim = np.size(X, axis=1)
+++++++++--        self.KMax = kmax
+++++++++--
+++++++++--    def fit(self):
+++++++++--        k = 1
+++++++++--        X = self.X
+++++++++--        M = self.dim
+++++++++--        num = self.num
+++++++++--
+++++++++--        while(1):
+++++++++--            ok = k
+++++++++--
+++++++++--            #Improve Params
+++++++++--            kmeans = KMeans(n_clusters=k).fit(X)
+++++++++--            labels = kmeans.labels_
+++++++++--            m = kmeans.cluster_centers_
+++++++++--
+++++++++--            #Improve Structure
+++++++++--            #Calculate BIC
+++++++++--            p = M + 1
+++++++++--
+++++++++--            obic = np.zeros(k)
+++++++++--
+++++++++--            for i in range(k):
+++++++++--                rn = np.size(np.where(labels == i))
+++++++++--                var = np.sum((X[labels == i] - m[i])**2)/float(rn - 1)
+++++++++--                obic[i] = self.loglikelihood(rn, rn, var, M, 1) - p/2.0*mt.log(rn)
+++++++++--
+++++++++--            #Split each cluster into two subclusters and calculate BIC of each splitted cluster
+++++++++--            sk = 2 #The number of subclusters
+++++++++--            nbic = np.zeros(k)
+++++++++--            addk = 0
+++++++++--
+++++++++--            for i in range(k):
+++++++++--                ci = X[labels == i]
+++++++++--                r = np.size(np.where(labels == i))
+++++++++--
+++++++++--                kmeans = KMeans(n_clusters=sk).fit(ci)
+++++++++--                ci_labels = kmeans.labels_
+++++++++--                sm = kmeans.cluster_centers_
+++++++++--
+++++++++--                for l in range(sk):
+++++++++--                    rn = np.size(np.where(ci_labels == l))
+++++++++--                    var = np.sum((ci[ci_labels == l] - sm[l])**2)/float(rn - sk)
+++++++++--                    nbic[i] += self.loglikelihood(r, rn, var, M, sk)
+++++++++--
+++++++++--                p = sk * (M + 1)
+++++++++--                nbic[i] -= p/2.0*mt.log(r)
+++++++++--
+++++++++--                if obic[i] < nbic[i]:
+++++++++--                    addk += 1
+++++++++--
+++++++++--            k += addk
+++++++++--
+++++++++--            if ok == k or k >= self.KMax:
+++++++++--                break
+++++++++--
+++++++++--
+++++++++--        #Calculate labels and centroids
+++++++++--        kmeans = KMeans(n_clusters=k).fit(X)
+++++++++--        self.labels = kmeans.labels_
+++++++++--        self.k = k
+++++++++--        self.m = kmeans.cluster_centers_
+++++++++--
+++++++++--
+++++++++--def cluster_trajectories(trajectories):
+++++++++--    xmeans_instance = XMeans(trajectories, kmax=10)
+++++++++--    xmeans_instance.fit()
+++++++++--
+++++++++--    clusters = xmeans_instance.labels
+++++++++--    return clusters
+++++++++--
+++++++++--def cluster_trajectories_2(trajectories):
++++++++++ 
++++++++++ def cluster_trajectories(trajectories, n_clusters=10):
++++++++++-    """TODO"""
+++++++++++    """
+++++++++++    Cluster trajectories using X-means.
+++++++++++    
+++++++++++    Args:
+++++++++++    - trajectories: np.array, shape (n_trajectories, encoding_dim)
+++++++++++    - n_clusters: int, max number of clusters
+++++++++++    
+++++++++++    Returns:
+++++++++++    - idxs_per_cluster: list, trajectory idxs per cluster idxs
+++++++++++    - clusters: np.array, shape (n_trajectories), cluster idxs per trajectory idx
+++++++++++    """ 
++++++++++ 
+++++++++ -    # Prepare initial centers - amount of initial centers defines amount of clusters from which X-Means will
+++++++++ -    # start analysis.
+++++++++--    amount_initial_centers = 2
+++++++++--    initial_centers = kmeans_plusplus_initializer(trajectories, amount_initial_centers).initialize()
+++++++++--    
+++++++++++    # Set 2 initial cluster centers
++++++++++     amount_initial_centers = 2
++++++++++     initial_centers = kmeans_plusplus_initializer(trajectories, amount_initial_centers).initialize()
++++++++++     
+++++++++ -    # Create instance of X-Means algorithm. The algorithm will start analysis from 2 clusters, the maximum
+++++++++ -    # number of clusters that can be allocated is 10.
+++++++++--    xmeans_instance = xmeans(trajectories, initial_centers, 10)
+++++++++--    xmeans_instance.process()
+++++++++--    
+++++++++--    # Extract clustering results: clusters
+++++++++--    idxs_per_cluster = xmeans_instance.get_clusters()
+++++++++--
+++++++++--    clusters = []
+++++++++--    for i in range(len(trajectories)):
+++++++++--        for j in range(len(idxs_per_cluster)):
+++++++++--            if i in idxs_per_cluster[j]: clusters.append(j)
+++++++++--
+++++++++--    return idxs_per_cluster, np.array(clusters)
+++++++++++    # Run X-means
++++++++++     xmeans_instance = xmeans(trajectories, initial_centers, n_clusters)
++++++++++     xmeans_instance.process()
++++++++++     
++++++++++     # Extract clustering results: clusters
++++++++++     idxs_per_cluster = xmeans_instance.get_clusters()
++++++++++ 
+++++++++++    # Turn list of trajectory idxs per cluster to array of cluster idx per trajectory idx
++++++++++     clusters = []
++++++++++     for i in range(len(trajectories)):
++++++++++         for j in range(len(idxs_per_cluster)):
++++++++++             if i in idxs_per_cluster[j]: clusters.append(j)
++++++++++ 
++++++++++     return idxs_per_cluster, np.array(clusters)
+++++++++ - 
+++++++++--# https://github.com/sascha-kirch/ML_Notebooks/blob/main/Softmax_Temperature.ipynb
+++++++++--def softmax(x, temp):
+++++++++--    """Compute softmax values for each sets of scores in x."""
+++++++++--    return np.exp(np.divide(x,temp)) / np.sum(np.exp(np.divide(x,temp)))
+++++++++--
+++++++++--def generate_data_embedding(trajectory_embeddings, normalizing_factor=1, temperature=1):
+++++++++--    embedding = np.sum(trajectory_embeddings, axis=0) / normalizing_factor
+++++++++--    embedding = softmax(embedding, temperature)
+++++++++--    return embedding
+++++++++--
+++++++++--def embed_trajectory(gpt, discretizer, observations, actions, rewards, preprocess_fn):
+++++++++--    context = []
+++++++++--
+++++++++--    for i in range(len(observations)):
+++++++++--        observation = observations[i]
+++++++++--        action = actions[i]
+++++++++--        reward = rewards[i]
+++++++++--
+++++++++--        observation = preprocess_fn(observation)
+++++++++--
+++++++++--        # print(observation)
+++++++++--        prefix = make_prefix(discretizer, context, observation, True)
+++++++++--        # print("prefix", prefix.shape)
+++++++++--
+++++++++--        out = forward(gpt, prefix)
+++++++++--        # print("out", out.shape)
+++++++++--        context = update_context(context, discretizer, observation, action, reward, len(observations))
+++++++++--        # print("cotext", context)
+++++++++--    
+++++++++--    emb = []
+++++++++--    for context_step in context:
+++++++++--        emb.append(context_step.numpy())
+++++++++--    emb = np.array(emb)
+++++++++--    emb = np.mean(emb, axis=0)[0]
+++++++++--
+++++++++--    return emb
+++++++++--
+++++++++--
+++++++++--def create_complementary_dataset(dataset, idxs, trajectory_length=10):
+++++++++--    observations = []
+++++++++--    actions = []
+++++++++--    rewards = []
+++++++++--    terminals = []
+++++++++--    for i in range(1000):
+++++++++--        if i not in idxs:
+++++++++--            observations += list(dataset.observations[1000*i:1000*i+trajectory_length])
+++++++++--            actions += list(dataset.actions[1000*i:1000*i+trajectory_length])
+++++++++--            rewards += list(dataset.rewards[1000*i:1000*i+trajectory_length])
+++++++++--            terminals += list(dataset.terminals[1000*i:1000*i+trajectory_length])
+++++++++--
+++++++++--    new_dataset = d3rlpy.dataset.MDPDataset(
+++++++++--        observations=np.array(observations),
+++++++++--        actions=np.array(actions),
+++++++++--        rewards=np.array(rewards),
+++++++++--        terminals=np.array(terminals)
+++++++++--    )
+++++++++--    return new_dataset
+++++++++--    
+++++++++--
+++++++++--
+++++++++--
+++++++++--def main():
+++++++++--    # args = Parser().parse_args('plan')
+++++++++--
+++++++++--    #######################
+++++++++--    ####### models ########
+++++++++--    #######################
+++++++++--
+++++++++--
+++++++++--
+++++++++--
+++++++++--
+++++++++--    # print(args.dataset)
+++++++++--
+++++++++--    # dataset = utils.load_from_config(args.logbase, args.dataset, args.gpt_loadpath,
+++++++++--    #         'data_config.pkl')
+++++++++--
+++++++++--
+++++++++--    # gpt, gpt_epoch = utils.load_model(args.logbase, args.dataset, args.gpt_loadpath,
+++++++++--    #         epoch=args.gpt_epoch, device=args.device)
+++++++++--
+++++++++--    # env = datasets.load_environment(args.dataset)
+++++++++--
+++++++++--    # discretizer = dataset.discretizer
+++++++++--
+++++++++--    # preprocess_fn = datasets.get_preprocess_fn(env.name)
+++++++++--
+++++++++--    # #######################
+++++++++--    # ####### dataset #######
+++++++++--    # #######################
+++++++++--
+++++++++--    # # env = datasets.load_environment(args.dataset)
+++++++++--    # discretizer = dataset.discretizer
+++++++++--    # preprocess_fn = datasets.get_preprocess_fn(env.name)
+++++++++--
+++++++++--    # # dataset
+++++++++--    dataset_d3, env = d3rlpy.datasets.get_dataset("halfcheetah-medium-v2")
+++++++++--
+++++++++--    # env = gym.make('halfcheetah-medium-v2')
+++++++++--    # dataset_d4 = d4rl.qlearning_dataset(env)
+++++++++--
+++++++++--    # # checks to see if d3rl & d4rl datasets are equal
+++++++++--    # print(np.allclose(dataset_d3.actions[100], dataset_d4['actions'][100]))
+++++++++--
+++++++++--    # # dr4rl has same trajectories, just cut off 1 element before the end
+++++++++--    # for j in range(1000):
+++++++++--    #     for i in range(999):
+++++++++--    #         if dataset_d4['rewards'][j * 999 + i] != dataset_d3.rewards[j * 1000 + i]: print("yo", i)
+++++++++--
+++++++++--    # #######################
+++++++++--    # ###### main loop ######
+++++++++--    # #######################
+++++++++--
+++++++++--    trajectory_length = 10 # 10 = max
+++++++++--
+++++++++--    # embeddings = []
+++++++++--    # for i in range(1000):
+++++++++--    #     observations = dataset_d3.observations[1000*i:1000*i+trajectory_length]
+++++++++--    #     actions = dataset_d3.actions[1000*i:1000*i+trajectory_length]
+++++++++--    #     rewards = dataset_d3.rewards[1000*i:1000*i+trajectory_length]
+++++++++--    #     terminals = dataset_d3.terminals[1000*i:1000*i+trajectory_length]
+++++++++--    #     emb = embed_trajectory(gpt, discretizer, observations, actions, rewards, preprocess_fn)
+++++++++--    #     embeddings.append(emb)
+++++++++--    # embeddings = np.array(embeddings)
+++++++++--    # np.save("embeddings.npy", embeddings)
+++++++++--    # print(embeddings)
+++++++++--
+++++++++--    embeddings = np.load("embeddings.npy")
+++++++++--
+++++++++--    pca = PCA(n_components=2)
+++++++++--    pca = PCA(n_components=2)
+++++++++--    pca_embeddings = pca.fit_transform(embeddings)
+++++++++--    np.save("pca.py", pca_embeddings)
+++++++++--
+++++++++--    idxs_per_cluster, clusters = cluster_trajectories_2(embeddings)
+++++++++--    # print(clusters)
+++++++++--    # return
+++++++++--    np.save("clusters.npy", clusters)
+++++++++--
+++++++++--    import matplotlib.pyplot as plt
+++++++++--
+++++++++--    d_orig = generate_data_embedding(embeddings)
+++++++++--    unique_clusters = np.unique(clusters)
+++++++++--    
+++++++++--    d_j = []
+++++++++--    complementary_datasets = []
+++++++++--    for j in np.sort(unique_clusters):
+++++++++--        print(j)
+++++++++--        d_j.append(generate_data_embedding(embeddings[clusters != j]))
+++++++++--        plt.scatter(pca_embeddings[clusters == j][:,0], pca_embeddings[clusters == j][:,1], label=j)
+++++++++--        complementary_datasets.append(create_complementary_dataset(dataset_d3, idxs_per_cluster[j], trajectory_length))
+++++++++--    
+++++++++--    original_dataset = create_complementary_dataset(dataset_d3, [], trajectory_length)
+++++++++--
+++++++++--    print(complementary_datasets, original_dataset)
+++++++++--
+++++++++--    plt.legend()
+++++++++--    plt.show()
+++++++++--
+++++++++--    agent_orig = d3rlpy.algos.SAC(
+++++++++--        actor_learning_rate=3e-4,
+++++++++--        critic_learning_rate=3e-4,
+++++++++--        temp_learning_rate=3e-4,
+++++++++--        batch_size=256)
+++++++++--
+++++++++--    print(agent_orig)
+++++++++--
+++++++++--    training_steps = 1000
+++++++++--
+++++++++--    agent_orig.fit(original_dataset, n_steps=training_steps)
+++++++++--
+++++++++--    agents_compl = []
+++++++++--
+++++++++--    for dset in complementary_datasets:
+++++++++--        agent = d3rlpy.algos.SAC(
+++++++++--            actor_learning_rate=3e-4,
+++++++++--            critic_learning_rate=3e-4,
+++++++++--            temp_learning_rate=3e-4,
+++++++++--            batch_size=256)
+++++++++--        agent.fit(dset, n_steps=training_steps)
+++++++++--        agents_compl.append(agent)
+++++++++--
+++++++++--    action_orig = agent_orig.predict(dataset_d3.observations[0])
+++++++++--
+++++++++--    actions_compl = []
+++++++++--    for agent in agents_compl:
+++++++++--        actions_compl.append(agent.predict(dataset_d3.observations[0]))
+++++++++--    
+++++++++--    action_dists = []
+++++++++--    for action in actions_compl:
+++++++++--        action_dists.append(np.linalg.norm(action_orig-action))
+++++++++--
+++++++++--    k = 3
+++++++++--    topk = np.argpartition(action_dists, -k)[-k:]
+++++++++--
+++++++++--    d_w = {}
+++++++++--    for idx in topk:
+++++++++--        d_w[idx] = wasserstein_distance(d_j[idx], d_orig)
+++++++++--
+++++++++--    cluster_assignment = min(d_w, key=d_w.get)
+++++++++--    print("explanation assigned to cluster", cluster_assignment)
+++++++++--
+++++++++--    
+++++++++--def assignment_test():
+++++++++--    action_orig = np.random.rand(10)
+++++++++--    d_orig = np.random.rand(5)
+++++++++--
+++++++++--    actions_compl = np.random.rand(6,10)
+++++++++--    d_j = np.random.rand(6,5)
+++++++++--
+++++++++--    action_dists = []
+++++++++--    for action in actions_compl:
+++++++++--        action_dists.append(np.linalg.norm(action_orig-action))
+++++++++--
+++++++++--    print(action_dists)
+++++++++--
+++++++++--    k = 3
+++++++++--    topk = np.argpartition(action_dists, -k)[-k:]
+++++++++--
+++++++++--    print(topk)
+++++++++--
+++++++++--    d_w = {}
+++++++++--    for idx in topk:
+++++++++--        d_w[idx] = wasserstein_distance(d_j[idx], d_orig)
+++++++++--
+++++++++--    print(d_w)
+++++++++--
+++++++++--    cluster_assignment = min(d_w, key=d_w.get)
+++++++++--    print("explanation assigned to cluster", cluster_assignment)
+++++++++--
+++++++++--
+++++++++--if __name__ == "__main__":
+++++++++--    # main()
+++++++++--    assignment_test()
+++++++++-diff --git a/halfcheetah/trajectory.egg-info/PKG-INFO b/halfcheetah/trajectory.egg-info/PKG-INFO
+++++++++-index 452c6cb..2603850 100644
+++++++++---- a/halfcheetah/trajectory.egg-info/PKG-INFO
+++++++++-+++ b/halfcheetah/trajectory.egg-info/PKG-INFO
+++++++++-@@ -1,4 +1,11 @@
+++++++++- Metadata-Version: 2.1
+++++++++- Name: trajectory
+++++++++- Version: 0.0.0
+++++++++-+Summary: UNKNOWN
+++++++++-+Home-page: UNKNOWN
+++++++++-+License: UNKNOWN
+++++++++-+Platform: UNKNOWN
+++++++++- License-File: LICENSE
+++++++++ +
+++++++++-+UNKNOWN
+++++++++ +
++++++++++ def softmax(x, temp):
++++++++++-    """TODO"""
+++++++++++    """
+++++++++++    Softmax with temperature using max-trick.
+++++++++++    
+++++++++++    Args:
+++++++++++    - x: np.array, shape (n_data, dim_data)
+++++++++++    - temp: int, softmax temperature
+++++++++++    
+++++++++++    Returns:
+++++++++++    - softmax_x: np.array: shape (dim_data)
+++++++++++    """ 
++++++++++     max_x = np.max(x)
++++++++++-    return np.exp(np.divide(x-max_x,temp)) / np.sum(np.exp(np.divide(x-max_x,temp)))
+++++++++++    softmax_x = np.exp(np.divide(x-max_x,temp)) / np.sum(np.exp(np.divide(x-max_x,temp)))
+++++++++++    return softmax_x
+++++++++++
++++++++++ 
++++++++++ def generate_data_embedding(trajectory_embeddings, temperature=10000):
++++++++++-    """TODO"""
+++++++++++    """
+++++++++++    Generate data embedding (sum+softmax) for set of encoded trajectories.
+++++++++++    
+++++++++++    Args:
+++++++++++    - trajectory_embeddings: np.array, shape (n_data, dim_data)
+++++++++++    - temperature: int, softmax temperature
+++++++++++    
+++++++++++    Returns:
+++++++++++    - embedding: np.array, shape (dim_data)
+++++++++++    """ 
++++++++++ 
++++++++++     embedding = np.sum(trajectory_embeddings, axis=0)
++++++++++     embedding = softmax(embedding, temperature)
++++++++++     
++++++++++-
++++++++++     return embedding
++++++++++ 
+++++++++++
++++++++++ def embed_trajectory(gpt, discretizer, observations, actions, rewards, preprocess_fn):
++++++++++-    """TODO"""
+++++++++++    """
+++++++++++    Encode trajectory using a trajectory transformer with a sliding window.
+++++++++++    
+++++++++++    Args:
+++++++++++    - gpt: trajectory transformer
+++++++++++    - discretizer: environment discretizer
+++++++++++    - observations: trajectory observations
+++++++++++    - actions: trajectory actions
+++++++++++    - rewards: trajectory rewards
+++++++++++    - preprocess_fn: observations preprocessing functions
+++++++++++    
+++++++++++    Returns:
+++++++++++    - embedding: np.array, shape (hidden_dim), encoded trajectory
+++++++++++    """ 
++++++++++ 
++++++++++     context = []
++++++++++-
++++++++++     output = []
++++++++++ 
++++++++++     for i in range(len(observations)):
++++++++++@@ -76,12 +118,12 @@ def embed_trajectory(gpt, discretizer, observations, actions, rewards, preproces
++++++++++         action = actions[i]
++++++++++         reward = rewards[i]
++++++++++ 
+++++++++++        # Preprocess, discretize & forward through trajectory transformer
++++++++++         observation = preprocess_fn(observation)
++++++++++-
++++++++++         prefix = make_prefix(discretizer, context, observation, True)
++++++++++-
++++++++++         out = forward(gpt, prefix)
++++++++++ 
+++++++++++        # Sliding window
++++++++++         if len(context) >= 9:
++++++++++             context.pop(0)
++++++++++             if len(output) == 0:
++++++++++@@ -91,8 +133,10 @@ def embed_trajectory(gpt, discretizer, observations, actions, rewards, preproces
++++++++++ 
++++++++++         context = update_context(context, discretizer, observation, action, reward, len(observations))
++++++++++ 
++++++++++-    emb = np.mean(output, axis=0)
++++++++++-    return emb
+++++++++++    # Embedding is the average of encoded states
+++++++++++    embedding = np.mean(output, axis=0)
+++++++++++    return embedding
+++++++++++
++++++++++ 
++++++++++ def create_complementary_dataset(dataset, idxs, trajectory_length=10, inverse=False):
++++++++++     """TODO"""
++++++++++@@ -142,7 +186,7 @@ def main():
++++++++++ 
++++++++++     ### IMPORTANT DEFINITIONS XRL SCRIPT ###
++++++++++ 
++++++++++-    load_embeddings = False
+++++++++++    load_embeddings = True
++++++++++     load_clusters = True
++++++++++     load_agents = True
++++++++++     generate_human_study = False
+++++++++ diff --git a/halfcheetah/trajectory.egg-info/SOURCES.txt b/halfcheetah/trajectory.egg-info/SOURCES.txt
+++++++++-index 4474d85..84e8e3a 100644
++++++++++index 84e8e3a..4474d85 100644
+++++++++ --- a/halfcheetah/trajectory.egg-info/SOURCES.txt
+++++++++ +++ b/halfcheetah/trajectory.egg-info/SOURCES.txt
+++++++++-@@ -30,4 +30,5 @@ trajectory/utils/serialization.py
++++++++++@@ -30,5 +30,4 @@ trajectory/utils/serialization.py
+++++++++  trajectory/utils/setup.py
+++++++++  trajectory/utils/timer.py
+++++++++  trajectory/utils/training.py
+++++++++ -trajectory/utils/video.py
++++++++++-trajectory_aaa/__init__.py
+++++++++ \ No newline at end of file
+++++++++ +trajectory/utils/video.py
+++++++++-+trajectory_aaa/__init__.py
+++++++++ \ No newline at end of file
+++++++++ diff --git a/halfcheetah/trajectory.egg-info/top_level.txt b/halfcheetah/trajectory.egg-info/top_level.txt
+++++++++-index ce65198..1d5271f 100644
++++++++++index 1d5271f..ce65198 100644
+++++++++ --- a/halfcheetah/trajectory.egg-info/top_level.txt
+++++++++ +++ b/halfcheetah/trajectory.egg-info/top_level.txt
+++++++++-@@ -1 +1,2 @@
++++++++++@@ -1,2 +1 @@
+++++++++  trajectory
+++++++++-+trajectory_aaa
++++++++ \ No newline at end of file
++++++++-diff --git a/halfcheetah/pca.py.npy b/halfcheetah/pca.py.npy
++++++++-deleted file mode 100644
++++++++-index bb19150..0000000
++++++++-Binary files a/halfcheetah/pca.py.npy and /dev/null differ
++++++++-diff --git a/halfcheetah/plotting/bar.png b/halfcheetah/plotting/bar.png
++++++++-deleted file mode 100644
++++++++-index 3679667..0000000
++++++++-Binary files a/halfcheetah/plotting/bar.png and /dev/null differ
++++++++-diff --git a/halfcheetah/plotting/plot.py b/halfcheetah/plotting/plot.py
++++++++-deleted file mode 100644
++++++++-index 163d0e4..0000000
++++++++---- a/halfcheetah/plotting/plot.py
++++++++-+++ /dev/null
++++++++-@@ -1,74 +0,0 @@
++++++++--import numpy as np
++++++++--import matplotlib
++++++++--import matplotlib.pyplot as plt
++++++++--import pdb
++++++++--
++++++++--from plotting.scores import means
++++++++--
++++++++--class Colors:
++++++++--	grey = '#B4B4B4'
++++++++--	gold = '#F6C781'
++++++++--	red = '#EC7C7D'
++++++++--	blue = '#70ABCC'
++++++++--
++++++++--LABELS = {
++++++++--	# 'BC': 'Behavior\nCloning',
++++++++--	# 'MBOP': 'Model-Based\nOffline Planning',
++++++++--	# 'BRAC': 'Behavior-Reg.\nActor-Critic',
++++++++--	# 'CQL': 'Conservative\nQ-Learning',
++++++++--}
++++++++--
++++++++--def get_mean(results, exclude=None):
++++++++--	'''
++++++++--		results : { environment: score, ... }
++++++++--	'''
++++++++--	filtered = {
++++++++--		k: v for k, v in results.items()
++++++++--		if (not exclude) or (exclude and exclude not in k)
++++++++--	}
++++++++--	return np.mean(list(filtered.values()))
++++++++--
++++++++--if __name__ == '__main__':
++++++++--
++++++++--	#################
++++++++--	## latex
++++++++--	#################
++++++++--	matplotlib.rc('font', **{'family': 'serif', 'serif': ['Computer Modern']})
++++++++--	matplotlib.rc('text', usetex=True)
++++++++--	matplotlib.rcParams['text.latex.preamble']=[r"\usepackage{amsmath}"]
++++++++--	#################
++++++++--
++++++++--	fig = plt.gcf()
++++++++--	ax = plt.gca()
++++++++--	fig.set_size_inches(7.5, 2.5)
++++++++--
++++++++--	means = {k: get_mean(v, exclude='ant') for k, v in means.items()}
++++++++--	print(means)
++++++++--
++++++++--	algs = ['BC', 'MBOP', 'BRAC', 'CQL', 'Decision\nTransformer', 'Trajectory\nTransformer']
++++++++--	vals = [means[alg] for alg in algs]
++++++++--
++++++++--	colors = [
++++++++--		Colors.grey, Colors.gold,
++++++++--		Colors.red, Colors.red, Colors.blue, Colors.blue
++++++++--	]
++++++++--
++++++++--	labels = [LABELS.get(alg, alg) for alg in algs]
++++++++--	plt.bar(labels, vals, color=colors, edgecolor=Colors.gold, lw=0)
++++++++--	plt.ylabel('Average normalized return', labelpad=15)
++++++++--	# plt.title('Offline RL Results')
++++++++--
++++++++--	legend_labels = ['Behavior Cloning', 'Trajectory Optimization', 'Temporal Difference', 'Sequence Modeling']
++++++++--	colors = [Colors.grey, Colors.gold, Colors.red, Colors.blue]
++++++++--	handles = [plt.Rectangle((0,0),1,1, color=color) for label, color in zip(legend_labels, colors)]
++++++++--	plt.legend(handles, legend_labels, ncol=4,
++++++++--		bbox_to_anchor=(1.07, -.18), fancybox=False, framealpha=0, shadow=False, columnspacing=1.5, handlelength=1.5)
++++++++--
++++++++--	matplotlib.rcParams['hatch.linewidth'] = 7.5
++++++++--	# ax.patches[-1].set_hatch('/')
++++++++--
++++++++--	ax.spines['right'].set_visible(False)
++++++++--	ax.spines['top'].set_visible(False)
++++++++--
++++++++--	# plt.savefig('plotting/bar.pdf', bbox_inches='tight')
++++++++--	plt.savefig('plotting/bar.png', bbox_inches='tight', dpi=500)
++++++++-diff --git a/halfcheetah/plotting/read_results.py b/halfcheetah/plotting/read_results.py
++++++++-deleted file mode 100644
++++++++-index 5a5fb62..0000000
++++++++---- a/halfcheetah/plotting/read_results.py
++++++++-+++ /dev/null
++++++++-@@ -1,70 +0,0 @@
++++++++--import os
++++++++--import glob
++++++++--import numpy as np
++++++++--import json
++++++++--import pdb
++++++++--
++++++++--import trajectory.utils as utils
++++++++--
++++++++--DATASETS = [
++++++++--	f'{env}-{buffer}'
++++++++--	for env in ['hopper', 'walker2d', 'halfcheetah', 'ant']
++++++++--	for buffer in ['medium-expert-v2', 'medium-v2', 'medium-replay-v2']
++++++++--]
++++++++--
++++++++--LOGBASE = 'logs'
++++++++--TRIAL = '*'
++++++++--EXP_NAME = 'plans/pretrained'
++++++++--
++++++++--def load_results(paths):
++++++++--	'''
++++++++--		paths : path to directory containing experiment trials
++++++++--	'''
++++++++--	scores = []
++++++++--	for i, path in enumerate(sorted(paths)):
++++++++--		score = load_result(path)
++++++++--		if score is None:
++++++++--			print(f'Skipping {path}')
++++++++--			continue
++++++++--		scores.append(score)
++++++++--
++++++++--		suffix = path.split('/')[-1]
++++++++--
++++++++--	mean = np.mean(scores)
++++++++--	err = np.std(scores) / np.sqrt(len(scores))
++++++++--	return mean, err, scores
++++++++--
++++++++--def load_result(path):
++++++++--	'''
++++++++--		path : path to experiment directory; expects `rollout.json` to be in directory
++++++++--	'''
++++++++--	fullpath = os.path.join(path, 'rollout.json')
++++++++--	suffix = path.split('/')[-1]
++++++++--
++++++++--	if not os.path.exists(fullpath):
++++++++--		return None
++++++++--
++++++++--	results = json.load(open(fullpath, 'rb'))
++++++++--	score = results['score']
++++++++--	return score * 100
++++++++--
++++++++--#######################
++++++++--######## setup ########
++++++++--#######################
++++++++--
++++++++--if __name__ == '__main__':
++++++++--
++++++++--	class Parser(utils.Parser):
++++++++--	    dataset: str = None
++++++++--
++++++++--	args = Parser().parse_args()
++++++++--
++++++++--	for dataset in ([args.dataset] if args.dataset else DATASETS):
++++++++--		subdirs = glob.glob(os.path.join(LOGBASE, dataset, EXP_NAME))
++++++++--
++++++++--		for subdir in subdirs:
++++++++--			reldir = subdir.split('/')[-1]
++++++++--			paths = glob.glob(os.path.join(subdir, TRIAL))
++++++++--
++++++++--			mean, err, scores = load_results(paths)
++++++++--			print(f'{dataset.ljust(30)} | {subdir.ljust(50)} | {len(scores)} scores \n    {mean:.2f} +/- {err:.2f}\n')
++++++++-diff --git a/halfcheetah/plotting/scores.py b/halfcheetah/plotting/scores.py
++++++++-deleted file mode 100644
++++++++-index f1917f7..0000000
++++++++---- a/halfcheetah/plotting/scores.py
++++++++-+++ /dev/null
++++++++-@@ -1,123 +0,0 @@
++++++++--means = {
++++++++--	'Trajectory\nTransformer': {
++++++++--		##
++++++++--		'halfcheetah-medium-expert-v2': 95.0,
++++++++--		'hopper-medium-expert-v2': 110.0,
++++++++--		'walker2d-medium-expert-v2': 101.9,
++++++++--		'ant-medium-expert-v2': 116.1,
++++++++--		##
++++++++--		'halfcheetah-medium-v2': 46.9,
++++++++--		'hopper-medium-v2': 61.1,
++++++++--		'walker2d-medium-v2': 79.0,
++++++++--		'ant-medium-v2': 83.1,
++++++++--		##
++++++++--		'halfcheetah-medium-replay-v2': 41.9,
++++++++--		'hopper-medium-replay-v2': 91.5,
++++++++--		'walker2d-medium-replay-v2': 82.6,
++++++++--		'ant-medium-replay-v2': 77.0,
++++++++--	},
++++++++--	'Decision\nTransformer': {
++++++++--		##
++++++++--		'halfcheetah-medium-expert-v2': 86.8,
++++++++--		'hopper-medium-expert-v2': 107.6,
++++++++--		'walker2d-medium-expert-v2': 108.1,
++++++++--		##
++++++++--		'halfcheetah-medium-v2': 42.6,
++++++++--		'hopper-medium-v2': 67.6,
++++++++--		'walker2d-medium-v2': 74.0,
++++++++--		##
++++++++--		'halfcheetah-medium-replay-v2': 36.6,
++++++++--		'hopper-medium-replay-v2': 82.7,
++++++++--		'walker2d-medium-replay-v2': 66.6,
++++++++--	},
++++++++--	'CQL': {
++++++++--		##
++++++++--		'halfcheetah-medium-expert-v2': 91.6,
++++++++--		'hopper-medium-expert-v2': 105.4,
++++++++--		'walker2d-medium-expert-v2': 108.8,
++++++++--		##
++++++++--		'halfcheetah-medium-v2': 44.0,
++++++++--		'hopper-medium-v2': 58.5,
++++++++--		'walker2d-medium-v2': 72.5,
++++++++--		##
++++++++--		'halfcheetah-medium-replay-v2': 45.5,
++++++++--		'hopper-medium-replay-v2': 95.0,
++++++++--		'walker2d-medium-replay-v2': 77.2,
++++++++--	},
++++++++--	'MOPO': {
++++++++--		##
++++++++--		'halfcheetah-medium-expert-v2': 63.3,
++++++++--		'hopper-medium-expert-v2': 23.7,
++++++++--		'walker2d-medium-expert-v2': 44.6,
++++++++--		##
++++++++--		'halfcheetah-medium-v2': 42.3,
++++++++--		'hopper-medium-v2': 28.0,
++++++++--		'walker2d-medium-v2': 17.8,
++++++++--		##
++++++++--		'halfcheetah-medium-replay-v2': 53.1,
++++++++--		'hopper-medium-replay-v2': 67.5,
++++++++--		'walker2d-medium-replay-v2':39.0,
++++++++--	},
++++++++--	'MBOP': {
++++++++--		##
++++++++--		'halfcheetah-medium-expert-v2': 105.9,
++++++++--		'hopper-medium-expert-v2': 55.1,
++++++++--		'walker2d-medium-expert-v2': 70.2,
++++++++--		##
++++++++--		'halfcheetah-medium-v2': 44.6,
++++++++--		'hopper-medium-v2': 48.8,
++++++++--		'walker2d-medium-v2': 41.0,
++++++++--		##
++++++++--		'halfcheetah-medium-replay-v2': 42.3,
++++++++--		'hopper-medium-replay-v2': 12.4,
++++++++--		'walker2d-medium-replay-v2': 9.7,
++++++++--	},
++++++++--	'BRAC': {
++++++++--		##
++++++++--		'halfcheetah-medium-expert-v2': 41.9,
++++++++--		'hopper-medium-expert-v2': 0.9,
++++++++--		'walker2d-medium-expert-v2': 81.6,
++++++++--		##
++++++++--		'halfcheetah-medium-v2': 46.3,
++++++++--		'hopper-medium-v2': 31.3,
++++++++--		'walker2d-medium-v2': 81.1,
++++++++--		##
++++++++--		'halfcheetah-medium-replay-v2': 47.7,
++++++++--		'hopper-medium-replay-v2': 0.6,
++++++++--		'walker2d-medium-replay-v2': 0.9,
++++++++--	},
++++++++--	'BC': {
++++++++--		##
++++++++--		'halfcheetah-medium-expert-v2': 59.9,
++++++++--		'hopper-medium-expert-v2': 79.6,
++++++++--		'walker2d-medium-expert-v2': 36.6,
++++++++--		##
++++++++--		'halfcheetah-medium-v2': 43.1,
++++++++--		'hopper-medium-v2': 63.9,
++++++++--		'walker2d-medium-v2': 77.3,
++++++++--		##
++++++++--		'halfcheetah-medium-replay-v2': 4.3,
++++++++--		'hopper-medium-replay-v2': 27.6,
++++++++--		'walker2d-medium-replay-v2': 36.9,
++++++++--	},
++++++++--}
++++++++--
++++++++--errors = {
++++++++--	'Trajectory\nTransformer': {
++++++++--		##
++++++++--		'halfcheetah-medium-expert-v2': 0.2,
++++++++--		'hopper-medium-expert-v2': 2.7,
++++++++--		'walker2d-medium-expert-v2': 6.8,
++++++++--		'ant-medium-expert-v2': 9.0,
++++++++--		##
++++++++--		'halfcheetah-medium-v2': 0.4,
++++++++--		'hopper-medium-v2': 3.6,
++++++++--		'walker2d-medium-v2': 2.8,
++++++++--		'ant-medium-v2': 7.3,
++++++++--		##
++++++++--		'halfcheetah-medium-replay-v2': 2.5,
++++++++--		'hopper-medium-replay-v2': 3.6,
++++++++--		'walker2d-medium-replay-v2': 6.9,
++++++++--		'ant-medium-replay-v2': 6.8,
++++++++--	},
++++++++--}
++++++++++-trajectory_aaa
++++++++++diff --git a/seaquest/readme.md b/seaquest/readme.md
++++++++++index 84e53f8..53561f9 100644
++++++++++--- a/seaquest/readme.md
+++++++++++++ b/seaquest/readme.md
++++++++++@@ -10,4 +10,4 @@ pip install git+https://github.com/takuseno/d4rl-atari
++++++++++ pip install "gym[atari, accept-rom-license]"
++++++++++ pip install pyclustering
++++++++++ pip install seaborn
++++++++++-pip install d3rlpy==1.1.1
++++++++++\ No newline at end of file
+++++++++++pip install d3rlpy==1.1.1
++++++++ \ No newline at end of file
++++++++-diff --git a/halfcheetah/plotting/table.py b/halfcheetah/plotting/table.py
++++++++-deleted file mode 100644
++++++++-index eae74e6..0000000
++++++++---- a/halfcheetah/plotting/table.py
++++++++-+++ /dev/null
++++++++-@@ -1,127 +0,0 @@
++++++++--import numpy as np
++++++++--import pdb
++++++++--
++++++++--from plotting.plot import get_mean
++++++++--from plotting.scores import (
++++++++--	means as MEANS,
++++++++--	errors as ERRORS,
++++++++--)
++++++++--
++++++++--ALGORITHM_STRINGS = {
++++++++--	'Trajectory\nTransformer': 'TT (Ours)',
++++++++--	'Decision\nTransformer': 'DT',	
++++++++--}
++++++++--
++++++++--BUFFER_STRINGS = {
++++++++--	'medium-expert': 'Medium-Expert',
++++++++--	'medium': 'Medium',
++++++++--	'medium-replay': 'Medium-Replay',	
++++++++--}
++++++++--
++++++++--ENVIRONMENT_STRINGS = {
++++++++--	'halfcheetah': 'HalfCheetah',
++++++++--	'hopper': 'Hopper',
++++++++--	'walker2d': 'Walker2d',
++++++++--	'ant': 'Ant',
++++++++--}
++++++++--
++++++++--SHOW_ERRORS = ['Trajectory\nTransformer']
++++++++--
++++++++--def get_result(algorithm, buffer, environment, version='v2'):
++++++++--	key = f'{environment}-{buffer}-{version}'
++++++++--	mean = MEANS[algorithm].get(key, '-')
++++++++--	if algorithm in SHOW_ERRORS:
++++++++--		error = ERRORS[algorithm].get(key)
++++++++--		return (mean, error)
++++++++--	else:
++++++++--		return mean
++++++++--
++++++++--def format_result(result):
++++++++--	if type(result) == tuple:
++++++++--		mean, std = result
++++++++--		return f'${mean}$ \\scriptsize{{\\raisebox{{1pt}}{{$\\pm {std}$}}}}'
++++++++--	else:
++++++++--		return f'${result}$'
++++++++--
++++++++--def format_row(buffer, environment, results):
++++++++--	buffer_str = BUFFER_STRINGS[buffer]
++++++++--	environment_str = ENVIRONMENT_STRINGS[environment]
++++++++--	results_str = ' & '.join(format_result(result) for result in results)
++++++++--	row = f'{buffer_str} & {environment_str} & {results_str} \\\\ \n'
++++++++--	return row
++++++++--
++++++++--def format_buffer_block(algorithms, buffer, environments):
++++++++--	block_str = '\\midrule\n'
++++++++--	for environment in environments:
++++++++--		results = [get_result(alg, buffer, environment) for alg in algorithms]
++++++++--		row_str = format_row(buffer, environment, results)
++++++++--		block_str += row_str
++++++++--	return block_str
++++++++--
++++++++--def format_algorithm(algorithm):
++++++++--	algorithm_str = ALGORITHM_STRINGS.get(algorithm, algorithm)
++++++++--	return f'\multicolumn{{1}}{{c}}{{\\bf {algorithm_str}}}'
++++++++--
++++++++--def format_algorithms(algorithms):
++++++++--	return ' & '.join(format_algorithm(algorithm) for algorithm in algorithms)
++++++++--
++++++++--def format_averages(means, label):
++++++++--	prefix = f'\\multicolumn{{2}}{{c}}{{\\bf Average ({label})}} & '
++++++++--	formatted = ' & '.join(str(mean) for mean in means)
++++++++--	return prefix + formatted
++++++++--
++++++++--def format_averages_block(algorithms):
++++++++--	means_filtered = [np.round(get_mean(MEANS[algorithm], exclude='ant'), 1) for algorithm in algorithms]
++++++++--	means_all = [np.round(get_mean(MEANS[algorithm], exclude=None), 1) for algorithm in algorithms]
++++++++--
++++++++--	means_all = [
++++++++--		means
++++++++--		if 'ant-medium-expert-v2' in MEANS[algorithm]
++++++++--		else '$-$'
++++++++--		for algorithm, means in zip(algorithms, means_all)
++++++++--	]
++++++++--
++++++++--	formatted_filtered = format_averages(means_filtered, 'without Ant')
++++++++--	formatted_all = format_averages(means_all, 'all settings')
++++++++--
++++++++--	formatted_block = (
++++++++--		f'{formatted_filtered} \\hspace{{.6cm}} \\\\ \n'
++++++++--		f'{formatted_all} \\hspace{{.6cm}} \\\\ \n'
++++++++--	)
++++++++--	return formatted_block
++++++++--
++++++++--def format_table(algorithms, buffers, environments):
++++++++--	justify_str = 'll' + 'r' * len(algorithms)
++++++++--	algorithm_str = format_algorithms(['Dataset', 'Environment'] + algorithms)
++++++++--	averages_str = format_averages_block(algorithms)
++++++++--	table_prefix = (
++++++++--		'\\begin{table*}[h]\n'
++++++++--		'\\centering\n'
++++++++--		'\\small\n'
++++++++--		f'\\begin{{tabular}}{{{justify_str}}}\n'
++++++++--		'\\toprule\n'
++++++++--		f'{algorithm_str} \\\\ \n'
++++++++--	)
++++++++--	table_suffix = (
++++++++--		'\\midrule\n'
++++++++--		f'{averages_str}'
++++++++--		'\\bottomrule\n'
++++++++--		'\\end{tabular}\n'
++++++++--		'\\label{table:d4rl}\n'
++++++++--		'\\end{table*}'
++++++++--	)
++++++++--	blocks = ''.join(format_buffer_block(algorithms, buffer, environments) for buffer in buffers)
++++++++--	table = (
++++++++--		f'{table_prefix}'
++++++++--		f'{blocks}'
++++++++--		f'{table_suffix}'
++++++++--	)
++++++++--	return table
++++++++--
++++++++--
++++++++--algorithms =['BC', 'MBOP', 'BRAC', 'CQL',  'Decision\nTransformer', 'Trajectory\nTransformer']
++++++++--buffers = ['medium-expert', 'medium', 'medium-replay']
++++++++--environments = ['halfcheetah', 'hopper', 'walker2d', 'ant']
++++++++--
++++++++--table = format_table(algorithms, buffers, environments)
++++++++--print(table)
++++++++-diff --git a/halfcheetah/scripts/plan.py b/halfcheetah/scripts/plan.py
++++++++-deleted file mode 100644
++++++++-index f13d4cc..0000000
++++++++---- a/halfcheetah/scripts/plan.py
++++++++-+++ /dev/null
++++++++-@@ -1,124 +0,0 @@
++++++++--import json
++++++++--import pdb
++++++++--from os.path import join
++++++++--
++++++++--import trajectory.utils as utils
++++++++--import trajectory.datasets as datasets
++++++++--from trajectory.search import (
++++++++--    beam_plan,
++++++++--    make_prefix,
++++++++--    extract_actions,
++++++++--    update_context,
++++++++--)
++++++++--
++++++++--class Parser(utils.Parser):
++++++++--    dataset: str = 'halfcheetah-medium-expert-v2'
++++++++--    config: str = 'config.offline'
++++++++--
++++++++--#######################
++++++++--######## setup ########
++++++++--#######################
++++++++--
++++++++--args = Parser().parse_args('plan')
++++++++--
++++++++--#######################
++++++++--####### models ########
++++++++--#######################
++++++++--
++++++++--dataset = utils.load_from_config(args.logbase, args.dataset, args.gpt_loadpath,
++++++++--        'data_config.pkl')
++++++++--
++++++++--gpt, gpt_epoch = utils.load_model(args.logbase, args.dataset, args.gpt_loadpath,
++++++++--        epoch=args.gpt_epoch, device=args.device)
++++++++--
++++++++--#######################
++++++++--####### dataset #######
++++++++--#######################
++++++++--
++++++++--env = datasets.load_environment(args.dataset)
++++++++--print('yo')
++++++++--renderer = utils.make_renderer(args)
++++++++--timer = utils.timer.Timer()
++++++++--
++++++++--discretizer = dataset.discretizer
++++++++--discount = dataset.discount
++++++++--observation_dim = dataset.observation_dim
++++++++--action_dim = dataset.action_dim
++++++++--
++++++++--value_fn = lambda x: discretizer.value_fn(x, args.percentile)
++++++++--preprocess_fn = datasets.get_preprocess_fn(env.name)
++++++++--
++++++++--print('yo2')
++++++++--
++++++++--#######################
++++++++--###### main loop ######
++++++++--#######################
++++++++--
++++++++--observation = env.reset()
++++++++--total_reward = 0
++++++++--
++++++++--## observations for rendering
++++++++--rollout = [observation.copy()]
++++++++--
++++++++--## previous (tokenized) transitions for conditioning transformer
++++++++--context = []
++++++++--
++++++++--T = env.max_episode_steps
++++++++--for t in range(T):
++++++++--
++++++++--    observation = preprocess_fn(observation)
++++++++--
++++++++--    if t % args.plan_freq == 0:
++++++++--        ## concatenate previous transitions and current observations to input to model
++++++++--        prefix = make_prefix(discretizer, context, observation, args.prefix_context)
++++++++--
++++++++--        ## sample sequence from model beginning with `prefix`
++++++++--        sequence = beam_plan(
++++++++--            gpt, value_fn, prefix,
++++++++--            args.horizon, args.beam_width, args.n_expand, observation_dim, action_dim,
++++++++--            discount, args.max_context_transitions, verbose=args.verbose,
++++++++--            k_obs=args.k_obs, k_act=args.k_act, cdf_obs=args.cdf_obs, cdf_act=args.cdf_act,
++++++++--        )
++++++++--
++++++++--    else:
++++++++--        sequence = sequence[1:]
++++++++--
++++++++--    ## [ horizon x transition_dim ] convert sampled tokens to continuous trajectory
++++++++--    sequence_recon = discretizer.reconstruct(sequence)
++++++++--
++++++++--    ## [ action_dim ] index into sampled trajectory to grab first action
++++++++--    action = extract_actions(sequence_recon, observation_dim, action_dim, t=0)
++++++++--
++++++++--    ## execute action in environment
++++++++--    next_observation, reward, terminal, _ = env.step(action)
++++++++--
++++++++--    ## update return
++++++++--    total_reward += reward
++++++++--    score = env.get_normalized_score(total_reward)
++++++++--
++++++++--    ## update rollout observations and context transitions
++++++++--    rollout.append(next_observation.copy())
++++++++--    context = update_context(context, discretizer, observation, action, reward, args.max_context_transitions)
++++++++--
++++++++--    print(
++++++++--        f'[ plan ] t: {t} / {T} | r: {reward:.2f} | R: {total_reward:.2f} | score: {score:.4f} | '
++++++++--        f'time: {timer():.2f} | {args.dataset} | {args.exp_name} | {args.suffix}\n'
++++++++--    )
++++++++--
++++++++--    ## visualization
++++++++--    if t % args.vis_freq == 0 or terminal or t == T:
++++++++--
++++++++--        ## save current plan
++++++++--        renderer.render_plan(join(args.savepath, f'{t}_plan.mp4'), sequence_recon, env.state_vector())
++++++++--
++++++++--        ## save rollout thus far
++++++++--        renderer.render_rollout(join(args.savepath, f'rollout.mp4'), rollout, fps=80)
++++++++--
++++++++--    if terminal: break
++++++++--
++++++++--    observation = next_observation
++++++++--
++++++++--## save result as a json file
++++++++--json_path = join(args.savepath, 'rollout.json')
++++++++--json_data = {'score': score, 'step': t, 'return': total_reward, 'term': terminal, 'gpt_epoch': gpt_epoch}
++++++++--json.dump(json_data, open(json_path, 'w'), indent=2, sort_keys=True)
++++++++-diff --git a/halfcheetah/scripts/train.py b/halfcheetah/scripts/train.py
++++++++-deleted file mode 100644
++++++++-index 04af8d7..0000000
++++++++---- a/halfcheetah/scripts/train.py
++++++++-+++ /dev/null
++++++++-@@ -1,122 +0,0 @@
++++++++--import os
++++++++--import numpy as np
++++++++--import torch
++++++++--import pdb
++++++++--
++++++++--import trajectory.utils as utils
++++++++--import trajectory.datasets as datasets
++++++++--from trajectory.models.transformers import GPT
++++++++--
++++++++--
++++++++--class Parser(utils.Parser):
++++++++--    dataset: str = 'halfcheetah-medium-expert-v2'
++++++++--    config: str = 'config.offline'
++++++++--
++++++++--#######################
++++++++--######## setup ########
++++++++--#######################
++++++++--
++++++++--args = Parser().parse_args('train')
++++++++--
++++++++--#######################
++++++++--####### dataset #######
++++++++--#######################
++++++++--
++++++++--env = datasets.load_environment(args.dataset)
++++++++--
++++++++--sequence_length = args.subsampled_sequence_length * args.step
++++++++--
++++++++--dataset_config = utils.Config(
++++++++--    datasets.DiscretizedDataset,
++++++++--    savepath=(args.savepath, 'data_config.pkl'),
++++++++--    env=args.dataset,
++++++++--    N=args.N,
++++++++--    penalty=args.termination_penalty,
++++++++--    sequence_length=sequence_length,
++++++++--    step=args.step,
++++++++--    discount=args.discount,
++++++++--    discretizer=args.discretizer,
++++++++--)
++++++++--
++++++++--dataset = dataset_config()
++++++++--obs_dim = dataset.observation_dim
++++++++--act_dim = dataset.action_dim
++++++++--transition_dim = dataset.joined_dim
++++++++--
++++++++--#######################
++++++++--######## model ########
++++++++--#######################
++++++++--
++++++++--block_size = args.subsampled_sequence_length * transition_dim - 1
++++++++--print(
++++++++--    f'Dataset size: {len(dataset)} | '
++++++++--    f'Joined dim: {transition_dim} '
++++++++--    f'(observation: {obs_dim}, action: {act_dim}) | Block size: {block_size}'
++++++++--)
++++++++--
++++++++--model_config = utils.Config(
++++++++--    GPT,
++++++++--    savepath=(args.savepath, 'model_config.pkl'),
++++++++--    ## discretization
++++++++--    vocab_size=args.N, block_size=block_size,
++++++++--    ## architecture
++++++++--    n_layer=args.n_layer, n_head=args.n_head, n_embd=args.n_embd*args.n_head,
++++++++--    ## dimensions
++++++++--    observation_dim=obs_dim, action_dim=act_dim, transition_dim=transition_dim,
++++++++--    ## loss weighting
++++++++--    action_weight=args.action_weight, reward_weight=args.reward_weight, value_weight=args.value_weight,
++++++++--    ## dropout probabilities
++++++++--    embd_pdrop=args.embd_pdrop, resid_pdrop=args.resid_pdrop, attn_pdrop=args.attn_pdrop,
++++++++--)
++++++++--
++++++++--model = model_config()
++++++++--model.to(args.device)
++++++++--
++++++++--#######################
++++++++--####### trainer #######
++++++++--#######################
++++++++--
++++++++--warmup_tokens = len(dataset) * block_size ## number of tokens seen per epoch
++++++++--final_tokens = 20 * warmup_tokens
++++++++--
++++++++--trainer_config = utils.Config(
++++++++--    utils.Trainer,
++++++++--    savepath=(args.savepath, 'trainer_config.pkl'),
++++++++--    # optimization parameters
++++++++--    batch_size=args.batch_size,
++++++++--    learning_rate=args.learning_rate,
++++++++--    betas=(0.9, 0.95),
++++++++--    grad_norm_clip=1.0,
++++++++--    weight_decay=0.1, # only applied on matmul weights
++++++++--    # learning rate decay: linear warmup followed by cosine decay to 10% of original
++++++++--    lr_decay=args.lr_decay,
++++++++--    warmup_tokens=warmup_tokens,
++++++++--    final_tokens=final_tokens,
++++++++--    ## dataloader
++++++++--    num_workers=0,
++++++++--    device=args.device,
++++++++--)
++++++++--
++++++++--trainer = trainer_config()
++++++++--
++++++++--#######################
++++++++--###### main loop ######
++++++++--#######################
++++++++--
++++++++--## scale number of epochs to keep number of updates constant
++++++++--n_epochs = int(1e6 / len(dataset) * args.n_epochs_ref)
++++++++--save_freq = int(n_epochs // args.n_saves)
++++++++--
++++++++--for epoch in range(n_epochs):
++++++++--    print(f'\nEpoch: {epoch} / {n_epochs} | {args.dataset} | {args.exp_name}')
++++++++--
++++++++--    trainer.train(model, dataset)
++++++++--
++++++++--    ## get greatest multiple of `save_freq` less than or equal to `save_epoch`
++++++++--    save_epoch = (epoch + 1) // save_freq * save_freq
++++++++--    statepath = os.path.join(args.savepath, f'state_{save_epoch}.pt')
++++++++--    print(f'Saving model to {statepath}')
++++++++--
++++++++--    ## save state to disk
++++++++--    state = model.state_dict()
++++++++--    torch.save(state, statepath)
++++++++-diff --git a/halfcheetah/scripts/xrl.py b/halfcheetah/scripts/xrl.py
++++++++-deleted file mode 100644
++++++++-index 134232a..0000000
++++++++---- a/halfcheetah/scripts/xrl.py
++++++++-+++ /dev/null
++++++++-@@ -1,372 +0,0 @@
++++++++--import json
++++++++--import pdb
++++++++--from os.path import join
++++++++--
++++++++--import trajectory.utils as utils
++++++++--import trajectory.datasets as datasets
++++++++--from trajectory.search import (
++++++++--    make_prefix,
++++++++--    update_context,
++++++++--)
++++++++--from trajectory.search.sampling import forward
++++++++--
++++++++--import gym
++++++++--import d4rl # Import required to register environments, you may need to also import the submodule
++++++++--import numpy as np
++++++++--import d3rlpy
++++++++--import math as mt
++++++++--from sklearn.cluster import KMeans
++++++++--from sklearn import datasets as skdatasets
++++++++--from sklearn.decomposition import PCA
++++++++--
++++++++--from pyclustering.cluster.xmeans import xmeans
++++++++--from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer
++++++++--
++++++++--from scipy.stats import wasserstein_distance
++++++++--
++++++++--class Parser(utils.Parser):
++++++++--    dataset: str = 'halfcheetah-medium-expert-v2'
++++++++--    config: str = 'config.offline'
++++++++--
+++++++++diff --git a/halfcheetah/scripts/xrl_v2.py b/halfcheetah/scripts/xrl_v2.py
+++++++++index 62a3d4d..b012599 100644
+++++++++--- a/halfcheetah/scripts/xrl_v2.py
++++++++++++ b/halfcheetah/scripts/xrl_v2.py
+++++++++@@ -21,54 +21,97 @@ from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer
+++++++++ from scipy.stats import wasserstein_distance
+++++++++ from moviepy.editor import VideoFileClip
+++++++++ 
++++++++++
+++++++++ class Parser(utils.Parser):
+++++++++     dataset: str = 'halfcheetah-medium-v2'
+++++++++     config: str = 'config.offline'
+++++++++ 
++++++++ -# utils
++++++++--    
++++++++--class XMeans:
++++++++--    def loglikelihood(self, r, rn, var, m, k):
++++++++--        l1 = - rn / 2.0 * mt.log(2 * mt.pi)
++++++++--        l2 = - rn * m / 2.0 * mt.log(var)
++++++++--        l3 = - (rn - k) / 2.0
++++++++--        l4 = rn * mt.log(rn)
++++++++--        l5 = - rn * mt.log(r)
++++++++--
++++++++--        return l1 + l2 + l3 + l4 + l5
++++++++--
++++++++--    def __init__(self, X, kmax = 20):
++++++++--        self.X = X
++++++++--        self.num = np.size(self.X, axis=0)
++++++++--        self.dim = np.size(X, axis=1)
++++++++--        self.KMax = kmax
++++++++--
++++++++--    def fit(self):
++++++++--        k = 1
++++++++--        X = self.X
++++++++--        M = self.dim
++++++++--        num = self.num
++++++++--
++++++++--        while(1):
++++++++--            ok = k
++++++++--
++++++++--            #Improve Params
++++++++--            kmeans = KMeans(n_clusters=k).fit(X)
++++++++--            labels = kmeans.labels_
++++++++--            m = kmeans.cluster_centers_
++++++++--
++++++++--            #Improve Structure
++++++++--            #Calculate BIC
++++++++--            p = M + 1
++++++++--
++++++++--            obic = np.zeros(k)
++++++++--
++++++++--            for i in range(k):
++++++++--                rn = np.size(np.where(labels == i))
++++++++--                var = np.sum((X[labels == i] - m[i])**2)/float(rn - 1)
++++++++--                obic[i] = self.loglikelihood(rn, rn, var, M, 1) - p/2.0*mt.log(rn)
++++++++--
++++++++--            #Split each cluster into two subclusters and calculate BIC of each splitted cluster
++++++++--            sk = 2 #The number of subclusters
++++++++--            nbic = np.zeros(k)
++++++++--            addk = 0
++++++++--
++++++++--            for i in range(k):
++++++++--                ci = X[labels == i]
++++++++--                r = np.size(np.where(labels == i))
++++++++--
++++++++--                kmeans = KMeans(n_clusters=sk).fit(ci)
++++++++--                ci_labels = kmeans.labels_
++++++++--                sm = kmeans.cluster_centers_
++++++++--
++++++++--                for l in range(sk):
++++++++--                    rn = np.size(np.where(ci_labels == l))
++++++++--                    var = np.sum((ci[ci_labels == l] - sm[l])**2)/float(rn - sk)
++++++++--                    nbic[i] += self.loglikelihood(r, rn, var, M, sk)
++++++++--
++++++++--                p = sk * (M + 1)
++++++++--                nbic[i] -= p/2.0*mt.log(r)
++++++++--
++++++++--                if obic[i] < nbic[i]:
++++++++--                    addk += 1
++++++++--
++++++++--            k += addk
++++++++--
++++++++--            if ok == k or k >= self.KMax:
++++++++--                break
++++++++--
++++++++--
++++++++--        #Calculate labels and centroids
++++++++--        kmeans = KMeans(n_clusters=k).fit(X)
++++++++--        self.labels = kmeans.labels_
++++++++--        self.k = k
++++++++--        self.m = kmeans.cluster_centers_
++++++++--
++++++++--
++++++++--def cluster_trajectories(trajectories):
++++++++--    xmeans_instance = XMeans(trajectories, kmax=10)
++++++++--    xmeans_instance.fit()
++++++++--
++++++++--    clusters = xmeans_instance.labels
++++++++--    return clusters
++++++++--
++++++++--def cluster_trajectories_2(trajectories):
+++++++++ 
+++++++++ def cluster_trajectories(trajectories, n_clusters=10):
+++++++++-    """TODO"""
++++++++++    """
++++++++++    Cluster trajectories using X-means.
++++++++++    
++++++++++    Args:
++++++++++    - trajectories: np.array, shape (n_trajectories, encoding_dim)
++++++++++    - n_clusters: int, max number of clusters
++++++++++    
++++++++++    Returns:
++++++++++    - idxs_per_cluster: list, trajectory idxs per cluster idxs
++++++++++    - clusters: np.array, shape (n_trajectories), cluster idxs per trajectory idx
++++++++++    """ 
+++++++++ 
++++++++ -    # Prepare initial centers - amount of initial centers defines amount of clusters from which X-Means will
++++++++ -    # start analysis.
++++++++--    amount_initial_centers = 2
++++++++--    initial_centers = kmeans_plusplus_initializer(trajectories, amount_initial_centers).initialize()
++++++++--    
++++++++++    # Set 2 initial cluster centers
+++++++++     amount_initial_centers = 2
+++++++++     initial_centers = kmeans_plusplus_initializer(trajectories, amount_initial_centers).initialize()
+++++++++     
++++++++ -    # Create instance of X-Means algorithm. The algorithm will start analysis from 2 clusters, the maximum
++++++++ -    # number of clusters that can be allocated is 10.
++++++++--    xmeans_instance = xmeans(trajectories, initial_centers, 10)
++++++++--    xmeans_instance.process()
++++++++--    
++++++++--    # Extract clustering results: clusters
++++++++--    idxs_per_cluster = xmeans_instance.get_clusters()
++++++++--
++++++++--    clusters = []
++++++++--    for i in range(len(trajectories)):
++++++++--        for j in range(len(idxs_per_cluster)):
++++++++--            if i in idxs_per_cluster[j]: clusters.append(j)
++++++++--
++++++++--    return idxs_per_cluster, np.array(clusters)
++++++++++    # Run X-means
+++++++++     xmeans_instance = xmeans(trajectories, initial_centers, n_clusters)
+++++++++     xmeans_instance.process()
+++++++++     
+++++++++     # Extract clustering results: clusters
+++++++++     idxs_per_cluster = xmeans_instance.get_clusters()
+++++++++ 
++++++++++    # Turn list of trajectory idxs per cluster to array of cluster idx per trajectory idx
+++++++++     clusters = []
+++++++++     for i in range(len(trajectories)):
+++++++++         for j in range(len(idxs_per_cluster)):
+++++++++             if i in idxs_per_cluster[j]: clusters.append(j)
+++++++++ 
+++++++++     return idxs_per_cluster, np.array(clusters)
++++++++ - 
++++++++--# https://github.com/sascha-kirch/ML_Notebooks/blob/main/Softmax_Temperature.ipynb
++++++++--def softmax(x, temp):
++++++++--    """Compute softmax values for each sets of scores in x."""
++++++++--    return np.exp(np.divide(x,temp)) / np.sum(np.exp(np.divide(x,temp)))
++++++++--
++++++++--def generate_data_embedding(trajectory_embeddings, normalizing_factor=1, temperature=1):
++++++++--    embedding = np.sum(trajectory_embeddings, axis=0) / normalizing_factor
++++++++--    embedding = softmax(embedding, temperature)
++++++++--    return embedding
++++++++--
++++++++--def embed_trajectory(gpt, discretizer, observations, actions, rewards, preprocess_fn):
++++++++--    context = []
++++++++--
++++++++--    for i in range(len(observations)):
++++++++--        observation = observations[i]
++++++++--        action = actions[i]
++++++++--        reward = rewards[i]
++++++++--
++++++++--        observation = preprocess_fn(observation)
++++++++--
++++++++--        # print(observation)
++++++++--        prefix = make_prefix(discretizer, context, observation, True)
++++++++--        # print("prefix", prefix.shape)
++++++++--
++++++++--        out = forward(gpt, prefix)
++++++++--        # print("out", out.shape)
++++++++--        context = update_context(context, discretizer, observation, action, reward, len(observations))
++++++++--        # print("cotext", context)
++++++++--    
++++++++--    emb = []
++++++++--    for context_step in context:
++++++++--        emb.append(context_step.numpy())
++++++++--    emb = np.array(emb)
++++++++--    emb = np.mean(emb, axis=0)[0]
++++++++--
++++++++++
++++++++++
+++++++++ def softmax(x, temp):
+++++++++-    """TODO"""
++++++++++    """
++++++++++    Softmax with temperature using max-trick.
++++++++++    
++++++++++    Args:
++++++++++    - x: np.array, shape (n_data, dim_data)
++++++++++    - temp: int, softmax temperature
++++++++++    
++++++++++    Returns:
++++++++++    - softmax_x: np.array: shape (dim_data)
++++++++++    """ 
++++++++++
+++++++++     max_x = np.max(x)
+++++++++-    return np.exp(np.divide(x-max_x,temp)) / np.sum(np.exp(np.divide(x-max_x,temp)))
++++++++++    softmax_x = np.exp(np.divide(x-max_x,temp)) / np.sum(np.exp(np.divide(x-max_x,temp)))
++++++++++    return softmax_x
++++++++++
+++++++++ 
+++++++++ def generate_data_embedding(trajectory_embeddings, temperature=10000):
+++++++++-    """TODO"""
++++++++++    """
++++++++++    Generate data embedding (sum+softmax) for set of encoded trajectories.
++++++++++    
++++++++++    Args:
++++++++++    - trajectory_embeddings: np.array, shape (n_data, dim_data)
++++++++++    - temperature: int, softmax temperature
++++++++++    
++++++++++    Returns:
++++++++++    - embedding: np.array, shape (dim_data)
++++++++++    """ 
+++++++++ 
+++++++++     embedding = np.sum(trajectory_embeddings, axis=0)
+++++++++     embedding = softmax(embedding, temperature)
+++++++++     
+++++++++-
+++++++++     return embedding
+++++++++ 
++++++++++
+++++++++ def embed_trajectory(gpt, discretizer, observations, actions, rewards, preprocess_fn):
+++++++++-    """TODO"""
++++++++++    """
++++++++++    Encode trajectory using a trajectory transformer with a sliding window.
++++++++++    
++++++++++    Args:
++++++++++    - gpt: trajectory transformer
++++++++++    - discretizer: environment discretizer
++++++++++    - observations: trajectory observations
++++++++++    - actions: trajectory actions
++++++++++    - rewards: trajectory rewards
++++++++++    - preprocess_fn: observations preprocessing functions
++++++++++    
++++++++++    Returns:
++++++++++    - embedding: np.array, shape (hidden_dim), encoded trajectory
++++++++++    """ 
+++++++++ 
+++++++++     context = []
+++++++++-
+++++++++     output = []
+++++++++ 
+++++++++     for i in range(len(observations)):
+++++++++@@ -76,12 +119,12 @@ def embed_trajectory(gpt, discretizer, observations, actions, rewards, preproces
+++++++++         action = actions[i]
+++++++++         reward = rewards[i]
+++++++++ 
++++++++++        # Preprocess, discretize & forward through trajectory transformer
+++++++++         observation = preprocess_fn(observation)
+++++++++-
+++++++++         prefix = make_prefix(discretizer, context, observation, True)
+++++++++-
+++++++++         out = forward(gpt, prefix)
+++++++++ 
++++++++++        # Sliding window
+++++++++         if len(context) >= 9:
+++++++++             context.pop(0)
+++++++++             if len(output) == 0:
+++++++++@@ -91,19 +134,33 @@ def embed_trajectory(gpt, discretizer, observations, actions, rewards, preproces
+++++++++ 
+++++++++         context = update_context(context, discretizer, observation, action, reward, len(observations))
+++++++++ 
+++++++++-    emb = np.mean(output, axis=0)
++++++++ -    return emb
++++++++--
++++++++--
++++++++--def create_complementary_dataset(dataset, idxs, trajectory_length=10):
++++++++--    observations = []
++++++++--    actions = []
++++++++--    rewards = []
++++++++--    terminals = []
++++++++--    for i in range(1000):
++++++++--        if i not in idxs:
++++++++--            observations += list(dataset.observations[1000*i:1000*i+trajectory_length])
++++++++--            actions += list(dataset.actions[1000*i:1000*i+trajectory_length])
++++++++--            rewards += list(dataset.rewards[1000*i:1000*i+trajectory_length])
++++++++--            terminals += list(dataset.terminals[1000*i:1000*i+trajectory_length])
++++++++--
++++++++--    new_dataset = d3rlpy.dataset.MDPDataset(
++++++++--        observations=np.array(observations),
++++++++--        actions=np.array(actions),
++++++++--        rewards=np.array(rewards),
++++++++--        terminals=np.array(terminals)
++++++++--    )
++++++++--    return new_dataset
++++++++--    
++++++++--
++++++++--
++++++++--
++++++++--def main():
++++++++--    # args = Parser().parse_args('plan')
++++++++--
++++++++--    #######################
++++++++--    ####### models ########
++++++++--    #######################
++++++++--
++++++++--
++++++++--
++++++++--
++++++++--
++++++++--    # print(args.dataset)
++++++++--
++++++++--    # dataset = utils.load_from_config(args.logbase, args.dataset, args.gpt_loadpath,
++++++++--    #         'data_config.pkl')
++++++++--
++++++++--
++++++++--    # gpt, gpt_epoch = utils.load_model(args.logbase, args.dataset, args.gpt_loadpath,
++++++++--    #         epoch=args.gpt_epoch, device=args.device)
++++++++--
++++++++--    # env = datasets.load_environment(args.dataset)
++++++++--
++++++++--    # discretizer = dataset.discretizer
++++++++--
++++++++--    # preprocess_fn = datasets.get_preprocess_fn(env.name)
++++++++--
++++++++--    # #######################
++++++++--    # ####### dataset #######
++++++++--    # #######################
++++++++--
++++++++--    # # env = datasets.load_environment(args.dataset)
++++++++--    # discretizer = dataset.discretizer
++++++++--    # preprocess_fn = datasets.get_preprocess_fn(env.name)
++++++++--
++++++++--    # # dataset
++++++++--    dataset_d3, env = d3rlpy.datasets.get_dataset("halfcheetah-medium-v2")
++++++++--
++++++++--    # env = gym.make('halfcheetah-medium-v2')
++++++++--    # dataset_d4 = d4rl.qlearning_dataset(env)
++++++++--
++++++++--    # # checks to see if d3rl & d4rl datasets are equal
++++++++--    # print(np.allclose(dataset_d3.actions[100], dataset_d4['actions'][100]))
++++++++--
++++++++--    # # dr4rl has same trajectories, just cut off 1 element before the end
++++++++--    # for j in range(1000):
++++++++--    #     for i in range(999):
++++++++--    #         if dataset_d4['rewards'][j * 999 + i] != dataset_d3.rewards[j * 1000 + i]: print("yo", i)
++++++++--
++++++++--    # #######################
++++++++--    # ###### main loop ######
++++++++--    # #######################
++++++++--
++++++++--    trajectory_length = 10 # 10 = max
++++++++--
++++++++--    # embeddings = []
++++++++--    # for i in range(1000):
++++++++--    #     observations = dataset_d3.observations[1000*i:1000*i+trajectory_length]
++++++++--    #     actions = dataset_d3.actions[1000*i:1000*i+trajectory_length]
++++++++--    #     rewards = dataset_d3.rewards[1000*i:1000*i+trajectory_length]
++++++++--    #     terminals = dataset_d3.terminals[1000*i:1000*i+trajectory_length]
++++++++--    #     emb = embed_trajectory(gpt, discretizer, observations, actions, rewards, preprocess_fn)
++++++++--    #     embeddings.append(emb)
++++++++--    # embeddings = np.array(embeddings)
++++++++--    # np.save("embeddings.npy", embeddings)
++++++++--    # print(embeddings)
++++++++--
++++++++--    embeddings = np.load("embeddings.npy")
++++++++--
++++++++--    pca = PCA(n_components=2)
++++++++--    pca = PCA(n_components=2)
++++++++--    pca_embeddings = pca.fit_transform(embeddings)
++++++++--    np.save("pca.py", pca_embeddings)
++++++++--
++++++++--    idxs_per_cluster, clusters = cluster_trajectories_2(embeddings)
++++++++--    # print(clusters)
++++++++--    # return
++++++++--    np.save("clusters.npy", clusters)
++++++++--
++++++++--    import matplotlib.pyplot as plt
++++++++--
++++++++--    d_orig = generate_data_embedding(embeddings)
++++++++--    unique_clusters = np.unique(clusters)
++++++++--    
++++++++--    d_j = []
++++++++--    complementary_datasets = []
++++++++--    for j in np.sort(unique_clusters):
++++++++--        print(j)
++++++++--        d_j.append(generate_data_embedding(embeddings[clusters != j]))
++++++++--        plt.scatter(pca_embeddings[clusters == j][:,0], pca_embeddings[clusters == j][:,1], label=j)
++++++++--        complementary_datasets.append(create_complementary_dataset(dataset_d3, idxs_per_cluster[j], trajectory_length))
++++++++--    
++++++++--    original_dataset = create_complementary_dataset(dataset_d3, [], trajectory_length)
++++++++--
++++++++--    print(complementary_datasets, original_dataset)
++++++++--
++++++++--    plt.legend()
++++++++--    plt.show()
++++++++--
++++++++--    agent_orig = d3rlpy.algos.SAC(
++++++++--        actor_learning_rate=3e-4,
++++++++--        critic_learning_rate=3e-4,
++++++++--        temp_learning_rate=3e-4,
++++++++--        batch_size=256)
++++++++--
++++++++--    print(agent_orig)
++++++++--
++++++++--    training_steps = 1000
++++++++--
++++++++--    agent_orig.fit(original_dataset, n_steps=training_steps)
++++++++--
++++++++--    agents_compl = []
++++++++--
++++++++--    for dset in complementary_datasets:
++++++++--        agent = d3rlpy.algos.SAC(
++++++++--            actor_learning_rate=3e-4,
++++++++--            critic_learning_rate=3e-4,
++++++++--            temp_learning_rate=3e-4,
++++++++--            batch_size=256)
++++++++--        agent.fit(dset, n_steps=training_steps)
++++++++--        agents_compl.append(agent)
++++++++--
++++++++--    action_orig = agent_orig.predict(dataset_d3.observations[0])
++++++++--
++++++++--    actions_compl = []
++++++++--    for agent in agents_compl:
++++++++--        actions_compl.append(agent.predict(dataset_d3.observations[0]))
++++++++--    
++++++++--    action_dists = []
++++++++--    for action in actions_compl:
++++++++--        action_dists.append(np.linalg.norm(action_orig-action))
++++++++--
++++++++--    k = 3
++++++++--    topk = np.argpartition(action_dists, -k)[-k:]
++++++++--
++++++++--    d_w = {}
++++++++--    for idx in topk:
++++++++--        d_w[idx] = wasserstein_distance(d_j[idx], d_orig)
++++++++--
++++++++--    cluster_assignment = min(d_w, key=d_w.get)
++++++++--    print("explanation assigned to cluster", cluster_assignment)
++++++++--
++++++++--    
++++++++--def assignment_test():
++++++++--    action_orig = np.random.rand(10)
++++++++--    d_orig = np.random.rand(5)
++++++++--
++++++++--    actions_compl = np.random.rand(6,10)
++++++++--    d_j = np.random.rand(6,5)
++++++++--
++++++++--    action_dists = []
++++++++--    for action in actions_compl:
++++++++--        action_dists.append(np.linalg.norm(action_orig-action))
++++++++--
++++++++--    print(action_dists)
++++++++--
++++++++--    k = 3
++++++++--    topk = np.argpartition(action_dists, -k)[-k:]
++++++++--
++++++++--    print(topk)
++++++++--
++++++++--    d_w = {}
++++++++--    for idx in topk:
++++++++--        d_w[idx] = wasserstein_distance(d_j[idx], d_orig)
++++++++--
++++++++--    print(d_w)
++++++++--
++++++++--    cluster_assignment = min(d_w, key=d_w.get)
++++++++--    print("explanation assigned to cluster", cluster_assignment)
++++++++--
++++++++--
++++++++--if __name__ == "__main__":
++++++++--    # main()
++++++++--    assignment_test()
++++++++-diff --git a/halfcheetah/trajectory.egg-info/PKG-INFO b/halfcheetah/trajectory.egg-info/PKG-INFO
++++++++-index 452c6cb..2603850 100644
++++++++---- a/halfcheetah/trajectory.egg-info/PKG-INFO
++++++++-+++ b/halfcheetah/trajectory.egg-info/PKG-INFO
++++++++-@@ -1,4 +1,11 @@
++++++++- Metadata-Version: 2.1
++++++++- Name: trajectory
++++++++- Version: 0.0.0
++++++++-+Summary: UNKNOWN
++++++++-+Home-page: UNKNOWN
++++++++-+License: UNKNOWN
++++++++-+Platform: UNKNOWN
++++++++- License-File: LICENSE
++++++++++    # Embedding is the average of encoded states
++++++++++    embedding = np.mean(output, axis=0)
++++++++++    return embedding
++++++++++
+++++++++ 
+++++++++ def create_complementary_dataset(dataset, idxs, trajectory_length=10, inverse=False):
+++++++++-    """TODO"""
++++++++++    """
++++++++++    Encode trajectory using a trajectory transformer with a sliding window.
++++++++++    
++++++++++    Args:
++++++++++    - dataset: MDPDataset, original d3rl dataset
++++++++++    - idxs: trajectory idxs to ignore (or include if inverse is True)
++++++++++    - trajectory_length: int, trajectory length
++++++++++    - inverse: bool, if True the dataset is not complementary
++++++++ +
++++++++-+UNKNOWN
++++++++++    Returns:
++++++++++    - new_dataset: MDPDataset, complementary dataset
++++++++++    """ 
+++++++++ 
+++++++++     observations = []
+++++++++     actions = []
+++++++++     rewards = []
+++++++++     terminals = []
+++++++++ 
+++++++++-    n_trajs = int(1000000/trajectory_length)
++++++++++    n_trajs = int(len(dataset.observations)/trajectory_length)
+++++++++     for i in range(n_trajs):
++++++++++        # If inverse is True, only include idxs. If not, leave out idxs
+++++++++         condition = i not in idxs
+++++++++         if inverse: condition = not condition
+++++++++ 
+++++++++@@ -112,6 +169,7 @@ def create_complementary_dataset(dataset, idxs, trajectory_length=10, inverse=Fa
+++++++++             actions += list(dataset.actions[trajectory_length*i:trajectory_length*(i+1)])
+++++++++             rewards += list(dataset.rewards[trajectory_length*i:trajectory_length*(i+1)])
+++++++++ 
++++++++++    # Trajectories end with a terminal state
+++++++++     terminals = np.tile([0]*(trajectory_length-1)+[1], int(len(observations)/trajectory_length))
+++++++++ 
+++++++++     new_dataset = d3rlpy.dataset.MDPDataset(
+++++++++@@ -124,6 +182,17 @@ def create_complementary_dataset(dataset, idxs, trajectory_length=10, inverse=Fa
+++++++++     
+++++++++ 
+++++++++ def clusters_to_idxs(clusters):
++++++++++    """
++++++++++    Helper function to turn array of cluster idxs per trajectory idxs to a list 
++++++++++    of trajectory idxs per cluster idx.
++++++++++    
++++++++++    Args:
++++++++++    - clusters: np.array, cluster idx per trajectory idx
++++++++ +
++++++++++    Returns:
++++++++++    - idxs_per_cluster: list, trajectory idxs per cluster idx
++++++++++    """ 
++++++++++
+++++++++     idxs_per_cluster = []
+++++++++     for i in np.sort(np.unique(clusters)):
+++++++++         idxs_per_cluster.append(list(np.argwhere(clusters == i).flatten()))
+++++++++@@ -142,7 +211,7 @@ def main():
+++++++++ 
+++++++++     ### IMPORTANT DEFINITIONS XRL SCRIPT ###
+++++++++ 
+++++++++-    load_embeddings = False
++++++++++    load_embeddings = True
+++++++++     load_clusters = True
+++++++++     load_agents = True
+++++++++     generate_human_study = False
++++++++ diff --git a/halfcheetah/trajectory.egg-info/SOURCES.txt b/halfcheetah/trajectory.egg-info/SOURCES.txt
++++++++-index 4474d85..84e8e3a 100644
+++++++++index 84e8e3a..4474d85 100644
++++++++ --- a/halfcheetah/trajectory.egg-info/SOURCES.txt
++++++++ +++ b/halfcheetah/trajectory.egg-info/SOURCES.txt
++++++++-@@ -30,4 +30,5 @@ trajectory/utils/serialization.py
+++++++++@@ -30,5 +30,4 @@ trajectory/utils/serialization.py
++++++++  trajectory/utils/setup.py
++++++++  trajectory/utils/timer.py
++++++++  trajectory/utils/training.py
++++++++ -trajectory/utils/video.py
+++++++++-trajectory_aaa/__init__.py
++++++++ \ No newline at end of file
++++++++ +trajectory/utils/video.py
++++++++-+trajectory_aaa/__init__.py
++++++++ \ No newline at end of file
++++++++ diff --git a/halfcheetah/trajectory.egg-info/top_level.txt b/halfcheetah/trajectory.egg-info/top_level.txt
++++++++-index ce65198..1d5271f 100644
+++++++++index 1d5271f..ce65198 100644
++++++++ --- a/halfcheetah/trajectory.egg-info/top_level.txt
++++++++ +++ b/halfcheetah/trajectory.egg-info/top_level.txt
++++++++-@@ -1 +1,2 @@
+++++++++@@ -1,2 +1 @@
++++++++  trajectory
++++++++-+trajectory_aaa
+++++++ \ No newline at end of file
+++++++-diff --git a/halfcheetah/pca.py.npy b/halfcheetah/pca.py.npy
+++++++-deleted file mode 100644
+++++++-index bb19150..0000000
+++++++-Binary files a/halfcheetah/pca.py.npy and /dev/null differ
+++++++-diff --git a/halfcheetah/plotting/bar.png b/halfcheetah/plotting/bar.png
+++++++-deleted file mode 100644
+++++++-index 3679667..0000000
+++++++-Binary files a/halfcheetah/plotting/bar.png and /dev/null differ
+++++++-diff --git a/halfcheetah/plotting/plot.py b/halfcheetah/plotting/plot.py
+++++++-deleted file mode 100644
+++++++-index 163d0e4..0000000
+++++++---- a/halfcheetah/plotting/plot.py
+++++++-+++ /dev/null
+++++++-@@ -1,74 +0,0 @@
+++++++--import numpy as np
+++++++--import matplotlib
+++++++--import matplotlib.pyplot as plt
+++++++--import pdb
+++++++--
+++++++--from plotting.scores import means
+++++++--
+++++++--class Colors:
+++++++--	grey = '#B4B4B4'
+++++++--	gold = '#F6C781'
+++++++--	red = '#EC7C7D'
+++++++--	blue = '#70ABCC'
+++++++--
+++++++--LABELS = {
+++++++--	# 'BC': 'Behavior\nCloning',
+++++++--	# 'MBOP': 'Model-Based\nOffline Planning',
+++++++--	# 'BRAC': 'Behavior-Reg.\nActor-Critic',
+++++++--	# 'CQL': 'Conservative\nQ-Learning',
+++++++--}
+++++++--
+++++++--def get_mean(results, exclude=None):
+++++++--	'''
+++++++--		results : { environment: score, ... }
+++++++--	'''
+++++++--	filtered = {
+++++++--		k: v for k, v in results.items()
+++++++--		if (not exclude) or (exclude and exclude not in k)
+++++++--	}
+++++++--	return np.mean(list(filtered.values()))
+++++++--
+++++++--if __name__ == '__main__':
+++++++--
+++++++--	#################
+++++++--	## latex
+++++++--	#################
+++++++--	matplotlib.rc('font', **{'family': 'serif', 'serif': ['Computer Modern']})
+++++++--	matplotlib.rc('text', usetex=True)
+++++++--	matplotlib.rcParams['text.latex.preamble']=[r"\usepackage{amsmath}"]
+++++++--	#################
+++++++--
+++++++--	fig = plt.gcf()
+++++++--	ax = plt.gca()
+++++++--	fig.set_size_inches(7.5, 2.5)
+++++++--
+++++++--	means = {k: get_mean(v, exclude='ant') for k, v in means.items()}
+++++++--	print(means)
+++++++--
+++++++--	algs = ['BC', 'MBOP', 'BRAC', 'CQL', 'Decision\nTransformer', 'Trajectory\nTransformer']
+++++++--	vals = [means[alg] for alg in algs]
+++++++--
+++++++--	colors = [
+++++++--		Colors.grey, Colors.gold,
+++++++--		Colors.red, Colors.red, Colors.blue, Colors.blue
+++++++--	]
+++++++--
+++++++--	labels = [LABELS.get(alg, alg) for alg in algs]
+++++++--	plt.bar(labels, vals, color=colors, edgecolor=Colors.gold, lw=0)
+++++++--	plt.ylabel('Average normalized return', labelpad=15)
+++++++--	# plt.title('Offline RL Results')
+++++++--
+++++++--	legend_labels = ['Behavior Cloning', 'Trajectory Optimization', 'Temporal Difference', 'Sequence Modeling']
+++++++--	colors = [Colors.grey, Colors.gold, Colors.red, Colors.blue]
+++++++--	handles = [plt.Rectangle((0,0),1,1, color=color) for label, color in zip(legend_labels, colors)]
+++++++--	plt.legend(handles, legend_labels, ncol=4,
+++++++--		bbox_to_anchor=(1.07, -.18), fancybox=False, framealpha=0, shadow=False, columnspacing=1.5, handlelength=1.5)
+++++++--
+++++++--	matplotlib.rcParams['hatch.linewidth'] = 7.5
+++++++--	# ax.patches[-1].set_hatch('/')
+++++++--
+++++++--	ax.spines['right'].set_visible(False)
+++++++--	ax.spines['top'].set_visible(False)
+++++++--
+++++++--	# plt.savefig('plotting/bar.pdf', bbox_inches='tight')
+++++++--	plt.savefig('plotting/bar.png', bbox_inches='tight', dpi=500)
+++++++-diff --git a/halfcheetah/plotting/read_results.py b/halfcheetah/plotting/read_results.py
+++++++-deleted file mode 100644
+++++++-index 5a5fb62..0000000
+++++++---- a/halfcheetah/plotting/read_results.py
+++++++-+++ /dev/null
+++++++-@@ -1,70 +0,0 @@
+++++++--import os
+++++++--import glob
+++++++--import numpy as np
+++++++--import json
+++++++--import pdb
+++++++--
+++++++--import trajectory.utils as utils
+++++++--
+++++++--DATASETS = [
+++++++--	f'{env}-{buffer}'
+++++++--	for env in ['hopper', 'walker2d', 'halfcheetah', 'ant']
+++++++--	for buffer in ['medium-expert-v2', 'medium-v2', 'medium-replay-v2']
+++++++--]
+++++++--
+++++++--LOGBASE = 'logs'
+++++++--TRIAL = '*'
+++++++--EXP_NAME = 'plans/pretrained'
+++++++--
+++++++--def load_results(paths):
+++++++--	'''
+++++++--		paths : path to directory containing experiment trials
+++++++--	'''
+++++++--	scores = []
+++++++--	for i, path in enumerate(sorted(paths)):
+++++++--		score = load_result(path)
+++++++--		if score is None:
+++++++--			print(f'Skipping {path}')
+++++++--			continue
+++++++--		scores.append(score)
+++++++--
+++++++--		suffix = path.split('/')[-1]
+++++++--
+++++++--	mean = np.mean(scores)
+++++++--	err = np.std(scores) / np.sqrt(len(scores))
+++++++--	return mean, err, scores
+++++++--
+++++++--def load_result(path):
+++++++--	'''
+++++++--		path : path to experiment directory; expects `rollout.json` to be in directory
+++++++--	'''
+++++++--	fullpath = os.path.join(path, 'rollout.json')
+++++++--	suffix = path.split('/')[-1]
+++++++--
+++++++--	if not os.path.exists(fullpath):
+++++++--		return None
+++++++--
+++++++--	results = json.load(open(fullpath, 'rb'))
+++++++--	score = results['score']
+++++++--	return score * 100
+++++++--
+++++++--#######################
+++++++--######## setup ########
+++++++--#######################
+++++++--
+++++++--if __name__ == '__main__':
+++++++--
+++++++--	class Parser(utils.Parser):
+++++++--	    dataset: str = None
+++++++--
+++++++--	args = Parser().parse_args()
+++++++--
+++++++--	for dataset in ([args.dataset] if args.dataset else DATASETS):
+++++++--		subdirs = glob.glob(os.path.join(LOGBASE, dataset, EXP_NAME))
+++++++--
+++++++--		for subdir in subdirs:
+++++++--			reldir = subdir.split('/')[-1]
+++++++--			paths = glob.glob(os.path.join(subdir, TRIAL))
+++++++--
+++++++--			mean, err, scores = load_results(paths)
+++++++--			print(f'{dataset.ljust(30)} | {subdir.ljust(50)} | {len(scores)} scores \n    {mean:.2f} +/- {err:.2f}\n')
+++++++-diff --git a/halfcheetah/plotting/scores.py b/halfcheetah/plotting/scores.py
+++++++-deleted file mode 100644
+++++++-index f1917f7..0000000
+++++++---- a/halfcheetah/plotting/scores.py
+++++++-+++ /dev/null
+++++++-@@ -1,123 +0,0 @@
+++++++--means = {
+++++++--	'Trajectory\nTransformer': {
+++++++--		##
+++++++--		'halfcheetah-medium-expert-v2': 95.0,
+++++++--		'hopper-medium-expert-v2': 110.0,
+++++++--		'walker2d-medium-expert-v2': 101.9,
+++++++--		'ant-medium-expert-v2': 116.1,
+++++++--		##
+++++++--		'halfcheetah-medium-v2': 46.9,
+++++++--		'hopper-medium-v2': 61.1,
+++++++--		'walker2d-medium-v2': 79.0,
+++++++--		'ant-medium-v2': 83.1,
+++++++--		##
+++++++--		'halfcheetah-medium-replay-v2': 41.9,
+++++++--		'hopper-medium-replay-v2': 91.5,
+++++++--		'walker2d-medium-replay-v2': 82.6,
+++++++--		'ant-medium-replay-v2': 77.0,
+++++++--	},
+++++++--	'Decision\nTransformer': {
+++++++--		##
+++++++--		'halfcheetah-medium-expert-v2': 86.8,
+++++++--		'hopper-medium-expert-v2': 107.6,
+++++++--		'walker2d-medium-expert-v2': 108.1,
+++++++--		##
+++++++--		'halfcheetah-medium-v2': 42.6,
+++++++--		'hopper-medium-v2': 67.6,
+++++++--		'walker2d-medium-v2': 74.0,
+++++++--		##
+++++++--		'halfcheetah-medium-replay-v2': 36.6,
+++++++--		'hopper-medium-replay-v2': 82.7,
+++++++--		'walker2d-medium-replay-v2': 66.6,
+++++++--	},
+++++++--	'CQL': {
+++++++--		##
+++++++--		'halfcheetah-medium-expert-v2': 91.6,
+++++++--		'hopper-medium-expert-v2': 105.4,
+++++++--		'walker2d-medium-expert-v2': 108.8,
+++++++--		##
+++++++--		'halfcheetah-medium-v2': 44.0,
+++++++--		'hopper-medium-v2': 58.5,
+++++++--		'walker2d-medium-v2': 72.5,
+++++++--		##
+++++++--		'halfcheetah-medium-replay-v2': 45.5,
+++++++--		'hopper-medium-replay-v2': 95.0,
+++++++--		'walker2d-medium-replay-v2': 77.2,
+++++++--	},
+++++++--	'MOPO': {
+++++++--		##
+++++++--		'halfcheetah-medium-expert-v2': 63.3,
+++++++--		'hopper-medium-expert-v2': 23.7,
+++++++--		'walker2d-medium-expert-v2': 44.6,
+++++++--		##
+++++++--		'halfcheetah-medium-v2': 42.3,
+++++++--		'hopper-medium-v2': 28.0,
+++++++--		'walker2d-medium-v2': 17.8,
+++++++--		##
+++++++--		'halfcheetah-medium-replay-v2': 53.1,
+++++++--		'hopper-medium-replay-v2': 67.5,
+++++++--		'walker2d-medium-replay-v2':39.0,
+++++++--	},
+++++++--	'MBOP': {
+++++++--		##
+++++++--		'halfcheetah-medium-expert-v2': 105.9,
+++++++--		'hopper-medium-expert-v2': 55.1,
+++++++--		'walker2d-medium-expert-v2': 70.2,
+++++++--		##
+++++++--		'halfcheetah-medium-v2': 44.6,
+++++++--		'hopper-medium-v2': 48.8,
+++++++--		'walker2d-medium-v2': 41.0,
+++++++--		##
+++++++--		'halfcheetah-medium-replay-v2': 42.3,
+++++++--		'hopper-medium-replay-v2': 12.4,
+++++++--		'walker2d-medium-replay-v2': 9.7,
+++++++--	},
+++++++--	'BRAC': {
+++++++--		##
+++++++--		'halfcheetah-medium-expert-v2': 41.9,
+++++++--		'hopper-medium-expert-v2': 0.9,
+++++++--		'walker2d-medium-expert-v2': 81.6,
+++++++--		##
+++++++--		'halfcheetah-medium-v2': 46.3,
+++++++--		'hopper-medium-v2': 31.3,
+++++++--		'walker2d-medium-v2': 81.1,
+++++++--		##
+++++++--		'halfcheetah-medium-replay-v2': 47.7,
+++++++--		'hopper-medium-replay-v2': 0.6,
+++++++--		'walker2d-medium-replay-v2': 0.9,
+++++++--	},
+++++++--	'BC': {
+++++++--		##
+++++++--		'halfcheetah-medium-expert-v2': 59.9,
+++++++--		'hopper-medium-expert-v2': 79.6,
+++++++--		'walker2d-medium-expert-v2': 36.6,
+++++++--		##
+++++++--		'halfcheetah-medium-v2': 43.1,
+++++++--		'hopper-medium-v2': 63.9,
+++++++--		'walker2d-medium-v2': 77.3,
+++++++--		##
+++++++--		'halfcheetah-medium-replay-v2': 4.3,
+++++++--		'hopper-medium-replay-v2': 27.6,
+++++++--		'walker2d-medium-replay-v2': 36.9,
+++++++--	},
+++++++--}
+++++++--
+++++++--errors = {
+++++++--	'Trajectory\nTransformer': {
+++++++--		##
+++++++--		'halfcheetah-medium-expert-v2': 0.2,
+++++++--		'hopper-medium-expert-v2': 2.7,
+++++++--		'walker2d-medium-expert-v2': 6.8,
+++++++--		'ant-medium-expert-v2': 9.0,
+++++++--		##
+++++++--		'halfcheetah-medium-v2': 0.4,
+++++++--		'hopper-medium-v2': 3.6,
+++++++--		'walker2d-medium-v2': 2.8,
+++++++--		'ant-medium-v2': 7.3,
+++++++--		##
+++++++--		'halfcheetah-medium-replay-v2': 2.5,
+++++++--		'hopper-medium-replay-v2': 3.6,
+++++++--		'walker2d-medium-replay-v2': 6.9,
+++++++--		'ant-medium-replay-v2': 6.8,
+++++++--	},
+++++++--}
+++++++++-trajectory_aaa
+++++++++diff --git a/seaquest/readme.md b/seaquest/readme.md
+++++++++index 84e53f8..53561f9 100644
+++++++++--- a/seaquest/readme.md
++++++++++++ b/seaquest/readme.md
+++++++++@@ -10,4 +10,4 @@ pip install git+https://github.com/takuseno/d4rl-atari
+++++++++ pip install "gym[atari, accept-rom-license]"
+++++++++ pip install pyclustering
+++++++++ pip install seaborn
+++++++++-pip install d3rlpy==1.1.1
+++++++++\ No newline at end of file
++++++++++pip install d3rlpy==1.1.1
+++++++ \ No newline at end of file
+++++++-diff --git a/halfcheetah/plotting/table.py b/halfcheetah/plotting/table.py
+++++++-deleted file mode 100644
+++++++-index eae74e6..0000000
+++++++---- a/halfcheetah/plotting/table.py
+++++++-+++ /dev/null
+++++++-@@ -1,127 +0,0 @@
+++++++--import numpy as np
+++++++--import pdb
+++++++--
+++++++--from plotting.plot import get_mean
+++++++--from plotting.scores import (
+++++++--	means as MEANS,
+++++++--	errors as ERRORS,
+++++++--)
+++++++--
+++++++--ALGORITHM_STRINGS = {
+++++++--	'Trajectory\nTransformer': 'TT (Ours)',
+++++++--	'Decision\nTransformer': 'DT',	
+++++++--}
+++++++--
+++++++--BUFFER_STRINGS = {
+++++++--	'medium-expert': 'Medium-Expert',
+++++++--	'medium': 'Medium',
+++++++--	'medium-replay': 'Medium-Replay',	
+++++++--}
+++++++--
+++++++--ENVIRONMENT_STRINGS = {
+++++++--	'halfcheetah': 'HalfCheetah',
+++++++--	'hopper': 'Hopper',
+++++++--	'walker2d': 'Walker2d',
+++++++--	'ant': 'Ant',
+++++++--}
+++++++--
+++++++--SHOW_ERRORS = ['Trajectory\nTransformer']
+++++++--
+++++++--def get_result(algorithm, buffer, environment, version='v2'):
+++++++--	key = f'{environment}-{buffer}-{version}'
+++++++--	mean = MEANS[algorithm].get(key, '-')
+++++++--	if algorithm in SHOW_ERRORS:
+++++++--		error = ERRORS[algorithm].get(key)
+++++++--		return (mean, error)
+++++++--	else:
+++++++--		return mean
+++++++--
+++++++--def format_result(result):
+++++++--	if type(result) == tuple:
+++++++--		mean, std = result
+++++++--		return f'${mean}$ \\scriptsize{{\\raisebox{{1pt}}{{$\\pm {std}$}}}}'
+++++++--	else:
+++++++--		return f'${result}$'
+++++++--
+++++++--def format_row(buffer, environment, results):
+++++++--	buffer_str = BUFFER_STRINGS[buffer]
+++++++--	environment_str = ENVIRONMENT_STRINGS[environment]
+++++++--	results_str = ' & '.join(format_result(result) for result in results)
+++++++--	row = f'{buffer_str} & {environment_str} & {results_str} \\\\ \n'
+++++++--	return row
+++++++--
+++++++--def format_buffer_block(algorithms, buffer, environments):
+++++++--	block_str = '\\midrule\n'
+++++++--	for environment in environments:
+++++++--		results = [get_result(alg, buffer, environment) for alg in algorithms]
+++++++--		row_str = format_row(buffer, environment, results)
+++++++--		block_str += row_str
+++++++--	return block_str
+++++++--
+++++++--def format_algorithm(algorithm):
+++++++--	algorithm_str = ALGORITHM_STRINGS.get(algorithm, algorithm)
+++++++--	return f'\multicolumn{{1}}{{c}}{{\\bf {algorithm_str}}}'
+++++++--
+++++++--def format_algorithms(algorithms):
+++++++--	return ' & '.join(format_algorithm(algorithm) for algorithm in algorithms)
+++++++--
+++++++--def format_averages(means, label):
+++++++--	prefix = f'\\multicolumn{{2}}{{c}}{{\\bf Average ({label})}} & '
+++++++--	formatted = ' & '.join(str(mean) for mean in means)
+++++++--	return prefix + formatted
+++++++--
+++++++--def format_averages_block(algorithms):
+++++++--	means_filtered = [np.round(get_mean(MEANS[algorithm], exclude='ant'), 1) for algorithm in algorithms]
+++++++--	means_all = [np.round(get_mean(MEANS[algorithm], exclude=None), 1) for algorithm in algorithms]
+++++++--
+++++++--	means_all = [
+++++++--		means
+++++++--		if 'ant-medium-expert-v2' in MEANS[algorithm]
+++++++--		else '$-$'
+++++++--		for algorithm, means in zip(algorithms, means_all)
+++++++--	]
+++++++--
+++++++--	formatted_filtered = format_averages(means_filtered, 'without Ant')
+++++++--	formatted_all = format_averages(means_all, 'all settings')
+++++++--
+++++++--	formatted_block = (
+++++++--		f'{formatted_filtered} \\hspace{{.6cm}} \\\\ \n'
+++++++--		f'{formatted_all} \\hspace{{.6cm}} \\\\ \n'
+++++++--	)
+++++++--	return formatted_block
+++++++--
+++++++--def format_table(algorithms, buffers, environments):
+++++++--	justify_str = 'll' + 'r' * len(algorithms)
+++++++--	algorithm_str = format_algorithms(['Dataset', 'Environment'] + algorithms)
+++++++--	averages_str = format_averages_block(algorithms)
+++++++--	table_prefix = (
+++++++--		'\\begin{table*}[h]\n'
+++++++--		'\\centering\n'
+++++++--		'\\small\n'
+++++++--		f'\\begin{{tabular}}{{{justify_str}}}\n'
+++++++--		'\\toprule\n'
+++++++--		f'{algorithm_str} \\\\ \n'
+++++++--	)
+++++++--	table_suffix = (
+++++++--		'\\midrule\n'
+++++++--		f'{averages_str}'
+++++++--		'\\bottomrule\n'
+++++++--		'\\end{tabular}\n'
+++++++--		'\\label{table:d4rl}\n'
+++++++--		'\\end{table*}'
+++++++--	)
+++++++--	blocks = ''.join(format_buffer_block(algorithms, buffer, environments) for buffer in buffers)
+++++++--	table = (
+++++++--		f'{table_prefix}'
+++++++--		f'{blocks}'
+++++++--		f'{table_suffix}'
+++++++--	)
+++++++--	return table
+++++++--
+++++++--
+++++++--algorithms =['BC', 'MBOP', 'BRAC', 'CQL',  'Decision\nTransformer', 'Trajectory\nTransformer']
+++++++--buffers = ['medium-expert', 'medium', 'medium-replay']
+++++++--environments = ['halfcheetah', 'hopper', 'walker2d', 'ant']
+++++++--
+++++++--table = format_table(algorithms, buffers, environments)
+++++++--print(table)
+++++++-diff --git a/halfcheetah/scripts/plan.py b/halfcheetah/scripts/plan.py
+++++++-deleted file mode 100644
+++++++-index f13d4cc..0000000
+++++++---- a/halfcheetah/scripts/plan.py
+++++++-+++ /dev/null
+++++++-@@ -1,124 +0,0 @@
+++++++--import json
+++++++--import pdb
+++++++--from os.path import join
+++++++--
+++++++--import trajectory.utils as utils
+++++++--import trajectory.datasets as datasets
+++++++--from trajectory.search import (
+++++++--    beam_plan,
+++++++--    make_prefix,
+++++++--    extract_actions,
+++++++--    update_context,
+++++++--)
+++++++--
+++++++--class Parser(utils.Parser):
+++++++--    dataset: str = 'halfcheetah-medium-expert-v2'
+++++++--    config: str = 'config.offline'
+++++++--
+++++++--#######################
+++++++--######## setup ########
+++++++--#######################
+++++++--
+++++++--args = Parser().parse_args('plan')
+++++++--
+++++++--#######################
+++++++--####### models ########
+++++++--#######################
+++++++--
+++++++--dataset = utils.load_from_config(args.logbase, args.dataset, args.gpt_loadpath,
+++++++--        'data_config.pkl')
+++++++--
+++++++--gpt, gpt_epoch = utils.load_model(args.logbase, args.dataset, args.gpt_loadpath,
+++++++--        epoch=args.gpt_epoch, device=args.device)
+++++++--
+++++++--#######################
+++++++--####### dataset #######
+++++++--#######################
+++++++--
+++++++--env = datasets.load_environment(args.dataset)
+++++++--print('yo')
+++++++--renderer = utils.make_renderer(args)
+++++++--timer = utils.timer.Timer()
+++++++--
+++++++--discretizer = dataset.discretizer
+++++++--discount = dataset.discount
+++++++--observation_dim = dataset.observation_dim
+++++++--action_dim = dataset.action_dim
+++++++--
+++++++--value_fn = lambda x: discretizer.value_fn(x, args.percentile)
+++++++--preprocess_fn = datasets.get_preprocess_fn(env.name)
+++++++--
+++++++--print('yo2')
+++++++--
+++++++--#######################
+++++++--###### main loop ######
+++++++--#######################
+++++++--
+++++++--observation = env.reset()
+++++++--total_reward = 0
+++++++--
+++++++--## observations for rendering
+++++++--rollout = [observation.copy()]
+++++++--
+++++++--## previous (tokenized) transitions for conditioning transformer
+++++++--context = []
+++++++--
+++++++--T = env.max_episode_steps
+++++++--for t in range(T):
+++++++--
+++++++--    observation = preprocess_fn(observation)
+++++++--
+++++++--    if t % args.plan_freq == 0:
+++++++--        ## concatenate previous transitions and current observations to input to model
+++++++--        prefix = make_prefix(discretizer, context, observation, args.prefix_context)
+++++++--
+++++++--        ## sample sequence from model beginning with `prefix`
+++++++--        sequence = beam_plan(
+++++++--            gpt, value_fn, prefix,
+++++++--            args.horizon, args.beam_width, args.n_expand, observation_dim, action_dim,
+++++++--            discount, args.max_context_transitions, verbose=args.verbose,
+++++++--            k_obs=args.k_obs, k_act=args.k_act, cdf_obs=args.cdf_obs, cdf_act=args.cdf_act,
+++++++--        )
+++++++--
+++++++--    else:
+++++++--        sequence = sequence[1:]
+++++++--
+++++++--    ## [ horizon x transition_dim ] convert sampled tokens to continuous trajectory
+++++++--    sequence_recon = discretizer.reconstruct(sequence)
+++++++--
+++++++--    ## [ action_dim ] index into sampled trajectory to grab first action
+++++++--    action = extract_actions(sequence_recon, observation_dim, action_dim, t=0)
+++++++--
+++++++--    ## execute action in environment
+++++++--    next_observation, reward, terminal, _ = env.step(action)
+++++++--
+++++++--    ## update return
+++++++--    total_reward += reward
+++++++--    score = env.get_normalized_score(total_reward)
+++++++--
+++++++--    ## update rollout observations and context transitions
+++++++--    rollout.append(next_observation.copy())
+++++++--    context = update_context(context, discretizer, observation, action, reward, args.max_context_transitions)
+++++++--
+++++++--    print(
+++++++--        f'[ plan ] t: {t} / {T} | r: {reward:.2f} | R: {total_reward:.2f} | score: {score:.4f} | '
+++++++--        f'time: {timer():.2f} | {args.dataset} | {args.exp_name} | {args.suffix}\n'
+++++++--    )
+++++++--
+++++++--    ## visualization
+++++++--    if t % args.vis_freq == 0 or terminal or t == T:
+++++++--
+++++++--        ## save current plan
+++++++--        renderer.render_plan(join(args.savepath, f'{t}_plan.mp4'), sequence_recon, env.state_vector())
+++++++--
+++++++--        ## save rollout thus far
+++++++--        renderer.render_rollout(join(args.savepath, f'rollout.mp4'), rollout, fps=80)
+++++++--
+++++++--    if terminal: break
+++++++--
+++++++--    observation = next_observation
+++++++--
+++++++--## save result as a json file
+++++++--json_path = join(args.savepath, 'rollout.json')
+++++++--json_data = {'score': score, 'step': t, 'return': total_reward, 'term': terminal, 'gpt_epoch': gpt_epoch}
+++++++--json.dump(json_data, open(json_path, 'w'), indent=2, sort_keys=True)
+++++++-diff --git a/halfcheetah/scripts/train.py b/halfcheetah/scripts/train.py
+++++++-deleted file mode 100644
+++++++-index 04af8d7..0000000
+++++++---- a/halfcheetah/scripts/train.py
+++++++-+++ /dev/null
+++++++-@@ -1,122 +0,0 @@
+++++++--import os
+++++++--import numpy as np
+++++++--import torch
+++++++--import pdb
+++++++--
+++++++--import trajectory.utils as utils
+++++++--import trajectory.datasets as datasets
+++++++--from trajectory.models.transformers import GPT
+++++++--
+++++++--
+++++++--class Parser(utils.Parser):
+++++++--    dataset: str = 'halfcheetah-medium-expert-v2'
+++++++--    config: str = 'config.offline'
+++++++--
+++++++--#######################
+++++++--######## setup ########
+++++++--#######################
+++++++--
+++++++--args = Parser().parse_args('train')
+++++++--
+++++++--#######################
+++++++--####### dataset #######
+++++++--#######################
+++++++--
+++++++--env = datasets.load_environment(args.dataset)
+++++++--
+++++++--sequence_length = args.subsampled_sequence_length * args.step
+++++++--
+++++++--dataset_config = utils.Config(
+++++++--    datasets.DiscretizedDataset,
+++++++--    savepath=(args.savepath, 'data_config.pkl'),
+++++++--    env=args.dataset,
+++++++--    N=args.N,
+++++++--    penalty=args.termination_penalty,
+++++++--    sequence_length=sequence_length,
+++++++--    step=args.step,
+++++++--    discount=args.discount,
+++++++--    discretizer=args.discretizer,
+++++++--)
+++++++--
+++++++--dataset = dataset_config()
+++++++--obs_dim = dataset.observation_dim
+++++++--act_dim = dataset.action_dim
+++++++--transition_dim = dataset.joined_dim
+++++++--
+++++++--#######################
+++++++--######## model ########
+++++++--#######################
+++++++--
+++++++--block_size = args.subsampled_sequence_length * transition_dim - 1
+++++++--print(
+++++++--    f'Dataset size: {len(dataset)} | '
+++++++--    f'Joined dim: {transition_dim} '
+++++++--    f'(observation: {obs_dim}, action: {act_dim}) | Block size: {block_size}'
+++++++--)
+++++++--
+++++++--model_config = utils.Config(
+++++++--    GPT,
+++++++--    savepath=(args.savepath, 'model_config.pkl'),
+++++++--    ## discretization
+++++++--    vocab_size=args.N, block_size=block_size,
+++++++--    ## architecture
+++++++--    n_layer=args.n_layer, n_head=args.n_head, n_embd=args.n_embd*args.n_head,
+++++++--    ## dimensions
+++++++--    observation_dim=obs_dim, action_dim=act_dim, transition_dim=transition_dim,
+++++++--    ## loss weighting
+++++++--    action_weight=args.action_weight, reward_weight=args.reward_weight, value_weight=args.value_weight,
+++++++--    ## dropout probabilities
+++++++--    embd_pdrop=args.embd_pdrop, resid_pdrop=args.resid_pdrop, attn_pdrop=args.attn_pdrop,
+++++++--)
+++++++--
+++++++--model = model_config()
+++++++--model.to(args.device)
+++++++--
+++++++--#######################
+++++++--####### trainer #######
+++++++--#######################
+++++++--
+++++++--warmup_tokens = len(dataset) * block_size ## number of tokens seen per epoch
+++++++--final_tokens = 20 * warmup_tokens
+++++++--
+++++++--trainer_config = utils.Config(
+++++++--    utils.Trainer,
+++++++--    savepath=(args.savepath, 'trainer_config.pkl'),
+++++++--    # optimization parameters
+++++++--    batch_size=args.batch_size,
+++++++--    learning_rate=args.learning_rate,
+++++++--    betas=(0.9, 0.95),
+++++++--    grad_norm_clip=1.0,
+++++++--    weight_decay=0.1, # only applied on matmul weights
+++++++--    # learning rate decay: linear warmup followed by cosine decay to 10% of original
+++++++--    lr_decay=args.lr_decay,
+++++++--    warmup_tokens=warmup_tokens,
+++++++--    final_tokens=final_tokens,
+++++++--    ## dataloader
+++++++--    num_workers=0,
+++++++--    device=args.device,
+++++++--)
+++++++--
+++++++--trainer = trainer_config()
+++++++--
+++++++--#######################
+++++++--###### main loop ######
+++++++--#######################
+++++++--
+++++++--## scale number of epochs to keep number of updates constant
+++++++--n_epochs = int(1e6 / len(dataset) * args.n_epochs_ref)
+++++++--save_freq = int(n_epochs // args.n_saves)
+++++++--
+++++++--for epoch in range(n_epochs):
+++++++--    print(f'\nEpoch: {epoch} / {n_epochs} | {args.dataset} | {args.exp_name}')
+++++++--
+++++++--    trainer.train(model, dataset)
+++++++--
+++++++--    ## get greatest multiple of `save_freq` less than or equal to `save_epoch`
+++++++--    save_epoch = (epoch + 1) // save_freq * save_freq
+++++++--    statepath = os.path.join(args.savepath, f'state_{save_epoch}.pt')
+++++++--    print(f'Saving model to {statepath}')
+++++++--
+++++++--    ## save state to disk
+++++++--    state = model.state_dict()
+++++++--    torch.save(state, statepath)
+++++++-diff --git a/halfcheetah/scripts/xrl.py b/halfcheetah/scripts/xrl.py
+++++++-deleted file mode 100644
+++++++-index 134232a..0000000
+++++++---- a/halfcheetah/scripts/xrl.py
+++++++-+++ /dev/null
+++++++-@@ -1,372 +0,0 @@
+++++++--import json
+++++++--import pdb
+++++++--from os.path import join
+++++++--
+++++++--import trajectory.utils as utils
+++++++--import trajectory.datasets as datasets
+++++++--from trajectory.search import (
+++++++--    make_prefix,
+++++++--    update_context,
+++++++--)
+++++++--from trajectory.search.sampling import forward
+++++++--
+++++++--import gym
+++++++--import d4rl # Import required to register environments, you may need to also import the submodule
+++++++--import numpy as np
+++++++--import d3rlpy
+++++++--import math as mt
+++++++--from sklearn.cluster import KMeans
+++++++--from sklearn import datasets as skdatasets
+++++++--from sklearn.decomposition import PCA
+++++++--
+++++++--from pyclustering.cluster.xmeans import xmeans
+++++++--from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer
+++++++--
+++++++--from scipy.stats import wasserstein_distance
+++++++--
+++++++--class Parser(utils.Parser):
+++++++--    dataset: str = 'halfcheetah-medium-expert-v2'
+++++++--    config: str = 'config.offline'
+++++++--
++++++++diff --git a/halfcheetah/scripts/xrl_v2.py b/halfcheetah/scripts/xrl_v2.py
++++++++index 62a3d4d..b012599 100644
++++++++--- a/halfcheetah/scripts/xrl_v2.py
+++++++++++ b/halfcheetah/scripts/xrl_v2.py
++++++++@@ -21,54 +21,97 @@ from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer
++++++++ from scipy.stats import wasserstein_distance
++++++++ from moviepy.editor import VideoFileClip
++++++++ 
+++++++++
++++++++ class Parser(utils.Parser):
++++++++     dataset: str = 'halfcheetah-medium-v2'
++++++++     config: str = 'config.offline'
++++++++ 
+++++++ -# utils
+++++++--    
+++++++--class XMeans:
+++++++--    def loglikelihood(self, r, rn, var, m, k):
+++++++--        l1 = - rn / 2.0 * mt.log(2 * mt.pi)
+++++++--        l2 = - rn * m / 2.0 * mt.log(var)
+++++++--        l3 = - (rn - k) / 2.0
+++++++--        l4 = rn * mt.log(rn)
+++++++--        l5 = - rn * mt.log(r)
+++++++--
+++++++--        return l1 + l2 + l3 + l4 + l5
+++++++--
+++++++--    def __init__(self, X, kmax = 20):
+++++++--        self.X = X
+++++++--        self.num = np.size(self.X, axis=0)
+++++++--        self.dim = np.size(X, axis=1)
+++++++--        self.KMax = kmax
+++++++--
+++++++--    def fit(self):
+++++++--        k = 1
+++++++--        X = self.X
+++++++--        M = self.dim
+++++++--        num = self.num
+++++++--
+++++++--        while(1):
+++++++--            ok = k
+++++++--
+++++++--            #Improve Params
+++++++--            kmeans = KMeans(n_clusters=k).fit(X)
+++++++--            labels = kmeans.labels_
+++++++--            m = kmeans.cluster_centers_
+++++++--
+++++++--            #Improve Structure
+++++++--            #Calculate BIC
+++++++--            p = M + 1
+++++++--
+++++++--            obic = np.zeros(k)
+++++++--
+++++++--            for i in range(k):
+++++++--                rn = np.size(np.where(labels == i))
+++++++--                var = np.sum((X[labels == i] - m[i])**2)/float(rn - 1)
+++++++--                obic[i] = self.loglikelihood(rn, rn, var, M, 1) - p/2.0*mt.log(rn)
+++++++--
+++++++--            #Split each cluster into two subclusters and calculate BIC of each splitted cluster
+++++++--            sk = 2 #The number of subclusters
+++++++--            nbic = np.zeros(k)
+++++++--            addk = 0
+++++++--
+++++++--            for i in range(k):
+++++++--                ci = X[labels == i]
+++++++--                r = np.size(np.where(labels == i))
+++++++--
+++++++--                kmeans = KMeans(n_clusters=sk).fit(ci)
+++++++--                ci_labels = kmeans.labels_
+++++++--                sm = kmeans.cluster_centers_
+++++++--
+++++++--                for l in range(sk):
+++++++--                    rn = np.size(np.where(ci_labels == l))
+++++++--                    var = np.sum((ci[ci_labels == l] - sm[l])**2)/float(rn - sk)
+++++++--                    nbic[i] += self.loglikelihood(r, rn, var, M, sk)
+++++++--
+++++++--                p = sk * (M + 1)
+++++++--                nbic[i] -= p/2.0*mt.log(r)
+++++++--
+++++++--                if obic[i] < nbic[i]:
+++++++--                    addk += 1
+++++++--
+++++++--            k += addk
+++++++--
+++++++--            if ok == k or k >= self.KMax:
+++++++--                break
+++++++--
+++++++--
+++++++--        #Calculate labels and centroids
+++++++--        kmeans = KMeans(n_clusters=k).fit(X)
+++++++--        self.labels = kmeans.labels_
+++++++--        self.k = k
+++++++--        self.m = kmeans.cluster_centers_
+++++++--
+++++++--
+++++++--def cluster_trajectories(trajectories):
+++++++--    xmeans_instance = XMeans(trajectories, kmax=10)
+++++++--    xmeans_instance.fit()
+++++++--
+++++++--    clusters = xmeans_instance.labels
+++++++--    return clusters
+++++++--
+++++++--def cluster_trajectories_2(trajectories):
++++++++ 
++++++++ def cluster_trajectories(trajectories, n_clusters=10):
++++++++-    """TODO"""
+++++++++    """
+++++++++    Cluster trajectories using X-means.
+++++++++    
+++++++++    Args:
+++++++++    - trajectories: np.array, shape (n_trajectories, encoding_dim)
+++++++++    - n_clusters: int, max number of clusters
+++++++++    
+++++++++    Returns:
+++++++++    - idxs_per_cluster: list, trajectory idxs per cluster idxs
+++++++++    - clusters: np.array, shape (n_trajectories), cluster idxs per trajectory idx
+++++++++    """ 
++++++++ 
+++++++ -    # Prepare initial centers - amount of initial centers defines amount of clusters from which X-Means will
+++++++ -    # start analysis.
+++++++--    amount_initial_centers = 2
+++++++--    initial_centers = kmeans_plusplus_initializer(trajectories, amount_initial_centers).initialize()
+++++++--    
+++++++++    # Set 2 initial cluster centers
++++++++     amount_initial_centers = 2
++++++++     initial_centers = kmeans_plusplus_initializer(trajectories, amount_initial_centers).initialize()
++++++++     
+++++++ -    # Create instance of X-Means algorithm. The algorithm will start analysis from 2 clusters, the maximum
+++++++ -    # number of clusters that can be allocated is 10.
+++++++--    xmeans_instance = xmeans(trajectories, initial_centers, 10)
+++++++--    xmeans_instance.process()
+++++++--    
+++++++--    # Extract clustering results: clusters
+++++++--    idxs_per_cluster = xmeans_instance.get_clusters()
+++++++--
+++++++--    clusters = []
+++++++--    for i in range(len(trajectories)):
+++++++--        for j in range(len(idxs_per_cluster)):
+++++++--            if i in idxs_per_cluster[j]: clusters.append(j)
+++++++--
+++++++--    return idxs_per_cluster, np.array(clusters)
+++++++++    # Run X-means
++++++++     xmeans_instance = xmeans(trajectories, initial_centers, n_clusters)
++++++++     xmeans_instance.process()
++++++++     
++++++++     # Extract clustering results: clusters
++++++++     idxs_per_cluster = xmeans_instance.get_clusters()
++++++++ 
+++++++++    # Turn list of trajectory idxs per cluster to array of cluster idx per trajectory idx
++++++++     clusters = []
++++++++     for i in range(len(trajectories)):
++++++++         for j in range(len(idxs_per_cluster)):
++++++++             if i in idxs_per_cluster[j]: clusters.append(j)
++++++++ 
++++++++     return idxs_per_cluster, np.array(clusters)
+++++++ - 
+++++++--# https://github.com/sascha-kirch/ML_Notebooks/blob/main/Softmax_Temperature.ipynb
+++++++--def softmax(x, temp):
+++++++--    """Compute softmax values for each sets of scores in x."""
+++++++--    return np.exp(np.divide(x,temp)) / np.sum(np.exp(np.divide(x,temp)))
+++++++--
+++++++--def generate_data_embedding(trajectory_embeddings, normalizing_factor=1, temperature=1):
+++++++--    embedding = np.sum(trajectory_embeddings, axis=0) / normalizing_factor
+++++++--    embedding = softmax(embedding, temperature)
+++++++--    return embedding
+++++++--
+++++++--def embed_trajectory(gpt, discretizer, observations, actions, rewards, preprocess_fn):
+++++++--    context = []
+++++++--
+++++++--    for i in range(len(observations)):
+++++++--        observation = observations[i]
+++++++--        action = actions[i]
+++++++--        reward = rewards[i]
+++++++--
+++++++--        observation = preprocess_fn(observation)
+++++++--
+++++++--        # print(observation)
+++++++--        prefix = make_prefix(discretizer, context, observation, True)
+++++++--        # print("prefix", prefix.shape)
+++++++--
+++++++--        out = forward(gpt, prefix)
+++++++--        # print("out", out.shape)
+++++++--        context = update_context(context, discretizer, observation, action, reward, len(observations))
+++++++--        # print("cotext", context)
+++++++--    
+++++++--    emb = []
+++++++--    for context_step in context:
+++++++--        emb.append(context_step.numpy())
+++++++--    emb = np.array(emb)
+++++++--    emb = np.mean(emb, axis=0)[0]
+++++++--
+++++++++
+++++++++
++++++++ def softmax(x, temp):
++++++++-    """TODO"""
+++++++++    """
+++++++++    Softmax with temperature using max-trick.
+++++++++    
+++++++++    Args:
+++++++++    - x: np.array, shape (n_data, dim_data)
+++++++++    - temp: int, softmax temperature
+++++++++    
+++++++++    Returns:
+++++++++    - softmax_x: np.array: shape (dim_data)
+++++++++    """ 
+++++++++
++++++++     max_x = np.max(x)
++++++++-    return np.exp(np.divide(x-max_x,temp)) / np.sum(np.exp(np.divide(x-max_x,temp)))
+++++++++    softmax_x = np.exp(np.divide(x-max_x,temp)) / np.sum(np.exp(np.divide(x-max_x,temp)))
+++++++++    return softmax_x
+++++++++
++++++++ 
++++++++ def generate_data_embedding(trajectory_embeddings, temperature=10000):
++++++++-    """TODO"""
+++++++++    """
+++++++++    Generate data embedding (sum+softmax) for set of encoded trajectories.
+++++++++    
+++++++++    Args:
+++++++++    - trajectory_embeddings: np.array, shape (n_data, dim_data)
+++++++++    - temperature: int, softmax temperature
+++++++++    
+++++++++    Returns:
+++++++++    - embedding: np.array, shape (dim_data)
+++++++++    """ 
++++++++ 
++++++++     embedding = np.sum(trajectory_embeddings, axis=0)
++++++++     embedding = softmax(embedding, temperature)
++++++++     
++++++++-
++++++++     return embedding
++++++++ 
+++++++++
++++++++ def embed_trajectory(gpt, discretizer, observations, actions, rewards, preprocess_fn):
++++++++-    """TODO"""
+++++++++    """
+++++++++    Encode trajectory using a trajectory transformer with a sliding window.
+++++++++    
+++++++++    Args:
+++++++++    - gpt: trajectory transformer
+++++++++    - discretizer: environment discretizer
+++++++++    - observations: trajectory observations
+++++++++    - actions: trajectory actions
+++++++++    - rewards: trajectory rewards
+++++++++    - preprocess_fn: observations preprocessing functions
+++++++++    
+++++++++    Returns:
+++++++++    - embedding: np.array, shape (hidden_dim), encoded trajectory
+++++++++    """ 
++++++++ 
++++++++     context = []
++++++++-
++++++++     output = []
++++++++ 
++++++++     for i in range(len(observations)):
++++++++@@ -76,12 +119,12 @@ def embed_trajectory(gpt, discretizer, observations, actions, rewards, preproces
++++++++         action = actions[i]
++++++++         reward = rewards[i]
++++++++ 
+++++++++        # Preprocess, discretize & forward through trajectory transformer
++++++++         observation = preprocess_fn(observation)
++++++++-
++++++++         prefix = make_prefix(discretizer, context, observation, True)
++++++++-
++++++++         out = forward(gpt, prefix)
++++++++ 
+++++++++        # Sliding window
++++++++         if len(context) >= 9:
++++++++             context.pop(0)
++++++++             if len(output) == 0:
++++++++@@ -91,19 +134,33 @@ def embed_trajectory(gpt, discretizer, observations, actions, rewards, preproces
++++++++ 
++++++++         context = update_context(context, discretizer, observation, action, reward, len(observations))
++++++++ 
++++++++-    emb = np.mean(output, axis=0)
+++++++ -    return emb
+++++++--
+++++++--
+++++++--def create_complementary_dataset(dataset, idxs, trajectory_length=10):
+++++++--    observations = []
+++++++--    actions = []
+++++++--    rewards = []
+++++++--    terminals = []
+++++++--    for i in range(1000):
+++++++--        if i not in idxs:
+++++++--            observations += list(dataset.observations[1000*i:1000*i+trajectory_length])
+++++++--            actions += list(dataset.actions[1000*i:1000*i+trajectory_length])
+++++++--            rewards += list(dataset.rewards[1000*i:1000*i+trajectory_length])
+++++++--            terminals += list(dataset.terminals[1000*i:1000*i+trajectory_length])
+++++++--
+++++++--    new_dataset = d3rlpy.dataset.MDPDataset(
+++++++--        observations=np.array(observations),
+++++++--        actions=np.array(actions),
+++++++--        rewards=np.array(rewards),
+++++++--        terminals=np.array(terminals)
+++++++--    )
+++++++--    return new_dataset
+++++++--    
+++++++--
+++++++--
+++++++--
+++++++--def main():
+++++++--    # args = Parser().parse_args('plan')
+++++++--
+++++++--    #######################
+++++++--    ####### models ########
+++++++--    #######################
+++++++--
+++++++--
+++++++--
+++++++--
+++++++--
+++++++--    # print(args.dataset)
+++++++--
+++++++--    # dataset = utils.load_from_config(args.logbase, args.dataset, args.gpt_loadpath,
+++++++--    #         'data_config.pkl')
+++++++--
+++++++--
+++++++--    # gpt, gpt_epoch = utils.load_model(args.logbase, args.dataset, args.gpt_loadpath,
+++++++--    #         epoch=args.gpt_epoch, device=args.device)
+++++++--
+++++++--    # env = datasets.load_environment(args.dataset)
+++++++--
+++++++--    # discretizer = dataset.discretizer
+++++++--
+++++++--    # preprocess_fn = datasets.get_preprocess_fn(env.name)
+++++++--
+++++++--    # #######################
+++++++--    # ####### dataset #######
+++++++--    # #######################
+++++++--
+++++++--    # # env = datasets.load_environment(args.dataset)
+++++++--    # discretizer = dataset.discretizer
+++++++--    # preprocess_fn = datasets.get_preprocess_fn(env.name)
+++++++--
+++++++--    # # dataset
+++++++--    dataset_d3, env = d3rlpy.datasets.get_dataset("halfcheetah-medium-v2")
+++++++--
+++++++--    # env = gym.make('halfcheetah-medium-v2')
+++++++--    # dataset_d4 = d4rl.qlearning_dataset(env)
+++++++--
+++++++--    # # checks to see if d3rl & d4rl datasets are equal
+++++++--    # print(np.allclose(dataset_d3.actions[100], dataset_d4['actions'][100]))
+++++++--
+++++++--    # # dr4rl has same trajectories, just cut off 1 element before the end
+++++++--    # for j in range(1000):
+++++++--    #     for i in range(999):
+++++++--    #         if dataset_d4['rewards'][j * 999 + i] != dataset_d3.rewards[j * 1000 + i]: print("yo", i)
+++++++--
+++++++--    # #######################
+++++++--    # ###### main loop ######
+++++++--    # #######################
+++++++--
+++++++--    trajectory_length = 10 # 10 = max
+++++++--
+++++++--    # embeddings = []
+++++++--    # for i in range(1000):
+++++++--    #     observations = dataset_d3.observations[1000*i:1000*i+trajectory_length]
+++++++--    #     actions = dataset_d3.actions[1000*i:1000*i+trajectory_length]
+++++++--    #     rewards = dataset_d3.rewards[1000*i:1000*i+trajectory_length]
+++++++--    #     terminals = dataset_d3.terminals[1000*i:1000*i+trajectory_length]
+++++++--    #     emb = embed_trajectory(gpt, discretizer, observations, actions, rewards, preprocess_fn)
+++++++--    #     embeddings.append(emb)
+++++++--    # embeddings = np.array(embeddings)
+++++++--    # np.save("embeddings.npy", embeddings)
+++++++--    # print(embeddings)
+++++++--
+++++++--    embeddings = np.load("embeddings.npy")
+++++++--
+++++++--    pca = PCA(n_components=2)
+++++++--    pca = PCA(n_components=2)
+++++++--    pca_embeddings = pca.fit_transform(embeddings)
+++++++--    np.save("pca.py", pca_embeddings)
+++++++--
+++++++--    idxs_per_cluster, clusters = cluster_trajectories_2(embeddings)
+++++++--    # print(clusters)
+++++++--    # return
+++++++--    np.save("clusters.npy", clusters)
+++++++--
+++++++--    import matplotlib.pyplot as plt
+++++++--
+++++++--    d_orig = generate_data_embedding(embeddings)
+++++++--    unique_clusters = np.unique(clusters)
+++++++--    
+++++++--    d_j = []
+++++++--    complementary_datasets = []
+++++++--    for j in np.sort(unique_clusters):
+++++++--        print(j)
+++++++--        d_j.append(generate_data_embedding(embeddings[clusters != j]))
+++++++--        plt.scatter(pca_embeddings[clusters == j][:,0], pca_embeddings[clusters == j][:,1], label=j)
+++++++--        complementary_datasets.append(create_complementary_dataset(dataset_d3, idxs_per_cluster[j], trajectory_length))
+++++++--    
+++++++--    original_dataset = create_complementary_dataset(dataset_d3, [], trajectory_length)
+++++++--
+++++++--    print(complementary_datasets, original_dataset)
+++++++--
+++++++--    plt.legend()
+++++++--    plt.show()
+++++++--
+++++++--    agent_orig = d3rlpy.algos.SAC(
+++++++--        actor_learning_rate=3e-4,
+++++++--        critic_learning_rate=3e-4,
+++++++--        temp_learning_rate=3e-4,
+++++++--        batch_size=256)
+++++++--
+++++++--    print(agent_orig)
+++++++--
+++++++--    training_steps = 1000
+++++++--
+++++++--    agent_orig.fit(original_dataset, n_steps=training_steps)
+++++++--
+++++++--    agents_compl = []
+++++++--
+++++++--    for dset in complementary_datasets:
+++++++--        agent = d3rlpy.algos.SAC(
+++++++--            actor_learning_rate=3e-4,
+++++++--            critic_learning_rate=3e-4,
+++++++--            temp_learning_rate=3e-4,
+++++++--            batch_size=256)
+++++++--        agent.fit(dset, n_steps=training_steps)
+++++++--        agents_compl.append(agent)
+++++++--
+++++++--    action_orig = agent_orig.predict(dataset_d3.observations[0])
+++++++--
+++++++--    actions_compl = []
+++++++--    for agent in agents_compl:
+++++++--        actions_compl.append(agent.predict(dataset_d3.observations[0]))
+++++++--    
+++++++--    action_dists = []
+++++++--    for action in actions_compl:
+++++++--        action_dists.append(np.linalg.norm(action_orig-action))
+++++++--
+++++++--    k = 3
+++++++--    topk = np.argpartition(action_dists, -k)[-k:]
+++++++--
+++++++--    d_w = {}
+++++++--    for idx in topk:
+++++++--        d_w[idx] = wasserstein_distance(d_j[idx], d_orig)
+++++++--
+++++++--    cluster_assignment = min(d_w, key=d_w.get)
+++++++--    print("explanation assigned to cluster", cluster_assignment)
+++++++--
+++++++--    
+++++++--def assignment_test():
+++++++--    action_orig = np.random.rand(10)
+++++++--    d_orig = np.random.rand(5)
+++++++--
+++++++--    actions_compl = np.random.rand(6,10)
+++++++--    d_j = np.random.rand(6,5)
+++++++--
+++++++--    action_dists = []
+++++++--    for action in actions_compl:
+++++++--        action_dists.append(np.linalg.norm(action_orig-action))
+++++++--
+++++++--    print(action_dists)
+++++++--
+++++++--    k = 3
+++++++--    topk = np.argpartition(action_dists, -k)[-k:]
+++++++--
+++++++--    print(topk)
+++++++--
+++++++--    d_w = {}
+++++++--    for idx in topk:
+++++++--        d_w[idx] = wasserstein_distance(d_j[idx], d_orig)
+++++++--
+++++++--    print(d_w)
+++++++--
+++++++--    cluster_assignment = min(d_w, key=d_w.get)
+++++++--    print("explanation assigned to cluster", cluster_assignment)
+++++++--
+++++++--
+++++++--if __name__ == "__main__":
+++++++--    # main()
+++++++--    assignment_test()
+++++++-diff --git a/halfcheetah/trajectory.egg-info/PKG-INFO b/halfcheetah/trajectory.egg-info/PKG-INFO
+++++++-index 452c6cb..2603850 100644
+++++++---- a/halfcheetah/trajectory.egg-info/PKG-INFO
+++++++-+++ b/halfcheetah/trajectory.egg-info/PKG-INFO
+++++++-@@ -1,4 +1,11 @@
+++++++- Metadata-Version: 2.1
+++++++- Name: trajectory
+++++++- Version: 0.0.0
+++++++-+Summary: UNKNOWN
+++++++-+Home-page: UNKNOWN
+++++++-+License: UNKNOWN
+++++++-+Platform: UNKNOWN
+++++++- License-File: LICENSE
+++++++++    # Embedding is the average of encoded states
+++++++++    embedding = np.mean(output, axis=0)
+++++++++    return embedding
+++++++++
++++++++ 
++++++++ def create_complementary_dataset(dataset, idxs, trajectory_length=10, inverse=False):
++++++++-    """TODO"""
+++++++++    """
+++++++++    Encode trajectory using a trajectory transformer with a sliding window.
+++++++++    
+++++++++    Args:
+++++++++    - dataset: MDPDataset, original d3rl dataset
+++++++++    - idxs: trajectory idxs to ignore (or include if inverse is True)
+++++++++    - trajectory_length: int, trajectory length
+++++++++    - inverse: bool, if True the dataset is not complementary
+++++++ +
+++++++-+UNKNOWN
+++++++++    Returns:
+++++++++    - new_dataset: MDPDataset, complementary dataset
+++++++++    """ 
++++++++ 
++++++++     observations = []
++++++++     actions = []
++++++++     rewards = []
++++++++     terminals = []
++++++++ 
++++++++-    n_trajs = int(1000000/trajectory_length)
+++++++++    n_trajs = int(len(dataset.observations)/trajectory_length)
++++++++     for i in range(n_trajs):
+++++++++        # If inverse is True, only include idxs. If not, leave out idxs
++++++++         condition = i not in idxs
++++++++         if inverse: condition = not condition
++++++++ 
++++++++@@ -112,6 +169,7 @@ def create_complementary_dataset(dataset, idxs, trajectory_length=10, inverse=Fa
++++++++             actions += list(dataset.actions[trajectory_length*i:trajectory_length*(i+1)])
++++++++             rewards += list(dataset.rewards[trajectory_length*i:trajectory_length*(i+1)])
++++++++ 
+++++++++    # Trajectories end with a terminal state
++++++++     terminals = np.tile([0]*(trajectory_length-1)+[1], int(len(observations)/trajectory_length))
++++++++ 
++++++++     new_dataset = d3rlpy.dataset.MDPDataset(
++++++++@@ -124,6 +182,17 @@ def create_complementary_dataset(dataset, idxs, trajectory_length=10, inverse=Fa
++++++++     
++++++++ 
++++++++ def clusters_to_idxs(clusters):
+++++++++    """
+++++++++    Helper function to turn array of cluster idxs per trajectory idxs to a list 
+++++++++    of trajectory idxs per cluster idx.
+++++++++    
+++++++++    Args:
+++++++++    - clusters: np.array, cluster idx per trajectory idx
+++++++ +
+++++++++    Returns:
+++++++++    - idxs_per_cluster: list, trajectory idxs per cluster idx
+++++++++    """ 
+++++++++
++++++++     idxs_per_cluster = []
++++++++     for i in np.sort(np.unique(clusters)):
++++++++         idxs_per_cluster.append(list(np.argwhere(clusters == i).flatten()))
++++++++@@ -142,7 +211,7 @@ def main():
++++++++ 
++++++++     ### IMPORTANT DEFINITIONS XRL SCRIPT ###
++++++++ 
++++++++-    load_embeddings = False
+++++++++    load_embeddings = True
++++++++     load_clusters = True
++++++++     load_agents = True
++++++++     generate_human_study = False
+++++++ diff --git a/halfcheetah/trajectory.egg-info/SOURCES.txt b/halfcheetah/trajectory.egg-info/SOURCES.txt
+++++++-index 4474d85..84e8e3a 100644
++++++++index 84e8e3a..4474d85 100644
+++++++ --- a/halfcheetah/trajectory.egg-info/SOURCES.txt
+++++++ +++ b/halfcheetah/trajectory.egg-info/SOURCES.txt
+++++++-@@ -30,4 +30,5 @@ trajectory/utils/serialization.py
++++++++@@ -30,5 +30,4 @@ trajectory/utils/serialization.py
+++++++  trajectory/utils/setup.py
+++++++  trajectory/utils/timer.py
+++++++  trajectory/utils/training.py
+++++++ -trajectory/utils/video.py
++++++++-trajectory_aaa/__init__.py
+++++++ \ No newline at end of file
+++++++ +trajectory/utils/video.py
+++++++-+trajectory_aaa/__init__.py
+++++++ \ No newline at end of file
+++++++ diff --git a/halfcheetah/trajectory.egg-info/top_level.txt b/halfcheetah/trajectory.egg-info/top_level.txt
+++++++-index ce65198..1d5271f 100644
++++++++index 1d5271f..ce65198 100644
+++++++ --- a/halfcheetah/trajectory.egg-info/top_level.txt
+++++++ +++ b/halfcheetah/trajectory.egg-info/top_level.txt
+++++++-@@ -1 +1,2 @@
++++++++@@ -1,2 +1 @@
+++++++  trajectory
+++++++-+trajectory_aaa
++++++ \ No newline at end of file
++++++-diff --git a/halfcheetah/pca.py.npy b/halfcheetah/pca.py.npy
++++++-deleted file mode 100644
++++++-index bb19150..0000000
++++++-Binary files a/halfcheetah/pca.py.npy and /dev/null differ
++++++-diff --git a/halfcheetah/plotting/bar.png b/halfcheetah/plotting/bar.png
++++++-deleted file mode 100644
++++++-index 3679667..0000000
++++++-Binary files a/halfcheetah/plotting/bar.png and /dev/null differ
++++++-diff --git a/halfcheetah/plotting/plot.py b/halfcheetah/plotting/plot.py
++++++-deleted file mode 100644
++++++-index 163d0e4..0000000
++++++---- a/halfcheetah/plotting/plot.py
++++++-+++ /dev/null
++++++-@@ -1,74 +0,0 @@
++++++--import numpy as np
++++++--import matplotlib
++++++--import matplotlib.pyplot as plt
++++++--import pdb
++++++--
++++++--from plotting.scores import means
++++++--
++++++--class Colors:
++++++--	grey = '#B4B4B4'
++++++--	gold = '#F6C781'
++++++--	red = '#EC7C7D'
++++++--	blue = '#70ABCC'
++++++--
++++++--LABELS = {
++++++--	# 'BC': 'Behavior\nCloning',
++++++--	# 'MBOP': 'Model-Based\nOffline Planning',
++++++--	# 'BRAC': 'Behavior-Reg.\nActor-Critic',
++++++--	# 'CQL': 'Conservative\nQ-Learning',
++++++--}
++++++--
++++++--def get_mean(results, exclude=None):
++++++--	'''
++++++--		results : { environment: score, ... }
++++++--	'''
++++++--	filtered = {
++++++--		k: v for k, v in results.items()
++++++--		if (not exclude) or (exclude and exclude not in k)
++++++--	}
++++++--	return np.mean(list(filtered.values()))
++++++--
++++++--if __name__ == '__main__':
++++++--
++++++--	#################
++++++--	## latex
++++++--	#################
++++++--	matplotlib.rc('font', **{'family': 'serif', 'serif': ['Computer Modern']})
++++++--	matplotlib.rc('text', usetex=True)
++++++--	matplotlib.rcParams['text.latex.preamble']=[r"\usepackage{amsmath}"]
++++++--	#################
++++++--
++++++--	fig = plt.gcf()
++++++--	ax = plt.gca()
++++++--	fig.set_size_inches(7.5, 2.5)
++++++--
++++++--	means = {k: get_mean(v, exclude='ant') for k, v in means.items()}
++++++--	print(means)
++++++--
++++++--	algs = ['BC', 'MBOP', 'BRAC', 'CQL', 'Decision\nTransformer', 'Trajectory\nTransformer']
++++++--	vals = [means[alg] for alg in algs]
++++++--
++++++--	colors = [
++++++--		Colors.grey, Colors.gold,
++++++--		Colors.red, Colors.red, Colors.blue, Colors.blue
++++++--	]
++++++--
++++++--	labels = [LABELS.get(alg, alg) for alg in algs]
++++++--	plt.bar(labels, vals, color=colors, edgecolor=Colors.gold, lw=0)
++++++--	plt.ylabel('Average normalized return', labelpad=15)
++++++--	# plt.title('Offline RL Results')
++++++--
++++++--	legend_labels = ['Behavior Cloning', 'Trajectory Optimization', 'Temporal Difference', 'Sequence Modeling']
++++++--	colors = [Colors.grey, Colors.gold, Colors.red, Colors.blue]
++++++--	handles = [plt.Rectangle((0,0),1,1, color=color) for label, color in zip(legend_labels, colors)]
++++++--	plt.legend(handles, legend_labels, ncol=4,
++++++--		bbox_to_anchor=(1.07, -.18), fancybox=False, framealpha=0, shadow=False, columnspacing=1.5, handlelength=1.5)
++++++--
++++++--	matplotlib.rcParams['hatch.linewidth'] = 7.5
++++++--	# ax.patches[-1].set_hatch('/')
++++++--
++++++--	ax.spines['right'].set_visible(False)
++++++--	ax.spines['top'].set_visible(False)
++++++--
++++++--	# plt.savefig('plotting/bar.pdf', bbox_inches='tight')
++++++--	plt.savefig('plotting/bar.png', bbox_inches='tight', dpi=500)
++++++-diff --git a/halfcheetah/plotting/read_results.py b/halfcheetah/plotting/read_results.py
++++++-deleted file mode 100644
++++++-index 5a5fb62..0000000
++++++---- a/halfcheetah/plotting/read_results.py
++++++-+++ /dev/null
++++++-@@ -1,70 +0,0 @@
++++++--import os
++++++--import glob
++++++--import numpy as np
++++++--import json
++++++--import pdb
++++++--
++++++--import trajectory.utils as utils
++++++--
++++++--DATASETS = [
++++++--	f'{env}-{buffer}'
++++++--	for env in ['hopper', 'walker2d', 'halfcheetah', 'ant']
++++++--	for buffer in ['medium-expert-v2', 'medium-v2', 'medium-replay-v2']
++++++--]
++++++--
++++++--LOGBASE = 'logs'
++++++--TRIAL = '*'
++++++--EXP_NAME = 'plans/pretrained'
++++++--
++++++--def load_results(paths):
++++++--	'''
++++++--		paths : path to directory containing experiment trials
++++++--	'''
++++++--	scores = []
++++++--	for i, path in enumerate(sorted(paths)):
++++++--		score = load_result(path)
++++++--		if score is None:
++++++--			print(f'Skipping {path}')
++++++--			continue
++++++--		scores.append(score)
++++++--
++++++--		suffix = path.split('/')[-1]
++++++--
++++++--	mean = np.mean(scores)
++++++--	err = np.std(scores) / np.sqrt(len(scores))
++++++--	return mean, err, scores
++++++--
++++++--def load_result(path):
++++++--	'''
++++++--		path : path to experiment directory; expects `rollout.json` to be in directory
++++++--	'''
++++++--	fullpath = os.path.join(path, 'rollout.json')
++++++--	suffix = path.split('/')[-1]
++++++--
++++++--	if not os.path.exists(fullpath):
++++++--		return None
++++++--
++++++--	results = json.load(open(fullpath, 'rb'))
++++++--	score = results['score']
++++++--	return score * 100
++++++--
++++++--#######################
++++++--######## setup ########
++++++--#######################
++++++--
++++++--if __name__ == '__main__':
++++++--
++++++--	class Parser(utils.Parser):
++++++--	    dataset: str = None
++++++--
++++++--	args = Parser().parse_args()
++++++--
++++++--	for dataset in ([args.dataset] if args.dataset else DATASETS):
++++++--		subdirs = glob.glob(os.path.join(LOGBASE, dataset, EXP_NAME))
++++++--
++++++--		for subdir in subdirs:
++++++--			reldir = subdir.split('/')[-1]
++++++--			paths = glob.glob(os.path.join(subdir, TRIAL))
++++++--
++++++--			mean, err, scores = load_results(paths)
++++++--			print(f'{dataset.ljust(30)} | {subdir.ljust(50)} | {len(scores)} scores \n    {mean:.2f} +/- {err:.2f}\n')
++++++-diff --git a/halfcheetah/plotting/scores.py b/halfcheetah/plotting/scores.py
++++++-deleted file mode 100644
++++++-index f1917f7..0000000
++++++---- a/halfcheetah/plotting/scores.py
++++++-+++ /dev/null
++++++-@@ -1,123 +0,0 @@
++++++--means = {
++++++--	'Trajectory\nTransformer': {
++++++--		##
++++++--		'halfcheetah-medium-expert-v2': 95.0,
++++++--		'hopper-medium-expert-v2': 110.0,
++++++--		'walker2d-medium-expert-v2': 101.9,
++++++--		'ant-medium-expert-v2': 116.1,
++++++--		##
++++++--		'halfcheetah-medium-v2': 46.9,
++++++--		'hopper-medium-v2': 61.1,
++++++--		'walker2d-medium-v2': 79.0,
++++++--		'ant-medium-v2': 83.1,
++++++--		##
++++++--		'halfcheetah-medium-replay-v2': 41.9,
++++++--		'hopper-medium-replay-v2': 91.5,
++++++--		'walker2d-medium-replay-v2': 82.6,
++++++--		'ant-medium-replay-v2': 77.0,
++++++--	},
++++++--	'Decision\nTransformer': {
++++++--		##
++++++--		'halfcheetah-medium-expert-v2': 86.8,
++++++--		'hopper-medium-expert-v2': 107.6,
++++++--		'walker2d-medium-expert-v2': 108.1,
++++++--		##
++++++--		'halfcheetah-medium-v2': 42.6,
++++++--		'hopper-medium-v2': 67.6,
++++++--		'walker2d-medium-v2': 74.0,
++++++--		##
++++++--		'halfcheetah-medium-replay-v2': 36.6,
++++++--		'hopper-medium-replay-v2': 82.7,
++++++--		'walker2d-medium-replay-v2': 66.6,
++++++--	},
++++++--	'CQL': {
++++++--		##
++++++--		'halfcheetah-medium-expert-v2': 91.6,
++++++--		'hopper-medium-expert-v2': 105.4,
++++++--		'walker2d-medium-expert-v2': 108.8,
++++++--		##
++++++--		'halfcheetah-medium-v2': 44.0,
++++++--		'hopper-medium-v2': 58.5,
++++++--		'walker2d-medium-v2': 72.5,
++++++--		##
++++++--		'halfcheetah-medium-replay-v2': 45.5,
++++++--		'hopper-medium-replay-v2': 95.0,
++++++--		'walker2d-medium-replay-v2': 77.2,
++++++--	},
++++++--	'MOPO': {
++++++--		##
++++++--		'halfcheetah-medium-expert-v2': 63.3,
++++++--		'hopper-medium-expert-v2': 23.7,
++++++--		'walker2d-medium-expert-v2': 44.6,
++++++--		##
++++++--		'halfcheetah-medium-v2': 42.3,
++++++--		'hopper-medium-v2': 28.0,
++++++--		'walker2d-medium-v2': 17.8,
++++++--		##
++++++--		'halfcheetah-medium-replay-v2': 53.1,
++++++--		'hopper-medium-replay-v2': 67.5,
++++++--		'walker2d-medium-replay-v2':39.0,
++++++--	},
++++++--	'MBOP': {
++++++--		##
++++++--		'halfcheetah-medium-expert-v2': 105.9,
++++++--		'hopper-medium-expert-v2': 55.1,
++++++--		'walker2d-medium-expert-v2': 70.2,
++++++--		##
++++++--		'halfcheetah-medium-v2': 44.6,
++++++--		'hopper-medium-v2': 48.8,
++++++--		'walker2d-medium-v2': 41.0,
++++++--		##
++++++--		'halfcheetah-medium-replay-v2': 42.3,
++++++--		'hopper-medium-replay-v2': 12.4,
++++++--		'walker2d-medium-replay-v2': 9.7,
++++++--	},
++++++--	'BRAC': {
++++++--		##
++++++--		'halfcheetah-medium-expert-v2': 41.9,
++++++--		'hopper-medium-expert-v2': 0.9,
++++++--		'walker2d-medium-expert-v2': 81.6,
++++++--		##
++++++--		'halfcheetah-medium-v2': 46.3,
++++++--		'hopper-medium-v2': 31.3,
++++++--		'walker2d-medium-v2': 81.1,
++++++--		##
++++++--		'halfcheetah-medium-replay-v2': 47.7,
++++++--		'hopper-medium-replay-v2': 0.6,
++++++--		'walker2d-medium-replay-v2': 0.9,
++++++--	},
++++++--	'BC': {
++++++--		##
++++++--		'halfcheetah-medium-expert-v2': 59.9,
++++++--		'hopper-medium-expert-v2': 79.6,
++++++--		'walker2d-medium-expert-v2': 36.6,
++++++--		##
++++++--		'halfcheetah-medium-v2': 43.1,
++++++--		'hopper-medium-v2': 63.9,
++++++--		'walker2d-medium-v2': 77.3,
++++++--		##
++++++--		'halfcheetah-medium-replay-v2': 4.3,
++++++--		'hopper-medium-replay-v2': 27.6,
++++++--		'walker2d-medium-replay-v2': 36.9,
++++++--	},
++++++--}
++++++--
++++++--errors = {
++++++--	'Trajectory\nTransformer': {
++++++--		##
++++++--		'halfcheetah-medium-expert-v2': 0.2,
++++++--		'hopper-medium-expert-v2': 2.7,
++++++--		'walker2d-medium-expert-v2': 6.8,
++++++--		'ant-medium-expert-v2': 9.0,
++++++--		##
++++++--		'halfcheetah-medium-v2': 0.4,
++++++--		'hopper-medium-v2': 3.6,
++++++--		'walker2d-medium-v2': 2.8,
++++++--		'ant-medium-v2': 7.3,
++++++--		##
++++++--		'halfcheetah-medium-replay-v2': 2.5,
++++++--		'hopper-medium-replay-v2': 3.6,
++++++--		'walker2d-medium-replay-v2': 6.9,
++++++--		'ant-medium-replay-v2': 6.8,
++++++--	},
++++++--}
++++++++-trajectory_aaa
++++++++diff --git a/seaquest/readme.md b/seaquest/readme.md
++++++++index 84e53f8..53561f9 100644
++++++++--- a/seaquest/readme.md
+++++++++++ b/seaquest/readme.md
++++++++@@ -10,4 +10,4 @@ pip install git+https://github.com/takuseno/d4rl-atari
++++++++ pip install "gym[atari, accept-rom-license]"
++++++++ pip install pyclustering
++++++++ pip install seaborn
++++++++-pip install d3rlpy==1.1.1
++++++++\ No newline at end of file
+++++++++pip install d3rlpy==1.1.1
++++++ \ No newline at end of file
++++++-diff --git a/halfcheetah/plotting/table.py b/halfcheetah/plotting/table.py
++++++-deleted file mode 100644
++++++-index eae74e6..0000000
++++++---- a/halfcheetah/plotting/table.py
++++++-+++ /dev/null
++++++-@@ -1,127 +0,0 @@
++++++--import numpy as np
++++++--import pdb
++++++--
++++++--from plotting.plot import get_mean
++++++--from plotting.scores import (
++++++--	means as MEANS,
++++++--	errors as ERRORS,
++++++--)
++++++--
++++++--ALGORITHM_STRINGS = {
++++++--	'Trajectory\nTransformer': 'TT (Ours)',
++++++--	'Decision\nTransformer': 'DT',	
++++++--}
++++++--
++++++--BUFFER_STRINGS = {
++++++--	'medium-expert': 'Medium-Expert',
++++++--	'medium': 'Medium',
++++++--	'medium-replay': 'Medium-Replay',	
++++++--}
++++++--
++++++--ENVIRONMENT_STRINGS = {
++++++--	'halfcheetah': 'HalfCheetah',
++++++--	'hopper': 'Hopper',
++++++--	'walker2d': 'Walker2d',
++++++--	'ant': 'Ant',
++++++--}
++++++--
++++++--SHOW_ERRORS = ['Trajectory\nTransformer']
++++++--
++++++--def get_result(algorithm, buffer, environment, version='v2'):
++++++--	key = f'{environment}-{buffer}-{version}'
++++++--	mean = MEANS[algorithm].get(key, '-')
++++++--	if algorithm in SHOW_ERRORS:
++++++--		error = ERRORS[algorithm].get(key)
++++++--		return (mean, error)
++++++--	else:
++++++--		return mean
++++++--
++++++--def format_result(result):
++++++--	if type(result) == tuple:
++++++--		mean, std = result
++++++--		return f'${mean}$ \\scriptsize{{\\raisebox{{1pt}}{{$\\pm {std}$}}}}'
++++++--	else:
++++++--		return f'${result}$'
++++++--
++++++--def format_row(buffer, environment, results):
++++++--	buffer_str = BUFFER_STRINGS[buffer]
++++++--	environment_str = ENVIRONMENT_STRINGS[environment]
++++++--	results_str = ' & '.join(format_result(result) for result in results)
++++++--	row = f'{buffer_str} & {environment_str} & {results_str} \\\\ \n'
++++++--	return row
++++++--
++++++--def format_buffer_block(algorithms, buffer, environments):
++++++--	block_str = '\\midrule\n'
++++++--	for environment in environments:
++++++--		results = [get_result(alg, buffer, environment) for alg in algorithms]
++++++--		row_str = format_row(buffer, environment, results)
++++++--		block_str += row_str
++++++--	return block_str
++++++--
++++++--def format_algorithm(algorithm):
++++++--	algorithm_str = ALGORITHM_STRINGS.get(algorithm, algorithm)
++++++--	return f'\multicolumn{{1}}{{c}}{{\\bf {algorithm_str}}}'
++++++--
++++++--def format_algorithms(algorithms):
++++++--	return ' & '.join(format_algorithm(algorithm) for algorithm in algorithms)
++++++--
++++++--def format_averages(means, label):
++++++--	prefix = f'\\multicolumn{{2}}{{c}}{{\\bf Average ({label})}} & '
++++++--	formatted = ' & '.join(str(mean) for mean in means)
++++++--	return prefix + formatted
++++++--
++++++--def format_averages_block(algorithms):
++++++--	means_filtered = [np.round(get_mean(MEANS[algorithm], exclude='ant'), 1) for algorithm in algorithms]
++++++--	means_all = [np.round(get_mean(MEANS[algorithm], exclude=None), 1) for algorithm in algorithms]
++++++--
++++++--	means_all = [
++++++--		means
++++++--		if 'ant-medium-expert-v2' in MEANS[algorithm]
++++++--		else '$-$'
++++++--		for algorithm, means in zip(algorithms, means_all)
++++++--	]
++++++--
++++++--	formatted_filtered = format_averages(means_filtered, 'without Ant')
++++++--	formatted_all = format_averages(means_all, 'all settings')
++++++--
++++++--	formatted_block = (
++++++--		f'{formatted_filtered} \\hspace{{.6cm}} \\\\ \n'
++++++--		f'{formatted_all} \\hspace{{.6cm}} \\\\ \n'
++++++--	)
++++++--	return formatted_block
++++++--
++++++--def format_table(algorithms, buffers, environments):
++++++--	justify_str = 'll' + 'r' * len(algorithms)
++++++--	algorithm_str = format_algorithms(['Dataset', 'Environment'] + algorithms)
++++++--	averages_str = format_averages_block(algorithms)
++++++--	table_prefix = (
++++++--		'\\begin{table*}[h]\n'
++++++--		'\\centering\n'
++++++--		'\\small\n'
++++++--		f'\\begin{{tabular}}{{{justify_str}}}\n'
++++++--		'\\toprule\n'
++++++--		f'{algorithm_str} \\\\ \n'
++++++--	)
++++++--	table_suffix = (
++++++--		'\\midrule\n'
++++++--		f'{averages_str}'
++++++--		'\\bottomrule\n'
++++++--		'\\end{tabular}\n'
++++++--		'\\label{table:d4rl}\n'
++++++--		'\\end{table*}'
++++++--	)
++++++--	blocks = ''.join(format_buffer_block(algorithms, buffer, environments) for buffer in buffers)
++++++--	table = (
++++++--		f'{table_prefix}'
++++++--		f'{blocks}'
++++++--		f'{table_suffix}'
++++++--	)
++++++--	return table
++++++--
++++++--
++++++--algorithms =['BC', 'MBOP', 'BRAC', 'CQL',  'Decision\nTransformer', 'Trajectory\nTransformer']
++++++--buffers = ['medium-expert', 'medium', 'medium-replay']
++++++--environments = ['halfcheetah', 'hopper', 'walker2d', 'ant']
++++++--
++++++--table = format_table(algorithms, buffers, environments)
++++++--print(table)
++++++-diff --git a/halfcheetah/scripts/plan.py b/halfcheetah/scripts/plan.py
++++++-deleted file mode 100644
++++++-index f13d4cc..0000000
++++++---- a/halfcheetah/scripts/plan.py
++++++-+++ /dev/null
++++++-@@ -1,124 +0,0 @@
++++++--import json
++++++--import pdb
++++++--from os.path import join
++++++--
++++++--import trajectory.utils as utils
++++++--import trajectory.datasets as datasets
++++++--from trajectory.search import (
++++++--    beam_plan,
++++++--    make_prefix,
++++++--    extract_actions,
++++++--    update_context,
++++++--)
++++++--
++++++--class Parser(utils.Parser):
++++++--    dataset: str = 'halfcheetah-medium-expert-v2'
++++++--    config: str = 'config.offline'
++++++--
++++++--#######################
++++++--######## setup ########
++++++--#######################
++++++--
++++++--args = Parser().parse_args('plan')
++++++--
++++++--#######################
++++++--####### models ########
++++++--#######################
++++++--
++++++--dataset = utils.load_from_config(args.logbase, args.dataset, args.gpt_loadpath,
++++++--        'data_config.pkl')
++++++--
++++++--gpt, gpt_epoch = utils.load_model(args.logbase, args.dataset, args.gpt_loadpath,
++++++--        epoch=args.gpt_epoch, device=args.device)
++++++--
++++++--#######################
++++++--####### dataset #######
++++++--#######################
++++++--
++++++--env = datasets.load_environment(args.dataset)
++++++--print('yo')
++++++--renderer = utils.make_renderer(args)
++++++--timer = utils.timer.Timer()
++++++--
++++++--discretizer = dataset.discretizer
++++++--discount = dataset.discount
++++++--observation_dim = dataset.observation_dim
++++++--action_dim = dataset.action_dim
++++++--
++++++--value_fn = lambda x: discretizer.value_fn(x, args.percentile)
++++++--preprocess_fn = datasets.get_preprocess_fn(env.name)
++++++--
++++++--print('yo2')
++++++--
++++++--#######################
++++++--###### main loop ######
++++++--#######################
++++++--
++++++--observation = env.reset()
++++++--total_reward = 0
++++++--
++++++--## observations for rendering
++++++--rollout = [observation.copy()]
++++++--
++++++--## previous (tokenized) transitions for conditioning transformer
++++++--context = []
++++++--
++++++--T = env.max_episode_steps
++++++--for t in range(T):
++++++--
++++++--    observation = preprocess_fn(observation)
++++++--
++++++--    if t % args.plan_freq == 0:
++++++--        ## concatenate previous transitions and current observations to input to model
++++++--        prefix = make_prefix(discretizer, context, observation, args.prefix_context)
++++++--
++++++--        ## sample sequence from model beginning with `prefix`
++++++--        sequence = beam_plan(
++++++--            gpt, value_fn, prefix,
++++++--            args.horizon, args.beam_width, args.n_expand, observation_dim, action_dim,
++++++--            discount, args.max_context_transitions, verbose=args.verbose,
++++++--            k_obs=args.k_obs, k_act=args.k_act, cdf_obs=args.cdf_obs, cdf_act=args.cdf_act,
++++++--        )
++++++--
++++++--    else:
++++++--        sequence = sequence[1:]
++++++--
++++++--    ## [ horizon x transition_dim ] convert sampled tokens to continuous trajectory
++++++--    sequence_recon = discretizer.reconstruct(sequence)
++++++--
++++++--    ## [ action_dim ] index into sampled trajectory to grab first action
++++++--    action = extract_actions(sequence_recon, observation_dim, action_dim, t=0)
++++++--
++++++--    ## execute action in environment
++++++--    next_observation, reward, terminal, _ = env.step(action)
++++++--
++++++--    ## update return
++++++--    total_reward += reward
++++++--    score = env.get_normalized_score(total_reward)
++++++--
++++++--    ## update rollout observations and context transitions
++++++--    rollout.append(next_observation.copy())
++++++--    context = update_context(context, discretizer, observation, action, reward, args.max_context_transitions)
++++++--
++++++--    print(
++++++--        f'[ plan ] t: {t} / {T} | r: {reward:.2f} | R: {total_reward:.2f} | score: {score:.4f} | '
++++++--        f'time: {timer():.2f} | {args.dataset} | {args.exp_name} | {args.suffix}\n'
++++++--    )
++++++--
++++++--    ## visualization
++++++--    if t % args.vis_freq == 0 or terminal or t == T:
++++++--
++++++--        ## save current plan
++++++--        renderer.render_plan(join(args.savepath, f'{t}_plan.mp4'), sequence_recon, env.state_vector())
++++++--
++++++--        ## save rollout thus far
++++++--        renderer.render_rollout(join(args.savepath, f'rollout.mp4'), rollout, fps=80)
++++++--
++++++--    if terminal: break
++++++--
++++++--    observation = next_observation
++++++--
++++++--## save result as a json file
++++++--json_path = join(args.savepath, 'rollout.json')
++++++--json_data = {'score': score, 'step': t, 'return': total_reward, 'term': terminal, 'gpt_epoch': gpt_epoch}
++++++--json.dump(json_data, open(json_path, 'w'), indent=2, sort_keys=True)
++++++-diff --git a/halfcheetah/scripts/train.py b/halfcheetah/scripts/train.py
++++++-deleted file mode 100644
++++++-index 04af8d7..0000000
++++++---- a/halfcheetah/scripts/train.py
++++++-+++ /dev/null
++++++-@@ -1,122 +0,0 @@
++++++--import os
++++++--import numpy as np
++++++--import torch
++++++--import pdb
++++++--
++++++--import trajectory.utils as utils
++++++--import trajectory.datasets as datasets
++++++--from trajectory.models.transformers import GPT
++++++--
++++++--
++++++--class Parser(utils.Parser):
++++++--    dataset: str = 'halfcheetah-medium-expert-v2'
++++++--    config: str = 'config.offline'
++++++--
++++++--#######################
++++++--######## setup ########
++++++--#######################
++++++--
++++++--args = Parser().parse_args('train')
++++++--
++++++--#######################
++++++--####### dataset #######
++++++--#######################
++++++--
++++++--env = datasets.load_environment(args.dataset)
++++++--
++++++--sequence_length = args.subsampled_sequence_length * args.step
++++++--
++++++--dataset_config = utils.Config(
++++++--    datasets.DiscretizedDataset,
++++++--    savepath=(args.savepath, 'data_config.pkl'),
++++++--    env=args.dataset,
++++++--    N=args.N,
++++++--    penalty=args.termination_penalty,
++++++--    sequence_length=sequence_length,
++++++--    step=args.step,
++++++--    discount=args.discount,
++++++--    discretizer=args.discretizer,
++++++--)
++++++--
++++++--dataset = dataset_config()
++++++--obs_dim = dataset.observation_dim
++++++--act_dim = dataset.action_dim
++++++--transition_dim = dataset.joined_dim
++++++--
++++++--#######################
++++++--######## model ########
++++++--#######################
++++++--
++++++--block_size = args.subsampled_sequence_length * transition_dim - 1
++++++--print(
++++++--    f'Dataset size: {len(dataset)} | '
++++++--    f'Joined dim: {transition_dim} '
++++++--    f'(observation: {obs_dim}, action: {act_dim}) | Block size: {block_size}'
++++++--)
++++++--
++++++--model_config = utils.Config(
++++++--    GPT,
++++++--    savepath=(args.savepath, 'model_config.pkl'),
++++++--    ## discretization
++++++--    vocab_size=args.N, block_size=block_size,
++++++--    ## architecture
++++++--    n_layer=args.n_layer, n_head=args.n_head, n_embd=args.n_embd*args.n_head,
++++++--    ## dimensions
++++++--    observation_dim=obs_dim, action_dim=act_dim, transition_dim=transition_dim,
++++++--    ## loss weighting
++++++--    action_weight=args.action_weight, reward_weight=args.reward_weight, value_weight=args.value_weight,
++++++--    ## dropout probabilities
++++++--    embd_pdrop=args.embd_pdrop, resid_pdrop=args.resid_pdrop, attn_pdrop=args.attn_pdrop,
++++++--)
++++++--
++++++--model = model_config()
++++++--model.to(args.device)
++++++--
++++++--#######################
++++++--####### trainer #######
++++++--#######################
++++++--
++++++--warmup_tokens = len(dataset) * block_size ## number of tokens seen per epoch
++++++--final_tokens = 20 * warmup_tokens
++++++--
++++++--trainer_config = utils.Config(
++++++--    utils.Trainer,
++++++--    savepath=(args.savepath, 'trainer_config.pkl'),
++++++--    # optimization parameters
++++++--    batch_size=args.batch_size,
++++++--    learning_rate=args.learning_rate,
++++++--    betas=(0.9, 0.95),
++++++--    grad_norm_clip=1.0,
++++++--    weight_decay=0.1, # only applied on matmul weights
++++++--    # learning rate decay: linear warmup followed by cosine decay to 10% of original
++++++--    lr_decay=args.lr_decay,
++++++--    warmup_tokens=warmup_tokens,
++++++--    final_tokens=final_tokens,
++++++--    ## dataloader
++++++--    num_workers=0,
++++++--    device=args.device,
++++++--)
++++++--
++++++--trainer = trainer_config()
++++++--
++++++--#######################
++++++--###### main loop ######
++++++--#######################
++++++--
++++++--## scale number of epochs to keep number of updates constant
++++++--n_epochs = int(1e6 / len(dataset) * args.n_epochs_ref)
++++++--save_freq = int(n_epochs // args.n_saves)
++++++--
++++++--for epoch in range(n_epochs):
++++++--    print(f'\nEpoch: {epoch} / {n_epochs} | {args.dataset} | {args.exp_name}')
++++++--
++++++--    trainer.train(model, dataset)
++++++--
++++++--    ## get greatest multiple of `save_freq` less than or equal to `save_epoch`
++++++--    save_epoch = (epoch + 1) // save_freq * save_freq
++++++--    statepath = os.path.join(args.savepath, f'state_{save_epoch}.pt')
++++++--    print(f'Saving model to {statepath}')
++++++--
++++++--    ## save state to disk
++++++--    state = model.state_dict()
++++++--    torch.save(state, statepath)
++++++-diff --git a/halfcheetah/scripts/xrl.py b/halfcheetah/scripts/xrl.py
++++++-deleted file mode 100644
++++++-index 134232a..0000000
++++++---- a/halfcheetah/scripts/xrl.py
++++++-+++ /dev/null
++++++-@@ -1,372 +0,0 @@
++++++--import json
++++++--import pdb
++++++--from os.path import join
++++++--
++++++--import trajectory.utils as utils
++++++--import trajectory.datasets as datasets
++++++--from trajectory.search import (
++++++--    make_prefix,
++++++--    update_context,
++++++--)
++++++--from trajectory.search.sampling import forward
++++++--
++++++--import gym
++++++--import d4rl # Import required to register environments, you may need to also import the submodule
++++++--import numpy as np
++++++--import d3rlpy
++++++--import math as mt
++++++--from sklearn.cluster import KMeans
++++++--from sklearn import datasets as skdatasets
++++++--from sklearn.decomposition import PCA
++++++--
++++++--from pyclustering.cluster.xmeans import xmeans
++++++--from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer
++++++--
++++++--from scipy.stats import wasserstein_distance
++++++--
++++++--class Parser(utils.Parser):
++++++--    dataset: str = 'halfcheetah-medium-expert-v2'
++++++--    config: str = 'config.offline'
++++++--
+++++++diff --git a/halfcheetah/scripts/xrl_v2.py b/halfcheetah/scripts/xrl_v2.py
+++++++index 62a3d4d..b012599 100644
+++++++--- a/halfcheetah/scripts/xrl_v2.py
++++++++++ b/halfcheetah/scripts/xrl_v2.py
+++++++@@ -21,54 +21,97 @@ from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer
+++++++ from scipy.stats import wasserstein_distance
+++++++ from moviepy.editor import VideoFileClip
+++++++ 
++++++++
+++++++ class Parser(utils.Parser):
+++++++     dataset: str = 'halfcheetah-medium-v2'
+++++++     config: str = 'config.offline'
+++++++ 
++++++ -# utils
++++++--    
++++++--class XMeans:
++++++--    def loglikelihood(self, r, rn, var, m, k):
++++++--        l1 = - rn / 2.0 * mt.log(2 * mt.pi)
++++++--        l2 = - rn * m / 2.0 * mt.log(var)
++++++--        l3 = - (rn - k) / 2.0
++++++--        l4 = rn * mt.log(rn)
++++++--        l5 = - rn * mt.log(r)
++++++--
++++++--        return l1 + l2 + l3 + l4 + l5
++++++--
++++++--    def __init__(self, X, kmax = 20):
++++++--        self.X = X
++++++--        self.num = np.size(self.X, axis=0)
++++++--        self.dim = np.size(X, axis=1)
++++++--        self.KMax = kmax
++++++--
++++++--    def fit(self):
++++++--        k = 1
++++++--        X = self.X
++++++--        M = self.dim
++++++--        num = self.num
++++++--
++++++--        while(1):
++++++--            ok = k
++++++--
++++++--            #Improve Params
++++++--            kmeans = KMeans(n_clusters=k).fit(X)
++++++--            labels = kmeans.labels_
++++++--            m = kmeans.cluster_centers_
++++++--
++++++--            #Improve Structure
++++++--            #Calculate BIC
++++++--            p = M + 1
++++++--
++++++--            obic = np.zeros(k)
++++++--
++++++--            for i in range(k):
++++++--                rn = np.size(np.where(labels == i))
++++++--                var = np.sum((X[labels == i] - m[i])**2)/float(rn - 1)
++++++--                obic[i] = self.loglikelihood(rn, rn, var, M, 1) - p/2.0*mt.log(rn)
++++++--
++++++--            #Split each cluster into two subclusters and calculate BIC of each splitted cluster
++++++--            sk = 2 #The number of subclusters
++++++--            nbic = np.zeros(k)
++++++--            addk = 0
++++++--
++++++--            for i in range(k):
++++++--                ci = X[labels == i]
++++++--                r = np.size(np.where(labels == i))
++++++--
++++++--                kmeans = KMeans(n_clusters=sk).fit(ci)
++++++--                ci_labels = kmeans.labels_
++++++--                sm = kmeans.cluster_centers_
++++++--
++++++--                for l in range(sk):
++++++--                    rn = np.size(np.where(ci_labels == l))
++++++--                    var = np.sum((ci[ci_labels == l] - sm[l])**2)/float(rn - sk)
++++++--                    nbic[i] += self.loglikelihood(r, rn, var, M, sk)
++++++--
++++++--                p = sk * (M + 1)
++++++--                nbic[i] -= p/2.0*mt.log(r)
++++++--
++++++--                if obic[i] < nbic[i]:
++++++--                    addk += 1
++++++--
++++++--            k += addk
++++++--
++++++--            if ok == k or k >= self.KMax:
++++++--                break
++++++--
++++++--
++++++--        #Calculate labels and centroids
++++++--        kmeans = KMeans(n_clusters=k).fit(X)
++++++--        self.labels = kmeans.labels_
++++++--        self.k = k
++++++--        self.m = kmeans.cluster_centers_
++++++--
++++++--
++++++--def cluster_trajectories(trajectories):
++++++--    xmeans_instance = XMeans(trajectories, kmax=10)
++++++--    xmeans_instance.fit()
++++++--
++++++--    clusters = xmeans_instance.labels
++++++--    return clusters
++++++--
++++++--def cluster_trajectories_2(trajectories):
+++++++ 
+++++++ def cluster_trajectories(trajectories, n_clusters=10):
+++++++-    """TODO"""
++++++++    """
++++++++    Cluster trajectories using X-means.
++++++++    
++++++++    Args:
++++++++    - trajectories: np.array, shape (n_trajectories, encoding_dim)
++++++++    - n_clusters: int, max number of clusters
++++++++    
++++++++    Returns:
++++++++    - idxs_per_cluster: list, trajectory idxs per cluster idxs
++++++++    - clusters: np.array, shape (n_trajectories), cluster idxs per trajectory idx
++++++++    """ 
+++++++ 
++++++ -    # Prepare initial centers - amount of initial centers defines amount of clusters from which X-Means will
++++++ -    # start analysis.
++++++--    amount_initial_centers = 2
++++++--    initial_centers = kmeans_plusplus_initializer(trajectories, amount_initial_centers).initialize()
++++++--    
++++++++    # Set 2 initial cluster centers
+++++++     amount_initial_centers = 2
+++++++     initial_centers = kmeans_plusplus_initializer(trajectories, amount_initial_centers).initialize()
+++++++     
++++++ -    # Create instance of X-Means algorithm. The algorithm will start analysis from 2 clusters, the maximum
++++++ -    # number of clusters that can be allocated is 10.
++++++--    xmeans_instance = xmeans(trajectories, initial_centers, 10)
++++++--    xmeans_instance.process()
++++++--    
++++++--    # Extract clustering results: clusters
++++++--    idxs_per_cluster = xmeans_instance.get_clusters()
++++++--
++++++--    clusters = []
++++++--    for i in range(len(trajectories)):
++++++--        for j in range(len(idxs_per_cluster)):
++++++--            if i in idxs_per_cluster[j]: clusters.append(j)
++++++--
++++++--    return idxs_per_cluster, np.array(clusters)
++++++++    # Run X-means
+++++++     xmeans_instance = xmeans(trajectories, initial_centers, n_clusters)
+++++++     xmeans_instance.process()
+++++++     
+++++++     # Extract clustering results: clusters
+++++++     idxs_per_cluster = xmeans_instance.get_clusters()
+++++++ 
++++++++    # Turn list of trajectory idxs per cluster to array of cluster idx per trajectory idx
+++++++     clusters = []
+++++++     for i in range(len(trajectories)):
+++++++         for j in range(len(idxs_per_cluster)):
+++++++             if i in idxs_per_cluster[j]: clusters.append(j)
+++++++ 
+++++++     return idxs_per_cluster, np.array(clusters)
++++++ - 
++++++--# https://github.com/sascha-kirch/ML_Notebooks/blob/main/Softmax_Temperature.ipynb
++++++--def softmax(x, temp):
++++++--    """Compute softmax values for each sets of scores in x."""
++++++--    return np.exp(np.divide(x,temp)) / np.sum(np.exp(np.divide(x,temp)))
++++++--
++++++--def generate_data_embedding(trajectory_embeddings, normalizing_factor=1, temperature=1):
++++++--    embedding = np.sum(trajectory_embeddings, axis=0) / normalizing_factor
++++++--    embedding = softmax(embedding, temperature)
++++++--    return embedding
++++++--
++++++--def embed_trajectory(gpt, discretizer, observations, actions, rewards, preprocess_fn):
++++++--    context = []
++++++--
++++++--    for i in range(len(observations)):
++++++--        observation = observations[i]
++++++--        action = actions[i]
++++++--        reward = rewards[i]
++++++--
++++++--        observation = preprocess_fn(observation)
++++++--
++++++--        # print(observation)
++++++--        prefix = make_prefix(discretizer, context, observation, True)
++++++--        # print("prefix", prefix.shape)
++++++--
++++++--        out = forward(gpt, prefix)
++++++--        # print("out", out.shape)
++++++--        context = update_context(context, discretizer, observation, action, reward, len(observations))
++++++--        # print("cotext", context)
++++++--    
++++++--    emb = []
++++++--    for context_step in context:
++++++--        emb.append(context_step.numpy())
++++++--    emb = np.array(emb)
++++++--    emb = np.mean(emb, axis=0)[0]
++++++--
++++++++
++++++++
+++++++ def softmax(x, temp):
+++++++-    """TODO"""
++++++++    """
++++++++    Softmax with temperature using max-trick.
++++++++    
++++++++    Args:
++++++++    - x: np.array, shape (n_data, dim_data)
++++++++    - temp: int, softmax temperature
++++++++    
++++++++    Returns:
++++++++    - softmax_x: np.array: shape (dim_data)
++++++++    """ 
++++++++
+++++++     max_x = np.max(x)
+++++++-    return np.exp(np.divide(x-max_x,temp)) / np.sum(np.exp(np.divide(x-max_x,temp)))
++++++++    softmax_x = np.exp(np.divide(x-max_x,temp)) / np.sum(np.exp(np.divide(x-max_x,temp)))
++++++++    return softmax_x
++++++++
+++++++ 
+++++++ def generate_data_embedding(trajectory_embeddings, temperature=10000):
+++++++-    """TODO"""
++++++++    """
++++++++    Generate data embedding (sum+softmax) for set of encoded trajectories.
++++++++    
++++++++    Args:
++++++++    - trajectory_embeddings: np.array, shape (n_data, dim_data)
++++++++    - temperature: int, softmax temperature
++++++++    
++++++++    Returns:
++++++++    - embedding: np.array, shape (dim_data)
++++++++    """ 
+++++++ 
+++++++     embedding = np.sum(trajectory_embeddings, axis=0)
+++++++     embedding = softmax(embedding, temperature)
+++++++     
+++++++-
+++++++     return embedding
+++++++ 
++++++++
+++++++ def embed_trajectory(gpt, discretizer, observations, actions, rewards, preprocess_fn):
+++++++-    """TODO"""
++++++++    """
++++++++    Encode trajectory using a trajectory transformer with a sliding window.
++++++++    
++++++++    Args:
++++++++    - gpt: trajectory transformer
++++++++    - discretizer: environment discretizer
++++++++    - observations: trajectory observations
++++++++    - actions: trajectory actions
++++++++    - rewards: trajectory rewards
++++++++    - preprocess_fn: observations preprocessing functions
++++++++    
++++++++    Returns:
++++++++    - embedding: np.array, shape (hidden_dim), encoded trajectory
++++++++    """ 
+++++++ 
+++++++     context = []
+++++++-
+++++++     output = []
+++++++ 
+++++++     for i in range(len(observations)):
+++++++@@ -76,12 +119,12 @@ def embed_trajectory(gpt, discretizer, observations, actions, rewards, preproces
+++++++         action = actions[i]
+++++++         reward = rewards[i]
+++++++ 
++++++++        # Preprocess, discretize & forward through trajectory transformer
+++++++         observation = preprocess_fn(observation)
+++++++-
+++++++         prefix = make_prefix(discretizer, context, observation, True)
+++++++-
+++++++         out = forward(gpt, prefix)
+++++++ 
++++++++        # Sliding window
+++++++         if len(context) >= 9:
+++++++             context.pop(0)
+++++++             if len(output) == 0:
+++++++@@ -91,19 +134,33 @@ def embed_trajectory(gpt, discretizer, observations, actions, rewards, preproces
+++++++ 
+++++++         context = update_context(context, discretizer, observation, action, reward, len(observations))
+++++++ 
+++++++-    emb = np.mean(output, axis=0)
++++++ -    return emb
++++++--
++++++--
++++++--def create_complementary_dataset(dataset, idxs, trajectory_length=10):
++++++--    observations = []
++++++--    actions = []
++++++--    rewards = []
++++++--    terminals = []
++++++--    for i in range(1000):
++++++--        if i not in idxs:
++++++--            observations += list(dataset.observations[1000*i:1000*i+trajectory_length])
++++++--            actions += list(dataset.actions[1000*i:1000*i+trajectory_length])
++++++--            rewards += list(dataset.rewards[1000*i:1000*i+trajectory_length])
++++++--            terminals += list(dataset.terminals[1000*i:1000*i+trajectory_length])
++++++--
++++++--    new_dataset = d3rlpy.dataset.MDPDataset(
++++++--        observations=np.array(observations),
++++++--        actions=np.array(actions),
++++++--        rewards=np.array(rewards),
++++++--        terminals=np.array(terminals)
++++++--    )
++++++--    return new_dataset
++++++--    
++++++--
++++++--
++++++--
++++++--def main():
++++++--    # args = Parser().parse_args('plan')
++++++--
++++++--    #######################
++++++--    ####### models ########
++++++--    #######################
++++++--
++++++--
++++++--
++++++--
++++++--
++++++--    # print(args.dataset)
++++++--
++++++--    # dataset = utils.load_from_config(args.logbase, args.dataset, args.gpt_loadpath,
++++++--    #         'data_config.pkl')
++++++--
++++++--
++++++--    # gpt, gpt_epoch = utils.load_model(args.logbase, args.dataset, args.gpt_loadpath,
++++++--    #         epoch=args.gpt_epoch, device=args.device)
++++++--
++++++--    # env = datasets.load_environment(args.dataset)
++++++--
++++++--    # discretizer = dataset.discretizer
++++++--
++++++--    # preprocess_fn = datasets.get_preprocess_fn(env.name)
++++++--
++++++--    # #######################
++++++--    # ####### dataset #######
++++++--    # #######################
++++++--
++++++--    # # env = datasets.load_environment(args.dataset)
++++++--    # discretizer = dataset.discretizer
++++++--    # preprocess_fn = datasets.get_preprocess_fn(env.name)
++++++--
++++++--    # # dataset
++++++--    dataset_d3, env = d3rlpy.datasets.get_dataset("halfcheetah-medium-v2")
++++++--
++++++--    # env = gym.make('halfcheetah-medium-v2')
++++++--    # dataset_d4 = d4rl.qlearning_dataset(env)
++++++--
++++++--    # # checks to see if d3rl & d4rl datasets are equal
++++++--    # print(np.allclose(dataset_d3.actions[100], dataset_d4['actions'][100]))
++++++--
++++++--    # # dr4rl has same trajectories, just cut off 1 element before the end
++++++--    # for j in range(1000):
++++++--    #     for i in range(999):
++++++--    #         if dataset_d4['rewards'][j * 999 + i] != dataset_d3.rewards[j * 1000 + i]: print("yo", i)
++++++--
++++++--    # #######################
++++++--    # ###### main loop ######
++++++--    # #######################
++++++--
++++++--    trajectory_length = 10 # 10 = max
++++++--
++++++--    # embeddings = []
++++++--    # for i in range(1000):
++++++--    #     observations = dataset_d3.observations[1000*i:1000*i+trajectory_length]
++++++--    #     actions = dataset_d3.actions[1000*i:1000*i+trajectory_length]
++++++--    #     rewards = dataset_d3.rewards[1000*i:1000*i+trajectory_length]
++++++--    #     terminals = dataset_d3.terminals[1000*i:1000*i+trajectory_length]
++++++--    #     emb = embed_trajectory(gpt, discretizer, observations, actions, rewards, preprocess_fn)
++++++--    #     embeddings.append(emb)
++++++--    # embeddings = np.array(embeddings)
++++++--    # np.save("embeddings.npy", embeddings)
++++++--    # print(embeddings)
++++++--
++++++--    embeddings = np.load("embeddings.npy")
++++++--
++++++--    pca = PCA(n_components=2)
++++++--    pca = PCA(n_components=2)
++++++--    pca_embeddings = pca.fit_transform(embeddings)
++++++--    np.save("pca.py", pca_embeddings)
++++++--
++++++--    idxs_per_cluster, clusters = cluster_trajectories_2(embeddings)
++++++--    # print(clusters)
++++++--    # return
++++++--    np.save("clusters.npy", clusters)
++++++--
++++++--    import matplotlib.pyplot as plt
++++++--
++++++--    d_orig = generate_data_embedding(embeddings)
++++++--    unique_clusters = np.unique(clusters)
++++++--    
++++++--    d_j = []
++++++--    complementary_datasets = []
++++++--    for j in np.sort(unique_clusters):
++++++--        print(j)
++++++--        d_j.append(generate_data_embedding(embeddings[clusters != j]))
++++++--        plt.scatter(pca_embeddings[clusters == j][:,0], pca_embeddings[clusters == j][:,1], label=j)
++++++--        complementary_datasets.append(create_complementary_dataset(dataset_d3, idxs_per_cluster[j], trajectory_length))
++++++--    
++++++--    original_dataset = create_complementary_dataset(dataset_d3, [], trajectory_length)
++++++--
++++++--    print(complementary_datasets, original_dataset)
++++++--
++++++--    plt.legend()
++++++--    plt.show()
++++++--
++++++--    agent_orig = d3rlpy.algos.SAC(
++++++--        actor_learning_rate=3e-4,
++++++--        critic_learning_rate=3e-4,
++++++--        temp_learning_rate=3e-4,
++++++--        batch_size=256)
++++++--
++++++--    print(agent_orig)
++++++--
++++++--    training_steps = 1000
++++++--
++++++--    agent_orig.fit(original_dataset, n_steps=training_steps)
++++++--
++++++--    agents_compl = []
++++++--
++++++--    for dset in complementary_datasets:
++++++--        agent = d3rlpy.algos.SAC(
++++++--            actor_learning_rate=3e-4,
++++++--            critic_learning_rate=3e-4,
++++++--            temp_learning_rate=3e-4,
++++++--            batch_size=256)
++++++--        agent.fit(dset, n_steps=training_steps)
++++++--        agents_compl.append(agent)
++++++--
++++++--    action_orig = agent_orig.predict(dataset_d3.observations[0])
++++++--
++++++--    actions_compl = []
++++++--    for agent in agents_compl:
++++++--        actions_compl.append(agent.predict(dataset_d3.observations[0]))
++++++--    
++++++--    action_dists = []
++++++--    for action in actions_compl:
++++++--        action_dists.append(np.linalg.norm(action_orig-action))
++++++--
++++++--    k = 3
++++++--    topk = np.argpartition(action_dists, -k)[-k:]
++++++--
++++++--    d_w = {}
++++++--    for idx in topk:
++++++--        d_w[idx] = wasserstein_distance(d_j[idx], d_orig)
++++++--
++++++--    cluster_assignment = min(d_w, key=d_w.get)
++++++--    print("explanation assigned to cluster", cluster_assignment)
++++++--
++++++--    
++++++--def assignment_test():
++++++--    action_orig = np.random.rand(10)
++++++--    d_orig = np.random.rand(5)
++++++--
++++++--    actions_compl = np.random.rand(6,10)
++++++--    d_j = np.random.rand(6,5)
++++++--
++++++--    action_dists = []
++++++--    for action in actions_compl:
++++++--        action_dists.append(np.linalg.norm(action_orig-action))
++++++--
++++++--    print(action_dists)
++++++--
++++++--    k = 3
++++++--    topk = np.argpartition(action_dists, -k)[-k:]
++++++--
++++++--    print(topk)
++++++--
++++++--    d_w = {}
++++++--    for idx in topk:
++++++--        d_w[idx] = wasserstein_distance(d_j[idx], d_orig)
++++++--
++++++--    print(d_w)
++++++--
++++++--    cluster_assignment = min(d_w, key=d_w.get)
++++++--    print("explanation assigned to cluster", cluster_assignment)
++++++--
++++++--
++++++--if __name__ == "__main__":
++++++--    # main()
++++++--    assignment_test()
++++++-diff --git a/halfcheetah/trajectory.egg-info/PKG-INFO b/halfcheetah/trajectory.egg-info/PKG-INFO
++++++-index 452c6cb..2603850 100644
++++++---- a/halfcheetah/trajectory.egg-info/PKG-INFO
++++++-+++ b/halfcheetah/trajectory.egg-info/PKG-INFO
++++++-@@ -1,4 +1,11 @@
++++++- Metadata-Version: 2.1
++++++- Name: trajectory
++++++- Version: 0.0.0
++++++-+Summary: UNKNOWN
++++++-+Home-page: UNKNOWN
++++++-+License: UNKNOWN
++++++-+Platform: UNKNOWN
++++++- License-File: LICENSE
++++++++    # Embedding is the average of encoded states
++++++++    embedding = np.mean(output, axis=0)
++++++++    return embedding
++++++++
+++++++ 
+++++++ def create_complementary_dataset(dataset, idxs, trajectory_length=10, inverse=False):
+++++++-    """TODO"""
++++++++    """
++++++++    Encode trajectory using a trajectory transformer with a sliding window.
++++++++    
++++++++    Args:
++++++++    - dataset: MDPDataset, original d3rl dataset
++++++++    - idxs: trajectory idxs to ignore (or include if inverse is True)
++++++++    - trajectory_length: int, trajectory length
++++++++    - inverse: bool, if True the dataset is not complementary
++++++ +
++++++-+UNKNOWN
++++++++    Returns:
++++++++    - new_dataset: MDPDataset, complementary dataset
++++++++    """ 
+++++++ 
+++++++     observations = []
+++++++     actions = []
+++++++     rewards = []
+++++++     terminals = []
+++++++ 
+++++++-    n_trajs = int(1000000/trajectory_length)
++++++++    n_trajs = int(len(dataset.observations)/trajectory_length)
+++++++     for i in range(n_trajs):
++++++++        # If inverse is True, only include idxs. If not, leave out idxs
+++++++         condition = i not in idxs
+++++++         if inverse: condition = not condition
+++++++ 
+++++++@@ -112,6 +169,7 @@ def create_complementary_dataset(dataset, idxs, trajectory_length=10, inverse=Fa
+++++++             actions += list(dataset.actions[trajectory_length*i:trajectory_length*(i+1)])
+++++++             rewards += list(dataset.rewards[trajectory_length*i:trajectory_length*(i+1)])
+++++++ 
++++++++    # Trajectories end with a terminal state
+++++++     terminals = np.tile([0]*(trajectory_length-1)+[1], int(len(observations)/trajectory_length))
+++++++ 
+++++++     new_dataset = d3rlpy.dataset.MDPDataset(
+++++++@@ -124,6 +182,17 @@ def create_complementary_dataset(dataset, idxs, trajectory_length=10, inverse=Fa
+++++++     
+++++++ 
+++++++ def clusters_to_idxs(clusters):
++++++++    """
++++++++    Helper function to turn array of cluster idxs per trajectory idxs to a list 
++++++++    of trajectory idxs per cluster idx.
++++++++    
++++++++    Args:
++++++++    - clusters: np.array, cluster idx per trajectory idx
++++++ +
++++++++    Returns:
++++++++    - idxs_per_cluster: list, trajectory idxs per cluster idx
++++++++    """ 
++++++++
+++++++     idxs_per_cluster = []
+++++++     for i in np.sort(np.unique(clusters)):
+++++++         idxs_per_cluster.append(list(np.argwhere(clusters == i).flatten()))
+++++++@@ -142,7 +211,7 @@ def main():
+++++++ 
+++++++     ### IMPORTANT DEFINITIONS XRL SCRIPT ###
+++++++ 
+++++++-    load_embeddings = False
++++++++    load_embeddings = True
+++++++     load_clusters = True
+++++++     load_agents = True
+++++++     generate_human_study = False
++++++ diff --git a/halfcheetah/trajectory.egg-info/SOURCES.txt b/halfcheetah/trajectory.egg-info/SOURCES.txt
++++++-index 4474d85..84e8e3a 100644
+++++++index 84e8e3a..4474d85 100644
++++++ --- a/halfcheetah/trajectory.egg-info/SOURCES.txt
++++++ +++ b/halfcheetah/trajectory.egg-info/SOURCES.txt
++++++-@@ -30,4 +30,5 @@ trajectory/utils/serialization.py
+++++++@@ -30,5 +30,4 @@ trajectory/utils/serialization.py
++++++  trajectory/utils/setup.py
++++++  trajectory/utils/timer.py
++++++  trajectory/utils/training.py
++++++ -trajectory/utils/video.py
+++++++-trajectory_aaa/__init__.py
++++++ \ No newline at end of file
++++++ +trajectory/utils/video.py
++++++-+trajectory_aaa/__init__.py
++++++ \ No newline at end of file
++++++ diff --git a/halfcheetah/trajectory.egg-info/top_level.txt b/halfcheetah/trajectory.egg-info/top_level.txt
++++++-index ce65198..1d5271f 100644
+++++++index 1d5271f..ce65198 100644
++++++ --- a/halfcheetah/trajectory.egg-info/top_level.txt
++++++ +++ b/halfcheetah/trajectory.egg-info/top_level.txt
++++++-@@ -1 +1,2 @@
+++++++@@ -1,2 +1 @@
++++++  trajectory
++++++-+trajectory_aaa
+++++ \ No newline at end of file
+++++-diff --git a/halfcheetah/pca.py.npy b/halfcheetah/pca.py.npy
+++++-deleted file mode 100644
+++++-index bb19150..0000000
+++++-Binary files a/halfcheetah/pca.py.npy and /dev/null differ
+++++-diff --git a/halfcheetah/plotting/bar.png b/halfcheetah/plotting/bar.png
+++++-deleted file mode 100644
+++++-index 3679667..0000000
+++++-Binary files a/halfcheetah/plotting/bar.png and /dev/null differ
+++++-diff --git a/halfcheetah/plotting/plot.py b/halfcheetah/plotting/plot.py
+++++-deleted file mode 100644
+++++-index 163d0e4..0000000
+++++---- a/halfcheetah/plotting/plot.py
+++++-+++ /dev/null
+++++-@@ -1,74 +0,0 @@
+++++--import numpy as np
+++++--import matplotlib
+++++--import matplotlib.pyplot as plt
+++++--import pdb
+++++--
+++++--from plotting.scores import means
+++++--
+++++--class Colors:
+++++--	grey = '#B4B4B4'
+++++--	gold = '#F6C781'
+++++--	red = '#EC7C7D'
+++++--	blue = '#70ABCC'
+++++--
+++++--LABELS = {
+++++--	# 'BC': 'Behavior\nCloning',
+++++--	# 'MBOP': 'Model-Based\nOffline Planning',
+++++--	# 'BRAC': 'Behavior-Reg.\nActor-Critic',
+++++--	# 'CQL': 'Conservative\nQ-Learning',
+++++--}
+++++--
+++++--def get_mean(results, exclude=None):
+++++--	'''
+++++--		results : { environment: score, ... }
+++++--	'''
+++++--	filtered = {
+++++--		k: v for k, v in results.items()
+++++--		if (not exclude) or (exclude and exclude not in k)
+++++--	}
+++++--	return np.mean(list(filtered.values()))
+++++--
+++++--if __name__ == '__main__':
+++++--
+++++--	#################
+++++--	## latex
+++++--	#################
+++++--	matplotlib.rc('font', **{'family': 'serif', 'serif': ['Computer Modern']})
+++++--	matplotlib.rc('text', usetex=True)
+++++--	matplotlib.rcParams['text.latex.preamble']=[r"\usepackage{amsmath}"]
+++++--	#################
+++++--
+++++--	fig = plt.gcf()
+++++--	ax = plt.gca()
+++++--	fig.set_size_inches(7.5, 2.5)
+++++--
+++++--	means = {k: get_mean(v, exclude='ant') for k, v in means.items()}
+++++--	print(means)
+++++--
+++++--	algs = ['BC', 'MBOP', 'BRAC', 'CQL', 'Decision\nTransformer', 'Trajectory\nTransformer']
+++++--	vals = [means[alg] for alg in algs]
+++++--
+++++--	colors = [
+++++--		Colors.grey, Colors.gold,
+++++--		Colors.red, Colors.red, Colors.blue, Colors.blue
+++++--	]
+++++--
+++++--	labels = [LABELS.get(alg, alg) for alg in algs]
+++++--	plt.bar(labels, vals, color=colors, edgecolor=Colors.gold, lw=0)
+++++--	plt.ylabel('Average normalized return', labelpad=15)
+++++--	# plt.title('Offline RL Results')
+++++--
+++++--	legend_labels = ['Behavior Cloning', 'Trajectory Optimization', 'Temporal Difference', 'Sequence Modeling']
+++++--	colors = [Colors.grey, Colors.gold, Colors.red, Colors.blue]
+++++--	handles = [plt.Rectangle((0,0),1,1, color=color) for label, color in zip(legend_labels, colors)]
+++++--	plt.legend(handles, legend_labels, ncol=4,
+++++--		bbox_to_anchor=(1.07, -.18), fancybox=False, framealpha=0, shadow=False, columnspacing=1.5, handlelength=1.5)
+++++--
+++++--	matplotlib.rcParams['hatch.linewidth'] = 7.5
+++++--	# ax.patches[-1].set_hatch('/')
+++++--
+++++--	ax.spines['right'].set_visible(False)
+++++--	ax.spines['top'].set_visible(False)
+++++--
+++++--	# plt.savefig('plotting/bar.pdf', bbox_inches='tight')
+++++--	plt.savefig('plotting/bar.png', bbox_inches='tight', dpi=500)
+++++-diff --git a/halfcheetah/plotting/read_results.py b/halfcheetah/plotting/read_results.py
+++++-deleted file mode 100644
+++++-index 5a5fb62..0000000
+++++---- a/halfcheetah/plotting/read_results.py
+++++-+++ /dev/null
+++++-@@ -1,70 +0,0 @@
+++++--import os
+++++--import glob
+++++--import numpy as np
+++++--import json
+++++--import pdb
+++++--
+++++--import trajectory.utils as utils
+++++--
+++++--DATASETS = [
+++++--	f'{env}-{buffer}'
+++++--	for env in ['hopper', 'walker2d', 'halfcheetah', 'ant']
+++++--	for buffer in ['medium-expert-v2', 'medium-v2', 'medium-replay-v2']
+++++--]
+++++--
+++++--LOGBASE = 'logs'
+++++--TRIAL = '*'
+++++--EXP_NAME = 'plans/pretrained'
+++++--
+++++--def load_results(paths):
+++++--	'''
+++++--		paths : path to directory containing experiment trials
+++++--	'''
+++++--	scores = []
+++++--	for i, path in enumerate(sorted(paths)):
+++++--		score = load_result(path)
+++++--		if score is None:
+++++--			print(f'Skipping {path}')
+++++--			continue
+++++--		scores.append(score)
+++++--
+++++--		suffix = path.split('/')[-1]
+++++--
+++++--	mean = np.mean(scores)
+++++--	err = np.std(scores) / np.sqrt(len(scores))
+++++--	return mean, err, scores
+++++--
+++++--def load_result(path):
+++++--	'''
+++++--		path : path to experiment directory; expects `rollout.json` to be in directory
+++++--	'''
+++++--	fullpath = os.path.join(path, 'rollout.json')
+++++--	suffix = path.split('/')[-1]
+++++--
+++++--	if not os.path.exists(fullpath):
+++++--		return None
+++++--
+++++--	results = json.load(open(fullpath, 'rb'))
+++++--	score = results['score']
+++++--	return score * 100
+++++--
+++++--#######################
+++++--######## setup ########
+++++--#######################
+++++--
+++++--if __name__ == '__main__':
+++++--
+++++--	class Parser(utils.Parser):
+++++--	    dataset: str = None
+++++--
+++++--	args = Parser().parse_args()
+++++--
+++++--	for dataset in ([args.dataset] if args.dataset else DATASETS):
+++++--		subdirs = glob.glob(os.path.join(LOGBASE, dataset, EXP_NAME))
+++++--
+++++--		for subdir in subdirs:
+++++--			reldir = subdir.split('/')[-1]
+++++--			paths = glob.glob(os.path.join(subdir, TRIAL))
+++++--
+++++--			mean, err, scores = load_results(paths)
+++++--			print(f'{dataset.ljust(30)} | {subdir.ljust(50)} | {len(scores)} scores \n    {mean:.2f} +/- {err:.2f}\n')
+++++-diff --git a/halfcheetah/plotting/scores.py b/halfcheetah/plotting/scores.py
+++++-deleted file mode 100644
+++++-index f1917f7..0000000
+++++---- a/halfcheetah/plotting/scores.py
+++++-+++ /dev/null
+++++-@@ -1,123 +0,0 @@
+++++--means = {
+++++--	'Trajectory\nTransformer': {
+++++--		##
+++++--		'halfcheetah-medium-expert-v2': 95.0,
+++++--		'hopper-medium-expert-v2': 110.0,
+++++--		'walker2d-medium-expert-v2': 101.9,
+++++--		'ant-medium-expert-v2': 116.1,
+++++--		##
+++++--		'halfcheetah-medium-v2': 46.9,
+++++--		'hopper-medium-v2': 61.1,
+++++--		'walker2d-medium-v2': 79.0,
+++++--		'ant-medium-v2': 83.1,
+++++--		##
+++++--		'halfcheetah-medium-replay-v2': 41.9,
+++++--		'hopper-medium-replay-v2': 91.5,
+++++--		'walker2d-medium-replay-v2': 82.6,
+++++--		'ant-medium-replay-v2': 77.0,
+++++--	},
+++++--	'Decision\nTransformer': {
+++++--		##
+++++--		'halfcheetah-medium-expert-v2': 86.8,
+++++--		'hopper-medium-expert-v2': 107.6,
+++++--		'walker2d-medium-expert-v2': 108.1,
+++++--		##
+++++--		'halfcheetah-medium-v2': 42.6,
+++++--		'hopper-medium-v2': 67.6,
+++++--		'walker2d-medium-v2': 74.0,
+++++--		##
+++++--		'halfcheetah-medium-replay-v2': 36.6,
+++++--		'hopper-medium-replay-v2': 82.7,
+++++--		'walker2d-medium-replay-v2': 66.6,
+++++--	},
+++++--	'CQL': {
+++++--		##
+++++--		'halfcheetah-medium-expert-v2': 91.6,
+++++--		'hopper-medium-expert-v2': 105.4,
+++++--		'walker2d-medium-expert-v2': 108.8,
+++++--		##
+++++--		'halfcheetah-medium-v2': 44.0,
+++++--		'hopper-medium-v2': 58.5,
+++++--		'walker2d-medium-v2': 72.5,
+++++--		##
+++++--		'halfcheetah-medium-replay-v2': 45.5,
+++++--		'hopper-medium-replay-v2': 95.0,
+++++--		'walker2d-medium-replay-v2': 77.2,
+++++--	},
+++++--	'MOPO': {
+++++--		##
+++++--		'halfcheetah-medium-expert-v2': 63.3,
+++++--		'hopper-medium-expert-v2': 23.7,
+++++--		'walker2d-medium-expert-v2': 44.6,
+++++--		##
+++++--		'halfcheetah-medium-v2': 42.3,
+++++--		'hopper-medium-v2': 28.0,
+++++--		'walker2d-medium-v2': 17.8,
+++++--		##
+++++--		'halfcheetah-medium-replay-v2': 53.1,
+++++--		'hopper-medium-replay-v2': 67.5,
+++++--		'walker2d-medium-replay-v2':39.0,
+++++--	},
+++++--	'MBOP': {
+++++--		##
+++++--		'halfcheetah-medium-expert-v2': 105.9,
+++++--		'hopper-medium-expert-v2': 55.1,
+++++--		'walker2d-medium-expert-v2': 70.2,
+++++--		##
+++++--		'halfcheetah-medium-v2': 44.6,
+++++--		'hopper-medium-v2': 48.8,
+++++--		'walker2d-medium-v2': 41.0,
+++++--		##
+++++--		'halfcheetah-medium-replay-v2': 42.3,
+++++--		'hopper-medium-replay-v2': 12.4,
+++++--		'walker2d-medium-replay-v2': 9.7,
+++++--	},
+++++--	'BRAC': {
+++++--		##
+++++--		'halfcheetah-medium-expert-v2': 41.9,
+++++--		'hopper-medium-expert-v2': 0.9,
+++++--		'walker2d-medium-expert-v2': 81.6,
+++++--		##
+++++--		'halfcheetah-medium-v2': 46.3,
+++++--		'hopper-medium-v2': 31.3,
+++++--		'walker2d-medium-v2': 81.1,
+++++--		##
+++++--		'halfcheetah-medium-replay-v2': 47.7,
+++++--		'hopper-medium-replay-v2': 0.6,
+++++--		'walker2d-medium-replay-v2': 0.9,
+++++--	},
+++++--	'BC': {
+++++--		##
+++++--		'halfcheetah-medium-expert-v2': 59.9,
+++++--		'hopper-medium-expert-v2': 79.6,
+++++--		'walker2d-medium-expert-v2': 36.6,
+++++--		##
+++++--		'halfcheetah-medium-v2': 43.1,
+++++--		'hopper-medium-v2': 63.9,
+++++--		'walker2d-medium-v2': 77.3,
+++++--		##
+++++--		'halfcheetah-medium-replay-v2': 4.3,
+++++--		'hopper-medium-replay-v2': 27.6,
+++++--		'walker2d-medium-replay-v2': 36.9,
+++++--	},
+++++--}
+++++--
+++++--errors = {
+++++--	'Trajectory\nTransformer': {
+++++--		##
+++++--		'halfcheetah-medium-expert-v2': 0.2,
+++++--		'hopper-medium-expert-v2': 2.7,
+++++--		'walker2d-medium-expert-v2': 6.8,
+++++--		'ant-medium-expert-v2': 9.0,
+++++--		##
+++++--		'halfcheetah-medium-v2': 0.4,
+++++--		'hopper-medium-v2': 3.6,
+++++--		'walker2d-medium-v2': 2.8,
+++++--		'ant-medium-v2': 7.3,
+++++--		##
+++++--		'halfcheetah-medium-replay-v2': 2.5,
+++++--		'hopper-medium-replay-v2': 3.6,
+++++--		'walker2d-medium-replay-v2': 6.9,
+++++--		'ant-medium-replay-v2': 6.8,
+++++--	},
+++++--}
+++++++-trajectory_aaa
+++++++diff --git a/seaquest/readme.md b/seaquest/readme.md
+++++++index 84e53f8..53561f9 100644
+++++++--- a/seaquest/readme.md
++++++++++ b/seaquest/readme.md
+++++++@@ -10,4 +10,4 @@ pip install git+https://github.com/takuseno/d4rl-atari
+++++++ pip install "gym[atari, accept-rom-license]"
+++++++ pip install pyclustering
+++++++ pip install seaborn
+++++++-pip install d3rlpy==1.1.1
+++++++\ No newline at end of file
++++++++pip install d3rlpy==1.1.1
+++++ \ No newline at end of file
+++++-diff --git a/halfcheetah/plotting/table.py b/halfcheetah/plotting/table.py
+++++-deleted file mode 100644
+++++-index eae74e6..0000000
+++++---- a/halfcheetah/plotting/table.py
+++++-+++ /dev/null
+++++-@@ -1,127 +0,0 @@
+++++--import numpy as np
+++++--import pdb
+++++--
+++++--from plotting.plot import get_mean
+++++--from plotting.scores import (
+++++--	means as MEANS,
+++++--	errors as ERRORS,
+++++--)
+++++--
+++++--ALGORITHM_STRINGS = {
+++++--	'Trajectory\nTransformer': 'TT (Ours)',
+++++--	'Decision\nTransformer': 'DT',	
+++++--}
+++++--
+++++--BUFFER_STRINGS = {
+++++--	'medium-expert': 'Medium-Expert',
+++++--	'medium': 'Medium',
+++++--	'medium-replay': 'Medium-Replay',	
+++++--}
+++++--
+++++--ENVIRONMENT_STRINGS = {
+++++--	'halfcheetah': 'HalfCheetah',
+++++--	'hopper': 'Hopper',
+++++--	'walker2d': 'Walker2d',
+++++--	'ant': 'Ant',
+++++--}
+++++--
+++++--SHOW_ERRORS = ['Trajectory\nTransformer']
+++++--
+++++--def get_result(algorithm, buffer, environment, version='v2'):
+++++--	key = f'{environment}-{buffer}-{version}'
+++++--	mean = MEANS[algorithm].get(key, '-')
+++++--	if algorithm in SHOW_ERRORS:
+++++--		error = ERRORS[algorithm].get(key)
+++++--		return (mean, error)
+++++--	else:
+++++--		return mean
+++++--
+++++--def format_result(result):
+++++--	if type(result) == tuple:
+++++--		mean, std = result
+++++--		return f'${mean}$ \\scriptsize{{\\raisebox{{1pt}}{{$\\pm {std}$}}}}'
+++++--	else:
+++++--		return f'${result}$'
+++++--
+++++--def format_row(buffer, environment, results):
+++++--	buffer_str = BUFFER_STRINGS[buffer]
+++++--	environment_str = ENVIRONMENT_STRINGS[environment]
+++++--	results_str = ' & '.join(format_result(result) for result in results)
+++++--	row = f'{buffer_str} & {environment_str} & {results_str} \\\\ \n'
+++++--	return row
+++++--
+++++--def format_buffer_block(algorithms, buffer, environments):
+++++--	block_str = '\\midrule\n'
+++++--	for environment in environments:
+++++--		results = [get_result(alg, buffer, environment) for alg in algorithms]
+++++--		row_str = format_row(buffer, environment, results)
+++++--		block_str += row_str
+++++--	return block_str
+++++--
+++++--def format_algorithm(algorithm):
+++++--	algorithm_str = ALGORITHM_STRINGS.get(algorithm, algorithm)
+++++--	return f'\multicolumn{{1}}{{c}}{{\\bf {algorithm_str}}}'
+++++--
+++++--def format_algorithms(algorithms):
+++++--	return ' & '.join(format_algorithm(algorithm) for algorithm in algorithms)
+++++--
+++++--def format_averages(means, label):
+++++--	prefix = f'\\multicolumn{{2}}{{c}}{{\\bf Average ({label})}} & '
+++++--	formatted = ' & '.join(str(mean) for mean in means)
+++++--	return prefix + formatted
+++++--
+++++--def format_averages_block(algorithms):
+++++--	means_filtered = [np.round(get_mean(MEANS[algorithm], exclude='ant'), 1) for algorithm in algorithms]
+++++--	means_all = [np.round(get_mean(MEANS[algorithm], exclude=None), 1) for algorithm in algorithms]
+++++--
+++++--	means_all = [
+++++--		means
+++++--		if 'ant-medium-expert-v2' in MEANS[algorithm]
+++++--		else '$-$'
+++++--		for algorithm, means in zip(algorithms, means_all)
+++++--	]
+++++--
+++++--	formatted_filtered = format_averages(means_filtered, 'without Ant')
+++++--	formatted_all = format_averages(means_all, 'all settings')
+++++--
+++++--	formatted_block = (
+++++--		f'{formatted_filtered} \\hspace{{.6cm}} \\\\ \n'
+++++--		f'{formatted_all} \\hspace{{.6cm}} \\\\ \n'
+++++--	)
+++++--	return formatted_block
+++++--
+++++--def format_table(algorithms, buffers, environments):
+++++--	justify_str = 'll' + 'r' * len(algorithms)
+++++--	algorithm_str = format_algorithms(['Dataset', 'Environment'] + algorithms)
+++++--	averages_str = format_averages_block(algorithms)
+++++--	table_prefix = (
+++++--		'\\begin{table*}[h]\n'
+++++--		'\\centering\n'
+++++--		'\\small\n'
+++++--		f'\\begin{{tabular}}{{{justify_str}}}\n'
+++++--		'\\toprule\n'
+++++--		f'{algorithm_str} \\\\ \n'
+++++--	)
+++++--	table_suffix = (
+++++--		'\\midrule\n'
+++++--		f'{averages_str}'
+++++--		'\\bottomrule\n'
+++++--		'\\end{tabular}\n'
+++++--		'\\label{table:d4rl}\n'
+++++--		'\\end{table*}'
+++++--	)
+++++--	blocks = ''.join(format_buffer_block(algorithms, buffer, environments) for buffer in buffers)
+++++--	table = (
+++++--		f'{table_prefix}'
+++++--		f'{blocks}'
+++++--		f'{table_suffix}'
+++++--	)
+++++--	return table
+++++--
+++++--
+++++--algorithms =['BC', 'MBOP', 'BRAC', 'CQL',  'Decision\nTransformer', 'Trajectory\nTransformer']
+++++--buffers = ['medium-expert', 'medium', 'medium-replay']
+++++--environments = ['halfcheetah', 'hopper', 'walker2d', 'ant']
+++++--
+++++--table = format_table(algorithms, buffers, environments)
+++++--print(table)
+++++-diff --git a/halfcheetah/scripts/plan.py b/halfcheetah/scripts/plan.py
+++++-deleted file mode 100644
+++++-index f13d4cc..0000000
+++++---- a/halfcheetah/scripts/plan.py
+++++-+++ /dev/null
+++++-@@ -1,124 +0,0 @@
+++++--import json
+++++--import pdb
+++++--from os.path import join
+++++--
+++++--import trajectory.utils as utils
+++++--import trajectory.datasets as datasets
+++++--from trajectory.search import (
+++++--    beam_plan,
+++++--    make_prefix,
+++++--    extract_actions,
+++++--    update_context,
+++++--)
+++++--
+++++--class Parser(utils.Parser):
+++++--    dataset: str = 'halfcheetah-medium-expert-v2'
+++++--    config: str = 'config.offline'
+++++--
+++++--#######################
+++++--######## setup ########
+++++--#######################
+++++--
+++++--args = Parser().parse_args('plan')
+++++--
+++++--#######################
+++++--####### models ########
+++++--#######################
+++++--
+++++--dataset = utils.load_from_config(args.logbase, args.dataset, args.gpt_loadpath,
+++++--        'data_config.pkl')
+++++--
+++++--gpt, gpt_epoch = utils.load_model(args.logbase, args.dataset, args.gpt_loadpath,
+++++--        epoch=args.gpt_epoch, device=args.device)
+++++--
+++++--#######################
+++++--####### dataset #######
+++++--#######################
+++++--
+++++--env = datasets.load_environment(args.dataset)
+++++--print('yo')
+++++--renderer = utils.make_renderer(args)
+++++--timer = utils.timer.Timer()
+++++--
+++++--discretizer = dataset.discretizer
+++++--discount = dataset.discount
+++++--observation_dim = dataset.observation_dim
+++++--action_dim = dataset.action_dim
+++++--
+++++--value_fn = lambda x: discretizer.value_fn(x, args.percentile)
+++++--preprocess_fn = datasets.get_preprocess_fn(env.name)
+++++--
+++++--print('yo2')
+++++--
+++++--#######################
+++++--###### main loop ######
+++++--#######################
+++++--
+++++--observation = env.reset()
+++++--total_reward = 0
+++++--
+++++--## observations for rendering
+++++--rollout = [observation.copy()]
+++++--
+++++--## previous (tokenized) transitions for conditioning transformer
+++++--context = []
+++++--
+++++--T = env.max_episode_steps
+++++--for t in range(T):
+++++--
+++++--    observation = preprocess_fn(observation)
+++++--
+++++--    if t % args.plan_freq == 0:
+++++--        ## concatenate previous transitions and current observations to input to model
+++++--        prefix = make_prefix(discretizer, context, observation, args.prefix_context)
+++++--
+++++--        ## sample sequence from model beginning with `prefix`
+++++--        sequence = beam_plan(
+++++--            gpt, value_fn, prefix,
+++++--            args.horizon, args.beam_width, args.n_expand, observation_dim, action_dim,
+++++--            discount, args.max_context_transitions, verbose=args.verbose,
+++++--            k_obs=args.k_obs, k_act=args.k_act, cdf_obs=args.cdf_obs, cdf_act=args.cdf_act,
+++++--        )
+++++--
+++++--    else:
+++++--        sequence = sequence[1:]
+++++--
+++++--    ## [ horizon x transition_dim ] convert sampled tokens to continuous trajectory
+++++--    sequence_recon = discretizer.reconstruct(sequence)
+++++--
+++++--    ## [ action_dim ] index into sampled trajectory to grab first action
+++++--    action = extract_actions(sequence_recon, observation_dim, action_dim, t=0)
+++++--
+++++--    ## execute action in environment
+++++--    next_observation, reward, terminal, _ = env.step(action)
+++++--
+++++--    ## update return
+++++--    total_reward += reward
+++++--    score = env.get_normalized_score(total_reward)
+++++--
+++++--    ## update rollout observations and context transitions
+++++--    rollout.append(next_observation.copy())
+++++--    context = update_context(context, discretizer, observation, action, reward, args.max_context_transitions)
+++++--
+++++--    print(
+++++--        f'[ plan ] t: {t} / {T} | r: {reward:.2f} | R: {total_reward:.2f} | score: {score:.4f} | '
+++++--        f'time: {timer():.2f} | {args.dataset} | {args.exp_name} | {args.suffix}\n'
+++++--    )
+++++--
+++++--    ## visualization
+++++--    if t % args.vis_freq == 0 or terminal or t == T:
+++++--
+++++--        ## save current plan
+++++--        renderer.render_plan(join(args.savepath, f'{t}_plan.mp4'), sequence_recon, env.state_vector())
+++++--
+++++--        ## save rollout thus far
+++++--        renderer.render_rollout(join(args.savepath, f'rollout.mp4'), rollout, fps=80)
+++++--
+++++--    if terminal: break
+++++--
+++++--    observation = next_observation
+++++--
+++++--## save result as a json file
+++++--json_path = join(args.savepath, 'rollout.json')
+++++--json_data = {'score': score, 'step': t, 'return': total_reward, 'term': terminal, 'gpt_epoch': gpt_epoch}
+++++--json.dump(json_data, open(json_path, 'w'), indent=2, sort_keys=True)
+++++-diff --git a/halfcheetah/scripts/train.py b/halfcheetah/scripts/train.py
+++++-deleted file mode 100644
+++++-index 04af8d7..0000000
+++++---- a/halfcheetah/scripts/train.py
+++++-+++ /dev/null
+++++-@@ -1,122 +0,0 @@
+++++--import os
+++++--import numpy as np
+++++--import torch
+++++--import pdb
+++++--
+++++--import trajectory.utils as utils
+++++--import trajectory.datasets as datasets
+++++--from trajectory.models.transformers import GPT
+++++--
+++++--
+++++--class Parser(utils.Parser):
+++++--    dataset: str = 'halfcheetah-medium-expert-v2'
+++++--    config: str = 'config.offline'
+++++--
+++++--#######################
+++++--######## setup ########
+++++--#######################
+++++--
+++++--args = Parser().parse_args('train')
+++++--
+++++--#######################
+++++--####### dataset #######
+++++--#######################
+++++--
+++++--env = datasets.load_environment(args.dataset)
+++++--
+++++--sequence_length = args.subsampled_sequence_length * args.step
+++++--
+++++--dataset_config = utils.Config(
+++++--    datasets.DiscretizedDataset,
+++++--    savepath=(args.savepath, 'data_config.pkl'),
+++++--    env=args.dataset,
+++++--    N=args.N,
+++++--    penalty=args.termination_penalty,
+++++--    sequence_length=sequence_length,
+++++--    step=args.step,
+++++--    discount=args.discount,
+++++--    discretizer=args.discretizer,
+++++--)
+++++--
+++++--dataset = dataset_config()
+++++--obs_dim = dataset.observation_dim
+++++--act_dim = dataset.action_dim
+++++--transition_dim = dataset.joined_dim
+++++--
+++++--#######################
+++++--######## model ########
+++++--#######################
+++++--
+++++--block_size = args.subsampled_sequence_length * transition_dim - 1
+++++--print(
+++++--    f'Dataset size: {len(dataset)} | '
+++++--    f'Joined dim: {transition_dim} '
+++++--    f'(observation: {obs_dim}, action: {act_dim}) | Block size: {block_size}'
+++++--)
+++++--
+++++--model_config = utils.Config(
+++++--    GPT,
+++++--    savepath=(args.savepath, 'model_config.pkl'),
+++++--    ## discretization
+++++--    vocab_size=args.N, block_size=block_size,
+++++--    ## architecture
+++++--    n_layer=args.n_layer, n_head=args.n_head, n_embd=args.n_embd*args.n_head,
+++++--    ## dimensions
+++++--    observation_dim=obs_dim, action_dim=act_dim, transition_dim=transition_dim,
+++++--    ## loss weighting
+++++--    action_weight=args.action_weight, reward_weight=args.reward_weight, value_weight=args.value_weight,
+++++--    ## dropout probabilities
+++++--    embd_pdrop=args.embd_pdrop, resid_pdrop=args.resid_pdrop, attn_pdrop=args.attn_pdrop,
+++++--)
+++++--
+++++--model = model_config()
+++++--model.to(args.device)
+++++--
+++++--#######################
+++++--####### trainer #######
+++++--#######################
+++++--
+++++--warmup_tokens = len(dataset) * block_size ## number of tokens seen per epoch
+++++--final_tokens = 20 * warmup_tokens
+++++--
+++++--trainer_config = utils.Config(
+++++--    utils.Trainer,
+++++--    savepath=(args.savepath, 'trainer_config.pkl'),
+++++--    # optimization parameters
+++++--    batch_size=args.batch_size,
+++++--    learning_rate=args.learning_rate,
+++++--    betas=(0.9, 0.95),
+++++--    grad_norm_clip=1.0,
+++++--    weight_decay=0.1, # only applied on matmul weights
+++++--    # learning rate decay: linear warmup followed by cosine decay to 10% of original
+++++--    lr_decay=args.lr_decay,
+++++--    warmup_tokens=warmup_tokens,
+++++--    final_tokens=final_tokens,
+++++--    ## dataloader
+++++--    num_workers=0,
+++++--    device=args.device,
+++++--)
+++++--
+++++--trainer = trainer_config()
+++++--
+++++--#######################
+++++--###### main loop ######
+++++--#######################
+++++--
+++++--## scale number of epochs to keep number of updates constant
+++++--n_epochs = int(1e6 / len(dataset) * args.n_epochs_ref)
+++++--save_freq = int(n_epochs // args.n_saves)
+++++--
+++++--for epoch in range(n_epochs):
+++++--    print(f'\nEpoch: {epoch} / {n_epochs} | {args.dataset} | {args.exp_name}')
+++++--
+++++--    trainer.train(model, dataset)
+++++--
+++++--    ## get greatest multiple of `save_freq` less than or equal to `save_epoch`
+++++--    save_epoch = (epoch + 1) // save_freq * save_freq
+++++--    statepath = os.path.join(args.savepath, f'state_{save_epoch}.pt')
+++++--    print(f'Saving model to {statepath}')
+++++--
+++++--    ## save state to disk
+++++--    state = model.state_dict()
+++++--    torch.save(state, statepath)
+++++-diff --git a/halfcheetah/scripts/xrl.py b/halfcheetah/scripts/xrl.py
+++++-deleted file mode 100644
+++++-index 134232a..0000000
+++++---- a/halfcheetah/scripts/xrl.py
+++++-+++ /dev/null
+++++-@@ -1,372 +0,0 @@
+++++--import json
+++++--import pdb
+++++--from os.path import join
+++++--
+++++--import trajectory.utils as utils
+++++--import trajectory.datasets as datasets
+++++--from trajectory.search import (
+++++--    make_prefix,
+++++--    update_context,
+++++--)
+++++--from trajectory.search.sampling import forward
+++++--
+++++--import gym
+++++--import d4rl # Import required to register environments, you may need to also import the submodule
+++++--import numpy as np
+++++--import d3rlpy
+++++--import math as mt
+++++--from sklearn.cluster import KMeans
+++++--from sklearn import datasets as skdatasets
+++++--from sklearn.decomposition import PCA
+++++--
+++++--from pyclustering.cluster.xmeans import xmeans
+++++--from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer
+++++--
+++++--from scipy.stats import wasserstein_distance
+++++--
+++++--class Parser(utils.Parser):
+++++--    dataset: str = 'halfcheetah-medium-expert-v2'
+++++--    config: str = 'config.offline'
+++++--
++++++diff --git a/halfcheetah/scripts/xrl_v2.py b/halfcheetah/scripts/xrl_v2.py
++++++index 62a3d4d..71cd19b 100644
++++++--- a/halfcheetah/scripts/xrl_v2.py
+++++++++ b/halfcheetah/scripts/xrl_v2.py
++++++@@ -21,54 +21,97 @@ from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer
++++++ from scipy.stats import wasserstein_distance
++++++ from moviepy.editor import VideoFileClip
++++++ 
+++++++
++++++ class Parser(utils.Parser):
++++++     dataset: str = 'halfcheetah-medium-v2'
++++++     config: str = 'config.offline'
++++++ 
+++++ -# utils
+++++--    
+++++--class XMeans:
+++++--    def loglikelihood(self, r, rn, var, m, k):
+++++--        l1 = - rn / 2.0 * mt.log(2 * mt.pi)
+++++--        l2 = - rn * m / 2.0 * mt.log(var)
+++++--        l3 = - (rn - k) / 2.0
+++++--        l4 = rn * mt.log(rn)
+++++--        l5 = - rn * mt.log(r)
+++++--
+++++--        return l1 + l2 + l3 + l4 + l5
+++++--
+++++--    def __init__(self, X, kmax = 20):
+++++--        self.X = X
+++++--        self.num = np.size(self.X, axis=0)
+++++--        self.dim = np.size(X, axis=1)
+++++--        self.KMax = kmax
+++++--
+++++--    def fit(self):
+++++--        k = 1
+++++--        X = self.X
+++++--        M = self.dim
+++++--        num = self.num
+++++--
+++++--        while(1):
+++++--            ok = k
+++++--
+++++--            #Improve Params
+++++--            kmeans = KMeans(n_clusters=k).fit(X)
+++++--            labels = kmeans.labels_
+++++--            m = kmeans.cluster_centers_
+++++--
+++++--            #Improve Structure
+++++--            #Calculate BIC
+++++--            p = M + 1
+++++--
+++++--            obic = np.zeros(k)
+++++--
+++++--            for i in range(k):
+++++--                rn = np.size(np.where(labels == i))
+++++--                var = np.sum((X[labels == i] - m[i])**2)/float(rn - 1)
+++++--                obic[i] = self.loglikelihood(rn, rn, var, M, 1) - p/2.0*mt.log(rn)
+++++--
+++++--            #Split each cluster into two subclusters and calculate BIC of each splitted cluster
+++++--            sk = 2 #The number of subclusters
+++++--            nbic = np.zeros(k)
+++++--            addk = 0
+++++--
+++++--            for i in range(k):
+++++--                ci = X[labels == i]
+++++--                r = np.size(np.where(labels == i))
+++++--
+++++--                kmeans = KMeans(n_clusters=sk).fit(ci)
+++++--                ci_labels = kmeans.labels_
+++++--                sm = kmeans.cluster_centers_
+++++--
+++++--                for l in range(sk):
+++++--                    rn = np.size(np.where(ci_labels == l))
+++++--                    var = np.sum((ci[ci_labels == l] - sm[l])**2)/float(rn - sk)
+++++--                    nbic[i] += self.loglikelihood(r, rn, var, M, sk)
+++++--
+++++--                p = sk * (M + 1)
+++++--                nbic[i] -= p/2.0*mt.log(r)
+++++--
+++++--                if obic[i] < nbic[i]:
+++++--                    addk += 1
+++++--
+++++--            k += addk
+++++--
+++++--            if ok == k or k >= self.KMax:
+++++--                break
+++++--
+++++--
+++++--        #Calculate labels and centroids
+++++--        kmeans = KMeans(n_clusters=k).fit(X)
+++++--        self.labels = kmeans.labels_
+++++--        self.k = k
+++++--        self.m = kmeans.cluster_centers_
+++++--
+++++--
+++++--def cluster_trajectories(trajectories):
+++++--    xmeans_instance = XMeans(trajectories, kmax=10)
+++++--    xmeans_instance.fit()
+++++--
+++++--    clusters = xmeans_instance.labels
+++++--    return clusters
+++++--
+++++--def cluster_trajectories_2(trajectories):
++++++ 
++++++ def cluster_trajectories(trajectories, n_clusters=10):
++++++-    """TODO"""
+++++++    """
+++++++    Cluster trajectories using X-means.
+++++++    
+++++++    Args:
+++++++    - trajectories: np.array, shape (n_trajectories, encoding_dim)
+++++++    - n_clusters: int, max number of clusters
+++++++    
+++++++    Returns:
+++++++    - idxs_per_cluster: list, trajectory idxs per cluster idxs
+++++++    - clusters: np.array, shape (n_trajectories), cluster idxs per trajectory idx
+++++++    """ 
++++++ 
+++++ -    # Prepare initial centers - amount of initial centers defines amount of clusters from which X-Means will
+++++ -    # start analysis.
+++++--    amount_initial_centers = 2
+++++--    initial_centers = kmeans_plusplus_initializer(trajectories, amount_initial_centers).initialize()
+++++--    
+++++++    # Set 2 initial cluster centers
++++++     amount_initial_centers = 2
++++++     initial_centers = kmeans_plusplus_initializer(trajectories, amount_initial_centers).initialize()
++++++     
+++++ -    # Create instance of X-Means algorithm. The algorithm will start analysis from 2 clusters, the maximum
+++++ -    # number of clusters that can be allocated is 10.
+++++--    xmeans_instance = xmeans(trajectories, initial_centers, 10)
+++++--    xmeans_instance.process()
+++++--    
+++++--    # Extract clustering results: clusters
+++++--    idxs_per_cluster = xmeans_instance.get_clusters()
+++++--
+++++--    clusters = []
+++++--    for i in range(len(trajectories)):
+++++--        for j in range(len(idxs_per_cluster)):
+++++--            if i in idxs_per_cluster[j]: clusters.append(j)
+++++--
+++++--    return idxs_per_cluster, np.array(clusters)
+++++++    # Run X-means
++++++     xmeans_instance = xmeans(trajectories, initial_centers, n_clusters)
++++++     xmeans_instance.process()
++++++     
++++++     # Extract clustering results: clusters
++++++     idxs_per_cluster = xmeans_instance.get_clusters()
++++++ 
+++++++    # Turn list of trajectory idxs per cluster to array of cluster idx per trajectory idx
++++++     clusters = []
++++++     for i in range(len(trajectories)):
++++++         for j in range(len(idxs_per_cluster)):
++++++             if i in idxs_per_cluster[j]: clusters.append(j)
++++++ 
++++++     return idxs_per_cluster, np.array(clusters)
+++++ - 
+++++--# https://github.com/sascha-kirch/ML_Notebooks/blob/main/Softmax_Temperature.ipynb
+++++--def softmax(x, temp):
+++++--    """Compute softmax values for each sets of scores in x."""
+++++--    return np.exp(np.divide(x,temp)) / np.sum(np.exp(np.divide(x,temp)))
+++++--
+++++--def generate_data_embedding(trajectory_embeddings, normalizing_factor=1, temperature=1):
+++++--    embedding = np.sum(trajectory_embeddings, axis=0) / normalizing_factor
+++++--    embedding = softmax(embedding, temperature)
+++++--    return embedding
+++++--
+++++--def embed_trajectory(gpt, discretizer, observations, actions, rewards, preprocess_fn):
+++++--    context = []
+++++--
+++++--    for i in range(len(observations)):
+++++--        observation = observations[i]
+++++--        action = actions[i]
+++++--        reward = rewards[i]
+++++--
+++++--        observation = preprocess_fn(observation)
+++++--
+++++--        # print(observation)
+++++--        prefix = make_prefix(discretizer, context, observation, True)
+++++--        # print("prefix", prefix.shape)
+++++--
+++++--        out = forward(gpt, prefix)
+++++--        # print("out", out.shape)
+++++--        context = update_context(context, discretizer, observation, action, reward, len(observations))
+++++--        # print("cotext", context)
+++++--    
+++++--    emb = []
+++++--    for context_step in context:
+++++--        emb.append(context_step.numpy())
+++++--    emb = np.array(emb)
+++++--    emb = np.mean(emb, axis=0)[0]
+++++--
+++++++
+++++++
++++++ def softmax(x, temp):
++++++-    """TODO"""
+++++++    """
+++++++    Softmax with temperature using max-trick.
+++++++    
+++++++    Args:
+++++++    - x: np.array, shape (n_data, dim_data)
+++++++    - temp: int, softmax temperature
+++++++    
+++++++    Returns:
+++++++    - softmax_x: np.array: shape (dim_data)
+++++++    """ 
+++++++
++++++     max_x = np.max(x)
++++++-    return np.exp(np.divide(x-max_x,temp)) / np.sum(np.exp(np.divide(x-max_x,temp)))
+++++++    softmax_x = np.exp(np.divide(x-max_x,temp)) / np.sum(np.exp(np.divide(x-max_x,temp)))
+++++++    return softmax_x
+++++++
++++++ 
++++++ def generate_data_embedding(trajectory_embeddings, temperature=10000):
++++++-    """TODO"""
+++++++    """
+++++++    Generate data embedding (sum+softmax) for set of encoded trajectories.
+++++++    
+++++++    Args:
+++++++    - trajectory_embeddings: np.array, shape (n_data, dim_data)
+++++++    - temperature: int, softmax temperature
+++++++    
+++++++    Returns:
+++++++    - embedding: np.array, shape (dim_data)
+++++++    """ 
++++++ 
++++++     embedding = np.sum(trajectory_embeddings, axis=0)
++++++     embedding = softmax(embedding, temperature)
++++++     
++++++-
++++++     return embedding
++++++ 
+++++++
++++++ def embed_trajectory(gpt, discretizer, observations, actions, rewards, preprocess_fn):
++++++-    """TODO"""
+++++++    """
+++++++    Encode trajectory using a trajectory transformer with a sliding window.
+++++++    
+++++++    Args:
+++++++    - gpt: trajectory transformer
+++++++    - discretizer: environment discretizer
+++++++    - observations: trajectory observations
+++++++    - actions: trajectory actions
+++++++    - rewards: trajectory rewards
+++++++    - preprocess_fn: observations preprocessing functions
+++++++    
+++++++    Returns:
+++++++    - embedding: np.array, shape (hidden_dim), encoded trajectory
+++++++    """ 
++++++ 
++++++     context = []
++++++-
++++++     output = []
++++++ 
++++++     for i in range(len(observations)):
++++++@@ -76,12 +119,12 @@ def embed_trajectory(gpt, discretizer, observations, actions, rewards, preproces
++++++         action = actions[i]
++++++         reward = rewards[i]
++++++ 
+++++++        # Preprocess, discretize & forward through trajectory transformer
++++++         observation = preprocess_fn(observation)
++++++-
++++++         prefix = make_prefix(discretizer, context, observation, True)
++++++-
++++++         out = forward(gpt, prefix)
++++++ 
+++++++        # Sliding window
++++++         if len(context) >= 9:
++++++             context.pop(0)
++++++             if len(output) == 0:
++++++@@ -91,19 +134,33 @@ def embed_trajectory(gpt, discretizer, observations, actions, rewards, preproces
++++++ 
++++++         context = update_context(context, discretizer, observation, action, reward, len(observations))
++++++ 
++++++-    emb = np.mean(output, axis=0)
+++++ -    return emb
+++++--
+++++--
+++++--def create_complementary_dataset(dataset, idxs, trajectory_length=10):
+++++--    observations = []
+++++--    actions = []
+++++--    rewards = []
+++++--    terminals = []
+++++--    for i in range(1000):
+++++--        if i not in idxs:
+++++--            observations += list(dataset.observations[1000*i:1000*i+trajectory_length])
+++++--            actions += list(dataset.actions[1000*i:1000*i+trajectory_length])
+++++--            rewards += list(dataset.rewards[1000*i:1000*i+trajectory_length])
+++++--            terminals += list(dataset.terminals[1000*i:1000*i+trajectory_length])
+++++--
+++++--    new_dataset = d3rlpy.dataset.MDPDataset(
+++++--        observations=np.array(observations),
+++++--        actions=np.array(actions),
+++++--        rewards=np.array(rewards),
+++++--        terminals=np.array(terminals)
+++++--    )
+++++--    return new_dataset
+++++--    
+++++--
+++++--
+++++--
+++++--def main():
+++++--    # args = Parser().parse_args('plan')
+++++--
+++++--    #######################
+++++--    ####### models ########
+++++--    #######################
+++++--
+++++--
+++++--
+++++--
+++++--
+++++--    # print(args.dataset)
+++++--
+++++--    # dataset = utils.load_from_config(args.logbase, args.dataset, args.gpt_loadpath,
+++++--    #         'data_config.pkl')
+++++--
+++++--
+++++--    # gpt, gpt_epoch = utils.load_model(args.logbase, args.dataset, args.gpt_loadpath,
+++++--    #         epoch=args.gpt_epoch, device=args.device)
+++++--
+++++--    # env = datasets.load_environment(args.dataset)
+++++--
+++++--    # discretizer = dataset.discretizer
+++++--
+++++--    # preprocess_fn = datasets.get_preprocess_fn(env.name)
+++++--
+++++--    # #######################
+++++--    # ####### dataset #######
+++++--    # #######################
+++++--
+++++--    # # env = datasets.load_environment(args.dataset)
+++++--    # discretizer = dataset.discretizer
+++++--    # preprocess_fn = datasets.get_preprocess_fn(env.name)
+++++--
+++++--    # # dataset
+++++--    dataset_d3, env = d3rlpy.datasets.get_dataset("halfcheetah-medium-v2")
+++++--
+++++--    # env = gym.make('halfcheetah-medium-v2')
+++++--    # dataset_d4 = d4rl.qlearning_dataset(env)
+++++--
+++++--    # # checks to see if d3rl & d4rl datasets are equal
+++++--    # print(np.allclose(dataset_d3.actions[100], dataset_d4['actions'][100]))
+++++--
+++++--    # # dr4rl has same trajectories, just cut off 1 element before the end
+++++--    # for j in range(1000):
+++++--    #     for i in range(999):
+++++--    #         if dataset_d4['rewards'][j * 999 + i] != dataset_d3.rewards[j * 1000 + i]: print("yo", i)
+++++--
+++++--    # #######################
+++++--    # ###### main loop ######
+++++--    # #######################
+++++--
+++++--    trajectory_length = 10 # 10 = max
+++++--
+++++--    # embeddings = []
+++++--    # for i in range(1000):
+++++--    #     observations = dataset_d3.observations[1000*i:1000*i+trajectory_length]
+++++--    #     actions = dataset_d3.actions[1000*i:1000*i+trajectory_length]
+++++--    #     rewards = dataset_d3.rewards[1000*i:1000*i+trajectory_length]
+++++--    #     terminals = dataset_d3.terminals[1000*i:1000*i+trajectory_length]
+++++--    #     emb = embed_trajectory(gpt, discretizer, observations, actions, rewards, preprocess_fn)
+++++--    #     embeddings.append(emb)
+++++--    # embeddings = np.array(embeddings)
+++++--    # np.save("embeddings.npy", embeddings)
+++++--    # print(embeddings)
+++++--
+++++--    embeddings = np.load("embeddings.npy")
+++++--
+++++--    pca = PCA(n_components=2)
+++++--    pca = PCA(n_components=2)
+++++--    pca_embeddings = pca.fit_transform(embeddings)
+++++--    np.save("pca.py", pca_embeddings)
+++++--
+++++--    idxs_per_cluster, clusters = cluster_trajectories_2(embeddings)
+++++--    # print(clusters)
+++++--    # return
+++++--    np.save("clusters.npy", clusters)
+++++--
+++++--    import matplotlib.pyplot as plt
+++++--
+++++--    d_orig = generate_data_embedding(embeddings)
+++++--    unique_clusters = np.unique(clusters)
+++++--    
+++++--    d_j = []
+++++--    complementary_datasets = []
+++++--    for j in np.sort(unique_clusters):
+++++--        print(j)
+++++--        d_j.append(generate_data_embedding(embeddings[clusters != j]))
+++++--        plt.scatter(pca_embeddings[clusters == j][:,0], pca_embeddings[clusters == j][:,1], label=j)
+++++--        complementary_datasets.append(create_complementary_dataset(dataset_d3, idxs_per_cluster[j], trajectory_length))
+++++--    
+++++--    original_dataset = create_complementary_dataset(dataset_d3, [], trajectory_length)
+++++--
+++++--    print(complementary_datasets, original_dataset)
+++++--
+++++--    plt.legend()
+++++--    plt.show()
+++++--
+++++--    agent_orig = d3rlpy.algos.SAC(
+++++--        actor_learning_rate=3e-4,
+++++--        critic_learning_rate=3e-4,
+++++--        temp_learning_rate=3e-4,
+++++--        batch_size=256)
+++++--
+++++--    print(agent_orig)
+++++--
+++++--    training_steps = 1000
+++++--
+++++--    agent_orig.fit(original_dataset, n_steps=training_steps)
+++++--
+++++--    agents_compl = []
+++++--
+++++--    for dset in complementary_datasets:
+++++--        agent = d3rlpy.algos.SAC(
+++++--            actor_learning_rate=3e-4,
+++++--            critic_learning_rate=3e-4,
+++++--            temp_learning_rate=3e-4,
+++++--            batch_size=256)
+++++--        agent.fit(dset, n_steps=training_steps)
+++++--        agents_compl.append(agent)
+++++--
+++++--    action_orig = agent_orig.predict(dataset_d3.observations[0])
+++++--
+++++--    actions_compl = []
+++++--    for agent in agents_compl:
+++++--        actions_compl.append(agent.predict(dataset_d3.observations[0]))
+++++--    
+++++--    action_dists = []
+++++--    for action in actions_compl:
+++++--        action_dists.append(np.linalg.norm(action_orig-action))
+++++--
+++++--    k = 3
+++++--    topk = np.argpartition(action_dists, -k)[-k:]
+++++--
+++++--    d_w = {}
+++++--    for idx in topk:
+++++--        d_w[idx] = wasserstein_distance(d_j[idx], d_orig)
+++++--
+++++--    cluster_assignment = min(d_w, key=d_w.get)
+++++--    print("explanation assigned to cluster", cluster_assignment)
+++++--
+++++--    
+++++--def assignment_test():
+++++--    action_orig = np.random.rand(10)
+++++--    d_orig = np.random.rand(5)
+++++--
+++++--    actions_compl = np.random.rand(6,10)
+++++--    d_j = np.random.rand(6,5)
+++++--
+++++--    action_dists = []
+++++--    for action in actions_compl:
+++++--        action_dists.append(np.linalg.norm(action_orig-action))
+++++--
+++++--    print(action_dists)
+++++--
+++++--    k = 3
+++++--    topk = np.argpartition(action_dists, -k)[-k:]
+++++--
+++++--    print(topk)
+++++--
+++++--    d_w = {}
+++++--    for idx in topk:
+++++--        d_w[idx] = wasserstein_distance(d_j[idx], d_orig)
+++++--
+++++--    print(d_w)
+++++--
+++++--    cluster_assignment = min(d_w, key=d_w.get)
+++++--    print("explanation assigned to cluster", cluster_assignment)
+++++--
+++++--
+++++--if __name__ == "__main__":
+++++--    # main()
+++++--    assignment_test()
+++++-diff --git a/halfcheetah/trajectory.egg-info/PKG-INFO b/halfcheetah/trajectory.egg-info/PKG-INFO
+++++-index 452c6cb..2603850 100644
+++++---- a/halfcheetah/trajectory.egg-info/PKG-INFO
+++++-+++ b/halfcheetah/trajectory.egg-info/PKG-INFO
+++++-@@ -1,4 +1,11 @@
+++++- Metadata-Version: 2.1
+++++- Name: trajectory
+++++- Version: 0.0.0
+++++-+Summary: UNKNOWN
+++++-+Home-page: UNKNOWN
+++++-+License: UNKNOWN
+++++-+Platform: UNKNOWN
+++++- License-File: LICENSE
+++++++    # Embedding is the average of encoded states
+++++++    embedding = np.mean(output, axis=0)
+++++++    return embedding
+++++ +
+++++-+UNKNOWN
++++++ 
++++++ def create_complementary_dataset(dataset, idxs, trajectory_length=10, inverse=False):
++++++-    """TODO"""
+++++++    """
+++++++    Encode trajectory using a trajectory transformer with a sliding window.
+++++++    
+++++++    Args:
+++++++    - dataset: MDPDataset, original d3rl dataset
+++++++    - idxs: trajectory idxs to ignore (or include if inverse is True)
+++++++    - trajectory_length: int, trajectory length
+++++++    - inverse: bool, if True the dataset is not complementary
+++++ +
+++++++    Returns:
+++++++    - new_dataset: MDPDataset, complementary dataset
+++++++    """ 
++++++ 
++++++     observations = []
++++++     actions = []
++++++     rewards = []
++++++     terminals = []
++++++ 
++++++-    n_trajs = int(1000000/trajectory_length)
+++++++    n_trajs = int(len(dataset.observations)/trajectory_length)
++++++     for i in range(n_trajs):
+++++++        # If inverse is True, only include idxs. If not, leave out idxs
++++++         condition = i not in idxs
++++++         if inverse: condition = not condition
++++++ 
++++++@@ -112,6 +169,7 @@ def create_complementary_dataset(dataset, idxs, trajectory_length=10, inverse=Fa
++++++             actions += list(dataset.actions[trajectory_length*i:trajectory_length*(i+1)])
++++++             rewards += list(dataset.rewards[trajectory_length*i:trajectory_length*(i+1)])
++++++ 
+++++++    # Trajectories end with a terminal state
++++++     terminals = np.tile([0]*(trajectory_length-1)+[1], int(len(observations)/trajectory_length))
++++++ 
++++++     new_dataset = d3rlpy.dataset.MDPDataset(
++++++@@ -124,6 +182,17 @@ def create_complementary_dataset(dataset, idxs, trajectory_length=10, inverse=Fa
++++++     
++++++ 
++++++ def clusters_to_idxs(clusters):
+++++++    """
+++++++    Helper function to turn array of cluster idxs per trajectory idxs to a list 
+++++++    of trajectory idxs per cluster idx.
+++++++    
+++++++    Args:
+++++++    - clusters: np.array, cluster idx per trajectory idx
+++++++
+++++++    Returns:
+++++++    - idxs_per_cluster: list, trajectory idxs per cluster idx
+++++++    """ 
+++++++
++++++     idxs_per_cluster = []
++++++     for i in np.sort(np.unique(clusters)):
++++++         idxs_per_cluster.append(list(np.argwhere(clusters == i).flatten()))
++++++@@ -138,11 +207,11 @@ def main():
++++++ 
++++++     ### DATASET ###
++++++ 
++++++-    dataset_d3, env = d3rlpy.datasets.get_dataset("halfcheetah-medium-v2")
+++++++    dataset_d3, env = d3rlpy.datasets.get_dataset("bullet-halfcheetah-medium-v2")
++++++ 
++++++     ### IMPORTANT DEFINITIONS XRL SCRIPT ###
++++++ 
++++++-    load_embeddings = False
+++++++    load_embeddings = True
++++++     load_clusters = True
++++++     load_agents = True
++++++     generate_human_study = False
+++++ diff --git a/halfcheetah/trajectory.egg-info/SOURCES.txt b/halfcheetah/trajectory.egg-info/SOURCES.txt
+++++-index 4474d85..84e8e3a 100644
++++++index 84e8e3a..4474d85 100644
+++++ --- a/halfcheetah/trajectory.egg-info/SOURCES.txt
+++++ +++ b/halfcheetah/trajectory.egg-info/SOURCES.txt
+++++-@@ -30,4 +30,5 @@ trajectory/utils/serialization.py
++++++@@ -30,5 +30,4 @@ trajectory/utils/serialization.py
+++++  trajectory/utils/setup.py
+++++  trajectory/utils/timer.py
+++++  trajectory/utils/training.py
+++++ -trajectory/utils/video.py
++++++-trajectory_aaa/__init__.py
+++++ \ No newline at end of file
+++++ +trajectory/utils/video.py
+++++-+trajectory_aaa/__init__.py
+++++ \ No newline at end of file
+++++ diff --git a/halfcheetah/trajectory.egg-info/top_level.txt b/halfcheetah/trajectory.egg-info/top_level.txt
+++++-index ce65198..1d5271f 100644
++++++index 1d5271f..ce65198 100644
+++++ --- a/halfcheetah/trajectory.egg-info/top_level.txt
+++++ +++ b/halfcheetah/trajectory.egg-info/top_level.txt
+++++-@@ -1 +1,2 @@
++++++@@ -1,2 +1 @@
+++++  trajectory
+++++-+trajectory_aaa
++++ \ No newline at end of file
++++-diff --git a/halfcheetah/pca.py.npy b/halfcheetah/pca.py.npy
++++-deleted file mode 100644
++++-index bb19150..0000000
++++-Binary files a/halfcheetah/pca.py.npy and /dev/null differ
++++-diff --git a/halfcheetah/plotting/bar.png b/halfcheetah/plotting/bar.png
++++-deleted file mode 100644
++++-index 3679667..0000000
++++-Binary files a/halfcheetah/plotting/bar.png and /dev/null differ
++++-diff --git a/halfcheetah/plotting/plot.py b/halfcheetah/plotting/plot.py
++++-deleted file mode 100644
++++-index 163d0e4..0000000
++++---- a/halfcheetah/plotting/plot.py
++++-+++ /dev/null
++++-@@ -1,74 +0,0 @@
++++--import numpy as np
++++--import matplotlib
++++--import matplotlib.pyplot as plt
++++--import pdb
++++--
++++--from plotting.scores import means
++++--
++++--class Colors:
++++--	grey = '#B4B4B4'
++++--	gold = '#F6C781'
++++--	red = '#EC7C7D'
++++--	blue = '#70ABCC'
++++--
++++--LABELS = {
++++--	# 'BC': 'Behavior\nCloning',
++++--	# 'MBOP': 'Model-Based\nOffline Planning',
++++--	# 'BRAC': 'Behavior-Reg.\nActor-Critic',
++++--	# 'CQL': 'Conservative\nQ-Learning',
++++--}
++++--
++++--def get_mean(results, exclude=None):
++++--	'''
++++--		results : { environment: score, ... }
++++--	'''
++++--	filtered = {
++++--		k: v for k, v in results.items()
++++--		if (not exclude) or (exclude and exclude not in k)
++++--	}
++++--	return np.mean(list(filtered.values()))
++++--
++++--if __name__ == '__main__':
++++--
++++--	#################
++++--	## latex
++++--	#################
++++--	matplotlib.rc('font', **{'family': 'serif', 'serif': ['Computer Modern']})
++++--	matplotlib.rc('text', usetex=True)
++++--	matplotlib.rcParams['text.latex.preamble']=[r"\usepackage{amsmath}"]
++++--	#################
++++--
++++--	fig = plt.gcf()
++++--	ax = plt.gca()
++++--	fig.set_size_inches(7.5, 2.5)
++++--
++++--	means = {k: get_mean(v, exclude='ant') for k, v in means.items()}
++++--	print(means)
++++--
++++--	algs = ['BC', 'MBOP', 'BRAC', 'CQL', 'Decision\nTransformer', 'Trajectory\nTransformer']
++++--	vals = [means[alg] for alg in algs]
++++--
++++--	colors = [
++++--		Colors.grey, Colors.gold,
++++--		Colors.red, Colors.red, Colors.blue, Colors.blue
++++--	]
++++--
++++--	labels = [LABELS.get(alg, alg) for alg in algs]
++++--	plt.bar(labels, vals, color=colors, edgecolor=Colors.gold, lw=0)
++++--	plt.ylabel('Average normalized return', labelpad=15)
++++--	# plt.title('Offline RL Results')
++++--
++++--	legend_labels = ['Behavior Cloning', 'Trajectory Optimization', 'Temporal Difference', 'Sequence Modeling']
++++--	colors = [Colors.grey, Colors.gold, Colors.red, Colors.blue]
++++--	handles = [plt.Rectangle((0,0),1,1, color=color) for label, color in zip(legend_labels, colors)]
++++--	plt.legend(handles, legend_labels, ncol=4,
++++--		bbox_to_anchor=(1.07, -.18), fancybox=False, framealpha=0, shadow=False, columnspacing=1.5, handlelength=1.5)
++++--
++++--	matplotlib.rcParams['hatch.linewidth'] = 7.5
++++--	# ax.patches[-1].set_hatch('/')
++++--
++++--	ax.spines['right'].set_visible(False)
++++--	ax.spines['top'].set_visible(False)
++++--
++++--	# plt.savefig('plotting/bar.pdf', bbox_inches='tight')
++++--	plt.savefig('plotting/bar.png', bbox_inches='tight', dpi=500)
++++-diff --git a/halfcheetah/plotting/read_results.py b/halfcheetah/plotting/read_results.py
++++-deleted file mode 100644
++++-index 5a5fb62..0000000
++++---- a/halfcheetah/plotting/read_results.py
++++-+++ /dev/null
++++-@@ -1,70 +0,0 @@
++++--import os
++++--import glob
++++--import numpy as np
++++--import json
++++--import pdb
++++--
++++--import trajectory.utils as utils
++++--
++++--DATASETS = [
++++--	f'{env}-{buffer}'
++++--	for env in ['hopper', 'walker2d', 'halfcheetah', 'ant']
++++--	for buffer in ['medium-expert-v2', 'medium-v2', 'medium-replay-v2']
++++--]
++++--
++++--LOGBASE = 'logs'
++++--TRIAL = '*'
++++--EXP_NAME = 'plans/pretrained'
++++--
++++--def load_results(paths):
++++--	'''
++++--		paths : path to directory containing experiment trials
++++--	'''
++++--	scores = []
++++--	for i, path in enumerate(sorted(paths)):
++++--		score = load_result(path)
++++--		if score is None:
++++--			print(f'Skipping {path}')
++++--			continue
++++--		scores.append(score)
++++--
++++--		suffix = path.split('/')[-1]
++++--
++++--	mean = np.mean(scores)
++++--	err = np.std(scores) / np.sqrt(len(scores))
++++--	return mean, err, scores
++++--
++++--def load_result(path):
++++--	'''
++++--		path : path to experiment directory; expects `rollout.json` to be in directory
++++--	'''
++++--	fullpath = os.path.join(path, 'rollout.json')
++++--	suffix = path.split('/')[-1]
++++--
++++--	if not os.path.exists(fullpath):
++++--		return None
++++--
++++--	results = json.load(open(fullpath, 'rb'))
++++--	score = results['score']
++++--	return score * 100
++++--
++++--#######################
++++--######## setup ########
++++--#######################
++++--
++++--if __name__ == '__main__':
++++--
++++--	class Parser(utils.Parser):
++++--	    dataset: str = None
++++--
++++--	args = Parser().parse_args()
++++--
++++--	for dataset in ([args.dataset] if args.dataset else DATASETS):
++++--		subdirs = glob.glob(os.path.join(LOGBASE, dataset, EXP_NAME))
++++--
++++--		for subdir in subdirs:
++++--			reldir = subdir.split('/')[-1]
++++--			paths = glob.glob(os.path.join(subdir, TRIAL))
++++--
++++--			mean, err, scores = load_results(paths)
++++--			print(f'{dataset.ljust(30)} | {subdir.ljust(50)} | {len(scores)} scores \n    {mean:.2f} +/- {err:.2f}\n')
++++-diff --git a/halfcheetah/plotting/scores.py b/halfcheetah/plotting/scores.py
++++-deleted file mode 100644
++++-index f1917f7..0000000
++++---- a/halfcheetah/plotting/scores.py
++++-+++ /dev/null
++++-@@ -1,123 +0,0 @@
++++--means = {
++++--	'Trajectory\nTransformer': {
++++--		##
++++--		'halfcheetah-medium-expert-v2': 95.0,
++++--		'hopper-medium-expert-v2': 110.0,
++++--		'walker2d-medium-expert-v2': 101.9,
++++--		'ant-medium-expert-v2': 116.1,
++++--		##
++++--		'halfcheetah-medium-v2': 46.9,
++++--		'hopper-medium-v2': 61.1,
++++--		'walker2d-medium-v2': 79.0,
++++--		'ant-medium-v2': 83.1,
++++--		##
++++--		'halfcheetah-medium-replay-v2': 41.9,
++++--		'hopper-medium-replay-v2': 91.5,
++++--		'walker2d-medium-replay-v2': 82.6,
++++--		'ant-medium-replay-v2': 77.0,
++++--	},
++++--	'Decision\nTransformer': {
++++--		##
++++--		'halfcheetah-medium-expert-v2': 86.8,
++++--		'hopper-medium-expert-v2': 107.6,
++++--		'walker2d-medium-expert-v2': 108.1,
++++--		##
++++--		'halfcheetah-medium-v2': 42.6,
++++--		'hopper-medium-v2': 67.6,
++++--		'walker2d-medium-v2': 74.0,
++++--		##
++++--		'halfcheetah-medium-replay-v2': 36.6,
++++--		'hopper-medium-replay-v2': 82.7,
++++--		'walker2d-medium-replay-v2': 66.6,
++++--	},
++++--	'CQL': {
++++--		##
++++--		'halfcheetah-medium-expert-v2': 91.6,
++++--		'hopper-medium-expert-v2': 105.4,
++++--		'walker2d-medium-expert-v2': 108.8,
++++--		##
++++--		'halfcheetah-medium-v2': 44.0,
++++--		'hopper-medium-v2': 58.5,
++++--		'walker2d-medium-v2': 72.5,
++++--		##
++++--		'halfcheetah-medium-replay-v2': 45.5,
++++--		'hopper-medium-replay-v2': 95.0,
++++--		'walker2d-medium-replay-v2': 77.2,
++++--	},
++++--	'MOPO': {
++++--		##
++++--		'halfcheetah-medium-expert-v2': 63.3,
++++--		'hopper-medium-expert-v2': 23.7,
++++--		'walker2d-medium-expert-v2': 44.6,
++++--		##
++++--		'halfcheetah-medium-v2': 42.3,
++++--		'hopper-medium-v2': 28.0,
++++--		'walker2d-medium-v2': 17.8,
++++--		##
++++--		'halfcheetah-medium-replay-v2': 53.1,
++++--		'hopper-medium-replay-v2': 67.5,
++++--		'walker2d-medium-replay-v2':39.0,
++++--	},
++++--	'MBOP': {
++++--		##
++++--		'halfcheetah-medium-expert-v2': 105.9,
++++--		'hopper-medium-expert-v2': 55.1,
++++--		'walker2d-medium-expert-v2': 70.2,
++++--		##
++++--		'halfcheetah-medium-v2': 44.6,
++++--		'hopper-medium-v2': 48.8,
++++--		'walker2d-medium-v2': 41.0,
++++--		##
++++--		'halfcheetah-medium-replay-v2': 42.3,
++++--		'hopper-medium-replay-v2': 12.4,
++++--		'walker2d-medium-replay-v2': 9.7,
++++--	},
++++--	'BRAC': {
++++--		##
++++--		'halfcheetah-medium-expert-v2': 41.9,
++++--		'hopper-medium-expert-v2': 0.9,
++++--		'walker2d-medium-expert-v2': 81.6,
++++--		##
++++--		'halfcheetah-medium-v2': 46.3,
++++--		'hopper-medium-v2': 31.3,
++++--		'walker2d-medium-v2': 81.1,
++++--		##
++++--		'halfcheetah-medium-replay-v2': 47.7,
++++--		'hopper-medium-replay-v2': 0.6,
++++--		'walker2d-medium-replay-v2': 0.9,
++++--	},
++++--	'BC': {
++++--		##
++++--		'halfcheetah-medium-expert-v2': 59.9,
++++--		'hopper-medium-expert-v2': 79.6,
++++--		'walker2d-medium-expert-v2': 36.6,
++++--		##
++++--		'halfcheetah-medium-v2': 43.1,
++++--		'hopper-medium-v2': 63.9,
++++--		'walker2d-medium-v2': 77.3,
++++--		##
++++--		'halfcheetah-medium-replay-v2': 4.3,
++++--		'hopper-medium-replay-v2': 27.6,
++++--		'walker2d-medium-replay-v2': 36.9,
++++--	},
++++--}
++++--
++++--errors = {
++++--	'Trajectory\nTransformer': {
++++--		##
++++--		'halfcheetah-medium-expert-v2': 0.2,
++++--		'hopper-medium-expert-v2': 2.7,
++++--		'walker2d-medium-expert-v2': 6.8,
++++--		'ant-medium-expert-v2': 9.0,
++++--		##
++++--		'halfcheetah-medium-v2': 0.4,
++++--		'hopper-medium-v2': 3.6,
++++--		'walker2d-medium-v2': 2.8,
++++--		'ant-medium-v2': 7.3,
++++--		##
++++--		'halfcheetah-medium-replay-v2': 2.5,
++++--		'hopper-medium-replay-v2': 3.6,
++++--		'walker2d-medium-replay-v2': 6.9,
++++--		'ant-medium-replay-v2': 6.8,
++++--	},
++++--}
++++++-trajectory_aaa
++++++diff --git a/seaquest/readme.md b/seaquest/readme.md
++++++index 84e53f8..53561f9 100644
++++++--- a/seaquest/readme.md
+++++++++ b/seaquest/readme.md
++++++@@ -10,4 +10,4 @@ pip install git+https://github.com/takuseno/d4rl-atari
++++++ pip install "gym[atari, accept-rom-license]"
++++++ pip install pyclustering
++++++ pip install seaborn
++++++-pip install d3rlpy==1.1.1
++++++\ No newline at end of file
+++++++pip install d3rlpy==1.1.1
++++ \ No newline at end of file
++++-diff --git a/halfcheetah/plotting/table.py b/halfcheetah/plotting/table.py
++++-deleted file mode 100644
++++-index eae74e6..0000000
++++---- a/halfcheetah/plotting/table.py
++++-+++ /dev/null
++++-@@ -1,127 +0,0 @@
++++--import numpy as np
++++--import pdb
++++--
++++--from plotting.plot import get_mean
++++--from plotting.scores import (
++++--	means as MEANS,
++++--	errors as ERRORS,
++++--)
++++--
++++--ALGORITHM_STRINGS = {
++++--	'Trajectory\nTransformer': 'TT (Ours)',
++++--	'Decision\nTransformer': 'DT',	
++++--}
++++--
++++--BUFFER_STRINGS = {
++++--	'medium-expert': 'Medium-Expert',
++++--	'medium': 'Medium',
++++--	'medium-replay': 'Medium-Replay',	
++++--}
++++--
++++--ENVIRONMENT_STRINGS = {
++++--	'halfcheetah': 'HalfCheetah',
++++--	'hopper': 'Hopper',
++++--	'walker2d': 'Walker2d',
++++--	'ant': 'Ant',
++++--}
++++--
++++--SHOW_ERRORS = ['Trajectory\nTransformer']
++++--
++++--def get_result(algorithm, buffer, environment, version='v2'):
++++--	key = f'{environment}-{buffer}-{version}'
++++--	mean = MEANS[algorithm].get(key, '-')
++++--	if algorithm in SHOW_ERRORS:
++++--		error = ERRORS[algorithm].get(key)
++++--		return (mean, error)
++++--	else:
++++--		return mean
++++--
++++--def format_result(result):
++++--	if type(result) == tuple:
++++--		mean, std = result
++++--		return f'${mean}$ \\scriptsize{{\\raisebox{{1pt}}{{$\\pm {std}$}}}}'
++++--	else:
++++--		return f'${result}$'
++++--
++++--def format_row(buffer, environment, results):
++++--	buffer_str = BUFFER_STRINGS[buffer]
++++--	environment_str = ENVIRONMENT_STRINGS[environment]
++++--	results_str = ' & '.join(format_result(result) for result in results)
++++--	row = f'{buffer_str} & {environment_str} & {results_str} \\\\ \n'
++++--	return row
++++--
++++--def format_buffer_block(algorithms, buffer, environments):
++++--	block_str = '\\midrule\n'
++++--	for environment in environments:
++++--		results = [get_result(alg, buffer, environment) for alg in algorithms]
++++--		row_str = format_row(buffer, environment, results)
++++--		block_str += row_str
++++--	return block_str
++++--
++++--def format_algorithm(algorithm):
++++--	algorithm_str = ALGORITHM_STRINGS.get(algorithm, algorithm)
++++--	return f'\multicolumn{{1}}{{c}}{{\\bf {algorithm_str}}}'
++++--
++++--def format_algorithms(algorithms):
++++--	return ' & '.join(format_algorithm(algorithm) for algorithm in algorithms)
++++--
++++--def format_averages(means, label):
++++--	prefix = f'\\multicolumn{{2}}{{c}}{{\\bf Average ({label})}} & '
++++--	formatted = ' & '.join(str(mean) for mean in means)
++++--	return prefix + formatted
++++--
++++--def format_averages_block(algorithms):
++++--	means_filtered = [np.round(get_mean(MEANS[algorithm], exclude='ant'), 1) for algorithm in algorithms]
++++--	means_all = [np.round(get_mean(MEANS[algorithm], exclude=None), 1) for algorithm in algorithms]
++++--
++++--	means_all = [
++++--		means
++++--		if 'ant-medium-expert-v2' in MEANS[algorithm]
++++--		else '$-$'
++++--		for algorithm, means in zip(algorithms, means_all)
++++--	]
++++--
++++--	formatted_filtered = format_averages(means_filtered, 'without Ant')
++++--	formatted_all = format_averages(means_all, 'all settings')
++++--
++++--	formatted_block = (
++++--		f'{formatted_filtered} \\hspace{{.6cm}} \\\\ \n'
++++--		f'{formatted_all} \\hspace{{.6cm}} \\\\ \n'
++++--	)
++++--	return formatted_block
++++--
++++--def format_table(algorithms, buffers, environments):
++++--	justify_str = 'll' + 'r' * len(algorithms)
++++--	algorithm_str = format_algorithms(['Dataset', 'Environment'] + algorithms)
++++--	averages_str = format_averages_block(algorithms)
++++--	table_prefix = (
++++--		'\\begin{table*}[h]\n'
++++--		'\\centering\n'
++++--		'\\small\n'
++++--		f'\\begin{{tabular}}{{{justify_str}}}\n'
++++--		'\\toprule\n'
++++--		f'{algorithm_str} \\\\ \n'
++++--	)
++++--	table_suffix = (
++++--		'\\midrule\n'
++++--		f'{averages_str}'
++++--		'\\bottomrule\n'
++++--		'\\end{tabular}\n'
++++--		'\\label{table:d4rl}\n'
++++--		'\\end{table*}'
++++--	)
++++--	blocks = ''.join(format_buffer_block(algorithms, buffer, environments) for buffer in buffers)
++++--	table = (
++++--		f'{table_prefix}'
++++--		f'{blocks}'
++++--		f'{table_suffix}'
++++--	)
++++--	return table
++++--
++++--
++++--algorithms =['BC', 'MBOP', 'BRAC', 'CQL',  'Decision\nTransformer', 'Trajectory\nTransformer']
++++--buffers = ['medium-expert', 'medium', 'medium-replay']
++++--environments = ['halfcheetah', 'hopper', 'walker2d', 'ant']
++++--
++++--table = format_table(algorithms, buffers, environments)
++++--print(table)
++++-diff --git a/halfcheetah/scripts/plan.py b/halfcheetah/scripts/plan.py
++++-deleted file mode 100644
++++-index f13d4cc..0000000
++++---- a/halfcheetah/scripts/plan.py
++++-+++ /dev/null
++++-@@ -1,124 +0,0 @@
++++--import json
++++--import pdb
++++--from os.path import join
++++--
++++--import trajectory.utils as utils
++++--import trajectory.datasets as datasets
++++--from trajectory.search import (
++++--    beam_plan,
++++--    make_prefix,
++++--    extract_actions,
++++--    update_context,
++++--)
++++--
++++--class Parser(utils.Parser):
++++--    dataset: str = 'halfcheetah-medium-expert-v2'
++++--    config: str = 'config.offline'
++++--
++++--#######################
++++--######## setup ########
++++--#######################
++++--
++++--args = Parser().parse_args('plan')
++++--
++++--#######################
++++--####### models ########
++++--#######################
++++--
++++--dataset = utils.load_from_config(args.logbase, args.dataset, args.gpt_loadpath,
++++--        'data_config.pkl')
++++--
++++--gpt, gpt_epoch = utils.load_model(args.logbase, args.dataset, args.gpt_loadpath,
++++--        epoch=args.gpt_epoch, device=args.device)
++++--
++++--#######################
++++--####### dataset #######
++++--#######################
++++--
++++--env = datasets.load_environment(args.dataset)
++++--print('yo')
++++--renderer = utils.make_renderer(args)
++++--timer = utils.timer.Timer()
++++--
++++--discretizer = dataset.discretizer
++++--discount = dataset.discount
++++--observation_dim = dataset.observation_dim
++++--action_dim = dataset.action_dim
++++--
++++--value_fn = lambda x: discretizer.value_fn(x, args.percentile)
++++--preprocess_fn = datasets.get_preprocess_fn(env.name)
++++--
++++--print('yo2')
++++--
++++--#######################
++++--###### main loop ######
++++--#######################
++++--
++++--observation = env.reset()
++++--total_reward = 0
++++--
++++--## observations for rendering
++++--rollout = [observation.copy()]
++++--
++++--## previous (tokenized) transitions for conditioning transformer
++++--context = []
++++--
++++--T = env.max_episode_steps
++++--for t in range(T):
++++--
++++--    observation = preprocess_fn(observation)
++++--
++++--    if t % args.plan_freq == 0:
++++--        ## concatenate previous transitions and current observations to input to model
++++--        prefix = make_prefix(discretizer, context, observation, args.prefix_context)
++++--
++++--        ## sample sequence from model beginning with `prefix`
++++--        sequence = beam_plan(
++++--            gpt, value_fn, prefix,
++++--            args.horizon, args.beam_width, args.n_expand, observation_dim, action_dim,
++++--            discount, args.max_context_transitions, verbose=args.verbose,
++++--            k_obs=args.k_obs, k_act=args.k_act, cdf_obs=args.cdf_obs, cdf_act=args.cdf_act,
++++--        )
++++--
++++--    else:
++++--        sequence = sequence[1:]
++++--
++++--    ## [ horizon x transition_dim ] convert sampled tokens to continuous trajectory
++++--    sequence_recon = discretizer.reconstruct(sequence)
++++--
++++--    ## [ action_dim ] index into sampled trajectory to grab first action
++++--    action = extract_actions(sequence_recon, observation_dim, action_dim, t=0)
++++--
++++--    ## execute action in environment
++++--    next_observation, reward, terminal, _ = env.step(action)
++++--
++++--    ## update return
++++--    total_reward += reward
++++--    score = env.get_normalized_score(total_reward)
++++--
++++--    ## update rollout observations and context transitions
++++--    rollout.append(next_observation.copy())
++++--    context = update_context(context, discretizer, observation, action, reward, args.max_context_transitions)
++++--
++++--    print(
++++--        f'[ plan ] t: {t} / {T} | r: {reward:.2f} | R: {total_reward:.2f} | score: {score:.4f} | '
++++--        f'time: {timer():.2f} | {args.dataset} | {args.exp_name} | {args.suffix}\n'
++++--    )
++++--
++++--    ## visualization
++++--    if t % args.vis_freq == 0 or terminal or t == T:
++++--
++++--        ## save current plan
++++--        renderer.render_plan(join(args.savepath, f'{t}_plan.mp4'), sequence_recon, env.state_vector())
++++--
++++--        ## save rollout thus far
++++--        renderer.render_rollout(join(args.savepath, f'rollout.mp4'), rollout, fps=80)
++++--
++++--    if terminal: break
++++--
++++--    observation = next_observation
++++--
++++--## save result as a json file
++++--json_path = join(args.savepath, 'rollout.json')
++++--json_data = {'score': score, 'step': t, 'return': total_reward, 'term': terminal, 'gpt_epoch': gpt_epoch}
++++--json.dump(json_data, open(json_path, 'w'), indent=2, sort_keys=True)
++++-diff --git a/halfcheetah/scripts/train.py b/halfcheetah/scripts/train.py
++++-deleted file mode 100644
++++-index 04af8d7..0000000
++++---- a/halfcheetah/scripts/train.py
++++-+++ /dev/null
++++-@@ -1,122 +0,0 @@
++++--import os
++++--import numpy as np
++++--import torch
++++--import pdb
++++--
++++--import trajectory.utils as utils
++++--import trajectory.datasets as datasets
++++--from trajectory.models.transformers import GPT
++++--
++++--
++++--class Parser(utils.Parser):
++++--    dataset: str = 'halfcheetah-medium-expert-v2'
++++--    config: str = 'config.offline'
++++--
++++--#######################
++++--######## setup ########
++++--#######################
++++--
++++--args = Parser().parse_args('train')
++++--
++++--#######################
++++--####### dataset #######
++++--#######################
++++--
++++--env = datasets.load_environment(args.dataset)
++++--
++++--sequence_length = args.subsampled_sequence_length * args.step
++++--
++++--dataset_config = utils.Config(
++++--    datasets.DiscretizedDataset,
++++--    savepath=(args.savepath, 'data_config.pkl'),
++++--    env=args.dataset,
++++--    N=args.N,
++++--    penalty=args.termination_penalty,
++++--    sequence_length=sequence_length,
++++--    step=args.step,
++++--    discount=args.discount,
++++--    discretizer=args.discretizer,
++++--)
++++--
++++--dataset = dataset_config()
++++--obs_dim = dataset.observation_dim
++++--act_dim = dataset.action_dim
++++--transition_dim = dataset.joined_dim
++++--
++++--#######################
++++--######## model ########
++++--#######################
++++--
++++--block_size = args.subsampled_sequence_length * transition_dim - 1
++++--print(
++++--    f'Dataset size: {len(dataset)} | '
++++--    f'Joined dim: {transition_dim} '
++++--    f'(observation: {obs_dim}, action: {act_dim}) | Block size: {block_size}'
++++--)
++++--
++++--model_config = utils.Config(
++++--    GPT,
++++--    savepath=(args.savepath, 'model_config.pkl'),
++++--    ## discretization
++++--    vocab_size=args.N, block_size=block_size,
++++--    ## architecture
++++--    n_layer=args.n_layer, n_head=args.n_head, n_embd=args.n_embd*args.n_head,
++++--    ## dimensions
++++--    observation_dim=obs_dim, action_dim=act_dim, transition_dim=transition_dim,
++++--    ## loss weighting
++++--    action_weight=args.action_weight, reward_weight=args.reward_weight, value_weight=args.value_weight,
++++--    ## dropout probabilities
++++--    embd_pdrop=args.embd_pdrop, resid_pdrop=args.resid_pdrop, attn_pdrop=args.attn_pdrop,
++++--)
++++--
++++--model = model_config()
++++--model.to(args.device)
++++--
++++--#######################
++++--####### trainer #######
++++--#######################
++++--
++++--warmup_tokens = len(dataset) * block_size ## number of tokens seen per epoch
++++--final_tokens = 20 * warmup_tokens
++++--
++++--trainer_config = utils.Config(
++++--    utils.Trainer,
++++--    savepath=(args.savepath, 'trainer_config.pkl'),
++++--    # optimization parameters
++++--    batch_size=args.batch_size,
++++--    learning_rate=args.learning_rate,
++++--    betas=(0.9, 0.95),
++++--    grad_norm_clip=1.0,
++++--    weight_decay=0.1, # only applied on matmul weights
++++--    # learning rate decay: linear warmup followed by cosine decay to 10% of original
++++--    lr_decay=args.lr_decay,
++++--    warmup_tokens=warmup_tokens,
++++--    final_tokens=final_tokens,
++++--    ## dataloader
++++--    num_workers=0,
++++--    device=args.device,
++++--)
++++--
++++--trainer = trainer_config()
++++--
++++--#######################
++++--###### main loop ######
++++--#######################
++++--
++++--## scale number of epochs to keep number of updates constant
++++--n_epochs = int(1e6 / len(dataset) * args.n_epochs_ref)
++++--save_freq = int(n_epochs // args.n_saves)
++++--
++++--for epoch in range(n_epochs):
++++--    print(f'\nEpoch: {epoch} / {n_epochs} | {args.dataset} | {args.exp_name}')
++++--
++++--    trainer.train(model, dataset)
++++--
++++--    ## get greatest multiple of `save_freq` less than or equal to `save_epoch`
++++--    save_epoch = (epoch + 1) // save_freq * save_freq
++++--    statepath = os.path.join(args.savepath, f'state_{save_epoch}.pt')
++++--    print(f'Saving model to {statepath}')
++++--
++++--    ## save state to disk
++++--    state = model.state_dict()
++++--    torch.save(state, statepath)
++++-diff --git a/halfcheetah/scripts/xrl.py b/halfcheetah/scripts/xrl.py
++++-deleted file mode 100644
++++-index 134232a..0000000
++++---- a/halfcheetah/scripts/xrl.py
++++-+++ /dev/null
++++-@@ -1,372 +0,0 @@
++++--import json
++++--import pdb
++++--from os.path import join
++++--
++++--import trajectory.utils as utils
++++--import trajectory.datasets as datasets
++++--from trajectory.search import (
++++--    make_prefix,
++++--    update_context,
++++--)
++++--from trajectory.search.sampling import forward
++++--
++++--import gym
++++--import d4rl # Import required to register environments, you may need to also import the submodule
++++--import numpy as np
++++--import d3rlpy
++++--import math as mt
++++--from sklearn.cluster import KMeans
++++--from sklearn import datasets as skdatasets
++++--from sklearn.decomposition import PCA
++++--
++++--from pyclustering.cluster.xmeans import xmeans
++++--from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer
++++--
++++--from scipy.stats import wasserstein_distance
++++--
++++--class Parser(utils.Parser):
++++--    dataset: str = 'halfcheetah-medium-expert-v2'
++++--    config: str = 'config.offline'
++++--
+++++diff --git a/halfcheetah/scripts/xrl_v2.py b/halfcheetah/scripts/xrl_v2.py
+++++index 62a3d4d..b012599 100644
+++++--- a/halfcheetah/scripts/xrl_v2.py
++++++++ b/halfcheetah/scripts/xrl_v2.py
+++++@@ -21,54 +21,97 @@ from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer
+++++ from scipy.stats import wasserstein_distance
+++++ from moviepy.editor import VideoFileClip
+++++ 
++++++
+++++ class Parser(utils.Parser):
+++++     dataset: str = 'halfcheetah-medium-v2'
+++++     config: str = 'config.offline'
+++++ 
++++ -# utils
++++--    
++++--class XMeans:
++++--    def loglikelihood(self, r, rn, var, m, k):
++++--        l1 = - rn / 2.0 * mt.log(2 * mt.pi)
++++--        l2 = - rn * m / 2.0 * mt.log(var)
++++--        l3 = - (rn - k) / 2.0
++++--        l4 = rn * mt.log(rn)
++++--        l5 = - rn * mt.log(r)
++++--
++++--        return l1 + l2 + l3 + l4 + l5
++++--
++++--    def __init__(self, X, kmax = 20):
++++--        self.X = X
++++--        self.num = np.size(self.X, axis=0)
++++--        self.dim = np.size(X, axis=1)
++++--        self.KMax = kmax
++++--
++++--    def fit(self):
++++--        k = 1
++++--        X = self.X
++++--        M = self.dim
++++--        num = self.num
++++--
++++--        while(1):
++++--            ok = k
++++--
++++--            #Improve Params
++++--            kmeans = KMeans(n_clusters=k).fit(X)
++++--            labels = kmeans.labels_
++++--            m = kmeans.cluster_centers_
++++--
++++--            #Improve Structure
++++--            #Calculate BIC
++++--            p = M + 1
++++--
++++--            obic = np.zeros(k)
++++--
++++--            for i in range(k):
++++--                rn = np.size(np.where(labels == i))
++++--                var = np.sum((X[labels == i] - m[i])**2)/float(rn - 1)
++++--                obic[i] = self.loglikelihood(rn, rn, var, M, 1) - p/2.0*mt.log(rn)
++++--
++++--            #Split each cluster into two subclusters and calculate BIC of each splitted cluster
++++--            sk = 2 #The number of subclusters
++++--            nbic = np.zeros(k)
++++--            addk = 0
++++--
++++--            for i in range(k):
++++--                ci = X[labels == i]
++++--                r = np.size(np.where(labels == i))
++++--
++++--                kmeans = KMeans(n_clusters=sk).fit(ci)
++++--                ci_labels = kmeans.labels_
++++--                sm = kmeans.cluster_centers_
++++--
++++--                for l in range(sk):
++++--                    rn = np.size(np.where(ci_labels == l))
++++--                    var = np.sum((ci[ci_labels == l] - sm[l])**2)/float(rn - sk)
++++--                    nbic[i] += self.loglikelihood(r, rn, var, M, sk)
++++--
++++--                p = sk * (M + 1)
++++--                nbic[i] -= p/2.0*mt.log(r)
++++--
++++--                if obic[i] < nbic[i]:
++++--                    addk += 1
++++--
++++--            k += addk
++++--
++++--            if ok == k or k >= self.KMax:
++++--                break
++++--
++++--
++++--        #Calculate labels and centroids
++++--        kmeans = KMeans(n_clusters=k).fit(X)
++++--        self.labels = kmeans.labels_
++++--        self.k = k
++++--        self.m = kmeans.cluster_centers_
++++--
++++--
++++--def cluster_trajectories(trajectories):
++++--    xmeans_instance = XMeans(trajectories, kmax=10)
++++--    xmeans_instance.fit()
++++--
++++--    clusters = xmeans_instance.labels
++++--    return clusters
++++--
++++--def cluster_trajectories_2(trajectories):
+++++ 
+++++ def cluster_trajectories(trajectories, n_clusters=10):
+++++-    """TODO"""
++++++    """
++++++    Cluster trajectories using X-means.
++++++    
++++++    Args:
++++++    - trajectories: np.array, shape (n_trajectories, encoding_dim)
++++++    - n_clusters: int, max number of clusters
++++++    
++++++    Returns:
++++++    - idxs_per_cluster: list, trajectory idxs per cluster idxs
++++++    - clusters: np.array, shape (n_trajectories), cluster idxs per trajectory idx
++++++    """ 
+++++ 
++++ -    # Prepare initial centers - amount of initial centers defines amount of clusters from which X-Means will
++++ -    # start analysis.
++++--    amount_initial_centers = 2
++++--    initial_centers = kmeans_plusplus_initializer(trajectories, amount_initial_centers).initialize()
++++--    
++++++    # Set 2 initial cluster centers
+++++     amount_initial_centers = 2
+++++     initial_centers = kmeans_plusplus_initializer(trajectories, amount_initial_centers).initialize()
+++++     
++++ -    # Create instance of X-Means algorithm. The algorithm will start analysis from 2 clusters, the maximum
++++ -    # number of clusters that can be allocated is 10.
++++--    xmeans_instance = xmeans(trajectories, initial_centers, 10)
++++--    xmeans_instance.process()
++++--    
++++--    # Extract clustering results: clusters
++++--    idxs_per_cluster = xmeans_instance.get_clusters()
++++--
++++--    clusters = []
++++--    for i in range(len(trajectories)):
++++--        for j in range(len(idxs_per_cluster)):
++++--            if i in idxs_per_cluster[j]: clusters.append(j)
++++--
++++--    return idxs_per_cluster, np.array(clusters)
++++++    # Run X-means
+++++     xmeans_instance = xmeans(trajectories, initial_centers, n_clusters)
+++++     xmeans_instance.process()
+++++     
+++++     # Extract clustering results: clusters
+++++     idxs_per_cluster = xmeans_instance.get_clusters()
+++++ 
++++++    # Turn list of trajectory idxs per cluster to array of cluster idx per trajectory idx
+++++     clusters = []
+++++     for i in range(len(trajectories)):
+++++         for j in range(len(idxs_per_cluster)):
+++++             if i in idxs_per_cluster[j]: clusters.append(j)
+++++ 
+++++     return idxs_per_cluster, np.array(clusters)
++++ - 
++++--# https://github.com/sascha-kirch/ML_Notebooks/blob/main/Softmax_Temperature.ipynb
++++--def softmax(x, temp):
++++--    """Compute softmax values for each sets of scores in x."""
++++--    return np.exp(np.divide(x,temp)) / np.sum(np.exp(np.divide(x,temp)))
++++--
++++--def generate_data_embedding(trajectory_embeddings, normalizing_factor=1, temperature=1):
++++--    embedding = np.sum(trajectory_embeddings, axis=0) / normalizing_factor
++++--    embedding = softmax(embedding, temperature)
++++--    return embedding
++++--
++++--def embed_trajectory(gpt, discretizer, observations, actions, rewards, preprocess_fn):
++++--    context = []
++++--
++++--    for i in range(len(observations)):
++++--        observation = observations[i]
++++--        action = actions[i]
++++--        reward = rewards[i]
++++--
++++--        observation = preprocess_fn(observation)
++++--
++++--        # print(observation)
++++--        prefix = make_prefix(discretizer, context, observation, True)
++++--        # print("prefix", prefix.shape)
++++--
++++--        out = forward(gpt, prefix)
++++--        # print("out", out.shape)
++++--        context = update_context(context, discretizer, observation, action, reward, len(observations))
++++--        # print("cotext", context)
++++--    
++++--    emb = []
++++--    for context_step in context:
++++--        emb.append(context_step.numpy())
++++--    emb = np.array(emb)
++++--    emb = np.mean(emb, axis=0)[0]
++++--
++++++
++++++
+++++ def softmax(x, temp):
+++++-    """TODO"""
++++++    """
++++++    Softmax with temperature using max-trick.
++++++    
++++++    Args:
++++++    - x: np.array, shape (n_data, dim_data)
++++++    - temp: int, softmax temperature
++++++    
++++++    Returns:
++++++    - softmax_x: np.array: shape (dim_data)
++++++    """ 
++++++
+++++     max_x = np.max(x)
+++++-    return np.exp(np.divide(x-max_x,temp)) / np.sum(np.exp(np.divide(x-max_x,temp)))
++++++    softmax_x = np.exp(np.divide(x-max_x,temp)) / np.sum(np.exp(np.divide(x-max_x,temp)))
++++++    return softmax_x
++++++
+++++ 
+++++ def generate_data_embedding(trajectory_embeddings, temperature=10000):
+++++-    """TODO"""
++++++    """
++++++    Generate data embedding (sum+softmax) for set of encoded trajectories.
++++++    
++++++    Args:
++++++    - trajectory_embeddings: np.array, shape (n_data, dim_data)
++++++    - temperature: int, softmax temperature
++++++    
++++++    Returns:
++++++    - embedding: np.array, shape (dim_data)
++++++    """ 
+++++ 
+++++     embedding = np.sum(trajectory_embeddings, axis=0)
+++++     embedding = softmax(embedding, temperature)
+++++     
+++++-
+++++     return embedding
+++++ 
++++++
+++++ def embed_trajectory(gpt, discretizer, observations, actions, rewards, preprocess_fn):
+++++-    """TODO"""
++++++    """
++++++    Encode trajectory using a trajectory transformer with a sliding window.
++++++    
++++++    Args:
++++++    - gpt: trajectory transformer
++++++    - discretizer: environment discretizer
++++++    - observations: trajectory observations
++++++    - actions: trajectory actions
++++++    - rewards: trajectory rewards
++++++    - preprocess_fn: observations preprocessing functions
++++++    
++++++    Returns:
++++++    - embedding: np.array, shape (hidden_dim), encoded trajectory
++++++    """ 
+++++ 
+++++     context = []
+++++-
+++++     output = []
+++++ 
+++++     for i in range(len(observations)):
+++++@@ -76,12 +119,12 @@ def embed_trajectory(gpt, discretizer, observations, actions, rewards, preproces
+++++         action = actions[i]
+++++         reward = rewards[i]
+++++ 
++++++        # Preprocess, discretize & forward through trajectory transformer
+++++         observation = preprocess_fn(observation)
+++++-
+++++         prefix = make_prefix(discretizer, context, observation, True)
+++++-
+++++         out = forward(gpt, prefix)
+++++ 
++++++        # Sliding window
+++++         if len(context) >= 9:
+++++             context.pop(0)
+++++             if len(output) == 0:
+++++@@ -91,19 +134,33 @@ def embed_trajectory(gpt, discretizer, observations, actions, rewards, preproces
+++++ 
+++++         context = update_context(context, discretizer, observation, action, reward, len(observations))
+++++ 
+++++-    emb = np.mean(output, axis=0)
++++ -    return emb
++++--
++++--
++++--def create_complementary_dataset(dataset, idxs, trajectory_length=10):
++++--    observations = []
++++--    actions = []
++++--    rewards = []
++++--    terminals = []
++++--    for i in range(1000):
++++--        if i not in idxs:
++++--            observations += list(dataset.observations[1000*i:1000*i+trajectory_length])
++++--            actions += list(dataset.actions[1000*i:1000*i+trajectory_length])
++++--            rewards += list(dataset.rewards[1000*i:1000*i+trajectory_length])
++++--            terminals += list(dataset.terminals[1000*i:1000*i+trajectory_length])
++++--
++++--    new_dataset = d3rlpy.dataset.MDPDataset(
++++--        observations=np.array(observations),
++++--        actions=np.array(actions),
++++--        rewards=np.array(rewards),
++++--        terminals=np.array(terminals)
++++--    )
++++--    return new_dataset
++++--    
++++--
++++--
++++--
++++--def main():
++++--    # args = Parser().parse_args('plan')
++++--
++++--    #######################
++++--    ####### models ########
++++--    #######################
++++--
++++--
++++--
++++--
++++--
++++--    # print(args.dataset)
++++--
++++--    # dataset = utils.load_from_config(args.logbase, args.dataset, args.gpt_loadpath,
++++--    #         'data_config.pkl')
++++--
++++--
++++--    # gpt, gpt_epoch = utils.load_model(args.logbase, args.dataset, args.gpt_loadpath,
++++--    #         epoch=args.gpt_epoch, device=args.device)
++++--
++++--    # env = datasets.load_environment(args.dataset)
++++--
++++--    # discretizer = dataset.discretizer
++++--
++++--    # preprocess_fn = datasets.get_preprocess_fn(env.name)
++++--
++++--    # #######################
++++--    # ####### dataset #######
++++--    # #######################
++++--
++++--    # # env = datasets.load_environment(args.dataset)
++++--    # discretizer = dataset.discretizer
++++--    # preprocess_fn = datasets.get_preprocess_fn(env.name)
++++--
++++--    # # dataset
++++--    dataset_d3, env = d3rlpy.datasets.get_dataset("halfcheetah-medium-v2")
++++--
++++--    # env = gym.make('halfcheetah-medium-v2')
++++--    # dataset_d4 = d4rl.qlearning_dataset(env)
++++--
++++--    # # checks to see if d3rl & d4rl datasets are equal
++++--    # print(np.allclose(dataset_d3.actions[100], dataset_d4['actions'][100]))
++++--
++++--    # # dr4rl has same trajectories, just cut off 1 element before the end
++++--    # for j in range(1000):
++++--    #     for i in range(999):
++++--    #         if dataset_d4['rewards'][j * 999 + i] != dataset_d3.rewards[j * 1000 + i]: print("yo", i)
++++--
++++--    # #######################
++++--    # ###### main loop ######
++++--    # #######################
++++--
++++--    trajectory_length = 10 # 10 = max
++++--
++++--    # embeddings = []
++++--    # for i in range(1000):
++++--    #     observations = dataset_d3.observations[1000*i:1000*i+trajectory_length]
++++--    #     actions = dataset_d3.actions[1000*i:1000*i+trajectory_length]
++++--    #     rewards = dataset_d3.rewards[1000*i:1000*i+trajectory_length]
++++--    #     terminals = dataset_d3.terminals[1000*i:1000*i+trajectory_length]
++++--    #     emb = embed_trajectory(gpt, discretizer, observations, actions, rewards, preprocess_fn)
++++--    #     embeddings.append(emb)
++++--    # embeddings = np.array(embeddings)
++++--    # np.save("embeddings.npy", embeddings)
++++--    # print(embeddings)
++++--
++++--    embeddings = np.load("embeddings.npy")
++++--
++++--    pca = PCA(n_components=2)
++++--    pca = PCA(n_components=2)
++++--    pca_embeddings = pca.fit_transform(embeddings)
++++--    np.save("pca.py", pca_embeddings)
++++--
++++--    idxs_per_cluster, clusters = cluster_trajectories_2(embeddings)
++++--    # print(clusters)
++++--    # return
++++--    np.save("clusters.npy", clusters)
++++--
++++--    import matplotlib.pyplot as plt
++++--
++++--    d_orig = generate_data_embedding(embeddings)
++++--    unique_clusters = np.unique(clusters)
++++--    
++++--    d_j = []
++++--    complementary_datasets = []
++++--    for j in np.sort(unique_clusters):
++++--        print(j)
++++--        d_j.append(generate_data_embedding(embeddings[clusters != j]))
++++--        plt.scatter(pca_embeddings[clusters == j][:,0], pca_embeddings[clusters == j][:,1], label=j)
++++--        complementary_datasets.append(create_complementary_dataset(dataset_d3, idxs_per_cluster[j], trajectory_length))
++++--    
++++--    original_dataset = create_complementary_dataset(dataset_d3, [], trajectory_length)
++++--
++++--    print(complementary_datasets, original_dataset)
++++--
++++--    plt.legend()
++++--    plt.show()
++++--
++++--    agent_orig = d3rlpy.algos.SAC(
++++--        actor_learning_rate=3e-4,
++++--        critic_learning_rate=3e-4,
++++--        temp_learning_rate=3e-4,
++++--        batch_size=256)
++++--
++++--    print(agent_orig)
++++--
++++--    training_steps = 1000
++++--
++++--    agent_orig.fit(original_dataset, n_steps=training_steps)
++++--
++++--    agents_compl = []
++++--
++++--    for dset in complementary_datasets:
++++--        agent = d3rlpy.algos.SAC(
++++--            actor_learning_rate=3e-4,
++++--            critic_learning_rate=3e-4,
++++--            temp_learning_rate=3e-4,
++++--            batch_size=256)
++++--        agent.fit(dset, n_steps=training_steps)
++++--        agents_compl.append(agent)
++++--
++++--    action_orig = agent_orig.predict(dataset_d3.observations[0])
++++--
++++--    actions_compl = []
++++--    for agent in agents_compl:
++++--        actions_compl.append(agent.predict(dataset_d3.observations[0]))
++++--    
++++--    action_dists = []
++++--    for action in actions_compl:
++++--        action_dists.append(np.linalg.norm(action_orig-action))
++++--
++++--    k = 3
++++--    topk = np.argpartition(action_dists, -k)[-k:]
++++--
++++--    d_w = {}
++++--    for idx in topk:
++++--        d_w[idx] = wasserstein_distance(d_j[idx], d_orig)
++++--
++++--    cluster_assignment = min(d_w, key=d_w.get)
++++--    print("explanation assigned to cluster", cluster_assignment)
++++--
++++--    
++++--def assignment_test():
++++--    action_orig = np.random.rand(10)
++++--    d_orig = np.random.rand(5)
++++--
++++--    actions_compl = np.random.rand(6,10)
++++--    d_j = np.random.rand(6,5)
++++--
++++--    action_dists = []
++++--    for action in actions_compl:
++++--        action_dists.append(np.linalg.norm(action_orig-action))
++++--
++++--    print(action_dists)
++++--
++++--    k = 3
++++--    topk = np.argpartition(action_dists, -k)[-k:]
++++--
++++--    print(topk)
++++--
++++--    d_w = {}
++++--    for idx in topk:
++++--        d_w[idx] = wasserstein_distance(d_j[idx], d_orig)
++++--
++++--    print(d_w)
++++--
++++--    cluster_assignment = min(d_w, key=d_w.get)
++++--    print("explanation assigned to cluster", cluster_assignment)
++++--
++++--
++++--if __name__ == "__main__":
++++--    # main()
++++--    assignment_test()
++++-diff --git a/halfcheetah/trajectory.egg-info/PKG-INFO b/halfcheetah/trajectory.egg-info/PKG-INFO
++++-index 452c6cb..2603850 100644
++++---- a/halfcheetah/trajectory.egg-info/PKG-INFO
++++-+++ b/halfcheetah/trajectory.egg-info/PKG-INFO
++++-@@ -1,4 +1,11 @@
++++- Metadata-Version: 2.1
++++- Name: trajectory
++++- Version: 0.0.0
++++-+Summary: UNKNOWN
++++-+Home-page: UNKNOWN
++++-+License: UNKNOWN
++++-+Platform: UNKNOWN
++++- License-File: LICENSE
++++++    # Embedding is the average of encoded states
++++++    embedding = np.mean(output, axis=0)
++++++    return embedding
++++++
+++++ 
+++++ def create_complementary_dataset(dataset, idxs, trajectory_length=10, inverse=False):
+++++-    """TODO"""
++++++    """
++++++    Encode trajectory using a trajectory transformer with a sliding window.
++++++    
++++++    Args:
++++++    - dataset: MDPDataset, original d3rl dataset
++++++    - idxs: trajectory idxs to ignore (or include if inverse is True)
++++++    - trajectory_length: int, trajectory length
++++++    - inverse: bool, if True the dataset is not complementary
++++ +
++++-+UNKNOWN
++++++    Returns:
++++++    - new_dataset: MDPDataset, complementary dataset
++++++    """ 
+++++ 
+++++     observations = []
+++++     actions = []
+++++     rewards = []
+++++     terminals = []
+++++ 
+++++-    n_trajs = int(1000000/trajectory_length)
++++++    n_trajs = int(len(dataset.observations)/trajectory_length)
+++++     for i in range(n_trajs):
++++++        # If inverse is True, only include idxs. If not, leave out idxs
+++++         condition = i not in idxs
+++++         if inverse: condition = not condition
+++++ 
+++++@@ -112,6 +169,7 @@ def create_complementary_dataset(dataset, idxs, trajectory_length=10, inverse=Fa
+++++             actions += list(dataset.actions[trajectory_length*i:trajectory_length*(i+1)])
+++++             rewards += list(dataset.rewards[trajectory_length*i:trajectory_length*(i+1)])
+++++ 
++++++    # Trajectories end with a terminal state
+++++     terminals = np.tile([0]*(trajectory_length-1)+[1], int(len(observations)/trajectory_length))
+++++ 
+++++     new_dataset = d3rlpy.dataset.MDPDataset(
+++++@@ -124,6 +182,17 @@ def create_complementary_dataset(dataset, idxs, trajectory_length=10, inverse=Fa
+++++     
+++++ 
+++++ def clusters_to_idxs(clusters):
++++++    """
++++++    Helper function to turn array of cluster idxs per trajectory idxs to a list 
++++++    of trajectory idxs per cluster idx.
++++++    
++++++    Args:
++++++    - clusters: np.array, cluster idx per trajectory idx
++++ +
++++++    Returns:
++++++    - idxs_per_cluster: list, trajectory idxs per cluster idx
++++++    """ 
++++++
+++++     idxs_per_cluster = []
+++++     for i in np.sort(np.unique(clusters)):
+++++         idxs_per_cluster.append(list(np.argwhere(clusters == i).flatten()))
+++++@@ -142,7 +211,7 @@ def main():
+++++ 
+++++     ### IMPORTANT DEFINITIONS XRL SCRIPT ###
+++++ 
+++++-    load_embeddings = False
++++++    load_embeddings = True
+++++     load_clusters = True
+++++     load_agents = True
+++++     generate_human_study = False
++++ diff --git a/halfcheetah/trajectory.egg-info/SOURCES.txt b/halfcheetah/trajectory.egg-info/SOURCES.txt
++++-index 4474d85..84e8e3a 100644
+++++index 84e8e3a..4474d85 100644
++++ --- a/halfcheetah/trajectory.egg-info/SOURCES.txt
++++ +++ b/halfcheetah/trajectory.egg-info/SOURCES.txt
++++-@@ -30,4 +30,5 @@ trajectory/utils/serialization.py
+++++@@ -30,5 +30,4 @@ trajectory/utils/serialization.py
++++  trajectory/utils/setup.py
++++  trajectory/utils/timer.py
++++  trajectory/utils/training.py
++++ -trajectory/utils/video.py
+++++-trajectory_aaa/__init__.py
++++ \ No newline at end of file
++++ +trajectory/utils/video.py
++++-+trajectory_aaa/__init__.py
++++ \ No newline at end of file
++++ diff --git a/halfcheetah/trajectory.egg-info/top_level.txt b/halfcheetah/trajectory.egg-info/top_level.txt
++++-index ce65198..1d5271f 100644
+++++index 1d5271f..ce65198 100644
++++ --- a/halfcheetah/trajectory.egg-info/top_level.txt
++++ +++ b/halfcheetah/trajectory.egg-info/top_level.txt
++++-@@ -1 +1,2 @@
+++++@@ -1,2 +1 @@
++++  trajectory
++++-+trajectory_aaa
+++ \ No newline at end of file
+++-diff --git a/halfcheetah/pca.py.npy b/halfcheetah/pca.py.npy
+++-deleted file mode 100644
+++-index bb19150..0000000
+++-Binary files a/halfcheetah/pca.py.npy and /dev/null differ
+++-diff --git a/halfcheetah/plotting/bar.png b/halfcheetah/plotting/bar.png
+++-deleted file mode 100644
+++-index 3679667..0000000
+++-Binary files a/halfcheetah/plotting/bar.png and /dev/null differ
+++-diff --git a/halfcheetah/plotting/plot.py b/halfcheetah/plotting/plot.py
+++-deleted file mode 100644
+++-index 163d0e4..0000000
+++---- a/halfcheetah/plotting/plot.py
+++-+++ /dev/null
+++-@@ -1,74 +0,0 @@
+++--import numpy as np
+++--import matplotlib
+++--import matplotlib.pyplot as plt
+++--import pdb
+++--
+++--from plotting.scores import means
+++--
+++--class Colors:
+++--	grey = '#B4B4B4'
+++--	gold = '#F6C781'
+++--	red = '#EC7C7D'
+++--	blue = '#70ABCC'
+++--
+++--LABELS = {
+++--	# 'BC': 'Behavior\nCloning',
+++--	# 'MBOP': 'Model-Based\nOffline Planning',
+++--	# 'BRAC': 'Behavior-Reg.\nActor-Critic',
+++--	# 'CQL': 'Conservative\nQ-Learning',
+++--}
+++--
+++--def get_mean(results, exclude=None):
+++--	'''
+++--		results : { environment: score, ... }
+++--	'''
+++--	filtered = {
+++--		k: v for k, v in results.items()
+++--		if (not exclude) or (exclude and exclude not in k)
+++--	}
+++--	return np.mean(list(filtered.values()))
+++--
+++--if __name__ == '__main__':
+++--
+++--	#################
+++--	## latex
+++--	#################
+++--	matplotlib.rc('font', **{'family': 'serif', 'serif': ['Computer Modern']})
+++--	matplotlib.rc('text', usetex=True)
+++--	matplotlib.rcParams['text.latex.preamble']=[r"\usepackage{amsmath}"]
+++--	#################
+++--
+++--	fig = plt.gcf()
+++--	ax = plt.gca()
+++--	fig.set_size_inches(7.5, 2.5)
+++--
+++--	means = {k: get_mean(v, exclude='ant') for k, v in means.items()}
+++--	print(means)
+++--
+++--	algs = ['BC', 'MBOP', 'BRAC', 'CQL', 'Decision\nTransformer', 'Trajectory\nTransformer']
+++--	vals = [means[alg] for alg in algs]
+++--
+++--	colors = [
+++--		Colors.grey, Colors.gold,
+++--		Colors.red, Colors.red, Colors.blue, Colors.blue
+++--	]
+++--
+++--	labels = [LABELS.get(alg, alg) for alg in algs]
+++--	plt.bar(labels, vals, color=colors, edgecolor=Colors.gold, lw=0)
+++--	plt.ylabel('Average normalized return', labelpad=15)
+++--	# plt.title('Offline RL Results')
+++--
+++--	legend_labels = ['Behavior Cloning', 'Trajectory Optimization', 'Temporal Difference', 'Sequence Modeling']
+++--	colors = [Colors.grey, Colors.gold, Colors.red, Colors.blue]
+++--	handles = [plt.Rectangle((0,0),1,1, color=color) for label, color in zip(legend_labels, colors)]
+++--	plt.legend(handles, legend_labels, ncol=4,
+++--		bbox_to_anchor=(1.07, -.18), fancybox=False, framealpha=0, shadow=False, columnspacing=1.5, handlelength=1.5)
+++--
+++--	matplotlib.rcParams['hatch.linewidth'] = 7.5
+++--	# ax.patches[-1].set_hatch('/')
+++--
+++--	ax.spines['right'].set_visible(False)
+++--	ax.spines['top'].set_visible(False)
+++--
+++--	# plt.savefig('plotting/bar.pdf', bbox_inches='tight')
+++--	plt.savefig('plotting/bar.png', bbox_inches='tight', dpi=500)
+++-diff --git a/halfcheetah/plotting/read_results.py b/halfcheetah/plotting/read_results.py
+++-deleted file mode 100644
+++-index 5a5fb62..0000000
+++---- a/halfcheetah/plotting/read_results.py
+++-+++ /dev/null
+++-@@ -1,70 +0,0 @@
+++--import os
+++--import glob
+++--import numpy as np
+++--import json
+++--import pdb
+++--
+++--import trajectory.utils as utils
+++--
+++--DATASETS = [
+++--	f'{env}-{buffer}'
+++--	for env in ['hopper', 'walker2d', 'halfcheetah', 'ant']
+++--	for buffer in ['medium-expert-v2', 'medium-v2', 'medium-replay-v2']
+++--]
+++--
+++--LOGBASE = 'logs'
+++--TRIAL = '*'
+++--EXP_NAME = 'plans/pretrained'
+++--
+++--def load_results(paths):
+++--	'''
+++--		paths : path to directory containing experiment trials
+++--	'''
+++--	scores = []
+++--	for i, path in enumerate(sorted(paths)):
+++--		score = load_result(path)
+++--		if score is None:
+++--			print(f'Skipping {path}')
+++--			continue
+++--		scores.append(score)
+++--
+++--		suffix = path.split('/')[-1]
+++--
+++--	mean = np.mean(scores)
+++--	err = np.std(scores) / np.sqrt(len(scores))
+++--	return mean, err, scores
+++--
+++--def load_result(path):
+++--	'''
+++--		path : path to experiment directory; expects `rollout.json` to be in directory
+++--	'''
+++--	fullpath = os.path.join(path, 'rollout.json')
+++--	suffix = path.split('/')[-1]
+++--
+++--	if not os.path.exists(fullpath):
+++--		return None
+++--
+++--	results = json.load(open(fullpath, 'rb'))
+++--	score = results['score']
+++--	return score * 100
+++--
+++--#######################
+++--######## setup ########
+++--#######################
+++--
+++--if __name__ == '__main__':
+++--
+++--	class Parser(utils.Parser):
+++--	    dataset: str = None
+++--
+++--	args = Parser().parse_args()
+++--
+++--	for dataset in ([args.dataset] if args.dataset else DATASETS):
+++--		subdirs = glob.glob(os.path.join(LOGBASE, dataset, EXP_NAME))
+++--
+++--		for subdir in subdirs:
+++--			reldir = subdir.split('/')[-1]
+++--			paths = glob.glob(os.path.join(subdir, TRIAL))
+++--
+++--			mean, err, scores = load_results(paths)
+++--			print(f'{dataset.ljust(30)} | {subdir.ljust(50)} | {len(scores)} scores \n    {mean:.2f} +/- {err:.2f}\n')
+++-diff --git a/halfcheetah/plotting/scores.py b/halfcheetah/plotting/scores.py
+++-deleted file mode 100644
+++-index f1917f7..0000000
+++---- a/halfcheetah/plotting/scores.py
+++-+++ /dev/null
+++-@@ -1,123 +0,0 @@
+++--means = {
+++--	'Trajectory\nTransformer': {
+++--		##
+++--		'halfcheetah-medium-expert-v2': 95.0,
+++--		'hopper-medium-expert-v2': 110.0,
+++--		'walker2d-medium-expert-v2': 101.9,
+++--		'ant-medium-expert-v2': 116.1,
+++--		##
+++--		'halfcheetah-medium-v2': 46.9,
+++--		'hopper-medium-v2': 61.1,
+++--		'walker2d-medium-v2': 79.0,
+++--		'ant-medium-v2': 83.1,
+++--		##
+++--		'halfcheetah-medium-replay-v2': 41.9,
+++--		'hopper-medium-replay-v2': 91.5,
+++--		'walker2d-medium-replay-v2': 82.6,
+++--		'ant-medium-replay-v2': 77.0,
+++--	},
+++--	'Decision\nTransformer': {
+++--		##
+++--		'halfcheetah-medium-expert-v2': 86.8,
+++--		'hopper-medium-expert-v2': 107.6,
+++--		'walker2d-medium-expert-v2': 108.1,
+++--		##
+++--		'halfcheetah-medium-v2': 42.6,
+++--		'hopper-medium-v2': 67.6,
+++--		'walker2d-medium-v2': 74.0,
+++--		##
+++--		'halfcheetah-medium-replay-v2': 36.6,
+++--		'hopper-medium-replay-v2': 82.7,
+++--		'walker2d-medium-replay-v2': 66.6,
+++--	},
+++--	'CQL': {
+++--		##
+++--		'halfcheetah-medium-expert-v2': 91.6,
+++--		'hopper-medium-expert-v2': 105.4,
+++--		'walker2d-medium-expert-v2': 108.8,
+++--		##
+++--		'halfcheetah-medium-v2': 44.0,
+++--		'hopper-medium-v2': 58.5,
+++--		'walker2d-medium-v2': 72.5,
+++--		##
+++--		'halfcheetah-medium-replay-v2': 45.5,
+++--		'hopper-medium-replay-v2': 95.0,
+++--		'walker2d-medium-replay-v2': 77.2,
+++--	},
+++--	'MOPO': {
+++--		##
+++--		'halfcheetah-medium-expert-v2': 63.3,
+++--		'hopper-medium-expert-v2': 23.7,
+++--		'walker2d-medium-expert-v2': 44.6,
+++--		##
+++--		'halfcheetah-medium-v2': 42.3,
+++--		'hopper-medium-v2': 28.0,
+++--		'walker2d-medium-v2': 17.8,
+++--		##
+++--		'halfcheetah-medium-replay-v2': 53.1,
+++--		'hopper-medium-replay-v2': 67.5,
+++--		'walker2d-medium-replay-v2':39.0,
+++--	},
+++--	'MBOP': {
+++--		##
+++--		'halfcheetah-medium-expert-v2': 105.9,
+++--		'hopper-medium-expert-v2': 55.1,
+++--		'walker2d-medium-expert-v2': 70.2,
+++--		##
+++--		'halfcheetah-medium-v2': 44.6,
+++--		'hopper-medium-v2': 48.8,
+++--		'walker2d-medium-v2': 41.0,
+++--		##
+++--		'halfcheetah-medium-replay-v2': 42.3,
+++--		'hopper-medium-replay-v2': 12.4,
+++--		'walker2d-medium-replay-v2': 9.7,
+++--	},
+++--	'BRAC': {
+++--		##
+++--		'halfcheetah-medium-expert-v2': 41.9,
+++--		'hopper-medium-expert-v2': 0.9,
+++--		'walker2d-medium-expert-v2': 81.6,
+++--		##
+++--		'halfcheetah-medium-v2': 46.3,
+++--		'hopper-medium-v2': 31.3,
+++--		'walker2d-medium-v2': 81.1,
+++--		##
+++--		'halfcheetah-medium-replay-v2': 47.7,
+++--		'hopper-medium-replay-v2': 0.6,
+++--		'walker2d-medium-replay-v2': 0.9,
+++--	},
+++--	'BC': {
+++--		##
+++--		'halfcheetah-medium-expert-v2': 59.9,
+++--		'hopper-medium-expert-v2': 79.6,
+++--		'walker2d-medium-expert-v2': 36.6,
+++--		##
+++--		'halfcheetah-medium-v2': 43.1,
+++--		'hopper-medium-v2': 63.9,
+++--		'walker2d-medium-v2': 77.3,
+++--		##
+++--		'halfcheetah-medium-replay-v2': 4.3,
+++--		'hopper-medium-replay-v2': 27.6,
+++--		'walker2d-medium-replay-v2': 36.9,
+++--	},
+++--}
+++--
+++--errors = {
+++--	'Trajectory\nTransformer': {
+++--		##
+++--		'halfcheetah-medium-expert-v2': 0.2,
+++--		'hopper-medium-expert-v2': 2.7,
+++--		'walker2d-medium-expert-v2': 6.8,
+++--		'ant-medium-expert-v2': 9.0,
+++--		##
+++--		'halfcheetah-medium-v2': 0.4,
+++--		'hopper-medium-v2': 3.6,
+++--		'walker2d-medium-v2': 2.8,
+++--		'ant-medium-v2': 7.3,
+++--		##
+++--		'halfcheetah-medium-replay-v2': 2.5,
+++--		'hopper-medium-replay-v2': 3.6,
+++--		'walker2d-medium-replay-v2': 6.9,
+++--		'ant-medium-replay-v2': 6.8,
+++--	},
+++--}
+++++-trajectory_aaa
+++++diff --git a/seaquest/readme.md b/seaquest/readme.md
+++++index 84e53f8..53561f9 100644
+++++--- a/seaquest/readme.md
++++++++ b/seaquest/readme.md
+++++@@ -10,4 +10,4 @@ pip install git+https://github.com/takuseno/d4rl-atari
+++++ pip install "gym[atari, accept-rom-license]"
+++++ pip install pyclustering
+++++ pip install seaborn
+++++-pip install d3rlpy==1.1.1
+++++\ No newline at end of file
++++++pip install d3rlpy==1.1.1
+++ \ No newline at end of file
+++-diff --git a/halfcheetah/plotting/table.py b/halfcheetah/plotting/table.py
++++diff --git a/halfcheetah/scripts/xrl_v2.py b/halfcheetah/scripts/xrl_v2.py
+++ deleted file mode 100644
+++-index eae74e6..0000000
+++---- a/halfcheetah/plotting/table.py
++++index 62a3d4d..0000000
++++--- a/halfcheetah/scripts/xrl_v2.py
+++ +++ /dev/null
+++-@@ -1,127 +0,0 @@
+++--import numpy as np
+++--import pdb
+++--
+++--from plotting.plot import get_mean
+++--from plotting.scores import (
+++--	means as MEANS,
+++--	errors as ERRORS,
+++--)
+++--
+++--ALGORITHM_STRINGS = {
+++--	'Trajectory\nTransformer': 'TT (Ours)',
+++--	'Decision\nTransformer': 'DT',	
+++--}
+++--
+++--BUFFER_STRINGS = {
+++--	'medium-expert': 'Medium-Expert',
+++--	'medium': 'Medium',
+++--	'medium-replay': 'Medium-Replay',	
+++--}
+++--
+++--ENVIRONMENT_STRINGS = {
+++--	'halfcheetah': 'HalfCheetah',
+++--	'hopper': 'Hopper',
+++--	'walker2d': 'Walker2d',
+++--	'ant': 'Ant',
+++--}
+++--
+++--SHOW_ERRORS = ['Trajectory\nTransformer']
+++--
+++--def get_result(algorithm, buffer, environment, version='v2'):
+++--	key = f'{environment}-{buffer}-{version}'
+++--	mean = MEANS[algorithm].get(key, '-')
+++--	if algorithm in SHOW_ERRORS:
+++--		error = ERRORS[algorithm].get(key)
+++--		return (mean, error)
+++--	else:
+++--		return mean
+++--
+++--def format_result(result):
+++--	if type(result) == tuple:
+++--		mean, std = result
+++--		return f'${mean}$ \\scriptsize{{\\raisebox{{1pt}}{{$\\pm {std}$}}}}'
+++--	else:
+++--		return f'${result}$'
+++--
+++--def format_row(buffer, environment, results):
+++--	buffer_str = BUFFER_STRINGS[buffer]
+++--	environment_str = ENVIRONMENT_STRINGS[environment]
+++--	results_str = ' & '.join(format_result(result) for result in results)
+++--	row = f'{buffer_str} & {environment_str} & {results_str} \\\\ \n'
+++--	return row
+++--
+++--def format_buffer_block(algorithms, buffer, environments):
+++--	block_str = '\\midrule\n'
+++--	for environment in environments:
+++--		results = [get_result(alg, buffer, environment) for alg in algorithms]
+++--		row_str = format_row(buffer, environment, results)
+++--		block_str += row_str
+++--	return block_str
+++--
+++--def format_algorithm(algorithm):
+++--	algorithm_str = ALGORITHM_STRINGS.get(algorithm, algorithm)
+++--	return f'\multicolumn{{1}}{{c}}{{\\bf {algorithm_str}}}'
+++--
+++--def format_algorithms(algorithms):
+++--	return ' & '.join(format_algorithm(algorithm) for algorithm in algorithms)
+++--
+++--def format_averages(means, label):
+++--	prefix = f'\\multicolumn{{2}}{{c}}{{\\bf Average ({label})}} & '
+++--	formatted = ' & '.join(str(mean) for mean in means)
+++--	return prefix + formatted
+++--
+++--def format_averages_block(algorithms):
+++--	means_filtered = [np.round(get_mean(MEANS[algorithm], exclude='ant'), 1) for algorithm in algorithms]
+++--	means_all = [np.round(get_mean(MEANS[algorithm], exclude=None), 1) for algorithm in algorithms]
+++--
+++--	means_all = [
+++--		means
+++--		if 'ant-medium-expert-v2' in MEANS[algorithm]
+++--		else '$-$'
+++--		for algorithm, means in zip(algorithms, means_all)
+++--	]
+++--
+++--	formatted_filtered = format_averages(means_filtered, 'without Ant')
+++--	formatted_all = format_averages(means_all, 'all settings')
+++--
+++--	formatted_block = (
+++--		f'{formatted_filtered} \\hspace{{.6cm}} \\\\ \n'
+++--		f'{formatted_all} \\hspace{{.6cm}} \\\\ \n'
+++--	)
+++--	return formatted_block
+++--
+++--def format_table(algorithms, buffers, environments):
+++--	justify_str = 'll' + 'r' * len(algorithms)
+++--	algorithm_str = format_algorithms(['Dataset', 'Environment'] + algorithms)
+++--	averages_str = format_averages_block(algorithms)
+++--	table_prefix = (
+++--		'\\begin{table*}[h]\n'
+++--		'\\centering\n'
+++--		'\\small\n'
+++--		f'\\begin{{tabular}}{{{justify_str}}}\n'
+++--		'\\toprule\n'
+++--		f'{algorithm_str} \\\\ \n'
+++--	)
+++--	table_suffix = (
+++--		'\\midrule\n'
+++--		f'{averages_str}'
+++--		'\\bottomrule\n'
+++--		'\\end{tabular}\n'
+++--		'\\label{table:d4rl}\n'
+++--		'\\end{table*}'
+++--	)
+++--	blocks = ''.join(format_buffer_block(algorithms, buffer, environments) for buffer in buffers)
+++--	table = (
+++--		f'{table_prefix}'
+++--		f'{blocks}'
+++--		f'{table_suffix}'
+++--	)
+++--	return table
+++--
+++--
+++--algorithms =['BC', 'MBOP', 'BRAC', 'CQL',  'Decision\nTransformer', 'Trajectory\nTransformer']
+++--buffers = ['medium-expert', 'medium', 'medium-replay']
+++--environments = ['halfcheetah', 'hopper', 'walker2d', 'ant']
+++--
+++--table = format_table(algorithms, buffers, environments)
+++--print(table)
+++-diff --git a/halfcheetah/scripts/plan.py b/halfcheetah/scripts/plan.py
+++-deleted file mode 100644
+++-index f13d4cc..0000000
+++---- a/halfcheetah/scripts/plan.py
+++-+++ /dev/null
+++-@@ -1,124 +0,0 @@
+++--import json
+++--import pdb
+++--from os.path import join
+++--
+++--import trajectory.utils as utils
+++--import trajectory.datasets as datasets
+++--from trajectory.search import (
+++--    beam_plan,
+++--    make_prefix,
+++--    extract_actions,
+++--    update_context,
+++--)
+++--
+++--class Parser(utils.Parser):
+++--    dataset: str = 'halfcheetah-medium-expert-v2'
+++--    config: str = 'config.offline'
+++--
+++--#######################
+++--######## setup ########
+++--#######################
+++--
+++--args = Parser().parse_args('plan')
+++--
+++--#######################
+++--####### models ########
+++--#######################
+++--
+++--dataset = utils.load_from_config(args.logbase, args.dataset, args.gpt_loadpath,
+++--        'data_config.pkl')
+++--
+++--gpt, gpt_epoch = utils.load_model(args.logbase, args.dataset, args.gpt_loadpath,
+++--        epoch=args.gpt_epoch, device=args.device)
+++--
+++--#######################
+++--####### dataset #######
+++--#######################
+++--
+++--env = datasets.load_environment(args.dataset)
+++--print('yo')
+++--renderer = utils.make_renderer(args)
+++--timer = utils.timer.Timer()
+++--
+++--discretizer = dataset.discretizer
+++--discount = dataset.discount
+++--observation_dim = dataset.observation_dim
+++--action_dim = dataset.action_dim
+++--
+++--value_fn = lambda x: discretizer.value_fn(x, args.percentile)
+++--preprocess_fn = datasets.get_preprocess_fn(env.name)
+++--
+++--print('yo2')
+++--
+++--#######################
+++--###### main loop ######
+++--#######################
+++--
+++--observation = env.reset()
+++--total_reward = 0
+++--
+++--## observations for rendering
+++--rollout = [observation.copy()]
++++@@ -1,442 +0,0 @@
++++-"""
++++-TODO HEADER
++++-"""
+++ -
+++--## previous (tokenized) transitions for conditioning transformer
+++--context = []
+++--
+++--T = env.max_episode_steps
+++--for t in range(T):
+++--
+++--    observation = preprocess_fn(observation)
+++--
+++--    if t % args.plan_freq == 0:
+++--        ## concatenate previous transitions and current observations to input to model
+++--        prefix = make_prefix(discretizer, context, observation, args.prefix_context)
+++--
+++--        ## sample sequence from model beginning with `prefix`
+++--        sequence = beam_plan(
+++--            gpt, value_fn, prefix,
+++--            args.horizon, args.beam_width, args.n_expand, observation_dim, action_dim,
+++--            discount, args.max_context_transitions, verbose=args.verbose,
+++--            k_obs=args.k_obs, k_act=args.k_act, cdf_obs=args.cdf_obs, cdf_act=args.cdf_act,
+++--        )
+++--
+++--    else:
+++--        sequence = sequence[1:]
+++--
+++--    ## [ horizon x transition_dim ] convert sampled tokens to continuous trajectory
+++--    sequence_recon = discretizer.reconstruct(sequence)
+++--
+++--    ## [ action_dim ] index into sampled trajectory to grab first action
+++--    action = extract_actions(sequence_recon, observation_dim, action_dim, t=0)
+++--
+++--    ## execute action in environment
+++--    next_observation, reward, terminal, _ = env.step(action)
+++--
+++--    ## update return
+++--    total_reward += reward
+++--    score = env.get_normalized_score(total_reward)
+++--
+++--    ## update rollout observations and context transitions
+++--    rollout.append(next_observation.copy())
+++--    context = update_context(context, discretizer, observation, action, reward, args.max_context_transitions)
+++--
+++--    print(
+++--        f'[ plan ] t: {t} / {T} | r: {reward:.2f} | R: {total_reward:.2f} | score: {score:.4f} | '
+++--        f'time: {timer():.2f} | {args.dataset} | {args.exp_name} | {args.suffix}\n'
+++--    )
+++--
+++--    ## visualization
+++--    if t % args.vis_freq == 0 or terminal or t == T:
+++--
+++--        ## save current plan
+++--        renderer.render_plan(join(args.savepath, f'{t}_plan.mp4'), sequence_recon, env.state_vector())
+++--
+++--        ## save rollout thus far
+++--        renderer.render_rollout(join(args.savepath, f'rollout.mp4'), rollout, fps=80)
+++--
+++--    if terminal: break
+++--
+++--    observation = next_observation
+++--
+++--## save result as a json file
+++--json_path = join(args.savepath, 'rollout.json')
+++--json_data = {'score': score, 'step': t, 'return': total_reward, 'term': terminal, 'gpt_epoch': gpt_epoch}
+++--json.dump(json_data, open(json_path, 'w'), indent=2, sort_keys=True)
+++-diff --git a/halfcheetah/scripts/train.py b/halfcheetah/scripts/train.py
+++-deleted file mode 100644
+++-index 04af8d7..0000000
+++---- a/halfcheetah/scripts/train.py
+++-+++ /dev/null
+++-@@ -1,122 +0,0 @@
+++ -import os
+++ -import numpy as np
+++--import torch
+++--import pdb
+++--
+++--import trajectory.utils as utils
+++--import trajectory.datasets as datasets
+++--from trajectory.models.transformers import GPT
+++--
+++--
+++--class Parser(utils.Parser):
+++--    dataset: str = 'halfcheetah-medium-expert-v2'
+++--    config: str = 'config.offline'
+++--
+++--#######################
+++--######## setup ########
+++--#######################
+++--
+++--args = Parser().parse_args('train')
+++--
+++--#######################
+++--####### dataset #######
+++--#######################
+++--
+++--env = datasets.load_environment(args.dataset)
+++--
+++--sequence_length = args.subsampled_sequence_length * args.step
+++--
+++--dataset_config = utils.Config(
+++--    datasets.DiscretizedDataset,
+++--    savepath=(args.savepath, 'data_config.pkl'),
+++--    env=args.dataset,
+++--    N=args.N,
+++--    penalty=args.termination_penalty,
+++--    sequence_length=sequence_length,
+++--    step=args.step,
+++--    discount=args.discount,
+++--    discretizer=args.discretizer,
+++--)
+++--
+++--dataset = dataset_config()
+++--obs_dim = dataset.observation_dim
+++--act_dim = dataset.action_dim
+++--transition_dim = dataset.joined_dim
+++--
+++--#######################
+++--######## model ########
+++--#######################
+++--
+++--block_size = args.subsampled_sequence_length * transition_dim - 1
+++--print(
+++--    f'Dataset size: {len(dataset)} | '
+++--    f'Joined dim: {transition_dim} '
+++--    f'(observation: {obs_dim}, action: {act_dim}) | Block size: {block_size}'
+++--)
+++--
+++--model_config = utils.Config(
+++--    GPT,
+++--    savepath=(args.savepath, 'model_config.pkl'),
+++--    ## discretization
+++--    vocab_size=args.N, block_size=block_size,
+++--    ## architecture
+++--    n_layer=args.n_layer, n_head=args.n_head, n_embd=args.n_embd*args.n_head,
+++--    ## dimensions
+++--    observation_dim=obs_dim, action_dim=act_dim, transition_dim=transition_dim,
+++--    ## loss weighting
+++--    action_weight=args.action_weight, reward_weight=args.reward_weight, value_weight=args.value_weight,
+++--    ## dropout probabilities
+++--    embd_pdrop=args.embd_pdrop, resid_pdrop=args.resid_pdrop, attn_pdrop=args.attn_pdrop,
+++--)
+++--
+++--model = model_config()
+++--model.to(args.device)
+++--
+++--#######################
+++--####### trainer #######
+++--#######################
+++--
+++--warmup_tokens = len(dataset) * block_size ## number of tokens seen per epoch
+++--final_tokens = 20 * warmup_tokens
+++--
+++--trainer_config = utils.Config(
+++--    utils.Trainer,
+++--    savepath=(args.savepath, 'trainer_config.pkl'),
+++--    # optimization parameters
+++--    batch_size=args.batch_size,
+++--    learning_rate=args.learning_rate,
+++--    betas=(0.9, 0.95),
+++--    grad_norm_clip=1.0,
+++--    weight_decay=0.1, # only applied on matmul weights
+++--    # learning rate decay: linear warmup followed by cosine decay to 10% of original
+++--    lr_decay=args.lr_decay,
+++--    warmup_tokens=warmup_tokens,
+++--    final_tokens=final_tokens,
+++--    ## dataloader
+++--    num_workers=0,
+++--    device=args.device,
+++--)
+++--
+++--trainer = trainer_config()
+++--
+++--#######################
+++--###### main loop ######
+++--#######################
+++--
+++--## scale number of epochs to keep number of updates constant
+++--n_epochs = int(1e6 / len(dataset) * args.n_epochs_ref)
+++--save_freq = int(n_epochs // args.n_saves)
+++--
+++--for epoch in range(n_epochs):
+++--    print(f'\nEpoch: {epoch} / {n_epochs} | {args.dataset} | {args.exp_name}')
+++--
+++--    trainer.train(model, dataset)
+++--
+++--    ## get greatest multiple of `save_freq` less than or equal to `save_epoch`
+++--    save_epoch = (epoch + 1) // save_freq * save_freq
+++--    statepath = os.path.join(args.savepath, f'state_{save_epoch}.pt')
+++--    print(f'Saving model to {statepath}')
+++--
+++--    ## save state to disk
+++--    state = model.state_dict()
+++--    torch.save(state, statepath)
+++-diff --git a/halfcheetah/scripts/xrl.py b/halfcheetah/scripts/xrl.py
+++-deleted file mode 100644
+++-index 134232a..0000000
+++---- a/halfcheetah/scripts/xrl.py
+++-+++ /dev/null
+++-@@ -1,372 +0,0 @@
+++--import json
+++--import pdb
+++--from os.path import join
++++-import matplotlib.pyplot as plt
++++-import d3rlpy
+++ -
+++ -import trajectory.utils as utils
+++ -import trajectory.datasets as datasets
+++@@ -852,112 +16928,21 @@ index 134232a..0000000
+++ -)
+++ -from trajectory.search.sampling import forward
+++ -
+++--import gym
+++--import d4rl # Import required to register environments, you may need to also import the submodule
+++--import numpy as np
+++--import d3rlpy
+++--import math as mt
+++--from sklearn.cluster import KMeans
+++--from sklearn import datasets as skdatasets
+++ -from sklearn.decomposition import PCA
+++--
+++ -from pyclustering.cluster.xmeans import xmeans
+++ -from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer
+++--
+++ -from scipy.stats import wasserstein_distance
++++-from moviepy.editor import VideoFileClip
+++ -
+++ -class Parser(utils.Parser):
+++--    dataset: str = 'halfcheetah-medium-expert-v2'
++++-    dataset: str = 'halfcheetah-medium-v2'
+++ -    config: str = 'config.offline'
+++ -
+++ -# utils
+++--    
+++--class XMeans:
+++--    def loglikelihood(self, r, rn, var, m, k):
+++--        l1 = - rn / 2.0 * mt.log(2 * mt.pi)
+++--        l2 = - rn * m / 2.0 * mt.log(var)
+++--        l3 = - (rn - k) / 2.0
+++--        l4 = rn * mt.log(rn)
+++--        l5 = - rn * mt.log(r)
+++ -
+++--        return l1 + l2 + l3 + l4 + l5
++++-def cluster_trajectories(trajectories, n_clusters=10):
++++-    """TODO"""
+++ -
+++--    def __init__(self, X, kmax = 20):
+++--        self.X = X
+++--        self.num = np.size(self.X, axis=0)
+++--        self.dim = np.size(X, axis=1)
+++--        self.KMax = kmax
+++--
+++--    def fit(self):
+++--        k = 1
+++--        X = self.X
+++--        M = self.dim
+++--        num = self.num
+++--
+++--        while(1):
+++--            ok = k
+++--
+++--            #Improve Params
+++--            kmeans = KMeans(n_clusters=k).fit(X)
+++--            labels = kmeans.labels_
+++--            m = kmeans.cluster_centers_
+++--
+++--            #Improve Structure
+++--            #Calculate BIC
+++--            p = M + 1
+++--
+++--            obic = np.zeros(k)
+++--
+++--            for i in range(k):
+++--                rn = np.size(np.where(labels == i))
+++--                var = np.sum((X[labels == i] - m[i])**2)/float(rn - 1)
+++--                obic[i] = self.loglikelihood(rn, rn, var, M, 1) - p/2.0*mt.log(rn)
+++--
+++--            #Split each cluster into two subclusters and calculate BIC of each splitted cluster
+++--            sk = 2 #The number of subclusters
+++--            nbic = np.zeros(k)
+++--            addk = 0
+++--
+++--            for i in range(k):
+++--                ci = X[labels == i]
+++--                r = np.size(np.where(labels == i))
+++--
+++--                kmeans = KMeans(n_clusters=sk).fit(ci)
+++--                ci_labels = kmeans.labels_
+++--                sm = kmeans.cluster_centers_
+++--
+++--                for l in range(sk):
+++--                    rn = np.size(np.where(ci_labels == l))
+++--                    var = np.sum((ci[ci_labels == l] - sm[l])**2)/float(rn - sk)
+++--                    nbic[i] += self.loglikelihood(r, rn, var, M, sk)
+++--
+++--                p = sk * (M + 1)
+++--                nbic[i] -= p/2.0*mt.log(r)
+++--
+++--                if obic[i] < nbic[i]:
+++--                    addk += 1
+++--
+++--            k += addk
+++--
+++--            if ok == k or k >= self.KMax:
+++--                break
+++--
+++--
+++--        #Calculate labels and centroids
+++--        kmeans = KMeans(n_clusters=k).fit(X)
+++--        self.labels = kmeans.labels_
+++--        self.k = k
+++--        self.m = kmeans.cluster_centers_
+++--
+++--
+++--def cluster_trajectories(trajectories):
+++--    xmeans_instance = XMeans(trajectories, kmax=10)
+++--    xmeans_instance.fit()
+++--
+++--    clusters = xmeans_instance.labels
+++--    return clusters
+++--
+++--def cluster_trajectories_2(trajectories):
+++ -    # Prepare initial centers - amount of initial centers defines amount of clusters from which X-Means will
+++ -    # start analysis.
+++ -    amount_initial_centers = 2
+++@@ -965,7 +16950,7 @@ index 134232a..0000000
+++ -    
+++ -    # Create instance of X-Means algorithm. The algorithm will start analysis from 2 clusters, the maximum
+++ -    # number of clusters that can be allocated is 10.
+++--    xmeans_instance = xmeans(trajectories, initial_centers, 10)
++++-    xmeans_instance = xmeans(trajectories, initial_centers, n_clusters)
+++ -    xmeans_instance.process()
+++ -    
+++ -    # Extract clustering results: clusters
+++@@ -978,19 +16963,27 @@ index 134232a..0000000
+++ -
+++ -    return idxs_per_cluster, np.array(clusters)
+++ - 
+++--# https://github.com/sascha-kirch/ML_Notebooks/blob/main/Softmax_Temperature.ipynb
+++ -def softmax(x, temp):
+++--    """Compute softmax values for each sets of scores in x."""
+++--    return np.exp(np.divide(x,temp)) / np.sum(np.exp(np.divide(x,temp)))
++++-    """TODO"""
++++-    max_x = np.max(x)
++++-    return np.exp(np.divide(x-max_x,temp)) / np.sum(np.exp(np.divide(x-max_x,temp)))
++++-
++++-def generate_data_embedding(trajectory_embeddings, temperature=10000):
++++-    """TODO"""
+++ -
+++--def generate_data_embedding(trajectory_embeddings, normalizing_factor=1, temperature=1):
+++--    embedding = np.sum(trajectory_embeddings, axis=0) / normalizing_factor
++++-    embedding = np.sum(trajectory_embeddings, axis=0)
+++ -    embedding = softmax(embedding, temperature)
++++-    
++++-
+++ -    return embedding
+++ -
+++ -def embed_trajectory(gpt, discretizer, observations, actions, rewards, preprocess_fn):
++++-    """TODO"""
++++-
+++ -    context = []
+++ -
++++-    output = []
++++-
+++ -    for i in range(len(observations)):
+++ -        observation = observations[i]
+++ -        action = actions[i]
+++@@ -998,253 +16991,396 @@ index 134232a..0000000
+++ -
+++ -        observation = preprocess_fn(observation)
+++ -
+++--        # print(observation)
+++ -        prefix = make_prefix(discretizer, context, observation, True)
+++--        # print("prefix", prefix.shape)
+++ -
+++ -        out = forward(gpt, prefix)
+++--        # print("out", out.shape)
++++-
++++-        if len(context) >= 9:
++++-            context.pop(0)
++++-            if len(output) == 0:
++++-                output = out.detach().numpy()[0]
++++-            else:
++++-                output = np.concatenate((output, out.detach().numpy()[0][217:]), axis=0)
++++-
+++ -        context = update_context(context, discretizer, observation, action, reward, len(observations))
+++--        # print("cotext", context)
+++--    
+++--    emb = []
+++--    for context_step in context:
+++--        emb.append(context_step.numpy())
+++--    emb = np.array(emb)
+++--    emb = np.mean(emb, axis=0)[0]
+++ -
++++-    emb = np.mean(output, axis=0)
+++ -    return emb
+++ -
++++-def create_complementary_dataset(dataset, idxs, trajectory_length=10, inverse=False):
++++-    """TODO"""
+++ -
+++--def create_complementary_dataset(dataset, idxs, trajectory_length=10):
+++ -    observations = []
+++ -    actions = []
+++ -    rewards = []
+++ -    terminals = []
+++--    for i in range(1000):
+++--        if i not in idxs:
+++--            observations += list(dataset.observations[1000*i:1000*i+trajectory_length])
+++--            actions += list(dataset.actions[1000*i:1000*i+trajectory_length])
+++--            rewards += list(dataset.rewards[1000*i:1000*i+trajectory_length])
+++--            terminals += list(dataset.terminals[1000*i:1000*i+trajectory_length])
++++-
++++-    n_trajs = int(1000000/trajectory_length)
++++-    for i in range(n_trajs):
++++-        condition = i not in idxs
++++-        if inverse: condition = not condition
++++-
++++-        if condition:
++++-            observations += list(dataset.observations[trajectory_length*i:trajectory_length*(i+1)])
++++-            actions += list(dataset.actions[trajectory_length*i:trajectory_length*(i+1)])
++++-            rewards += list(dataset.rewards[trajectory_length*i:trajectory_length*(i+1)])
++++-
++++-    terminals = np.tile([0]*(trajectory_length-1)+[1], int(len(observations)/trajectory_length))
+++ -
+++ -    new_dataset = d3rlpy.dataset.MDPDataset(
+++ -        observations=np.array(observations),
+++ -        actions=np.array(actions),
+++ -        rewards=np.array(rewards),
+++--        terminals=np.array(terminals)
++++-        terminals=np.array(terminals),
+++ -    )
+++ -    return new_dataset
+++ -    
+++ -
++++-def clusters_to_idxs(clusters):
++++-    idxs_per_cluster = []
++++-    for i in np.sort(np.unique(clusters)):
++++-        idxs_per_cluster.append(list(np.argwhere(clusters == i).flatten()))
++++-    
++++-    return idxs_per_cluster
+++ -
+++ -
+++ -def main():
+++--    # args = Parser().parse_args('plan')
+++--
+++--    #######################
+++--    ####### models ########
+++--    #######################
+++--
+++--
+++--
+++--
+++ -
+++--    # print(args.dataset)
++++-    args = Parser().parse_args('plan')
+++ -
+++--    # dataset = utils.load_from_config(args.logbase, args.dataset, args.gpt_loadpath,
+++--    #         'data_config.pkl')
+++ -
++++-    ### DATASET ###
+++ -
+++--    # gpt, gpt_epoch = utils.load_model(args.logbase, args.dataset, args.gpt_loadpath,
+++--    #         epoch=args.gpt_epoch, device=args.device)
+++--
+++--    # env = datasets.load_environment(args.dataset)
+++--
+++--    # discretizer = dataset.discretizer
+++--
+++--    # preprocess_fn = datasets.get_preprocess_fn(env.name)
+++--
+++--    # #######################
+++--    # ####### dataset #######
+++--    # #######################
++++-    dataset_d3, env = d3rlpy.datasets.get_dataset("halfcheetah-medium-v2")
+++ -
+++--    # # env = datasets.load_environment(args.dataset)
+++--    # discretizer = dataset.discretizer
+++--    # preprocess_fn = datasets.get_preprocess_fn(env.name)
++++-    ### IMPORTANT DEFINITIONS XRL SCRIPT ###
+++ -
+++--    # # dataset
+++--    dataset_d3, env = d3rlpy.datasets.get_dataset("halfcheetah-medium-v2")
++++-    load_embeddings = False
++++-    load_clusters = True
++++-    load_agents = True
++++-    generate_human_study = False
++++-    
++++-    seed = 4 
++++-    trajectory_length = 25 # 10 = max
++++-    n_clusters = 10
++++-    k = 3
++++-    temperature = 10000
++++-    logging_folder = f"results/v2_models_100k_{seed}"
++++-    training_steps = 100000
+++ -
+++--    # env = gym.make('halfcheetah-medium-v2')
+++--    # dataset_d4 = d4rl.qlearning_dataset(env)
++++-    d3rlpy.seed(seed)
+++ -
+++--    # # checks to see if d3rl & d4rl datasets are equal
+++--    # print(np.allclose(dataset_d3.actions[100], dataset_d4['actions'][100]))
++++-    if load_embeddings:
++++-        embeddings = np.load(f"{logging_folder}/embeddings.npy")
++++-    else:
++++-   
++++-        ### TRAJECTORY TRANSFORMER ###
++++-    
++++-        dataset = utils.load_from_config(args.logbase, args.dataset, args.gpt_loadpath,
++++-                'data_config.pkl')
++++-        gpt, gpt_epoch = utils.load_model(args.logbase, args.dataset, args.gpt_loadpath,
++++-                epoch=args.gpt_epoch, device=args.device)
++++-        env = datasets.load_environment(args.dataset)
++++-        discretizer = dataset.discretizer
++++-        preprocess_fn = datasets.get_preprocess_fn(env.name)
++++-    
++++-        ### TRAJECTORY EMBEDDINGS ###
++++-    
++++-        embeddings = []
++++-        n_trajs = int(1000000/trajectory_length)
++++-        for i in range(n_trajs):
++++-            observations = dataset_d3.observations[trajectory_length*i:trajectory_length*(i+1)]
++++-            actions = dataset_d3.actions[trajectory_length*i:trajectory_length*(i+1)]
++++-            rewards = dataset_d3.rewards[trajectory_length*i:trajectory_length*(i+1)]
++++-            terminals = dataset_d3.terminals[trajectory_length*i:trajectory_length*(i+1)]
++++-            emb = embed_trajectory(gpt, discretizer, observations, actions, rewards, preprocess_fn)
++++-            embeddings.append(emb)
++++-        embeddings = np.array(embeddings)
++++-        np.save(f"{logging_folder}/embeddings.npy", embeddings)
+++ -
+++--    # # dr4rl has same trajectories, just cut off 1 element before the end
+++--    # for j in range(1000):
+++--    #     for i in range(999):
+++--    #         if dataset_d4['rewards'][j * 999 + i] != dataset_d3.rewards[j * 1000 + i]: print("yo", i)
++++-    print("embeddings ready")
+++ -
+++--    # #######################
+++--    # ###### main loop ######
+++--    # #######################
++++-    ### TRAJECTORY CLUSTERS ###
+++ -
+++--    trajectory_length = 10 # 10 = max
++++-    if load_clusters:
++++-        clusters = np.load(f"{logging_folder}/clusters.npy")
++++-        idxs_per_cluster = clusters_to_idxs(clusters)
++++-    else:
++++-        idxs_per_cluster, clusters = cluster_trajectories(embeddings, n_clusters)
++++-        np.save(f"{logging_folder}/clusters.npy", clusters)
+++ -
+++--    # embeddings = []
+++--    # for i in range(1000):
+++--    #     observations = dataset_d3.observations[1000*i:1000*i+trajectory_length]
+++--    #     actions = dataset_d3.actions[1000*i:1000*i+trajectory_length]
+++--    #     rewards = dataset_d3.rewards[1000*i:1000*i+trajectory_length]
+++--    #     terminals = dataset_d3.terminals[1000*i:1000*i+trajectory_length]
+++--    #     emb = embed_trajectory(gpt, discretizer, observations, actions, rewards, preprocess_fn)
+++--    #     embeddings.append(emb)
+++--    # embeddings = np.array(embeddings)
+++--    # np.save("embeddings.npy", embeddings)
+++--    # print(embeddings)
++++-    print("clusters ready")
+++ -
+++--    embeddings = np.load("embeddings.npy")
++++-    ### PCA (solely for visualization) ###
++++- 
++++-    pca_idxs = np.random.choice(len(embeddings), 500, replace=False)
+++ -
+++ -    pca = PCA(n_components=2)
+++--    pca = PCA(n_components=2)
+++--    pca_embeddings = pca.fit_transform(embeddings)
+++--    np.save("pca.py", pca_embeddings)
++++-    pca_embeddings = pca.fit_transform(embeddings[pca_idxs])
++++-    pca_clusters = clusters[pca_idxs]
+++ -
+++--    idxs_per_cluster, clusters = cluster_trajectories_2(embeddings)
+++--    # print(clusters)
+++--    # return
+++--    np.save("clusters.npy", clusters)
++++-    print("pca ready")
+++ -
+++--    import matplotlib.pyplot as plt
++++-    ### COMPLEMENTARY DATASETS & CLUSTER EMBEDDINGS (also plotting PCA) ###
++++-    d_orig = generate_data_embedding(embeddings, temperature=temperature)
+++ -
+++--    d_orig = generate_data_embedding(embeddings)
+++ -    unique_clusters = np.unique(clusters)
+++--    
+++ -    d_j = []
+++ -    complementary_datasets = []
++++-    cluster_datasets = []
++++-    fig, ax = plt.subplots(figsize=(5,4))
+++ -    for j in np.sort(unique_clusters):
+++--        print(j)
+++--        d_j.append(generate_data_embedding(embeddings[clusters != j]))
+++--        plt.scatter(pca_embeddings[clusters == j][:,0], pca_embeddings[clusters == j][:,1], label=j)
++++-        d_j.append(generate_data_embedding(embeddings[clusters != j], temperature=temperature))
++++-        ax.scatter(pca_embeddings[pca_clusters == j][:,0], pca_embeddings[pca_clusters == j][:,1], label=j)
+++ -        complementary_datasets.append(create_complementary_dataset(dataset_d3, idxs_per_cluster[j], trajectory_length))
++++-        cluster_datasets.append(create_complementary_dataset(dataset_d3, idxs_per_cluster[j], trajectory_length, inverse=True))
+++ -    
+++ -    original_dataset = create_complementary_dataset(dataset_d3, [], trajectory_length)
+++ -
+++--    print(complementary_datasets, original_dataset)
++++-    ax.legend(title="$c_j$", bbox_to_anchor=(0.5, 1.2), loc="lower center", ncol=5)
++++-    ax.set_xlabel("feature 1")
++++-    ax.set_ylabel("feature 2")
++++-    ax.spines['top'].set_visible(False)
++++-    ax.spines['right'].set_visible(False)
++++-    plt.title("Trajectory Clustering HalfCheetah")
++++-    plt.tight_layout()
++++-
++++-    plt.savefig(f"{logging_folder}/pca.pdf")
++++-
++++-    print("complementary datasets ready")
+++ -
+++--    plt.legend()
+++--    plt.show()
++++-    ### AGENT TRAINING (original & complementary) ###
+++ -
+++ -    agent_orig = d3rlpy.algos.SAC(
+++ -        actor_learning_rate=3e-4,
+++ -        critic_learning_rate=3e-4,
+++ -        temp_learning_rate=3e-4,
+++--        batch_size=256)
++++-        batch_size=512)
+++ -
+++--    print(agent_orig)
+++--
+++--    training_steps = 1000
+++--
+++--    agent_orig.fit(original_dataset, n_steps=training_steps)
++++-    if load_agents:
++++-        agent_orig.build_with_dataset(original_dataset)
++++-        agent_orig.load_model(f"{logging_folder}/agent_orig.pt")
++++-    else:
++++-        agent_orig.fit(original_dataset, n_steps=training_steps)
++++-        agent_orig.save_model(f"{logging_folder}/agent_orig.pt")
+++ -
+++ -    agents_compl = []
+++ -
+++--    for dset in complementary_datasets:
++++-    for i in range(len(complementary_datasets)):
+++ -        agent = d3rlpy.algos.SAC(
+++ -            actor_learning_rate=3e-4,
+++ -            critic_learning_rate=3e-4,
+++ -            temp_learning_rate=3e-4,
+++--            batch_size=256)
+++--        agent.fit(dset, n_steps=training_steps)
++++-            batch_size=512)
++++-        if load_agents:
++++-            agent.build_with_dataset(complementary_datasets[i])
++++-            agent.load_model(f"{logging_folder}/agent_compl_{i}.pt")
++++-        else:
++++-            agent.fit(complementary_datasets[i], n_steps=training_steps)
++++-            agent.save_model(f"{logging_folder}/agent_compl_{i}.pt")
+++ -        agents_compl.append(agent)
+++ -
+++--    action_orig = agent_orig.predict(dataset_d3.observations[0])
++++-    print("agents ready")
+++ -
+++--    actions_compl = []
+++--    for agent in agents_compl:
+++--        actions_compl.append(agent.predict(dataset_d3.observations[0]))
+++--    
+++--    action_dists = []
+++--    for action in actions_compl:
+++--        action_dists.append(np.linalg.norm(action_orig-action))
++++-    ### OBSERVATION EXPLANATION (cluster assignment) ###
+++ -
+++--    k = 3
+++--    topk = np.argpartition(action_dists, -k)[-k:]
++++-    original_state = np.random.get_state()
++++-    np.random.seed(0)
++++-    idxs_to_explain = np.random.choice(range(len(dataset_d3.observations)), 1000, replace=False)
++++-    np.random.set_state(original_state)
+++ -
+++--    d_w = {}
+++--    for idx in topk:
+++--        d_w[idx] = wasserstein_distance(d_j[idx], d_orig)
++++-    observations_to_explain = [dataset_d3.observations[i] for i in idxs_to_explain] 
+++ -
+++--    cluster_assignment = min(d_w, key=d_w.get)
+++--    print("explanation assigned to cluster", cluster_assignment)
++++-    ISVE = []
++++-    ISVE_orig = 0.
++++-    LMAAVD = []
++++-    ACM = []
++++-    NWD = []
++++-    CAF = [0] * len(d_j)
+++ -
+++--    
+++--def assignment_test():
+++--    action_orig = np.random.rand(10)
+++--    d_orig = np.random.rand(5)
++++-    if generate_human_study:
++++-        ctr = 0
++++-        unrelated_idxs = [690, 1520, 3030, 6050, 7080, 8030]
++++-        if not os.path.isdir(f"{logging_folder}/mp4s"): os.mkdir(f"{logging_folder}/mp4s")
++++-        if not os.path.isdir(f"{logging_folder}/gifs"): os.mkdir(f"{logging_folder}/gifs")
+++ -
+++--    actions_compl = np.random.rand(6,10)
+++--    d_j = np.random.rand(6,5)
++++-    for observation_to_explain in observations_to_explain:
++++-        action_orig = agent_orig.predict([observation_to_explain])
+++ -
+++--    action_dists = []
+++--    for action in actions_compl:
+++--        action_dists.append(np.linalg.norm(action_orig-action))
++++-        actions_compl = []
++++-        for agent in agents_compl:
++++-            actions_compl.append(agent.predict([observation_to_explain]))
+++ -
+++--    print(action_dists)
++++-        action_dists = []
++++-        for action in actions_compl:
++++-            action_dists.append(np.linalg.norm(action_orig-action))
+++ -
+++--    k = 3
+++--    topk = np.argpartition(action_dists, -k)[-k:]
++++-        topk = np.argpartition(action_dists, -k)[-k:]
++++-
++++-        d_w = {}
++++-        for idx in topk:
++++-            d_w[idx] = wasserstein_distance(d_j[idx], d_orig)
++++-
++++-        cluster_assignment = min(d_w, key=d_w.get)
++++-
++++-        ### OBSERVATION EXPLANATION (representing cluster with 1 trajectory) ###
++++-
++++-        distances_to_obs = [np.linalg.norm(observation_to_explain-obs) for obs in cluster_datasets[cluster_assignment].observations]
++++-        trajectory_to_assign = np.floor(np.argmin(distances_to_obs) / trajectory_length)
++++-        assigned_trajectory = np.arange(trajectory_to_assign * trajectory_length, (trajectory_to_assign+1) * trajectory_length)
+++ -
+++--    print(topk)
++++-        ### OBSERVATION EXPLANATION (metrics) ###
+++ -
+++--    d_w = {}
+++--    for idx in topk:
+++--        d_w[idx] = wasserstein_distance(d_j[idx], d_orig)
++++-        # Initial State Value Estimate
++++- 
++++-        V_s = 0.
++++-        for _ in range(10):
++++-            sampled_action = agent_orig.sample_action([observation_to_explain])
++++-            Q_sa = agent_orig.predict_value([observation_to_explain], [sampled_action[0]])[0]
++++-            V_s += Q_sa
++++-        ISVE_orig += V_s/10
++++- 
++++-        new_ISVE = []
++++-        for agent in agents_compl:
++++-            V_s = 0.
++++-            for _ in range(10):
++++-                sampled_action = agent.sample_action([observation_to_explain])
++++-                Q_sa = agent.predict_value([observation_to_explain], [sampled_action[0]])[0]
++++-                V_s += Q_sa
++++-            new_ISVE.append(V_s/10)
++++-        ISVE.append(new_ISVE)
++++- 
++++-        # Local Mean Absolute Action-Value Difference
++++-        Q_orig = agent_orig.predict_value([observation_to_explain], [action_orig[0]])
++++-        Q_j = [agent_orig.predict_value([observation_to_explain], [ac[0]]) for ac in actions_compl]
++++-        LMAAVD.append(np.abs(np.array(Q_j) - Q_orig).flatten())
++++- 
++++-        # Action Contrast Measure
++++-        ACM.append(action_dists)
++++- 
++++-        # Normalized Wasserstein distance (between cluster embeddings)
++++-        wasser = np.array([wasserstein_distance(d, d_orig) for d in d_j])
++++-        NWD.append(list((wasser-np.min(wasser))/(np.max(wasser)-np.min(wasser))))
++++- 
++++-        # Cluster attribution frequency
++++-        CAF[cluster_assignment] += 1
++++- 
++++-        if generate_human_study:
++++-            ### RENDERING ###
++++-            if not os.path.isdir(f"{logging_folder}/gifs/question_{ctr}"): os.mkdir(f"{logging_folder}/gifs/question_{ctr}")
+++ -
+++--    print(d_w)
++++-            rollout = dataset_d3.observations[idxs_to_explain[ctr]-25:idxs_to_explain[ctr]]
++++-            print(idxs_to_explain[ctr])
++++-            print(rollout)
++++-            renderer = utils.make_renderer(args)
++++-            rollout_savepath = f"{logging_folder}/mp4s/question_{ctr}/traj_to_explain.mp4"
++++-            renderer.render_rollout(rollout_savepath, rollout, fps=10)
++++-            videoClip = VideoFileClip(f"{logging_folder}/mp4s/question_{ctr}/traj_to_explain.mp4")
++++-            videoClip.write_gif(f"{logging_folder}/gifs/question_{ctr}/traj_to_explain.gif")
+++ -
+++--    cluster_assignment = min(d_w, key=d_w.get)
+++--    print("explanation assigned to cluster", cluster_assignment)
++++-
++++-            rollout = cluster_datasets[cluster_assignment].observations[assigned_trajectory.astype(int)]
++++-            print(rollout)
++++-            renderer = utils.make_renderer(args)
++++-            rollout_savepath = f"{logging_folder}/mp4s/question_{ctr}/traj_assigned_cluster_attr.mp4"
++++-            renderer.render_rollout(rollout_savepath, rollout, fps=10)
++++-            videoClip = VideoFileClip(f"{logging_folder}/mp4s/question_{ctr}/traj_assigned_cluster_attr.mp4")
++++-            videoClip.write_gif(f"{logging_folder}/gifs/question_{ctr}/traj_assigned_cluster_attr.gif")
++++-
++++-            random_trajs = np.random.randint(len(cluster_datasets[cluster_assignment])//25, size=3)
++++-
++++-            rollout = cluster_datasets[cluster_assignment].observations[random_trajs[0]*25:(random_trajs[0]+1)*25]
++++-            print(rollout)
++++-            renderer = utils.make_renderer(args)
++++-            rollout_savepath = f"{logging_folder}/mp4s/question_{ctr}/traj_assigned_cluster_1.mp4"
++++-            renderer.render_rollout(rollout_savepath, rollout, fps=10)
++++-            videoClip = VideoFileClip(f"{logging_folder}/mp4s/question_{ctr}/traj_assigned_cluster_1.mp4")
++++-            videoClip.write_gif(f"{logging_folder}/gifs/question_{ctr}/traj_assigned_cluster_1.gif")
++++-
++++-            rollout = cluster_datasets[cluster_assignment].observations[random_trajs[1]*25:(random_trajs[1]+1)*25]
++++-            print(rollout)
++++-            renderer = utils.make_renderer(args)
++++-            rollout_savepath = f"{logging_folder}/mp4s/question_{ctr}/traj_assigned_cluster_2.mp4"
++++-            renderer.render_rollout(rollout_savepath, rollout, fps=10)
++++-            videoClip = VideoFileClip(f"{logging_folder}/mp4s/question_{ctr}/traj_assigned_cluster_2.mp4")
++++-            videoClip.write_gif(f"{logging_folder}/gifs/question_{ctr}/traj_assigned_cluster_2.gif")
++++-
++++-
++++-            rollout = cluster_datasets[cluster_assignment].observations[random_trajs[2]*25:(random_trajs[2]+1)*25]
++++-            print(rollout)
++++-            renderer = utils.make_renderer(args)
++++-            rollout_savepath = f"{logging_folder}/mp4s/question_{ctr}/traj_assigned_cluster_3.mp4"
++++-            renderer.render_rollout(rollout_savepath, rollout, fps=10)
++++-            videoClip = VideoFileClip(f"{logging_folder}/mp4s/question_{ctr}/traj_assigned_cluster_3.mp4")
++++-            videoClip.write_gif(f"{logging_folder}/gifs/question_{ctr}/traj_assigned_cluster_3.gif")
++++-
++++-
++++-            different_cluster = 0 if cluster_assignment != 0 else 1
++++-            random_trajs = np.random.randint(len(cluster_datasets[different_cluster])//25, size=3)
++++-            rollout = cluster_datasets[different_cluster].observations[random_trajs[2]*25:(random_trajs[2]+1)*25]
++++-            print(rollout)
++++-            renderer = utils.make_renderer(args)
++++-            rollout_savepath = f"{logging_folder}/mp4s/question_{ctr}/traj_different_cluster.mp4"
++++-            renderer.render_rollout(rollout_savepath, rollout, fps=10)
++++-            videoClip = VideoFileClip(f"{logging_folder}/mp4s/question_{ctr}/traj_different_cluster.mp4")
++++-            videoClip.write_gif(f"{logging_folder}/gifs/question_{ctr}/traj_different_cluster.gif")
++++-
++++-            rollout = dataset_d3.observations[unrelated_idxs[ctr]-25:unrelated_idxs[ctr]]
++++-            print(rollout)
++++-            renderer = utils.make_renderer(args)
++++-            rollout_savepath = f"{logging_folder}/mp4s/question_{ctr}/traj_unrelated.mp4"
++++-            renderer.render_rollout(rollout_savepath, rollout, fps=10)
++++-            videoClip = VideoFileClip(f"{logging_folder}/mp4s/question_{ctr}/traj_unrelated.mp4")
++++-            videoClip.write_gif(f"{logging_folder}/gifs/question_{ctr}/traj_unrelated.gif")
++++-
++++-            ctr += 1
++++-
++++-    ### RESULTS ###
++++-    ISVE_orig /= len(observations_to_explain)
++++-    ISVE = np.mean(ISVE, axis=0)
++++-    LMAAVD = np.mean(LMAAVD, axis=0)
++++-    ACM = np.mean(ACM, axis=0)
++++-    NWD = np.mean(NWD, axis=0)
++++-    CAF = np.array(CAF) / np.sum(CAF)
++++-
++++-    print("ISVE orig:", ISVE_orig)
++++-    print("ISVE:",ISVE)
++++-    print("LMAAVD:",LMAAVD)
++++-    print("ACM:",ACM)
++++-    print("NWD:",NWD)
++++-    print("CAF:",CAF)
+++ -
+++ -
+++ -if __name__ == "__main__":
+++--    # main()
+++--    assignment_test()
+++-diff --git a/halfcheetah/trajectory.egg-info/PKG-INFO b/halfcheetah/trajectory.egg-info/PKG-INFO
+++-index 452c6cb..2603850 100644
+++---- a/halfcheetah/trajectory.egg-info/PKG-INFO
+++-+++ b/halfcheetah/trajectory.egg-info/PKG-INFO
+++-@@ -1,4 +1,11 @@
+++- Metadata-Version: 2.1
+++- Name: trajectory
+++- Version: 0.0.0
+++-+Summary: UNKNOWN
+++-+Home-page: UNKNOWN
+++-+License: UNKNOWN
+++-+Platform: UNKNOWN
+++- License-File: LICENSE
+++-+
+++-+UNKNOWN
+++-+
++++-    main()
+++ diff --git a/halfcheetah/trajectory.egg-info/SOURCES.txt b/halfcheetah/trajectory.egg-info/SOURCES.txt
+++-index 4474d85..84e8e3a 100644
++++index 84e8e3a..4474d85 100644
+++ --- a/halfcheetah/trajectory.egg-info/SOURCES.txt
+++ +++ b/halfcheetah/trajectory.egg-info/SOURCES.txt
+++-@@ -30,4 +30,5 @@ trajectory/utils/serialization.py
++++@@ -30,5 +30,4 @@ trajectory/utils/serialization.py
+++  trajectory/utils/setup.py
+++  trajectory/utils/timer.py
+++  trajectory/utils/training.py
+++ -trajectory/utils/video.py
++++-trajectory_aaa/__init__.py
+++ \ No newline at end of file
+++ +trajectory/utils/video.py
+++-+trajectory_aaa/__init__.py
+++ \ No newline at end of file
+++ diff --git a/halfcheetah/trajectory.egg-info/top_level.txt b/halfcheetah/trajectory.egg-info/top_level.txt
+++-index ce65198..1d5271f 100644
++++index 1d5271f..ce65198 100644
+++ --- a/halfcheetah/trajectory.egg-info/top_level.txt
+++ +++ b/halfcheetah/trajectory.egg-info/top_level.txt
+++-@@ -1 +1,2 @@
++++@@ -1,2 +1 @@
+++  trajectory
+++-+trajectory_aaa
++ \ No newline at end of file
++-diff --git a/halfcheetah/pca.py.npy b/halfcheetah/pca.py.npy
++-deleted file mode 100644
++-index bb19150..0000000
++-Binary files a/halfcheetah/pca.py.npy and /dev/null differ
++-diff --git a/halfcheetah/plotting/bar.png b/halfcheetah/plotting/bar.png
++-deleted file mode 100644
++-index 3679667..0000000
++-Binary files a/halfcheetah/plotting/bar.png and /dev/null differ
++-diff --git a/halfcheetah/plotting/plot.py b/halfcheetah/plotting/plot.py
++-deleted file mode 100644
++-index 163d0e4..0000000
++---- a/halfcheetah/plotting/plot.py
++-+++ /dev/null
++-@@ -1,74 +0,0 @@
++--import numpy as np
++--import matplotlib
++--import matplotlib.pyplot as plt
++--import pdb
++--
++--from plotting.scores import means
++--
++--class Colors:
++--	grey = '#B4B4B4'
++--	gold = '#F6C781'
++--	red = '#EC7C7D'
++--	blue = '#70ABCC'
++--
++--LABELS = {
++--	# 'BC': 'Behavior\nCloning',
++--	# 'MBOP': 'Model-Based\nOffline Planning',
++--	# 'BRAC': 'Behavior-Reg.\nActor-Critic',
++--	# 'CQL': 'Conservative\nQ-Learning',
++--}
++--
++--def get_mean(results, exclude=None):
++--	'''
++--		results : { environment: score, ... }
++--	'''
++--	filtered = {
++--		k: v for k, v in results.items()
++--		if (not exclude) or (exclude and exclude not in k)
++--	}
++--	return np.mean(list(filtered.values()))
++--
++--if __name__ == '__main__':
++--
++--	#################
++--	## latex
++--	#################
++--	matplotlib.rc('font', **{'family': 'serif', 'serif': ['Computer Modern']})
++--	matplotlib.rc('text', usetex=True)
++--	matplotlib.rcParams['text.latex.preamble']=[r"\usepackage{amsmath}"]
++--	#################
++--
++--	fig = plt.gcf()
++--	ax = plt.gca()
++--	fig.set_size_inches(7.5, 2.5)
++--
++--	means = {k: get_mean(v, exclude='ant') for k, v in means.items()}
++--	print(means)
++--
++--	algs = ['BC', 'MBOP', 'BRAC', 'CQL', 'Decision\nTransformer', 'Trajectory\nTransformer']
++--	vals = [means[alg] for alg in algs]
++--
++--	colors = [
++--		Colors.grey, Colors.gold,
++--		Colors.red, Colors.red, Colors.blue, Colors.blue
++--	]
++--
++--	labels = [LABELS.get(alg, alg) for alg in algs]
++--	plt.bar(labels, vals, color=colors, edgecolor=Colors.gold, lw=0)
++--	plt.ylabel('Average normalized return', labelpad=15)
++--	# plt.title('Offline RL Results')
++--
++--	legend_labels = ['Behavior Cloning', 'Trajectory Optimization', 'Temporal Difference', 'Sequence Modeling']
++--	colors = [Colors.grey, Colors.gold, Colors.red, Colors.blue]
++--	handles = [plt.Rectangle((0,0),1,1, color=color) for label, color in zip(legend_labels, colors)]
++--	plt.legend(handles, legend_labels, ncol=4,
++--		bbox_to_anchor=(1.07, -.18), fancybox=False, framealpha=0, shadow=False, columnspacing=1.5, handlelength=1.5)
++--
++--	matplotlib.rcParams['hatch.linewidth'] = 7.5
++--	# ax.patches[-1].set_hatch('/')
++--
++--	ax.spines['right'].set_visible(False)
++--	ax.spines['top'].set_visible(False)
++--
++--	# plt.savefig('plotting/bar.pdf', bbox_inches='tight')
++--	plt.savefig('plotting/bar.png', bbox_inches='tight', dpi=500)
++-diff --git a/halfcheetah/plotting/read_results.py b/halfcheetah/plotting/read_results.py
++-deleted file mode 100644
++-index 5a5fb62..0000000
++---- a/halfcheetah/plotting/read_results.py
++-+++ /dev/null
++-@@ -1,70 +0,0 @@
++--import os
++--import glob
++--import numpy as np
++--import json
++--import pdb
++--
++--import trajectory.utils as utils
++--
++--DATASETS = [
++--	f'{env}-{buffer}'
++--	for env in ['hopper', 'walker2d', 'halfcheetah', 'ant']
++--	for buffer in ['medium-expert-v2', 'medium-v2', 'medium-replay-v2']
++--]
++--
++--LOGBASE = 'logs'
++--TRIAL = '*'
++--EXP_NAME = 'plans/pretrained'
++--
++--def load_results(paths):
++--	'''
++--		paths : path to directory containing experiment trials
++--	'''
++--	scores = []
++--	for i, path in enumerate(sorted(paths)):
++--		score = load_result(path)
++--		if score is None:
++--			print(f'Skipping {path}')
++--			continue
++--		scores.append(score)
++--
++--		suffix = path.split('/')[-1]
++--
++--	mean = np.mean(scores)
++--	err = np.std(scores) / np.sqrt(len(scores))
++--	return mean, err, scores
++--
++--def load_result(path):
++--	'''
++--		path : path to experiment directory; expects `rollout.json` to be in directory
++--	'''
++--	fullpath = os.path.join(path, 'rollout.json')
++--	suffix = path.split('/')[-1]
++--
++--	if not os.path.exists(fullpath):
++--		return None
++--
++--	results = json.load(open(fullpath, 'rb'))
++--	score = results['score']
++--	return score * 100
++--
++--#######################
++--######## setup ########
++--#######################
++--
++--if __name__ == '__main__':
++--
++--	class Parser(utils.Parser):
++--	    dataset: str = None
++--
++--	args = Parser().parse_args()
++--
++--	for dataset in ([args.dataset] if args.dataset else DATASETS):
++--		subdirs = glob.glob(os.path.join(LOGBASE, dataset, EXP_NAME))
++--
++--		for subdir in subdirs:
++--			reldir = subdir.split('/')[-1]
++--			paths = glob.glob(os.path.join(subdir, TRIAL))
++--
++--			mean, err, scores = load_results(paths)
++--			print(f'{dataset.ljust(30)} | {subdir.ljust(50)} | {len(scores)} scores \n    {mean:.2f} +/- {err:.2f}\n')
++-diff --git a/halfcheetah/plotting/scores.py b/halfcheetah/plotting/scores.py
++-deleted file mode 100644
++-index f1917f7..0000000
++---- a/halfcheetah/plotting/scores.py
++-+++ /dev/null
++-@@ -1,123 +0,0 @@
++--means = {
++--	'Trajectory\nTransformer': {
++--		##
++--		'halfcheetah-medium-expert-v2': 95.0,
++--		'hopper-medium-expert-v2': 110.0,
++--		'walker2d-medium-expert-v2': 101.9,
++--		'ant-medium-expert-v2': 116.1,
++--		##
++--		'halfcheetah-medium-v2': 46.9,
++--		'hopper-medium-v2': 61.1,
++--		'walker2d-medium-v2': 79.0,
++--		'ant-medium-v2': 83.1,
++--		##
++--		'halfcheetah-medium-replay-v2': 41.9,
++--		'hopper-medium-replay-v2': 91.5,
++--		'walker2d-medium-replay-v2': 82.6,
++--		'ant-medium-replay-v2': 77.0,
++--	},
++--	'Decision\nTransformer': {
++--		##
++--		'halfcheetah-medium-expert-v2': 86.8,
++--		'hopper-medium-expert-v2': 107.6,
++--		'walker2d-medium-expert-v2': 108.1,
++--		##
++--		'halfcheetah-medium-v2': 42.6,
++--		'hopper-medium-v2': 67.6,
++--		'walker2d-medium-v2': 74.0,
++--		##
++--		'halfcheetah-medium-replay-v2': 36.6,
++--		'hopper-medium-replay-v2': 82.7,
++--		'walker2d-medium-replay-v2': 66.6,
++--	},
++--	'CQL': {
++--		##
++--		'halfcheetah-medium-expert-v2': 91.6,
++--		'hopper-medium-expert-v2': 105.4,
++--		'walker2d-medium-expert-v2': 108.8,
++--		##
++--		'halfcheetah-medium-v2': 44.0,
++--		'hopper-medium-v2': 58.5,
++--		'walker2d-medium-v2': 72.5,
++--		##
++--		'halfcheetah-medium-replay-v2': 45.5,
++--		'hopper-medium-replay-v2': 95.0,
++--		'walker2d-medium-replay-v2': 77.2,
++--	},
++--	'MOPO': {
++--		##
++--		'halfcheetah-medium-expert-v2': 63.3,
++--		'hopper-medium-expert-v2': 23.7,
++--		'walker2d-medium-expert-v2': 44.6,
++--		##
++--		'halfcheetah-medium-v2': 42.3,
++--		'hopper-medium-v2': 28.0,
++--		'walker2d-medium-v2': 17.8,
++--		##
++--		'halfcheetah-medium-replay-v2': 53.1,
++--		'hopper-medium-replay-v2': 67.5,
++--		'walker2d-medium-replay-v2':39.0,
++--	},
++--	'MBOP': {
++--		##
++--		'halfcheetah-medium-expert-v2': 105.9,
++--		'hopper-medium-expert-v2': 55.1,
++--		'walker2d-medium-expert-v2': 70.2,
++--		##
++--		'halfcheetah-medium-v2': 44.6,
++--		'hopper-medium-v2': 48.8,
++--		'walker2d-medium-v2': 41.0,
++--		##
++--		'halfcheetah-medium-replay-v2': 42.3,
++--		'hopper-medium-replay-v2': 12.4,
++--		'walker2d-medium-replay-v2': 9.7,
++--	},
++--	'BRAC': {
++--		##
++--		'halfcheetah-medium-expert-v2': 41.9,
++--		'hopper-medium-expert-v2': 0.9,
++--		'walker2d-medium-expert-v2': 81.6,
++--		##
++--		'halfcheetah-medium-v2': 46.3,
++--		'hopper-medium-v2': 31.3,
++--		'walker2d-medium-v2': 81.1,
++--		##
++--		'halfcheetah-medium-replay-v2': 47.7,
++--		'hopper-medium-replay-v2': 0.6,
++--		'walker2d-medium-replay-v2': 0.9,
++--	},
++--	'BC': {
++--		##
++--		'halfcheetah-medium-expert-v2': 59.9,
++--		'hopper-medium-expert-v2': 79.6,
++--		'walker2d-medium-expert-v2': 36.6,
++--		##
++--		'halfcheetah-medium-v2': 43.1,
++--		'hopper-medium-v2': 63.9,
++--		'walker2d-medium-v2': 77.3,
++--		##
++--		'halfcheetah-medium-replay-v2': 4.3,
++--		'hopper-medium-replay-v2': 27.6,
++--		'walker2d-medium-replay-v2': 36.9,
++--	},
++--}
++--
++--errors = {
++--	'Trajectory\nTransformer': {
++--		##
++--		'halfcheetah-medium-expert-v2': 0.2,
++--		'hopper-medium-expert-v2': 2.7,
++--		'walker2d-medium-expert-v2': 6.8,
++--		'ant-medium-expert-v2': 9.0,
++--		##
++--		'halfcheetah-medium-v2': 0.4,
++--		'hopper-medium-v2': 3.6,
++--		'walker2d-medium-v2': 2.8,
++--		'ant-medium-v2': 7.3,
++--		##
++--		'halfcheetah-medium-replay-v2': 2.5,
++--		'hopper-medium-replay-v2': 3.6,
++--		'walker2d-medium-replay-v2': 6.9,
++--		'ant-medium-replay-v2': 6.8,
++--	},
++--}
++++-trajectory_aaa
++++diff --git a/seaquest/readme.md b/seaquest/readme.md
++++index 84e53f8..53561f9 100644
++++--- a/seaquest/readme.md
+++++++ b/seaquest/readme.md
++++@@ -10,4 +10,4 @@ pip install git+https://github.com/takuseno/d4rl-atari
++++ pip install "gym[atari, accept-rom-license]"
++++ pip install pyclustering
++++ pip install seaborn
++++-pip install d3rlpy==1.1.1
++++\ No newline at end of file
+++++pip install d3rlpy==1.1.1
++ \ No newline at end of file
++-diff --git a/halfcheetah/plotting/table.py b/halfcheetah/plotting/table.py
+++diff --git a/halfcheetah/scripts/xrl_v2.py b/halfcheetah/scripts/xrl_v2.py
++ deleted file mode 100644
++-index eae74e6..0000000
++---- a/halfcheetah/plotting/table.py
+++index 62a3d4d..0000000
+++--- a/halfcheetah/scripts/xrl_v2.py
++ +++ /dev/null
++-@@ -1,127 +0,0 @@
++--import numpy as np
++--import pdb
++--
++--from plotting.plot import get_mean
++--from plotting.scores import (
++--	means as MEANS,
++--	errors as ERRORS,
++--)
++--
++--ALGORITHM_STRINGS = {
++--	'Trajectory\nTransformer': 'TT (Ours)',
++--	'Decision\nTransformer': 'DT',	
++--}
++--
++--BUFFER_STRINGS = {
++--	'medium-expert': 'Medium-Expert',
++--	'medium': 'Medium',
++--	'medium-replay': 'Medium-Replay',	
++--}
++--
++--ENVIRONMENT_STRINGS = {
++--	'halfcheetah': 'HalfCheetah',
++--	'hopper': 'Hopper',
++--	'walker2d': 'Walker2d',
++--	'ant': 'Ant',
++--}
++--
++--SHOW_ERRORS = ['Trajectory\nTransformer']
++--
++--def get_result(algorithm, buffer, environment, version='v2'):
++--	key = f'{environment}-{buffer}-{version}'
++--	mean = MEANS[algorithm].get(key, '-')
++--	if algorithm in SHOW_ERRORS:
++--		error = ERRORS[algorithm].get(key)
++--		return (mean, error)
++--	else:
++--		return mean
++--
++--def format_result(result):
++--	if type(result) == tuple:
++--		mean, std = result
++--		return f'${mean}$ \\scriptsize{{\\raisebox{{1pt}}{{$\\pm {std}$}}}}'
++--	else:
++--		return f'${result}$'
++--
++--def format_row(buffer, environment, results):
++--	buffer_str = BUFFER_STRINGS[buffer]
++--	environment_str = ENVIRONMENT_STRINGS[environment]
++--	results_str = ' & '.join(format_result(result) for result in results)
++--	row = f'{buffer_str} & {environment_str} & {results_str} \\\\ \n'
++--	return row
++--
++--def format_buffer_block(algorithms, buffer, environments):
++--	block_str = '\\midrule\n'
++--	for environment in environments:
++--		results = [get_result(alg, buffer, environment) for alg in algorithms]
++--		row_str = format_row(buffer, environment, results)
++--		block_str += row_str
++--	return block_str
++--
++--def format_algorithm(algorithm):
++--	algorithm_str = ALGORITHM_STRINGS.get(algorithm, algorithm)
++--	return f'\multicolumn{{1}}{{c}}{{\\bf {algorithm_str}}}'
++--
++--def format_algorithms(algorithms):
++--	return ' & '.join(format_algorithm(algorithm) for algorithm in algorithms)
++--
++--def format_averages(means, label):
++--	prefix = f'\\multicolumn{{2}}{{c}}{{\\bf Average ({label})}} & '
++--	formatted = ' & '.join(str(mean) for mean in means)
++--	return prefix + formatted
++--
++--def format_averages_block(algorithms):
++--	means_filtered = [np.round(get_mean(MEANS[algorithm], exclude='ant'), 1) for algorithm in algorithms]
++--	means_all = [np.round(get_mean(MEANS[algorithm], exclude=None), 1) for algorithm in algorithms]
++--
++--	means_all = [
++--		means
++--		if 'ant-medium-expert-v2' in MEANS[algorithm]
++--		else '$-$'
++--		for algorithm, means in zip(algorithms, means_all)
++--	]
++--
++--	formatted_filtered = format_averages(means_filtered, 'without Ant')
++--	formatted_all = format_averages(means_all, 'all settings')
++--
++--	formatted_block = (
++--		f'{formatted_filtered} \\hspace{{.6cm}} \\\\ \n'
++--		f'{formatted_all} \\hspace{{.6cm}} \\\\ \n'
++--	)
++--	return formatted_block
++--
++--def format_table(algorithms, buffers, environments):
++--	justify_str = 'll' + 'r' * len(algorithms)
++--	algorithm_str = format_algorithms(['Dataset', 'Environment'] + algorithms)
++--	averages_str = format_averages_block(algorithms)
++--	table_prefix = (
++--		'\\begin{table*}[h]\n'
++--		'\\centering\n'
++--		'\\small\n'
++--		f'\\begin{{tabular}}{{{justify_str}}}\n'
++--		'\\toprule\n'
++--		f'{algorithm_str} \\\\ \n'
++--	)
++--	table_suffix = (
++--		'\\midrule\n'
++--		f'{averages_str}'
++--		'\\bottomrule\n'
++--		'\\end{tabular}\n'
++--		'\\label{table:d4rl}\n'
++--		'\\end{table*}'
++--	)
++--	blocks = ''.join(format_buffer_block(algorithms, buffer, environments) for buffer in buffers)
++--	table = (
++--		f'{table_prefix}'
++--		f'{blocks}'
++--		f'{table_suffix}'
++--	)
++--	return table
++--
++--
++--algorithms =['BC', 'MBOP', 'BRAC', 'CQL',  'Decision\nTransformer', 'Trajectory\nTransformer']
++--buffers = ['medium-expert', 'medium', 'medium-replay']
++--environments = ['halfcheetah', 'hopper', 'walker2d', 'ant']
++--
++--table = format_table(algorithms, buffers, environments)
++--print(table)
++-diff --git a/halfcheetah/scripts/plan.py b/halfcheetah/scripts/plan.py
++-deleted file mode 100644
++-index f13d4cc..0000000
++---- a/halfcheetah/scripts/plan.py
++-+++ /dev/null
++-@@ -1,124 +0,0 @@
++--import json
++--import pdb
++--from os.path import join
++--
++--import trajectory.utils as utils
++--import trajectory.datasets as datasets
++--from trajectory.search import (
++--    beam_plan,
++--    make_prefix,
++--    extract_actions,
++--    update_context,
++--)
++--
++--class Parser(utils.Parser):
++--    dataset: str = 'halfcheetah-medium-expert-v2'
++--    config: str = 'config.offline'
++--
++--#######################
++--######## setup ########
++--#######################
++--
++--args = Parser().parse_args('plan')
++--
++--#######################
++--####### models ########
++--#######################
++--
++--dataset = utils.load_from_config(args.logbase, args.dataset, args.gpt_loadpath,
++--        'data_config.pkl')
++--
++--gpt, gpt_epoch = utils.load_model(args.logbase, args.dataset, args.gpt_loadpath,
++--        epoch=args.gpt_epoch, device=args.device)
++--
++--#######################
++--####### dataset #######
++--#######################
++--
++--env = datasets.load_environment(args.dataset)
++--print('yo')
++--renderer = utils.make_renderer(args)
++--timer = utils.timer.Timer()
++--
++--discretizer = dataset.discretizer
++--discount = dataset.discount
++--observation_dim = dataset.observation_dim
++--action_dim = dataset.action_dim
++--
++--value_fn = lambda x: discretizer.value_fn(x, args.percentile)
++--preprocess_fn = datasets.get_preprocess_fn(env.name)
++--
++--print('yo2')
++--
++--#######################
++--###### main loop ######
++--#######################
++--
++--observation = env.reset()
++--total_reward = 0
++--
++--## observations for rendering
++--rollout = [observation.copy()]
+++@@ -1,442 +0,0 @@
+++-"""
+++-TODO HEADER
+++-"""
++ -
++--## previous (tokenized) transitions for conditioning transformer
++--context = []
++--
++--T = env.max_episode_steps
++--for t in range(T):
++--
++--    observation = preprocess_fn(observation)
++--
++--    if t % args.plan_freq == 0:
++--        ## concatenate previous transitions and current observations to input to model
++--        prefix = make_prefix(discretizer, context, observation, args.prefix_context)
++--
++--        ## sample sequence from model beginning with `prefix`
++--        sequence = beam_plan(
++--            gpt, value_fn, prefix,
++--            args.horizon, args.beam_width, args.n_expand, observation_dim, action_dim,
++--            discount, args.max_context_transitions, verbose=args.verbose,
++--            k_obs=args.k_obs, k_act=args.k_act, cdf_obs=args.cdf_obs, cdf_act=args.cdf_act,
++--        )
++--
++--    else:
++--        sequence = sequence[1:]
++--
++--    ## [ horizon x transition_dim ] convert sampled tokens to continuous trajectory
++--    sequence_recon = discretizer.reconstruct(sequence)
++--
++--    ## [ action_dim ] index into sampled trajectory to grab first action
++--    action = extract_actions(sequence_recon, observation_dim, action_dim, t=0)
++--
++--    ## execute action in environment
++--    next_observation, reward, terminal, _ = env.step(action)
++--
++--    ## update return
++--    total_reward += reward
++--    score = env.get_normalized_score(total_reward)
++--
++--    ## update rollout observations and context transitions
++--    rollout.append(next_observation.copy())
++--    context = update_context(context, discretizer, observation, action, reward, args.max_context_transitions)
++--
++--    print(
++--        f'[ plan ] t: {t} / {T} | r: {reward:.2f} | R: {total_reward:.2f} | score: {score:.4f} | '
++--        f'time: {timer():.2f} | {args.dataset} | {args.exp_name} | {args.suffix}\n'
++--    )
++--
++--    ## visualization
++--    if t % args.vis_freq == 0 or terminal or t == T:
++--
++--        ## save current plan
++--        renderer.render_plan(join(args.savepath, f'{t}_plan.mp4'), sequence_recon, env.state_vector())
++--
++--        ## save rollout thus far
++--        renderer.render_rollout(join(args.savepath, f'rollout.mp4'), rollout, fps=80)
++--
++--    if terminal: break
++--
++--    observation = next_observation
++--
++--## save result as a json file
++--json_path = join(args.savepath, 'rollout.json')
++--json_data = {'score': score, 'step': t, 'return': total_reward, 'term': terminal, 'gpt_epoch': gpt_epoch}
++--json.dump(json_data, open(json_path, 'w'), indent=2, sort_keys=True)
++-diff --git a/halfcheetah/scripts/train.py b/halfcheetah/scripts/train.py
++-deleted file mode 100644
++-index 04af8d7..0000000
++---- a/halfcheetah/scripts/train.py
++-+++ /dev/null
++-@@ -1,122 +0,0 @@
++ -import os
++ -import numpy as np
++--import torch
++--import pdb
++--
++--import trajectory.utils as utils
++--import trajectory.datasets as datasets
++--from trajectory.models.transformers import GPT
++--
++--
++--class Parser(utils.Parser):
++--    dataset: str = 'halfcheetah-medium-expert-v2'
++--    config: str = 'config.offline'
++--
++--#######################
++--######## setup ########
++--#######################
++--
++--args = Parser().parse_args('train')
++--
++--#######################
++--####### dataset #######
++--#######################
++--
++--env = datasets.load_environment(args.dataset)
++--
++--sequence_length = args.subsampled_sequence_length * args.step
++--
++--dataset_config = utils.Config(
++--    datasets.DiscretizedDataset,
++--    savepath=(args.savepath, 'data_config.pkl'),
++--    env=args.dataset,
++--    N=args.N,
++--    penalty=args.termination_penalty,
++--    sequence_length=sequence_length,
++--    step=args.step,
++--    discount=args.discount,
++--    discretizer=args.discretizer,
++--)
++--
++--dataset = dataset_config()
++--obs_dim = dataset.observation_dim
++--act_dim = dataset.action_dim
++--transition_dim = dataset.joined_dim
++--
++--#######################
++--######## model ########
++--#######################
++--
++--block_size = args.subsampled_sequence_length * transition_dim - 1
++--print(
++--    f'Dataset size: {len(dataset)} | '
++--    f'Joined dim: {transition_dim} '
++--    f'(observation: {obs_dim}, action: {act_dim}) | Block size: {block_size}'
++--)
++--
++--model_config = utils.Config(
++--    GPT,
++--    savepath=(args.savepath, 'model_config.pkl'),
++--    ## discretization
++--    vocab_size=args.N, block_size=block_size,
++--    ## architecture
++--    n_layer=args.n_layer, n_head=args.n_head, n_embd=args.n_embd*args.n_head,
++--    ## dimensions
++--    observation_dim=obs_dim, action_dim=act_dim, transition_dim=transition_dim,
++--    ## loss weighting
++--    action_weight=args.action_weight, reward_weight=args.reward_weight, value_weight=args.value_weight,
++--    ## dropout probabilities
++--    embd_pdrop=args.embd_pdrop, resid_pdrop=args.resid_pdrop, attn_pdrop=args.attn_pdrop,
++--)
++--
++--model = model_config()
++--model.to(args.device)
++--
++--#######################
++--####### trainer #######
++--#######################
++--
++--warmup_tokens = len(dataset) * block_size ## number of tokens seen per epoch
++--final_tokens = 20 * warmup_tokens
++--
++--trainer_config = utils.Config(
++--    utils.Trainer,
++--    savepath=(args.savepath, 'trainer_config.pkl'),
++--    # optimization parameters
++--    batch_size=args.batch_size,
++--    learning_rate=args.learning_rate,
++--    betas=(0.9, 0.95),
++--    grad_norm_clip=1.0,
++--    weight_decay=0.1, # only applied on matmul weights
++--    # learning rate decay: linear warmup followed by cosine decay to 10% of original
++--    lr_decay=args.lr_decay,
++--    warmup_tokens=warmup_tokens,
++--    final_tokens=final_tokens,
++--    ## dataloader
++--    num_workers=0,
++--    device=args.device,
++--)
++--
++--trainer = trainer_config()
++--
++--#######################
++--###### main loop ######
++--#######################
++--
++--## scale number of epochs to keep number of updates constant
++--n_epochs = int(1e6 / len(dataset) * args.n_epochs_ref)
++--save_freq = int(n_epochs // args.n_saves)
++--
++--for epoch in range(n_epochs):
++--    print(f'\nEpoch: {epoch} / {n_epochs} | {args.dataset} | {args.exp_name}')
++--
++--    trainer.train(model, dataset)
++--
++--    ## get greatest multiple of `save_freq` less than or equal to `save_epoch`
++--    save_epoch = (epoch + 1) // save_freq * save_freq
++--    statepath = os.path.join(args.savepath, f'state_{save_epoch}.pt')
++--    print(f'Saving model to {statepath}')
++--
++--    ## save state to disk
++--    state = model.state_dict()
++--    torch.save(state, statepath)
++-diff --git a/halfcheetah/scripts/xrl.py b/halfcheetah/scripts/xrl.py
++-deleted file mode 100644
++-index 134232a..0000000
++---- a/halfcheetah/scripts/xrl.py
++-+++ /dev/null
++-@@ -1,372 +0,0 @@
++--import json
++--import pdb
++--from os.path import join
+++-import matplotlib.pyplot as plt
+++-import d3rlpy
++ -
++ -import trajectory.utils as utils
++ -import trajectory.datasets as datasets
++@@ -852,112 +18603,21 @@ index 134232a..0000000
++ -)
++ -from trajectory.search.sampling import forward
++ -
++--import gym
++--import d4rl # Import required to register environments, you may need to also import the submodule
++--import numpy as np
++--import d3rlpy
++--import math as mt
++--from sklearn.cluster import KMeans
++--from sklearn import datasets as skdatasets
++ -from sklearn.decomposition import PCA
++--
++ -from pyclustering.cluster.xmeans import xmeans
++ -from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer
++--
++ -from scipy.stats import wasserstein_distance
+++-from moviepy.editor import VideoFileClip
++ -
++ -class Parser(utils.Parser):
++--    dataset: str = 'halfcheetah-medium-expert-v2'
+++-    dataset: str = 'halfcheetah-medium-v2'
++ -    config: str = 'config.offline'
++ -
++ -# utils
++--    
++--class XMeans:
++--    def loglikelihood(self, r, rn, var, m, k):
++--        l1 = - rn / 2.0 * mt.log(2 * mt.pi)
++--        l2 = - rn * m / 2.0 * mt.log(var)
++--        l3 = - (rn - k) / 2.0
++--        l4 = rn * mt.log(rn)
++--        l5 = - rn * mt.log(r)
++ -
++--        return l1 + l2 + l3 + l4 + l5
+++-def cluster_trajectories(trajectories, n_clusters=10):
+++-    """TODO"""
++ -
++--    def __init__(self, X, kmax = 20):
++--        self.X = X
++--        self.num = np.size(self.X, axis=0)
++--        self.dim = np.size(X, axis=1)
++--        self.KMax = kmax
++--
++--    def fit(self):
++--        k = 1
++--        X = self.X
++--        M = self.dim
++--        num = self.num
++--
++--        while(1):
++--            ok = k
++--
++--            #Improve Params
++--            kmeans = KMeans(n_clusters=k).fit(X)
++--            labels = kmeans.labels_
++--            m = kmeans.cluster_centers_
++--
++--            #Improve Structure
++--            #Calculate BIC
++--            p = M + 1
++--
++--            obic = np.zeros(k)
++--
++--            for i in range(k):
++--                rn = np.size(np.where(labels == i))
++--                var = np.sum((X[labels == i] - m[i])**2)/float(rn - 1)
++--                obic[i] = self.loglikelihood(rn, rn, var, M, 1) - p/2.0*mt.log(rn)
++--
++--            #Split each cluster into two subclusters and calculate BIC of each splitted cluster
++--            sk = 2 #The number of subclusters
++--            nbic = np.zeros(k)
++--            addk = 0
++--
++--            for i in range(k):
++--                ci = X[labels == i]
++--                r = np.size(np.where(labels == i))
++--
++--                kmeans = KMeans(n_clusters=sk).fit(ci)
++--                ci_labels = kmeans.labels_
++--                sm = kmeans.cluster_centers_
++--
++--                for l in range(sk):
++--                    rn = np.size(np.where(ci_labels == l))
++--                    var = np.sum((ci[ci_labels == l] - sm[l])**2)/float(rn - sk)
++--                    nbic[i] += self.loglikelihood(r, rn, var, M, sk)
++--
++--                p = sk * (M + 1)
++--                nbic[i] -= p/2.0*mt.log(r)
++--
++--                if obic[i] < nbic[i]:
++--                    addk += 1
++--
++--            k += addk
++--
++--            if ok == k or k >= self.KMax:
++--                break
++--
++--
++--        #Calculate labels and centroids
++--        kmeans = KMeans(n_clusters=k).fit(X)
++--        self.labels = kmeans.labels_
++--        self.k = k
++--        self.m = kmeans.cluster_centers_
++--
++--
++--def cluster_trajectories(trajectories):
++--    xmeans_instance = XMeans(trajectories, kmax=10)
++--    xmeans_instance.fit()
++--
++--    clusters = xmeans_instance.labels
++--    return clusters
++--
++--def cluster_trajectories_2(trajectories):
++ -    # Prepare initial centers - amount of initial centers defines amount of clusters from which X-Means will
++ -    # start analysis.
++ -    amount_initial_centers = 2
++@@ -965,7 +18625,7 @@ index 134232a..0000000
++ -    
++ -    # Create instance of X-Means algorithm. The algorithm will start analysis from 2 clusters, the maximum
++ -    # number of clusters that can be allocated is 10.
++--    xmeans_instance = xmeans(trajectories, initial_centers, 10)
+++-    xmeans_instance = xmeans(trajectories, initial_centers, n_clusters)
++ -    xmeans_instance.process()
++ -    
++ -    # Extract clustering results: clusters
++@@ -978,19 +18638,27 @@ index 134232a..0000000
++ -
++ -    return idxs_per_cluster, np.array(clusters)
++ - 
++--# https://github.com/sascha-kirch/ML_Notebooks/blob/main/Softmax_Temperature.ipynb
++ -def softmax(x, temp):
++--    """Compute softmax values for each sets of scores in x."""
++--    return np.exp(np.divide(x,temp)) / np.sum(np.exp(np.divide(x,temp)))
+++-    """TODO"""
+++-    max_x = np.max(x)
+++-    return np.exp(np.divide(x-max_x,temp)) / np.sum(np.exp(np.divide(x-max_x,temp)))
+++-
+++-def generate_data_embedding(trajectory_embeddings, temperature=10000):
+++-    """TODO"""
++ -
++--def generate_data_embedding(trajectory_embeddings, normalizing_factor=1, temperature=1):
++--    embedding = np.sum(trajectory_embeddings, axis=0) / normalizing_factor
+++-    embedding = np.sum(trajectory_embeddings, axis=0)
++ -    embedding = softmax(embedding, temperature)
+++-    
+++-
++ -    return embedding
++ -
++ -def embed_trajectory(gpt, discretizer, observations, actions, rewards, preprocess_fn):
+++-    """TODO"""
+++-
++ -    context = []
++ -
+++-    output = []
+++-
++ -    for i in range(len(observations)):
++ -        observation = observations[i]
++ -        action = actions[i]
++@@ -998,253 +18666,396 @@ index 134232a..0000000
++ -
++ -        observation = preprocess_fn(observation)
++ -
++--        # print(observation)
++ -        prefix = make_prefix(discretizer, context, observation, True)
++--        # print("prefix", prefix.shape)
++ -
++ -        out = forward(gpt, prefix)
++--        # print("out", out.shape)
+++-
+++-        if len(context) >= 9:
+++-            context.pop(0)
+++-            if len(output) == 0:
+++-                output = out.detach().numpy()[0]
+++-            else:
+++-                output = np.concatenate((output, out.detach().numpy()[0][217:]), axis=0)
+++-
++ -        context = update_context(context, discretizer, observation, action, reward, len(observations))
++--        # print("cotext", context)
++--    
++--    emb = []
++--    for context_step in context:
++--        emb.append(context_step.numpy())
++--    emb = np.array(emb)
++--    emb = np.mean(emb, axis=0)[0]
++ -
+++-    emb = np.mean(output, axis=0)
++ -    return emb
++ -
+++-def create_complementary_dataset(dataset, idxs, trajectory_length=10, inverse=False):
+++-    """TODO"""
++ -
++--def create_complementary_dataset(dataset, idxs, trajectory_length=10):
++ -    observations = []
++ -    actions = []
++ -    rewards = []
++ -    terminals = []
++--    for i in range(1000):
++--        if i not in idxs:
++--            observations += list(dataset.observations[1000*i:1000*i+trajectory_length])
++--            actions += list(dataset.actions[1000*i:1000*i+trajectory_length])
++--            rewards += list(dataset.rewards[1000*i:1000*i+trajectory_length])
++--            terminals += list(dataset.terminals[1000*i:1000*i+trajectory_length])
+++-
+++-    n_trajs = int(1000000/trajectory_length)
+++-    for i in range(n_trajs):
+++-        condition = i not in idxs
+++-        if inverse: condition = not condition
+++-
+++-        if condition:
+++-            observations += list(dataset.observations[trajectory_length*i:trajectory_length*(i+1)])
+++-            actions += list(dataset.actions[trajectory_length*i:trajectory_length*(i+1)])
+++-            rewards += list(dataset.rewards[trajectory_length*i:trajectory_length*(i+1)])
+++-
+++-    terminals = np.tile([0]*(trajectory_length-1)+[1], int(len(observations)/trajectory_length))
++ -
++ -    new_dataset = d3rlpy.dataset.MDPDataset(
++ -        observations=np.array(observations),
++ -        actions=np.array(actions),
++ -        rewards=np.array(rewards),
++--        terminals=np.array(terminals)
+++-        terminals=np.array(terminals),
++ -    )
++ -    return new_dataset
++ -    
++ -
+++-def clusters_to_idxs(clusters):
+++-    idxs_per_cluster = []
+++-    for i in np.sort(np.unique(clusters)):
+++-        idxs_per_cluster.append(list(np.argwhere(clusters == i).flatten()))
+++-    
+++-    return idxs_per_cluster
++ -
++ -
++ -def main():
++--    # args = Parser().parse_args('plan')
++--
++--    #######################
++--    ####### models ########
++--    #######################
++--
++--
++--
++--
++ -
++--    # print(args.dataset)
+++-    args = Parser().parse_args('plan')
++ -
++--    # dataset = utils.load_from_config(args.logbase, args.dataset, args.gpt_loadpath,
++--    #         'data_config.pkl')
++ -
+++-    ### DATASET ###
++ -
++--    # gpt, gpt_epoch = utils.load_model(args.logbase, args.dataset, args.gpt_loadpath,
++--    #         epoch=args.gpt_epoch, device=args.device)
++--
++--    # env = datasets.load_environment(args.dataset)
++--
++--    # discretizer = dataset.discretizer
++--
++--    # preprocess_fn = datasets.get_preprocess_fn(env.name)
++--
++--    # #######################
++--    # ####### dataset #######
++--    # #######################
+++-    dataset_d3, env = d3rlpy.datasets.get_dataset("halfcheetah-medium-v2")
++ -
++--    # # env = datasets.load_environment(args.dataset)
++--    # discretizer = dataset.discretizer
++--    # preprocess_fn = datasets.get_preprocess_fn(env.name)
+++-    ### IMPORTANT DEFINITIONS XRL SCRIPT ###
++ -
++--    # # dataset
++--    dataset_d3, env = d3rlpy.datasets.get_dataset("halfcheetah-medium-v2")
+++-    load_embeddings = False
+++-    load_clusters = True
+++-    load_agents = True
+++-    generate_human_study = False
+++-    
+++-    seed = 4 
+++-    trajectory_length = 25 # 10 = max
+++-    n_clusters = 10
+++-    k = 3
+++-    temperature = 10000
+++-    logging_folder = f"results/v2_models_100k_{seed}"
+++-    training_steps = 100000
++ -
++--    # env = gym.make('halfcheetah-medium-v2')
++--    # dataset_d4 = d4rl.qlearning_dataset(env)
+++-    d3rlpy.seed(seed)
++ -
++--    # # checks to see if d3rl & d4rl datasets are equal
++--    # print(np.allclose(dataset_d3.actions[100], dataset_d4['actions'][100]))
+++-    if load_embeddings:
+++-        embeddings = np.load(f"{logging_folder}/embeddings.npy")
+++-    else:
+++-   
+++-        ### TRAJECTORY TRANSFORMER ###
+++-    
+++-        dataset = utils.load_from_config(args.logbase, args.dataset, args.gpt_loadpath,
+++-                'data_config.pkl')
+++-        gpt, gpt_epoch = utils.load_model(args.logbase, args.dataset, args.gpt_loadpath,
+++-                epoch=args.gpt_epoch, device=args.device)
+++-        env = datasets.load_environment(args.dataset)
+++-        discretizer = dataset.discretizer
+++-        preprocess_fn = datasets.get_preprocess_fn(env.name)
+++-    
+++-        ### TRAJECTORY EMBEDDINGS ###
+++-    
+++-        embeddings = []
+++-        n_trajs = int(1000000/trajectory_length)
+++-        for i in range(n_trajs):
+++-            observations = dataset_d3.observations[trajectory_length*i:trajectory_length*(i+1)]
+++-            actions = dataset_d3.actions[trajectory_length*i:trajectory_length*(i+1)]
+++-            rewards = dataset_d3.rewards[trajectory_length*i:trajectory_length*(i+1)]
+++-            terminals = dataset_d3.terminals[trajectory_length*i:trajectory_length*(i+1)]
+++-            emb = embed_trajectory(gpt, discretizer, observations, actions, rewards, preprocess_fn)
+++-            embeddings.append(emb)
+++-        embeddings = np.array(embeddings)
+++-        np.save(f"{logging_folder}/embeddings.npy", embeddings)
++ -
++--    # # dr4rl has same trajectories, just cut off 1 element before the end
++--    # for j in range(1000):
++--    #     for i in range(999):
++--    #         if dataset_d4['rewards'][j * 999 + i] != dataset_d3.rewards[j * 1000 + i]: print("yo", i)
+++-    print("embeddings ready")
++ -
++--    # #######################
++--    # ###### main loop ######
++--    # #######################
+++-    ### TRAJECTORY CLUSTERS ###
++ -
++--    trajectory_length = 10 # 10 = max
+++-    if load_clusters:
+++-        clusters = np.load(f"{logging_folder}/clusters.npy")
+++-        idxs_per_cluster = clusters_to_idxs(clusters)
+++-    else:
+++-        idxs_per_cluster, clusters = cluster_trajectories(embeddings, n_clusters)
+++-        np.save(f"{logging_folder}/clusters.npy", clusters)
++ -
++--    # embeddings = []
++--    # for i in range(1000):
++--    #     observations = dataset_d3.observations[1000*i:1000*i+trajectory_length]
++--    #     actions = dataset_d3.actions[1000*i:1000*i+trajectory_length]
++--    #     rewards = dataset_d3.rewards[1000*i:1000*i+trajectory_length]
++--    #     terminals = dataset_d3.terminals[1000*i:1000*i+trajectory_length]
++--    #     emb = embed_trajectory(gpt, discretizer, observations, actions, rewards, preprocess_fn)
++--    #     embeddings.append(emb)
++--    # embeddings = np.array(embeddings)
++--    # np.save("embeddings.npy", embeddings)
++--    # print(embeddings)
+++-    print("clusters ready")
++ -
++--    embeddings = np.load("embeddings.npy")
+++-    ### PCA (solely for visualization) ###
+++- 
+++-    pca_idxs = np.random.choice(len(embeddings), 500, replace=False)
++ -
++ -    pca = PCA(n_components=2)
++--    pca = PCA(n_components=2)
++--    pca_embeddings = pca.fit_transform(embeddings)
++--    np.save("pca.py", pca_embeddings)
+++-    pca_embeddings = pca.fit_transform(embeddings[pca_idxs])
+++-    pca_clusters = clusters[pca_idxs]
++ -
++--    idxs_per_cluster, clusters = cluster_trajectories_2(embeddings)
++--    # print(clusters)
++--    # return
++--    np.save("clusters.npy", clusters)
+++-    print("pca ready")
++ -
++--    import matplotlib.pyplot as plt
+++-    ### COMPLEMENTARY DATASETS & CLUSTER EMBEDDINGS (also plotting PCA) ###
+++-    d_orig = generate_data_embedding(embeddings, temperature=temperature)
++ -
++--    d_orig = generate_data_embedding(embeddings)
++ -    unique_clusters = np.unique(clusters)
++--    
++ -    d_j = []
++ -    complementary_datasets = []
+++-    cluster_datasets = []
+++-    fig, ax = plt.subplots(figsize=(5,4))
++ -    for j in np.sort(unique_clusters):
++--        print(j)
++--        d_j.append(generate_data_embedding(embeddings[clusters != j]))
++--        plt.scatter(pca_embeddings[clusters == j][:,0], pca_embeddings[clusters == j][:,1], label=j)
+++-        d_j.append(generate_data_embedding(embeddings[clusters != j], temperature=temperature))
+++-        ax.scatter(pca_embeddings[pca_clusters == j][:,0], pca_embeddings[pca_clusters == j][:,1], label=j)
++ -        complementary_datasets.append(create_complementary_dataset(dataset_d3, idxs_per_cluster[j], trajectory_length))
+++-        cluster_datasets.append(create_complementary_dataset(dataset_d3, idxs_per_cluster[j], trajectory_length, inverse=True))
++ -    
++ -    original_dataset = create_complementary_dataset(dataset_d3, [], trajectory_length)
++ -
++--    print(complementary_datasets, original_dataset)
+++-    ax.legend(title="$c_j$", bbox_to_anchor=(0.5, 1.2), loc="lower center", ncol=5)
+++-    ax.set_xlabel("feature 1")
+++-    ax.set_ylabel("feature 2")
+++-    ax.spines['top'].set_visible(False)
+++-    ax.spines['right'].set_visible(False)
+++-    plt.title("Trajectory Clustering HalfCheetah")
+++-    plt.tight_layout()
+++-
+++-    plt.savefig(f"{logging_folder}/pca.pdf")
+++-
+++-    print("complementary datasets ready")
++ -
++--    plt.legend()
++--    plt.show()
+++-    ### AGENT TRAINING (original & complementary) ###
++ -
++ -    agent_orig = d3rlpy.algos.SAC(
++ -        actor_learning_rate=3e-4,
++ -        critic_learning_rate=3e-4,
++ -        temp_learning_rate=3e-4,
++--        batch_size=256)
+++-        batch_size=512)
++ -
++--    print(agent_orig)
++--
++--    training_steps = 1000
++--
++--    agent_orig.fit(original_dataset, n_steps=training_steps)
+++-    if load_agents:
+++-        agent_orig.build_with_dataset(original_dataset)
+++-        agent_orig.load_model(f"{logging_folder}/agent_orig.pt")
+++-    else:
+++-        agent_orig.fit(original_dataset, n_steps=training_steps)
+++-        agent_orig.save_model(f"{logging_folder}/agent_orig.pt")
++ -
++ -    agents_compl = []
++ -
++--    for dset in complementary_datasets:
+++-    for i in range(len(complementary_datasets)):
++ -        agent = d3rlpy.algos.SAC(
++ -            actor_learning_rate=3e-4,
++ -            critic_learning_rate=3e-4,
++ -            temp_learning_rate=3e-4,
++--            batch_size=256)
++--        agent.fit(dset, n_steps=training_steps)
+++-            batch_size=512)
+++-        if load_agents:
+++-            agent.build_with_dataset(complementary_datasets[i])
+++-            agent.load_model(f"{logging_folder}/agent_compl_{i}.pt")
+++-        else:
+++-            agent.fit(complementary_datasets[i], n_steps=training_steps)
+++-            agent.save_model(f"{logging_folder}/agent_compl_{i}.pt")
++ -        agents_compl.append(agent)
++ -
++--    action_orig = agent_orig.predict(dataset_d3.observations[0])
+++-    print("agents ready")
++ -
++--    actions_compl = []
++--    for agent in agents_compl:
++--        actions_compl.append(agent.predict(dataset_d3.observations[0]))
++--    
++--    action_dists = []
++--    for action in actions_compl:
++--        action_dists.append(np.linalg.norm(action_orig-action))
+++-    ### OBSERVATION EXPLANATION (cluster assignment) ###
++ -
++--    k = 3
++--    topk = np.argpartition(action_dists, -k)[-k:]
+++-    original_state = np.random.get_state()
+++-    np.random.seed(0)
+++-    idxs_to_explain = np.random.choice(range(len(dataset_d3.observations)), 1000, replace=False)
+++-    np.random.set_state(original_state)
++ -
++--    d_w = {}
++--    for idx in topk:
++--        d_w[idx] = wasserstein_distance(d_j[idx], d_orig)
+++-    observations_to_explain = [dataset_d3.observations[i] for i in idxs_to_explain] 
++ -
++--    cluster_assignment = min(d_w, key=d_w.get)
++--    print("explanation assigned to cluster", cluster_assignment)
+++-    ISVE = []
+++-    ISVE_orig = 0.
+++-    LMAAVD = []
+++-    ACM = []
+++-    NWD = []
+++-    CAF = [0] * len(d_j)
++ -
++--    
++--def assignment_test():
++--    action_orig = np.random.rand(10)
++--    d_orig = np.random.rand(5)
+++-    if generate_human_study:
+++-        ctr = 0
+++-        unrelated_idxs = [690, 1520, 3030, 6050, 7080, 8030]
+++-        if not os.path.isdir(f"{logging_folder}/mp4s"): os.mkdir(f"{logging_folder}/mp4s")
+++-        if not os.path.isdir(f"{logging_folder}/gifs"): os.mkdir(f"{logging_folder}/gifs")
++ -
++--    actions_compl = np.random.rand(6,10)
++--    d_j = np.random.rand(6,5)
+++-    for observation_to_explain in observations_to_explain:
+++-        action_orig = agent_orig.predict([observation_to_explain])
++ -
++--    action_dists = []
++--    for action in actions_compl:
++--        action_dists.append(np.linalg.norm(action_orig-action))
+++-        actions_compl = []
+++-        for agent in agents_compl:
+++-            actions_compl.append(agent.predict([observation_to_explain]))
++ -
++--    print(action_dists)
+++-        action_dists = []
+++-        for action in actions_compl:
+++-            action_dists.append(np.linalg.norm(action_orig-action))
++ -
++--    k = 3
++--    topk = np.argpartition(action_dists, -k)[-k:]
+++-        topk = np.argpartition(action_dists, -k)[-k:]
+++-
+++-        d_w = {}
+++-        for idx in topk:
+++-            d_w[idx] = wasserstein_distance(d_j[idx], d_orig)
+++-
+++-        cluster_assignment = min(d_w, key=d_w.get)
+++-
+++-        ### OBSERVATION EXPLANATION (representing cluster with 1 trajectory) ###
+++-
+++-        distances_to_obs = [np.linalg.norm(observation_to_explain-obs) for obs in cluster_datasets[cluster_assignment].observations]
+++-        trajectory_to_assign = np.floor(np.argmin(distances_to_obs) / trajectory_length)
+++-        assigned_trajectory = np.arange(trajectory_to_assign * trajectory_length, (trajectory_to_assign+1) * trajectory_length)
++ -
++--    print(topk)
+++-        ### OBSERVATION EXPLANATION (metrics) ###
++ -
++--    d_w = {}
++--    for idx in topk:
++--        d_w[idx] = wasserstein_distance(d_j[idx], d_orig)
+++-        # Initial State Value Estimate
+++- 
+++-        V_s = 0.
+++-        for _ in range(10):
+++-            sampled_action = agent_orig.sample_action([observation_to_explain])
+++-            Q_sa = agent_orig.predict_value([observation_to_explain], [sampled_action[0]])[0]
+++-            V_s += Q_sa
+++-        ISVE_orig += V_s/10
+++- 
+++-        new_ISVE = []
+++-        for agent in agents_compl:
+++-            V_s = 0.
+++-            for _ in range(10):
+++-                sampled_action = agent.sample_action([observation_to_explain])
+++-                Q_sa = agent.predict_value([observation_to_explain], [sampled_action[0]])[0]
+++-                V_s += Q_sa
+++-            new_ISVE.append(V_s/10)
+++-        ISVE.append(new_ISVE)
+++- 
+++-        # Local Mean Absolute Action-Value Difference
+++-        Q_orig = agent_orig.predict_value([observation_to_explain], [action_orig[0]])
+++-        Q_j = [agent_orig.predict_value([observation_to_explain], [ac[0]]) for ac in actions_compl]
+++-        LMAAVD.append(np.abs(np.array(Q_j) - Q_orig).flatten())
+++- 
+++-        # Action Contrast Measure
+++-        ACM.append(action_dists)
+++- 
+++-        # Normalized Wasserstein distance (between cluster embeddings)
+++-        wasser = np.array([wasserstein_distance(d, d_orig) for d in d_j])
+++-        NWD.append(list((wasser-np.min(wasser))/(np.max(wasser)-np.min(wasser))))
+++- 
+++-        # Cluster attribution frequency
+++-        CAF[cluster_assignment] += 1
+++- 
+++-        if generate_human_study:
+++-            ### RENDERING ###
+++-            if not os.path.isdir(f"{logging_folder}/gifs/question_{ctr}"): os.mkdir(f"{logging_folder}/gifs/question_{ctr}")
++ -
++--    print(d_w)
+++-            rollout = dataset_d3.observations[idxs_to_explain[ctr]-25:idxs_to_explain[ctr]]
+++-            print(idxs_to_explain[ctr])
+++-            print(rollout)
+++-            renderer = utils.make_renderer(args)
+++-            rollout_savepath = f"{logging_folder}/mp4s/question_{ctr}/traj_to_explain.mp4"
+++-            renderer.render_rollout(rollout_savepath, rollout, fps=10)
+++-            videoClip = VideoFileClip(f"{logging_folder}/mp4s/question_{ctr}/traj_to_explain.mp4")
+++-            videoClip.write_gif(f"{logging_folder}/gifs/question_{ctr}/traj_to_explain.gif")
++ -
++--    cluster_assignment = min(d_w, key=d_w.get)
++--    print("explanation assigned to cluster", cluster_assignment)
+++-
+++-            rollout = cluster_datasets[cluster_assignment].observations[assigned_trajectory.astype(int)]
+++-            print(rollout)
+++-            renderer = utils.make_renderer(args)
+++-            rollout_savepath = f"{logging_folder}/mp4s/question_{ctr}/traj_assigned_cluster_attr.mp4"
+++-            renderer.render_rollout(rollout_savepath, rollout, fps=10)
+++-            videoClip = VideoFileClip(f"{logging_folder}/mp4s/question_{ctr}/traj_assigned_cluster_attr.mp4")
+++-            videoClip.write_gif(f"{logging_folder}/gifs/question_{ctr}/traj_assigned_cluster_attr.gif")
+++-
+++-            random_trajs = np.random.randint(len(cluster_datasets[cluster_assignment])//25, size=3)
+++-
+++-            rollout = cluster_datasets[cluster_assignment].observations[random_trajs[0]*25:(random_trajs[0]+1)*25]
+++-            print(rollout)
+++-            renderer = utils.make_renderer(args)
+++-            rollout_savepath = f"{logging_folder}/mp4s/question_{ctr}/traj_assigned_cluster_1.mp4"
+++-            renderer.render_rollout(rollout_savepath, rollout, fps=10)
+++-            videoClip = VideoFileClip(f"{logging_folder}/mp4s/question_{ctr}/traj_assigned_cluster_1.mp4")
+++-            videoClip.write_gif(f"{logging_folder}/gifs/question_{ctr}/traj_assigned_cluster_1.gif")
+++-
+++-            rollout = cluster_datasets[cluster_assignment].observations[random_trajs[1]*25:(random_trajs[1]+1)*25]
+++-            print(rollout)
+++-            renderer = utils.make_renderer(args)
+++-            rollout_savepath = f"{logging_folder}/mp4s/question_{ctr}/traj_assigned_cluster_2.mp4"
+++-            renderer.render_rollout(rollout_savepath, rollout, fps=10)
+++-            videoClip = VideoFileClip(f"{logging_folder}/mp4s/question_{ctr}/traj_assigned_cluster_2.mp4")
+++-            videoClip.write_gif(f"{logging_folder}/gifs/question_{ctr}/traj_assigned_cluster_2.gif")
+++-
+++-
+++-            rollout = cluster_datasets[cluster_assignment].observations[random_trajs[2]*25:(random_trajs[2]+1)*25]
+++-            print(rollout)
+++-            renderer = utils.make_renderer(args)
+++-            rollout_savepath = f"{logging_folder}/mp4s/question_{ctr}/traj_assigned_cluster_3.mp4"
+++-            renderer.render_rollout(rollout_savepath, rollout, fps=10)
+++-            videoClip = VideoFileClip(f"{logging_folder}/mp4s/question_{ctr}/traj_assigned_cluster_3.mp4")
+++-            videoClip.write_gif(f"{logging_folder}/gifs/question_{ctr}/traj_assigned_cluster_3.gif")
+++-
+++-
+++-            different_cluster = 0 if cluster_assignment != 0 else 1
+++-            random_trajs = np.random.randint(len(cluster_datasets[different_cluster])//25, size=3)
+++-            rollout = cluster_datasets[different_cluster].observations[random_trajs[2]*25:(random_trajs[2]+1)*25]
+++-            print(rollout)
+++-            renderer = utils.make_renderer(args)
+++-            rollout_savepath = f"{logging_folder}/mp4s/question_{ctr}/traj_different_cluster.mp4"
+++-            renderer.render_rollout(rollout_savepath, rollout, fps=10)
+++-            videoClip = VideoFileClip(f"{logging_folder}/mp4s/question_{ctr}/traj_different_cluster.mp4")
+++-            videoClip.write_gif(f"{logging_folder}/gifs/question_{ctr}/traj_different_cluster.gif")
+++-
+++-            rollout = dataset_d3.observations[unrelated_idxs[ctr]-25:unrelated_idxs[ctr]]
+++-            print(rollout)
+++-            renderer = utils.make_renderer(args)
+++-            rollout_savepath = f"{logging_folder}/mp4s/question_{ctr}/traj_unrelated.mp4"
+++-            renderer.render_rollout(rollout_savepath, rollout, fps=10)
+++-            videoClip = VideoFileClip(f"{logging_folder}/mp4s/question_{ctr}/traj_unrelated.mp4")
+++-            videoClip.write_gif(f"{logging_folder}/gifs/question_{ctr}/traj_unrelated.gif")
+++-
+++-            ctr += 1
+++-
+++-    ### RESULTS ###
+++-    ISVE_orig /= len(observations_to_explain)
+++-    ISVE = np.mean(ISVE, axis=0)
+++-    LMAAVD = np.mean(LMAAVD, axis=0)
+++-    ACM = np.mean(ACM, axis=0)
+++-    NWD = np.mean(NWD, axis=0)
+++-    CAF = np.array(CAF) / np.sum(CAF)
+++-
+++-    print("ISVE orig:", ISVE_orig)
+++-    print("ISVE:",ISVE)
+++-    print("LMAAVD:",LMAAVD)
+++-    print("ACM:",ACM)
+++-    print("NWD:",NWD)
+++-    print("CAF:",CAF)
++ -
++ -
++ -if __name__ == "__main__":
++--    # main()
++--    assignment_test()
++-diff --git a/halfcheetah/trajectory.egg-info/PKG-INFO b/halfcheetah/trajectory.egg-info/PKG-INFO
++-index 452c6cb..2603850 100644
++---- a/halfcheetah/trajectory.egg-info/PKG-INFO
++-+++ b/halfcheetah/trajectory.egg-info/PKG-INFO
++-@@ -1,4 +1,11 @@
++- Metadata-Version: 2.1
++- Name: trajectory
++- Version: 0.0.0
++-+Summary: UNKNOWN
++-+Home-page: UNKNOWN
++-+License: UNKNOWN
++-+Platform: UNKNOWN
++- License-File: LICENSE
++-+
++-+UNKNOWN
++-+
+++-    main()
++ diff --git a/halfcheetah/trajectory.egg-info/SOURCES.txt b/halfcheetah/trajectory.egg-info/SOURCES.txt
++-index 4474d85..84e8e3a 100644
+++index 84e8e3a..4474d85 100644
++ --- a/halfcheetah/trajectory.egg-info/SOURCES.txt
++ +++ b/halfcheetah/trajectory.egg-info/SOURCES.txt
++-@@ -30,4 +30,5 @@ trajectory/utils/serialization.py
+++@@ -30,5 +30,4 @@ trajectory/utils/serialization.py
++  trajectory/utils/setup.py
++  trajectory/utils/timer.py
++  trajectory/utils/training.py
++ -trajectory/utils/video.py
+++-trajectory_aaa/__init__.py
++ \ No newline at end of file
++ +trajectory/utils/video.py
++-+trajectory_aaa/__init__.py
++ \ No newline at end of file
++ diff --git a/halfcheetah/trajectory.egg-info/top_level.txt b/halfcheetah/trajectory.egg-info/top_level.txt
++-index ce65198..1d5271f 100644
+++index 1d5271f..ce65198 100644
++ --- a/halfcheetah/trajectory.egg-info/top_level.txt
++ +++ b/halfcheetah/trajectory.egg-info/top_level.txt
++-@@ -1 +1,2 @@
+++@@ -1,2 +1 @@
++  trajectory
++-+trajectory_aaa
+ \ No newline at end of file
+-diff --git a/halfcheetah/pca.py.npy b/halfcheetah/pca.py.npy
+-deleted file mode 100644
+-index bb19150..0000000
+-Binary files a/halfcheetah/pca.py.npy and /dev/null differ
+-diff --git a/halfcheetah/plotting/bar.png b/halfcheetah/plotting/bar.png
+-deleted file mode 100644
+-index 3679667..0000000
+-Binary files a/halfcheetah/plotting/bar.png and /dev/null differ
+-diff --git a/halfcheetah/plotting/plot.py b/halfcheetah/plotting/plot.py
+-deleted file mode 100644
+-index 163d0e4..0000000
+---- a/halfcheetah/plotting/plot.py
+-+++ /dev/null
+-@@ -1,74 +0,0 @@
+--import numpy as np
+--import matplotlib
+--import matplotlib.pyplot as plt
+--import pdb
+--
+--from plotting.scores import means
+--
+--class Colors:
+--	grey = '#B4B4B4'
+--	gold = '#F6C781'
+--	red = '#EC7C7D'
+--	blue = '#70ABCC'
+--
+--LABELS = {
+--	# 'BC': 'Behavior\nCloning',
+--	# 'MBOP': 'Model-Based\nOffline Planning',
+--	# 'BRAC': 'Behavior-Reg.\nActor-Critic',
+--	# 'CQL': 'Conservative\nQ-Learning',
+--}
+--
+--def get_mean(results, exclude=None):
+--	'''
+--		results : { environment: score, ... }
+--	'''
+--	filtered = {
+--		k: v for k, v in results.items()
+--		if (not exclude) or (exclude and exclude not in k)
+--	}
+--	return np.mean(list(filtered.values()))
+--
+--if __name__ == '__main__':
+--
+--	#################
+--	## latex
+--	#################
+--	matplotlib.rc('font', **{'family': 'serif', 'serif': ['Computer Modern']})
+--	matplotlib.rc('text', usetex=True)
+--	matplotlib.rcParams['text.latex.preamble']=[r"\usepackage{amsmath}"]
+--	#################
+--
+--	fig = plt.gcf()
+--	ax = plt.gca()
+--	fig.set_size_inches(7.5, 2.5)
+--
+--	means = {k: get_mean(v, exclude='ant') for k, v in means.items()}
+--	print(means)
+--
+--	algs = ['BC', 'MBOP', 'BRAC', 'CQL', 'Decision\nTransformer', 'Trajectory\nTransformer']
+--	vals = [means[alg] for alg in algs]
+--
+--	colors = [
+--		Colors.grey, Colors.gold,
+--		Colors.red, Colors.red, Colors.blue, Colors.blue
+--	]
+--
+--	labels = [LABELS.get(alg, alg) for alg in algs]
+--	plt.bar(labels, vals, color=colors, edgecolor=Colors.gold, lw=0)
+--	plt.ylabel('Average normalized return', labelpad=15)
+--	# plt.title('Offline RL Results')
+--
+--	legend_labels = ['Behavior Cloning', 'Trajectory Optimization', 'Temporal Difference', 'Sequence Modeling']
+--	colors = [Colors.grey, Colors.gold, Colors.red, Colors.blue]
+--	handles = [plt.Rectangle((0,0),1,1, color=color) for label, color in zip(legend_labels, colors)]
+--	plt.legend(handles, legend_labels, ncol=4,
+--		bbox_to_anchor=(1.07, -.18), fancybox=False, framealpha=0, shadow=False, columnspacing=1.5, handlelength=1.5)
+--
+--	matplotlib.rcParams['hatch.linewidth'] = 7.5
+--	# ax.patches[-1].set_hatch('/')
+--
+--	ax.spines['right'].set_visible(False)
+--	ax.spines['top'].set_visible(False)
+--
+--	# plt.savefig('plotting/bar.pdf', bbox_inches='tight')
+--	plt.savefig('plotting/bar.png', bbox_inches='tight', dpi=500)
+-diff --git a/halfcheetah/plotting/read_results.py b/halfcheetah/plotting/read_results.py
+-deleted file mode 100644
+-index 5a5fb62..0000000
+---- a/halfcheetah/plotting/read_results.py
+-+++ /dev/null
+-@@ -1,70 +0,0 @@
+--import os
+--import glob
+--import numpy as np
+--import json
+--import pdb
+--
+--import trajectory.utils as utils
+--
+--DATASETS = [
+--	f'{env}-{buffer}'
+--	for env in ['hopper', 'walker2d', 'halfcheetah', 'ant']
+--	for buffer in ['medium-expert-v2', 'medium-v2', 'medium-replay-v2']
+--]
+--
+--LOGBASE = 'logs'
+--TRIAL = '*'
+--EXP_NAME = 'plans/pretrained'
+--
+--def load_results(paths):
+--	'''
+--		paths : path to directory containing experiment trials
+--	'''
+--	scores = []
+--	for i, path in enumerate(sorted(paths)):
+--		score = load_result(path)
+--		if score is None:
+--			print(f'Skipping {path}')
+--			continue
+--		scores.append(score)
+--
+--		suffix = path.split('/')[-1]
+--
+--	mean = np.mean(scores)
+--	err = np.std(scores) / np.sqrt(len(scores))
+--	return mean, err, scores
+--
+--def load_result(path):
+--	'''
+--		path : path to experiment directory; expects `rollout.json` to be in directory
+--	'''
+--	fullpath = os.path.join(path, 'rollout.json')
+--	suffix = path.split('/')[-1]
+--
+--	if not os.path.exists(fullpath):
+--		return None
+--
+--	results = json.load(open(fullpath, 'rb'))
+--	score = results['score']
+--	return score * 100
+--
+--#######################
+--######## setup ########
+--#######################
+--
+--if __name__ == '__main__':
+--
+--	class Parser(utils.Parser):
+--	    dataset: str = None
+--
+--	args = Parser().parse_args()
+--
+--	for dataset in ([args.dataset] if args.dataset else DATASETS):
+--		subdirs = glob.glob(os.path.join(LOGBASE, dataset, EXP_NAME))
+--
+--		for subdir in subdirs:
+--			reldir = subdir.split('/')[-1]
+--			paths = glob.glob(os.path.join(subdir, TRIAL))
+--
+--			mean, err, scores = load_results(paths)
+--			print(f'{dataset.ljust(30)} | {subdir.ljust(50)} | {len(scores)} scores \n    {mean:.2f} +/- {err:.2f}\n')
+-diff --git a/halfcheetah/plotting/scores.py b/halfcheetah/plotting/scores.py
+-deleted file mode 100644
+-index f1917f7..0000000
+---- a/halfcheetah/plotting/scores.py
+-+++ /dev/null
+-@@ -1,123 +0,0 @@
+--means = {
+--	'Trajectory\nTransformer': {
+--		##
+--		'halfcheetah-medium-expert-v2': 95.0,
+--		'hopper-medium-expert-v2': 110.0,
+--		'walker2d-medium-expert-v2': 101.9,
+--		'ant-medium-expert-v2': 116.1,
+--		##
+--		'halfcheetah-medium-v2': 46.9,
+--		'hopper-medium-v2': 61.1,
+--		'walker2d-medium-v2': 79.0,
+--		'ant-medium-v2': 83.1,
+--		##
+--		'halfcheetah-medium-replay-v2': 41.9,
+--		'hopper-medium-replay-v2': 91.5,
+--		'walker2d-medium-replay-v2': 82.6,
+--		'ant-medium-replay-v2': 77.0,
+--	},
+--	'Decision\nTransformer': {
+--		##
+--		'halfcheetah-medium-expert-v2': 86.8,
+--		'hopper-medium-expert-v2': 107.6,
+--		'walker2d-medium-expert-v2': 108.1,
+--		##
+--		'halfcheetah-medium-v2': 42.6,
+--		'hopper-medium-v2': 67.6,
+--		'walker2d-medium-v2': 74.0,
+--		##
+--		'halfcheetah-medium-replay-v2': 36.6,
+--		'hopper-medium-replay-v2': 82.7,
+--		'walker2d-medium-replay-v2': 66.6,
+--	},
+--	'CQL': {
+--		##
+--		'halfcheetah-medium-expert-v2': 91.6,
+--		'hopper-medium-expert-v2': 105.4,
+--		'walker2d-medium-expert-v2': 108.8,
+--		##
+--		'halfcheetah-medium-v2': 44.0,
+--		'hopper-medium-v2': 58.5,
+--		'walker2d-medium-v2': 72.5,
+--		##
+--		'halfcheetah-medium-replay-v2': 45.5,
+--		'hopper-medium-replay-v2': 95.0,
+--		'walker2d-medium-replay-v2': 77.2,
+--	},
+--	'MOPO': {
+--		##
+--		'halfcheetah-medium-expert-v2': 63.3,
+--		'hopper-medium-expert-v2': 23.7,
+--		'walker2d-medium-expert-v2': 44.6,
+--		##
+--		'halfcheetah-medium-v2': 42.3,
+--		'hopper-medium-v2': 28.0,
+--		'walker2d-medium-v2': 17.8,
+--		##
+--		'halfcheetah-medium-replay-v2': 53.1,
+--		'hopper-medium-replay-v2': 67.5,
+--		'walker2d-medium-replay-v2':39.0,
+--	},
+--	'MBOP': {
+--		##
+--		'halfcheetah-medium-expert-v2': 105.9,
+--		'hopper-medium-expert-v2': 55.1,
+--		'walker2d-medium-expert-v2': 70.2,
+--		##
+--		'halfcheetah-medium-v2': 44.6,
+--		'hopper-medium-v2': 48.8,
+--		'walker2d-medium-v2': 41.0,
+--		##
+--		'halfcheetah-medium-replay-v2': 42.3,
+--		'hopper-medium-replay-v2': 12.4,
+--		'walker2d-medium-replay-v2': 9.7,
+--	},
+--	'BRAC': {
+--		##
+--		'halfcheetah-medium-expert-v2': 41.9,
+--		'hopper-medium-expert-v2': 0.9,
+--		'walker2d-medium-expert-v2': 81.6,
+--		##
+--		'halfcheetah-medium-v2': 46.3,
+--		'hopper-medium-v2': 31.3,
+--		'walker2d-medium-v2': 81.1,
+--		##
+--		'halfcheetah-medium-replay-v2': 47.7,
+--		'hopper-medium-replay-v2': 0.6,
+--		'walker2d-medium-replay-v2': 0.9,
+--	},
+--	'BC': {
+--		##
+--		'halfcheetah-medium-expert-v2': 59.9,
+--		'hopper-medium-expert-v2': 79.6,
+--		'walker2d-medium-expert-v2': 36.6,
+--		##
+--		'halfcheetah-medium-v2': 43.1,
+--		'hopper-medium-v2': 63.9,
+--		'walker2d-medium-v2': 77.3,
+--		##
+--		'halfcheetah-medium-replay-v2': 4.3,
+--		'hopper-medium-replay-v2': 27.6,
+--		'walker2d-medium-replay-v2': 36.9,
+--	},
+--}
+--
+--errors = {
+--	'Trajectory\nTransformer': {
+--		##
+--		'halfcheetah-medium-expert-v2': 0.2,
+--		'hopper-medium-expert-v2': 2.7,
+--		'walker2d-medium-expert-v2': 6.8,
+--		'ant-medium-expert-v2': 9.0,
+--		##
+--		'halfcheetah-medium-v2': 0.4,
+--		'hopper-medium-v2': 3.6,
+--		'walker2d-medium-v2': 2.8,
+--		'ant-medium-v2': 7.3,
+--		##
+--		'halfcheetah-medium-replay-v2': 2.5,
+--		'hopper-medium-replay-v2': 3.6,
+--		'walker2d-medium-replay-v2': 6.9,
+--		'ant-medium-replay-v2': 6.8,
+--	},
+--}
+++-trajectory_aaa
+++diff --git a/seaquest/readme.md b/seaquest/readme.md
+++index 84e53f8..53561f9 100644
+++--- a/seaquest/readme.md
++++++ b/seaquest/readme.md
+++@@ -10,4 +10,4 @@ pip install git+https://github.com/takuseno/d4rl-atari
+++ pip install "gym[atari, accept-rom-license]"
+++ pip install pyclustering
+++ pip install seaborn
+++-pip install d3rlpy==1.1.1
+++\ No newline at end of file
++++pip install d3rlpy==1.1.1
+ \ No newline at end of file
+-diff --git a/halfcheetah/plotting/table.py b/halfcheetah/plotting/table.py
++diff --git a/halfcheetah/scripts/xrl_v2.py b/halfcheetah/scripts/xrl_v2.py
+ deleted file mode 100644
+-index eae74e6..0000000
+---- a/halfcheetah/plotting/table.py
++index 62a3d4d..0000000
++--- a/halfcheetah/scripts/xrl_v2.py
+ +++ /dev/null
+-@@ -1,127 +0,0 @@
+--import numpy as np
+--import pdb
+--
+--from plotting.plot import get_mean
+--from plotting.scores import (
+--	means as MEANS,
+--	errors as ERRORS,
+--)
+--
+--ALGORITHM_STRINGS = {
+--	'Trajectory\nTransformer': 'TT (Ours)',
+--	'Decision\nTransformer': 'DT',	
+--}
+--
+--BUFFER_STRINGS = {
+--	'medium-expert': 'Medium-Expert',
+--	'medium': 'Medium',
+--	'medium-replay': 'Medium-Replay',	
+--}
+--
+--ENVIRONMENT_STRINGS = {
+--	'halfcheetah': 'HalfCheetah',
+--	'hopper': 'Hopper',
+--	'walker2d': 'Walker2d',
+--	'ant': 'Ant',
+--}
+--
+--SHOW_ERRORS = ['Trajectory\nTransformer']
+--
+--def get_result(algorithm, buffer, environment, version='v2'):
+--	key = f'{environment}-{buffer}-{version}'
+--	mean = MEANS[algorithm].get(key, '-')
+--	if algorithm in SHOW_ERRORS:
+--		error = ERRORS[algorithm].get(key)
+--		return (mean, error)
+--	else:
+--		return mean
+--
+--def format_result(result):
+--	if type(result) == tuple:
+--		mean, std = result
+--		return f'${mean}$ \\scriptsize{{\\raisebox{{1pt}}{{$\\pm {std}$}}}}'
+--	else:
+--		return f'${result}$'
+--
+--def format_row(buffer, environment, results):
+--	buffer_str = BUFFER_STRINGS[buffer]
+--	environment_str = ENVIRONMENT_STRINGS[environment]
+--	results_str = ' & '.join(format_result(result) for result in results)
+--	row = f'{buffer_str} & {environment_str} & {results_str} \\\\ \n'
+--	return row
+--
+--def format_buffer_block(algorithms, buffer, environments):
+--	block_str = '\\midrule\n'
+--	for environment in environments:
+--		results = [get_result(alg, buffer, environment) for alg in algorithms]
+--		row_str = format_row(buffer, environment, results)
+--		block_str += row_str
+--	return block_str
+--
+--def format_algorithm(algorithm):
+--	algorithm_str = ALGORITHM_STRINGS.get(algorithm, algorithm)
+--	return f'\multicolumn{{1}}{{c}}{{\\bf {algorithm_str}}}'
+--
+--def format_algorithms(algorithms):
+--	return ' & '.join(format_algorithm(algorithm) for algorithm in algorithms)
+--
+--def format_averages(means, label):
+--	prefix = f'\\multicolumn{{2}}{{c}}{{\\bf Average ({label})}} & '
+--	formatted = ' & '.join(str(mean) for mean in means)
+--	return prefix + formatted
+--
+--def format_averages_block(algorithms):
+--	means_filtered = [np.round(get_mean(MEANS[algorithm], exclude='ant'), 1) for algorithm in algorithms]
+--	means_all = [np.round(get_mean(MEANS[algorithm], exclude=None), 1) for algorithm in algorithms]
+--
+--	means_all = [
+--		means
+--		if 'ant-medium-expert-v2' in MEANS[algorithm]
+--		else '$-$'
+--		for algorithm, means in zip(algorithms, means_all)
+--	]
+--
+--	formatted_filtered = format_averages(means_filtered, 'without Ant')
+--	formatted_all = format_averages(means_all, 'all settings')
+--
+--	formatted_block = (
+--		f'{formatted_filtered} \\hspace{{.6cm}} \\\\ \n'
+--		f'{formatted_all} \\hspace{{.6cm}} \\\\ \n'
+--	)
+--	return formatted_block
+--
+--def format_table(algorithms, buffers, environments):
+--	justify_str = 'll' + 'r' * len(algorithms)
+--	algorithm_str = format_algorithms(['Dataset', 'Environment'] + algorithms)
+--	averages_str = format_averages_block(algorithms)
+--	table_prefix = (
+--		'\\begin{table*}[h]\n'
+--		'\\centering\n'
+--		'\\small\n'
+--		f'\\begin{{tabular}}{{{justify_str}}}\n'
+--		'\\toprule\n'
+--		f'{algorithm_str} \\\\ \n'
+--	)
+--	table_suffix = (
+--		'\\midrule\n'
+--		f'{averages_str}'
+--		'\\bottomrule\n'
+--		'\\end{tabular}\n'
+--		'\\label{table:d4rl}\n'
+--		'\\end{table*}'
+--	)
+--	blocks = ''.join(format_buffer_block(algorithms, buffer, environments) for buffer in buffers)
+--	table = (
+--		f'{table_prefix}'
+--		f'{blocks}'
+--		f'{table_suffix}'
+--	)
+--	return table
+--
+--
+--algorithms =['BC', 'MBOP', 'BRAC', 'CQL',  'Decision\nTransformer', 'Trajectory\nTransformer']
+--buffers = ['medium-expert', 'medium', 'medium-replay']
+--environments = ['halfcheetah', 'hopper', 'walker2d', 'ant']
+--
+--table = format_table(algorithms, buffers, environments)
+--print(table)
+-diff --git a/halfcheetah/scripts/plan.py b/halfcheetah/scripts/plan.py
+-deleted file mode 100644
+-index f13d4cc..0000000
+---- a/halfcheetah/scripts/plan.py
+-+++ /dev/null
+-@@ -1,124 +0,0 @@
+--import json
+--import pdb
+--from os.path import join
+--
+--import trajectory.utils as utils
+--import trajectory.datasets as datasets
+--from trajectory.search import (
+--    beam_plan,
+--    make_prefix,
+--    extract_actions,
+--    update_context,
+--)
+--
+--class Parser(utils.Parser):
+--    dataset: str = 'halfcheetah-medium-expert-v2'
+--    config: str = 'config.offline'
+--
+--#######################
+--######## setup ########
+--#######################
+--
+--args = Parser().parse_args('plan')
+--
+--#######################
+--####### models ########
+--#######################
+--
+--dataset = utils.load_from_config(args.logbase, args.dataset, args.gpt_loadpath,
+--        'data_config.pkl')
+--
+--gpt, gpt_epoch = utils.load_model(args.logbase, args.dataset, args.gpt_loadpath,
+--        epoch=args.gpt_epoch, device=args.device)
+--
+--#######################
+--####### dataset #######
+--#######################
+--
+--env = datasets.load_environment(args.dataset)
+--print('yo')
+--renderer = utils.make_renderer(args)
+--timer = utils.timer.Timer()
+--
+--discretizer = dataset.discretizer
+--discount = dataset.discount
+--observation_dim = dataset.observation_dim
+--action_dim = dataset.action_dim
+--
+--value_fn = lambda x: discretizer.value_fn(x, args.percentile)
+--preprocess_fn = datasets.get_preprocess_fn(env.name)
+--
+--print('yo2')
+--
+--#######################
+--###### main loop ######
+--#######################
+--
+--observation = env.reset()
+--total_reward = 0
+--
+--## observations for rendering
+--rollout = [observation.copy()]
++@@ -1,442 +0,0 @@
++-"""
++-TODO HEADER
++-"""
+ -
+--## previous (tokenized) transitions for conditioning transformer
+--context = []
+--
+--T = env.max_episode_steps
+--for t in range(T):
+--
+--    observation = preprocess_fn(observation)
+--
+--    if t % args.plan_freq == 0:
+--        ## concatenate previous transitions and current observations to input to model
+--        prefix = make_prefix(discretizer, context, observation, args.prefix_context)
+--
+--        ## sample sequence from model beginning with `prefix`
+--        sequence = beam_plan(
+--            gpt, value_fn, prefix,
+--            args.horizon, args.beam_width, args.n_expand, observation_dim, action_dim,
+--            discount, args.max_context_transitions, verbose=args.verbose,
+--            k_obs=args.k_obs, k_act=args.k_act, cdf_obs=args.cdf_obs, cdf_act=args.cdf_act,
+--        )
+--
+--    else:
+--        sequence = sequence[1:]
+--
+--    ## [ horizon x transition_dim ] convert sampled tokens to continuous trajectory
+--    sequence_recon = discretizer.reconstruct(sequence)
+--
+--    ## [ action_dim ] index into sampled trajectory to grab first action
+--    action = extract_actions(sequence_recon, observation_dim, action_dim, t=0)
+--
+--    ## execute action in environment
+--    next_observation, reward, terminal, _ = env.step(action)
+--
+--    ## update return
+--    total_reward += reward
+--    score = env.get_normalized_score(total_reward)
+--
+--    ## update rollout observations and context transitions
+--    rollout.append(next_observation.copy())
+--    context = update_context(context, discretizer, observation, action, reward, args.max_context_transitions)
+--
+--    print(
+--        f'[ plan ] t: {t} / {T} | r: {reward:.2f} | R: {total_reward:.2f} | score: {score:.4f} | '
+--        f'time: {timer():.2f} | {args.dataset} | {args.exp_name} | {args.suffix}\n'
+--    )
+--
+--    ## visualization
+--    if t % args.vis_freq == 0 or terminal or t == T:
+--
+--        ## save current plan
+--        renderer.render_plan(join(args.savepath, f'{t}_plan.mp4'), sequence_recon, env.state_vector())
+--
+--        ## save rollout thus far
+--        renderer.render_rollout(join(args.savepath, f'rollout.mp4'), rollout, fps=80)
+--
+--    if terminal: break
+--
+--    observation = next_observation
+--
+--## save result as a json file
+--json_path = join(args.savepath, 'rollout.json')
+--json_data = {'score': score, 'step': t, 'return': total_reward, 'term': terminal, 'gpt_epoch': gpt_epoch}
+--json.dump(json_data, open(json_path, 'w'), indent=2, sort_keys=True)
+-diff --git a/halfcheetah/scripts/train.py b/halfcheetah/scripts/train.py
+-deleted file mode 100644
+-index 04af8d7..0000000
+---- a/halfcheetah/scripts/train.py
+-+++ /dev/null
+-@@ -1,122 +0,0 @@
+ -import os
+ -import numpy as np
+--import torch
+--import pdb
+--
+--import trajectory.utils as utils
+--import trajectory.datasets as datasets
+--from trajectory.models.transformers import GPT
+--
+--
+--class Parser(utils.Parser):
+--    dataset: str = 'halfcheetah-medium-expert-v2'
+--    config: str = 'config.offline'
+--
+--#######################
+--######## setup ########
+--#######################
+--
+--args = Parser().parse_args('train')
+--
+--#######################
+--####### dataset #######
+--#######################
+--
+--env = datasets.load_environment(args.dataset)
+--
+--sequence_length = args.subsampled_sequence_length * args.step
+--
+--dataset_config = utils.Config(
+--    datasets.DiscretizedDataset,
+--    savepath=(args.savepath, 'data_config.pkl'),
+--    env=args.dataset,
+--    N=args.N,
+--    penalty=args.termination_penalty,
+--    sequence_length=sequence_length,
+--    step=args.step,
+--    discount=args.discount,
+--    discretizer=args.discretizer,
+--)
+--
+--dataset = dataset_config()
+--obs_dim = dataset.observation_dim
+--act_dim = dataset.action_dim
+--transition_dim = dataset.joined_dim
+--
+--#######################
+--######## model ########
+--#######################
+--
+--block_size = args.subsampled_sequence_length * transition_dim - 1
+--print(
+--    f'Dataset size: {len(dataset)} | '
+--    f'Joined dim: {transition_dim} '
+--    f'(observation: {obs_dim}, action: {act_dim}) | Block size: {block_size}'
+--)
+--
+--model_config = utils.Config(
+--    GPT,
+--    savepath=(args.savepath, 'model_config.pkl'),
+--    ## discretization
+--    vocab_size=args.N, block_size=block_size,
+--    ## architecture
+--    n_layer=args.n_layer, n_head=args.n_head, n_embd=args.n_embd*args.n_head,
+--    ## dimensions
+--    observation_dim=obs_dim, action_dim=act_dim, transition_dim=transition_dim,
+--    ## loss weighting
+--    action_weight=args.action_weight, reward_weight=args.reward_weight, value_weight=args.value_weight,
+--    ## dropout probabilities
+--    embd_pdrop=args.embd_pdrop, resid_pdrop=args.resid_pdrop, attn_pdrop=args.attn_pdrop,
+--)
+--
+--model = model_config()
+--model.to(args.device)
+--
+--#######################
+--####### trainer #######
+--#######################
+--
+--warmup_tokens = len(dataset) * block_size ## number of tokens seen per epoch
+--final_tokens = 20 * warmup_tokens
+--
+--trainer_config = utils.Config(
+--    utils.Trainer,
+--    savepath=(args.savepath, 'trainer_config.pkl'),
+--    # optimization parameters
+--    batch_size=args.batch_size,
+--    learning_rate=args.learning_rate,
+--    betas=(0.9, 0.95),
+--    grad_norm_clip=1.0,
+--    weight_decay=0.1, # only applied on matmul weights
+--    # learning rate decay: linear warmup followed by cosine decay to 10% of original
+--    lr_decay=args.lr_decay,
+--    warmup_tokens=warmup_tokens,
+--    final_tokens=final_tokens,
+--    ## dataloader
+--    num_workers=0,
+--    device=args.device,
+--)
+--
+--trainer = trainer_config()
+--
+--#######################
+--###### main loop ######
+--#######################
+--
+--## scale number of epochs to keep number of updates constant
+--n_epochs = int(1e6 / len(dataset) * args.n_epochs_ref)
+--save_freq = int(n_epochs // args.n_saves)
+--
+--for epoch in range(n_epochs):
+--    print(f'\nEpoch: {epoch} / {n_epochs} | {args.dataset} | {args.exp_name}')
+--
+--    trainer.train(model, dataset)
+--
+--    ## get greatest multiple of `save_freq` less than or equal to `save_epoch`
+--    save_epoch = (epoch + 1) // save_freq * save_freq
+--    statepath = os.path.join(args.savepath, f'state_{save_epoch}.pt')
+--    print(f'Saving model to {statepath}')
+--
+--    ## save state to disk
+--    state = model.state_dict()
+--    torch.save(state, statepath)
+-diff --git a/halfcheetah/scripts/xrl.py b/halfcheetah/scripts/xrl.py
+-deleted file mode 100644
+-index 134232a..0000000
+---- a/halfcheetah/scripts/xrl.py
+-+++ /dev/null
+-@@ -1,372 +0,0 @@
+--import json
+--import pdb
+--from os.path import join
++-import matplotlib.pyplot as plt
++-import d3rlpy
+ -
+ -import trajectory.utils as utils
+ -import trajectory.datasets as datasets
+@@ -852,112 +20278,21 @@ index 134232a..0000000
+ -)
+ -from trajectory.search.sampling import forward
+ -
+--import gym
+--import d4rl # Import required to register environments, you may need to also import the submodule
+--import numpy as np
+--import d3rlpy
+--import math as mt
+--from sklearn.cluster import KMeans
+--from sklearn import datasets as skdatasets
+ -from sklearn.decomposition import PCA
+--
+ -from pyclustering.cluster.xmeans import xmeans
+ -from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer
+--
+ -from scipy.stats import wasserstein_distance
++-from moviepy.editor import VideoFileClip
+ -
+ -class Parser(utils.Parser):
+--    dataset: str = 'halfcheetah-medium-expert-v2'
++-    dataset: str = 'halfcheetah-medium-v2'
+ -    config: str = 'config.offline'
+ -
+ -# utils
+--    
+--class XMeans:
+--    def loglikelihood(self, r, rn, var, m, k):
+--        l1 = - rn / 2.0 * mt.log(2 * mt.pi)
+--        l2 = - rn * m / 2.0 * mt.log(var)
+--        l3 = - (rn - k) / 2.0
+--        l4 = rn * mt.log(rn)
+--        l5 = - rn * mt.log(r)
+ -
+--        return l1 + l2 + l3 + l4 + l5
++-def cluster_trajectories(trajectories, n_clusters=10):
++-    """TODO"""
+ -
+--    def __init__(self, X, kmax = 20):
+--        self.X = X
+--        self.num = np.size(self.X, axis=0)
+--        self.dim = np.size(X, axis=1)
+--        self.KMax = kmax
+--
+--    def fit(self):
+--        k = 1
+--        X = self.X
+--        M = self.dim
+--        num = self.num
+--
+--        while(1):
+--            ok = k
+--
+--            #Improve Params
+--            kmeans = KMeans(n_clusters=k).fit(X)
+--            labels = kmeans.labels_
+--            m = kmeans.cluster_centers_
+--
+--            #Improve Structure
+--            #Calculate BIC
+--            p = M + 1
+--
+--            obic = np.zeros(k)
+--
+--            for i in range(k):
+--                rn = np.size(np.where(labels == i))
+--                var = np.sum((X[labels == i] - m[i])**2)/float(rn - 1)
+--                obic[i] = self.loglikelihood(rn, rn, var, M, 1) - p/2.0*mt.log(rn)
+--
+--            #Split each cluster into two subclusters and calculate BIC of each splitted cluster
+--            sk = 2 #The number of subclusters
+--            nbic = np.zeros(k)
+--            addk = 0
+--
+--            for i in range(k):
+--                ci = X[labels == i]
+--                r = np.size(np.where(labels == i))
+--
+--                kmeans = KMeans(n_clusters=sk).fit(ci)
+--                ci_labels = kmeans.labels_
+--                sm = kmeans.cluster_centers_
+--
+--                for l in range(sk):
+--                    rn = np.size(np.where(ci_labels == l))
+--                    var = np.sum((ci[ci_labels == l] - sm[l])**2)/float(rn - sk)
+--                    nbic[i] += self.loglikelihood(r, rn, var, M, sk)
+--
+--                p = sk * (M + 1)
+--                nbic[i] -= p/2.0*mt.log(r)
+--
+--                if obic[i] < nbic[i]:
+--                    addk += 1
+--
+--            k += addk
+--
+--            if ok == k or k >= self.KMax:
+--                break
+--
+--
+--        #Calculate labels and centroids
+--        kmeans = KMeans(n_clusters=k).fit(X)
+--        self.labels = kmeans.labels_
+--        self.k = k
+--        self.m = kmeans.cluster_centers_
+--
+--
+--def cluster_trajectories(trajectories):
+--    xmeans_instance = XMeans(trajectories, kmax=10)
+--    xmeans_instance.fit()
+--
+--    clusters = xmeans_instance.labels
+--    return clusters
+--
+--def cluster_trajectories_2(trajectories):
+ -    # Prepare initial centers - amount of initial centers defines amount of clusters from which X-Means will
+ -    # start analysis.
+ -    amount_initial_centers = 2
+@@ -965,7 +20300,7 @@ index 134232a..0000000
+ -    
+ -    # Create instance of X-Means algorithm. The algorithm will start analysis from 2 clusters, the maximum
+ -    # number of clusters that can be allocated is 10.
+--    xmeans_instance = xmeans(trajectories, initial_centers, 10)
++-    xmeans_instance = xmeans(trajectories, initial_centers, n_clusters)
+ -    xmeans_instance.process()
+ -    
+ -    # Extract clustering results: clusters
+@@ -978,19 +20313,27 @@ index 134232a..0000000
+ -
+ -    return idxs_per_cluster, np.array(clusters)
+ - 
+--# https://github.com/sascha-kirch/ML_Notebooks/blob/main/Softmax_Temperature.ipynb
+ -def softmax(x, temp):
+--    """Compute softmax values for each sets of scores in x."""
+--    return np.exp(np.divide(x,temp)) / np.sum(np.exp(np.divide(x,temp)))
++-    """TODO"""
++-    max_x = np.max(x)
++-    return np.exp(np.divide(x-max_x,temp)) / np.sum(np.exp(np.divide(x-max_x,temp)))
++-
++-def generate_data_embedding(trajectory_embeddings, temperature=10000):
++-    """TODO"""
+ -
+--def generate_data_embedding(trajectory_embeddings, normalizing_factor=1, temperature=1):
+--    embedding = np.sum(trajectory_embeddings, axis=0) / normalizing_factor
++-    embedding = np.sum(trajectory_embeddings, axis=0)
+ -    embedding = softmax(embedding, temperature)
++-    
++-
+ -    return embedding
+ -
+ -def embed_trajectory(gpt, discretizer, observations, actions, rewards, preprocess_fn):
++-    """TODO"""
++-
+ -    context = []
+ -
++-    output = []
++-
+ -    for i in range(len(observations)):
+ -        observation = observations[i]
+ -        action = actions[i]
+@@ -998,253 +20341,396 @@ index 134232a..0000000
+ -
+ -        observation = preprocess_fn(observation)
+ -
+--        # print(observation)
+ -        prefix = make_prefix(discretizer, context, observation, True)
+--        # print("prefix", prefix.shape)
+ -
+ -        out = forward(gpt, prefix)
+--        # print("out", out.shape)
++-
++-        if len(context) >= 9:
++-            context.pop(0)
++-            if len(output) == 0:
++-                output = out.detach().numpy()[0]
++-            else:
++-                output = np.concatenate((output, out.detach().numpy()[0][217:]), axis=0)
++-
+ -        context = update_context(context, discretizer, observation, action, reward, len(observations))
+--        # print("cotext", context)
+--    
+--    emb = []
+--    for context_step in context:
+--        emb.append(context_step.numpy())
+--    emb = np.array(emb)
+--    emb = np.mean(emb, axis=0)[0]
+ -
++-    emb = np.mean(output, axis=0)
+ -    return emb
+ -
++-def create_complementary_dataset(dataset, idxs, trajectory_length=10, inverse=False):
++-    """TODO"""
+ -
+--def create_complementary_dataset(dataset, idxs, trajectory_length=10):
+ -    observations = []
+ -    actions = []
+ -    rewards = []
+ -    terminals = []
+--    for i in range(1000):
+--        if i not in idxs:
+--            observations += list(dataset.observations[1000*i:1000*i+trajectory_length])
+--            actions += list(dataset.actions[1000*i:1000*i+trajectory_length])
+--            rewards += list(dataset.rewards[1000*i:1000*i+trajectory_length])
+--            terminals += list(dataset.terminals[1000*i:1000*i+trajectory_length])
++-
++-    n_trajs = int(1000000/trajectory_length)
++-    for i in range(n_trajs):
++-        condition = i not in idxs
++-        if inverse: condition = not condition
++-
++-        if condition:
++-            observations += list(dataset.observations[trajectory_length*i:trajectory_length*(i+1)])
++-            actions += list(dataset.actions[trajectory_length*i:trajectory_length*(i+1)])
++-            rewards += list(dataset.rewards[trajectory_length*i:trajectory_length*(i+1)])
++-
++-    terminals = np.tile([0]*(trajectory_length-1)+[1], int(len(observations)/trajectory_length))
+ -
+ -    new_dataset = d3rlpy.dataset.MDPDataset(
+ -        observations=np.array(observations),
+ -        actions=np.array(actions),
+ -        rewards=np.array(rewards),
+--        terminals=np.array(terminals)
++-        terminals=np.array(terminals),
+ -    )
+ -    return new_dataset
+ -    
+ -
++-def clusters_to_idxs(clusters):
++-    idxs_per_cluster = []
++-    for i in np.sort(np.unique(clusters)):
++-        idxs_per_cluster.append(list(np.argwhere(clusters == i).flatten()))
++-    
++-    return idxs_per_cluster
+ -
+ -
+ -def main():
+--    # args = Parser().parse_args('plan')
+--
+--    #######################
+--    ####### models ########
+--    #######################
+--
+--
+--
+--
+ -
+--    # print(args.dataset)
++-    args = Parser().parse_args('plan')
+ -
+--    # dataset = utils.load_from_config(args.logbase, args.dataset, args.gpt_loadpath,
+--    #         'data_config.pkl')
+ -
++-    ### DATASET ###
+ -
+--    # gpt, gpt_epoch = utils.load_model(args.logbase, args.dataset, args.gpt_loadpath,
+--    #         epoch=args.gpt_epoch, device=args.device)
+--
+--    # env = datasets.load_environment(args.dataset)
+--
+--    # discretizer = dataset.discretizer
+--
+--    # preprocess_fn = datasets.get_preprocess_fn(env.name)
+--
+--    # #######################
+--    # ####### dataset #######
+--    # #######################
++-    dataset_d3, env = d3rlpy.datasets.get_dataset("halfcheetah-medium-v2")
+ -
+--    # # env = datasets.load_environment(args.dataset)
+--    # discretizer = dataset.discretizer
+--    # preprocess_fn = datasets.get_preprocess_fn(env.name)
++-    ### IMPORTANT DEFINITIONS XRL SCRIPT ###
+ -
+--    # # dataset
+--    dataset_d3, env = d3rlpy.datasets.get_dataset("halfcheetah-medium-v2")
++-    load_embeddings = False
++-    load_clusters = True
++-    load_agents = True
++-    generate_human_study = False
++-    
++-    seed = 4 
++-    trajectory_length = 25 # 10 = max
++-    n_clusters = 10
++-    k = 3
++-    temperature = 10000
++-    logging_folder = f"results/v2_models_100k_{seed}"
++-    training_steps = 100000
+ -
+--    # env = gym.make('halfcheetah-medium-v2')
+--    # dataset_d4 = d4rl.qlearning_dataset(env)
++-    d3rlpy.seed(seed)
+ -
+--    # # checks to see if d3rl & d4rl datasets are equal
+--    # print(np.allclose(dataset_d3.actions[100], dataset_d4['actions'][100]))
++-    if load_embeddings:
++-        embeddings = np.load(f"{logging_folder}/embeddings.npy")
++-    else:
++-   
++-        ### TRAJECTORY TRANSFORMER ###
++-    
++-        dataset = utils.load_from_config(args.logbase, args.dataset, args.gpt_loadpath,
++-                'data_config.pkl')
++-        gpt, gpt_epoch = utils.load_model(args.logbase, args.dataset, args.gpt_loadpath,
++-                epoch=args.gpt_epoch, device=args.device)
++-        env = datasets.load_environment(args.dataset)
++-        discretizer = dataset.discretizer
++-        preprocess_fn = datasets.get_preprocess_fn(env.name)
++-    
++-        ### TRAJECTORY EMBEDDINGS ###
++-    
++-        embeddings = []
++-        n_trajs = int(1000000/trajectory_length)
++-        for i in range(n_trajs):
++-            observations = dataset_d3.observations[trajectory_length*i:trajectory_length*(i+1)]
++-            actions = dataset_d3.actions[trajectory_length*i:trajectory_length*(i+1)]
++-            rewards = dataset_d3.rewards[trajectory_length*i:trajectory_length*(i+1)]
++-            terminals = dataset_d3.terminals[trajectory_length*i:trajectory_length*(i+1)]
++-            emb = embed_trajectory(gpt, discretizer, observations, actions, rewards, preprocess_fn)
++-            embeddings.append(emb)
++-        embeddings = np.array(embeddings)
++-        np.save(f"{logging_folder}/embeddings.npy", embeddings)
+ -
+--    # # dr4rl has same trajectories, just cut off 1 element before the end
+--    # for j in range(1000):
+--    #     for i in range(999):
+--    #         if dataset_d4['rewards'][j * 999 + i] != dataset_d3.rewards[j * 1000 + i]: print("yo", i)
++-    print("embeddings ready")
+ -
+--    # #######################
+--    # ###### main loop ######
+--    # #######################
++-    ### TRAJECTORY CLUSTERS ###
+ -
+--    trajectory_length = 10 # 10 = max
++-    if load_clusters:
++-        clusters = np.load(f"{logging_folder}/clusters.npy")
++-        idxs_per_cluster = clusters_to_idxs(clusters)
++-    else:
++-        idxs_per_cluster, clusters = cluster_trajectories(embeddings, n_clusters)
++-        np.save(f"{logging_folder}/clusters.npy", clusters)
+ -
+--    # embeddings = []
+--    # for i in range(1000):
+--    #     observations = dataset_d3.observations[1000*i:1000*i+trajectory_length]
+--    #     actions = dataset_d3.actions[1000*i:1000*i+trajectory_length]
+--    #     rewards = dataset_d3.rewards[1000*i:1000*i+trajectory_length]
+--    #     terminals = dataset_d3.terminals[1000*i:1000*i+trajectory_length]
+--    #     emb = embed_trajectory(gpt, discretizer, observations, actions, rewards, preprocess_fn)
+--    #     embeddings.append(emb)
+--    # embeddings = np.array(embeddings)
+--    # np.save("embeddings.npy", embeddings)
+--    # print(embeddings)
++-    print("clusters ready")
+ -
+--    embeddings = np.load("embeddings.npy")
++-    ### PCA (solely for visualization) ###
++- 
++-    pca_idxs = np.random.choice(len(embeddings), 500, replace=False)
+ -
+ -    pca = PCA(n_components=2)
+--    pca = PCA(n_components=2)
+--    pca_embeddings = pca.fit_transform(embeddings)
+--    np.save("pca.py", pca_embeddings)
++-    pca_embeddings = pca.fit_transform(embeddings[pca_idxs])
++-    pca_clusters = clusters[pca_idxs]
+ -
+--    idxs_per_cluster, clusters = cluster_trajectories_2(embeddings)
+--    # print(clusters)
+--    # return
+--    np.save("clusters.npy", clusters)
++-    print("pca ready")
+ -
+--    import matplotlib.pyplot as plt
++-    ### COMPLEMENTARY DATASETS & CLUSTER EMBEDDINGS (also plotting PCA) ###
++-    d_orig = generate_data_embedding(embeddings, temperature=temperature)
+ -
+--    d_orig = generate_data_embedding(embeddings)
+ -    unique_clusters = np.unique(clusters)
+--    
+ -    d_j = []
+ -    complementary_datasets = []
++-    cluster_datasets = []
++-    fig, ax = plt.subplots(figsize=(5,4))
+ -    for j in np.sort(unique_clusters):
+--        print(j)
+--        d_j.append(generate_data_embedding(embeddings[clusters != j]))
+--        plt.scatter(pca_embeddings[clusters == j][:,0], pca_embeddings[clusters == j][:,1], label=j)
++-        d_j.append(generate_data_embedding(embeddings[clusters != j], temperature=temperature))
++-        ax.scatter(pca_embeddings[pca_clusters == j][:,0], pca_embeddings[pca_clusters == j][:,1], label=j)
+ -        complementary_datasets.append(create_complementary_dataset(dataset_d3, idxs_per_cluster[j], trajectory_length))
++-        cluster_datasets.append(create_complementary_dataset(dataset_d3, idxs_per_cluster[j], trajectory_length, inverse=True))
+ -    
+ -    original_dataset = create_complementary_dataset(dataset_d3, [], trajectory_length)
+ -
+--    print(complementary_datasets, original_dataset)
++-    ax.legend(title="$c_j$", bbox_to_anchor=(0.5, 1.2), loc="lower center", ncol=5)
++-    ax.set_xlabel("feature 1")
++-    ax.set_ylabel("feature 2")
++-    ax.spines['top'].set_visible(False)
++-    ax.spines['right'].set_visible(False)
++-    plt.title("Trajectory Clustering HalfCheetah")
++-    plt.tight_layout()
++-
++-    plt.savefig(f"{logging_folder}/pca.pdf")
++-
++-    print("complementary datasets ready")
+ -
+--    plt.legend()
+--    plt.show()
++-    ### AGENT TRAINING (original & complementary) ###
+ -
+ -    agent_orig = d3rlpy.algos.SAC(
+ -        actor_learning_rate=3e-4,
+ -        critic_learning_rate=3e-4,
+ -        temp_learning_rate=3e-4,
+--        batch_size=256)
++-        batch_size=512)
+ -
+--    print(agent_orig)
+--
+--    training_steps = 1000
+--
+--    agent_orig.fit(original_dataset, n_steps=training_steps)
++-    if load_agents:
++-        agent_orig.build_with_dataset(original_dataset)
++-        agent_orig.load_model(f"{logging_folder}/agent_orig.pt")
++-    else:
++-        agent_orig.fit(original_dataset, n_steps=training_steps)
++-        agent_orig.save_model(f"{logging_folder}/agent_orig.pt")
+ -
+ -    agents_compl = []
+ -
+--    for dset in complementary_datasets:
++-    for i in range(len(complementary_datasets)):
+ -        agent = d3rlpy.algos.SAC(
+ -            actor_learning_rate=3e-4,
+ -            critic_learning_rate=3e-4,
+ -            temp_learning_rate=3e-4,
+--            batch_size=256)
+--        agent.fit(dset, n_steps=training_steps)
++-            batch_size=512)
++-        if load_agents:
++-            agent.build_with_dataset(complementary_datasets[i])
++-            agent.load_model(f"{logging_folder}/agent_compl_{i}.pt")
++-        else:
++-            agent.fit(complementary_datasets[i], n_steps=training_steps)
++-            agent.save_model(f"{logging_folder}/agent_compl_{i}.pt")
+ -        agents_compl.append(agent)
+ -
+--    action_orig = agent_orig.predict(dataset_d3.observations[0])
++-    print("agents ready")
+ -
+--    actions_compl = []
+--    for agent in agents_compl:
+--        actions_compl.append(agent.predict(dataset_d3.observations[0]))
+--    
+--    action_dists = []
+--    for action in actions_compl:
+--        action_dists.append(np.linalg.norm(action_orig-action))
++-    ### OBSERVATION EXPLANATION (cluster assignment) ###
+ -
+--    k = 3
+--    topk = np.argpartition(action_dists, -k)[-k:]
++-    original_state = np.random.get_state()
++-    np.random.seed(0)
++-    idxs_to_explain = np.random.choice(range(len(dataset_d3.observations)), 1000, replace=False)
++-    np.random.set_state(original_state)
+ -
+--    d_w = {}
+--    for idx in topk:
+--        d_w[idx] = wasserstein_distance(d_j[idx], d_orig)
++-    observations_to_explain = [dataset_d3.observations[i] for i in idxs_to_explain] 
+ -
+--    cluster_assignment = min(d_w, key=d_w.get)
+--    print("explanation assigned to cluster", cluster_assignment)
++-    ISVE = []
++-    ISVE_orig = 0.
++-    LMAAVD = []
++-    ACM = []
++-    NWD = []
++-    CAF = [0] * len(d_j)
+ -
+--    
+--def assignment_test():
+--    action_orig = np.random.rand(10)
+--    d_orig = np.random.rand(5)
++-    if generate_human_study:
++-        ctr = 0
++-        unrelated_idxs = [690, 1520, 3030, 6050, 7080, 8030]
++-        if not os.path.isdir(f"{logging_folder}/mp4s"): os.mkdir(f"{logging_folder}/mp4s")
++-        if not os.path.isdir(f"{logging_folder}/gifs"): os.mkdir(f"{logging_folder}/gifs")
+ -
+--    actions_compl = np.random.rand(6,10)
+--    d_j = np.random.rand(6,5)
++-    for observation_to_explain in observations_to_explain:
++-        action_orig = agent_orig.predict([observation_to_explain])
+ -
+--    action_dists = []
+--    for action in actions_compl:
+--        action_dists.append(np.linalg.norm(action_orig-action))
++-        actions_compl = []
++-        for agent in agents_compl:
++-            actions_compl.append(agent.predict([observation_to_explain]))
+ -
+--    print(action_dists)
++-        action_dists = []
++-        for action in actions_compl:
++-            action_dists.append(np.linalg.norm(action_orig-action))
+ -
+--    k = 3
+--    topk = np.argpartition(action_dists, -k)[-k:]
++-        topk = np.argpartition(action_dists, -k)[-k:]
++-
++-        d_w = {}
++-        for idx in topk:
++-            d_w[idx] = wasserstein_distance(d_j[idx], d_orig)
++-
++-        cluster_assignment = min(d_w, key=d_w.get)
++-
++-        ### OBSERVATION EXPLANATION (representing cluster with 1 trajectory) ###
++-
++-        distances_to_obs = [np.linalg.norm(observation_to_explain-obs) for obs in cluster_datasets[cluster_assignment].observations]
++-        trajectory_to_assign = np.floor(np.argmin(distances_to_obs) / trajectory_length)
++-        assigned_trajectory = np.arange(trajectory_to_assign * trajectory_length, (trajectory_to_assign+1) * trajectory_length)
+ -
+--    print(topk)
++-        ### OBSERVATION EXPLANATION (metrics) ###
+ -
+--    d_w = {}
+--    for idx in topk:
+--        d_w[idx] = wasserstein_distance(d_j[idx], d_orig)
++-        # Initial State Value Estimate
++- 
++-        V_s = 0.
++-        for _ in range(10):
++-            sampled_action = agent_orig.sample_action([observation_to_explain])
++-            Q_sa = agent_orig.predict_value([observation_to_explain], [sampled_action[0]])[0]
++-            V_s += Q_sa
++-        ISVE_orig += V_s/10
++- 
++-        new_ISVE = []
++-        for agent in agents_compl:
++-            V_s = 0.
++-            for _ in range(10):
++-                sampled_action = agent.sample_action([observation_to_explain])
++-                Q_sa = agent.predict_value([observation_to_explain], [sampled_action[0]])[0]
++-                V_s += Q_sa
++-            new_ISVE.append(V_s/10)
++-        ISVE.append(new_ISVE)
++- 
++-        # Local Mean Absolute Action-Value Difference
++-        Q_orig = agent_orig.predict_value([observation_to_explain], [action_orig[0]])
++-        Q_j = [agent_orig.predict_value([observation_to_explain], [ac[0]]) for ac in actions_compl]
++-        LMAAVD.append(np.abs(np.array(Q_j) - Q_orig).flatten())
++- 
++-        # Action Contrast Measure
++-        ACM.append(action_dists)
++- 
++-        # Normalized Wasserstein distance (between cluster embeddings)
++-        wasser = np.array([wasserstein_distance(d, d_orig) for d in d_j])
++-        NWD.append(list((wasser-np.min(wasser))/(np.max(wasser)-np.min(wasser))))
++- 
++-        # Cluster attribution frequency
++-        CAF[cluster_assignment] += 1
++- 
++-        if generate_human_study:
++-            ### RENDERING ###
++-            if not os.path.isdir(f"{logging_folder}/gifs/question_{ctr}"): os.mkdir(f"{logging_folder}/gifs/question_{ctr}")
+ -
+--    print(d_w)
++-            rollout = dataset_d3.observations[idxs_to_explain[ctr]-25:idxs_to_explain[ctr]]
++-            print(idxs_to_explain[ctr])
++-            print(rollout)
++-            renderer = utils.make_renderer(args)
++-            rollout_savepath = f"{logging_folder}/mp4s/question_{ctr}/traj_to_explain.mp4"
++-            renderer.render_rollout(rollout_savepath, rollout, fps=10)
++-            videoClip = VideoFileClip(f"{logging_folder}/mp4s/question_{ctr}/traj_to_explain.mp4")
++-            videoClip.write_gif(f"{logging_folder}/gifs/question_{ctr}/traj_to_explain.gif")
+ -
+--    cluster_assignment = min(d_w, key=d_w.get)
+--    print("explanation assigned to cluster", cluster_assignment)
++-
++-            rollout = cluster_datasets[cluster_assignment].observations[assigned_trajectory.astype(int)]
++-            print(rollout)
++-            renderer = utils.make_renderer(args)
++-            rollout_savepath = f"{logging_folder}/mp4s/question_{ctr}/traj_assigned_cluster_attr.mp4"
++-            renderer.render_rollout(rollout_savepath, rollout, fps=10)
++-            videoClip = VideoFileClip(f"{logging_folder}/mp4s/question_{ctr}/traj_assigned_cluster_attr.mp4")
++-            videoClip.write_gif(f"{logging_folder}/gifs/question_{ctr}/traj_assigned_cluster_attr.gif")
++-
++-            random_trajs = np.random.randint(len(cluster_datasets[cluster_assignment])//25, size=3)
++-
++-            rollout = cluster_datasets[cluster_assignment].observations[random_trajs[0]*25:(random_trajs[0]+1)*25]
++-            print(rollout)
++-            renderer = utils.make_renderer(args)
++-            rollout_savepath = f"{logging_folder}/mp4s/question_{ctr}/traj_assigned_cluster_1.mp4"
++-            renderer.render_rollout(rollout_savepath, rollout, fps=10)
++-            videoClip = VideoFileClip(f"{logging_folder}/mp4s/question_{ctr}/traj_assigned_cluster_1.mp4")
++-            videoClip.write_gif(f"{logging_folder}/gifs/question_{ctr}/traj_assigned_cluster_1.gif")
++-
++-            rollout = cluster_datasets[cluster_assignment].observations[random_trajs[1]*25:(random_trajs[1]+1)*25]
++-            print(rollout)
++-            renderer = utils.make_renderer(args)
++-            rollout_savepath = f"{logging_folder}/mp4s/question_{ctr}/traj_assigned_cluster_2.mp4"
++-            renderer.render_rollout(rollout_savepath, rollout, fps=10)
++-            videoClip = VideoFileClip(f"{logging_folder}/mp4s/question_{ctr}/traj_assigned_cluster_2.mp4")
++-            videoClip.write_gif(f"{logging_folder}/gifs/question_{ctr}/traj_assigned_cluster_2.gif")
++-
++-
++-            rollout = cluster_datasets[cluster_assignment].observations[random_trajs[2]*25:(random_trajs[2]+1)*25]
++-            print(rollout)
++-            renderer = utils.make_renderer(args)
++-            rollout_savepath = f"{logging_folder}/mp4s/question_{ctr}/traj_assigned_cluster_3.mp4"
++-            renderer.render_rollout(rollout_savepath, rollout, fps=10)
++-            videoClip = VideoFileClip(f"{logging_folder}/mp4s/question_{ctr}/traj_assigned_cluster_3.mp4")
++-            videoClip.write_gif(f"{logging_folder}/gifs/question_{ctr}/traj_assigned_cluster_3.gif")
++-
++-
++-            different_cluster = 0 if cluster_assignment != 0 else 1
++-            random_trajs = np.random.randint(len(cluster_datasets[different_cluster])//25, size=3)
++-            rollout = cluster_datasets[different_cluster].observations[random_trajs[2]*25:(random_trajs[2]+1)*25]
++-            print(rollout)
++-            renderer = utils.make_renderer(args)
++-            rollout_savepath = f"{logging_folder}/mp4s/question_{ctr}/traj_different_cluster.mp4"
++-            renderer.render_rollout(rollout_savepath, rollout, fps=10)
++-            videoClip = VideoFileClip(f"{logging_folder}/mp4s/question_{ctr}/traj_different_cluster.mp4")
++-            videoClip.write_gif(f"{logging_folder}/gifs/question_{ctr}/traj_different_cluster.gif")
++-
++-            rollout = dataset_d3.observations[unrelated_idxs[ctr]-25:unrelated_idxs[ctr]]
++-            print(rollout)
++-            renderer = utils.make_renderer(args)
++-            rollout_savepath = f"{logging_folder}/mp4s/question_{ctr}/traj_unrelated.mp4"
++-            renderer.render_rollout(rollout_savepath, rollout, fps=10)
++-            videoClip = VideoFileClip(f"{logging_folder}/mp4s/question_{ctr}/traj_unrelated.mp4")
++-            videoClip.write_gif(f"{logging_folder}/gifs/question_{ctr}/traj_unrelated.gif")
++-
++-            ctr += 1
++-
++-    ### RESULTS ###
++-    ISVE_orig /= len(observations_to_explain)
++-    ISVE = np.mean(ISVE, axis=0)
++-    LMAAVD = np.mean(LMAAVD, axis=0)
++-    ACM = np.mean(ACM, axis=0)
++-    NWD = np.mean(NWD, axis=0)
++-    CAF = np.array(CAF) / np.sum(CAF)
++-
++-    print("ISVE orig:", ISVE_orig)
++-    print("ISVE:",ISVE)
++-    print("LMAAVD:",LMAAVD)
++-    print("ACM:",ACM)
++-    print("NWD:",NWD)
++-    print("CAF:",CAF)
+ -
+ -
+ -if __name__ == "__main__":
+--    # main()
+--    assignment_test()
+-diff --git a/halfcheetah/trajectory.egg-info/PKG-INFO b/halfcheetah/trajectory.egg-info/PKG-INFO
+-index 452c6cb..2603850 100644
+---- a/halfcheetah/trajectory.egg-info/PKG-INFO
+-+++ b/halfcheetah/trajectory.egg-info/PKG-INFO
+-@@ -1,4 +1,11 @@
+- Metadata-Version: 2.1
+- Name: trajectory
+- Version: 0.0.0
+-+Summary: UNKNOWN
+-+Home-page: UNKNOWN
+-+License: UNKNOWN
+-+Platform: UNKNOWN
+- License-File: LICENSE
+-+
+-+UNKNOWN
+-+
++-    main()
+ diff --git a/halfcheetah/trajectory.egg-info/SOURCES.txt b/halfcheetah/trajectory.egg-info/SOURCES.txt
+-index 4474d85..84e8e3a 100644
++index 84e8e3a..4474d85 100644
+ --- a/halfcheetah/trajectory.egg-info/SOURCES.txt
+ +++ b/halfcheetah/trajectory.egg-info/SOURCES.txt
+-@@ -30,4 +30,5 @@ trajectory/utils/serialization.py
++@@ -30,5 +30,4 @@ trajectory/utils/serialization.py
+  trajectory/utils/setup.py
+  trajectory/utils/timer.py
+  trajectory/utils/training.py
+ -trajectory/utils/video.py
++-trajectory_aaa/__init__.py
+ \ No newline at end of file
+ +trajectory/utils/video.py
+-+trajectory_aaa/__init__.py
+ \ No newline at end of file
+ diff --git a/halfcheetah/trajectory.egg-info/top_level.txt b/halfcheetah/trajectory.egg-info/top_level.txt
+-index ce65198..1d5271f 100644
++index 1d5271f..ce65198 100644
+ --- a/halfcheetah/trajectory.egg-info/top_level.txt
+ +++ b/halfcheetah/trajectory.egg-info/top_level.txt
+-@@ -1 +1,2 @@
++@@ -1,2 +1 @@
+  trajectory
+-+trajectory_aaa
 \ No newline at end of file
-diff --git a/halfcheetah/pca.py.npy b/halfcheetah/pca.py.npy
-deleted file mode 100644
-index bb19150..0000000
-Binary files a/halfcheetah/pca.py.npy and /dev/null differ
-diff --git a/halfcheetah/plotting/bar.png b/halfcheetah/plotting/bar.png
-deleted file mode 100644
-index 3679667..0000000
-Binary files a/halfcheetah/plotting/bar.png and /dev/null differ
-diff --git a/halfcheetah/plotting/plot.py b/halfcheetah/plotting/plot.py
-deleted file mode 100644
-index 163d0e4..0000000
---- a/halfcheetah/plotting/plot.py
-+++ /dev/null
-@@ -1,74 +0,0 @@
--import numpy as np
--import matplotlib
--import matplotlib.pyplot as plt
--import pdb
--
--from plotting.scores import means
--
--class Colors:
--	grey = '#B4B4B4'
--	gold = '#F6C781'
--	red = '#EC7C7D'
--	blue = '#70ABCC'
--
--LABELS = {
--	# 'BC': 'Behavior\nCloning',
--	# 'MBOP': 'Model-Based\nOffline Planning',
--	# 'BRAC': 'Behavior-Reg.\nActor-Critic',
--	# 'CQL': 'Conservative\nQ-Learning',
--}
--
--def get_mean(results, exclude=None):
--	'''
--		results : { environment: score, ... }
--	'''
--	filtered = {
--		k: v for k, v in results.items()
--		if (not exclude) or (exclude and exclude not in k)
--	}
--	return np.mean(list(filtered.values()))
--
--if __name__ == '__main__':
--
--	#################
--	## latex
--	#################
--	matplotlib.rc('font', **{'family': 'serif', 'serif': ['Computer Modern']})
--	matplotlib.rc('text', usetex=True)
--	matplotlib.rcParams['text.latex.preamble']=[r"\usepackage{amsmath}"]
--	#################
--
--	fig = plt.gcf()
--	ax = plt.gca()
--	fig.set_size_inches(7.5, 2.5)
--
--	means = {k: get_mean(v, exclude='ant') for k, v in means.items()}
--	print(means)
--
--	algs = ['BC', 'MBOP', 'BRAC', 'CQL', 'Decision\nTransformer', 'Trajectory\nTransformer']
--	vals = [means[alg] for alg in algs]
--
--	colors = [
--		Colors.grey, Colors.gold,
--		Colors.red, Colors.red, Colors.blue, Colors.blue
--	]
--
--	labels = [LABELS.get(alg, alg) for alg in algs]
--	plt.bar(labels, vals, color=colors, edgecolor=Colors.gold, lw=0)
--	plt.ylabel('Average normalized return', labelpad=15)
--	# plt.title('Offline RL Results')
--
--	legend_labels = ['Behavior Cloning', 'Trajectory Optimization', 'Temporal Difference', 'Sequence Modeling']
--	colors = [Colors.grey, Colors.gold, Colors.red, Colors.blue]
--	handles = [plt.Rectangle((0,0),1,1, color=color) for label, color in zip(legend_labels, colors)]
--	plt.legend(handles, legend_labels, ncol=4,
--		bbox_to_anchor=(1.07, -.18), fancybox=False, framealpha=0, shadow=False, columnspacing=1.5, handlelength=1.5)
--
--	matplotlib.rcParams['hatch.linewidth'] = 7.5
--	# ax.patches[-1].set_hatch('/')
--
--	ax.spines['right'].set_visible(False)
--	ax.spines['top'].set_visible(False)
--
--	# plt.savefig('plotting/bar.pdf', bbox_inches='tight')
--	plt.savefig('plotting/bar.png', bbox_inches='tight', dpi=500)
-diff --git a/halfcheetah/plotting/read_results.py b/halfcheetah/plotting/read_results.py
-deleted file mode 100644
-index 5a5fb62..0000000
---- a/halfcheetah/plotting/read_results.py
-+++ /dev/null
-@@ -1,70 +0,0 @@
--import os
--import glob
--import numpy as np
--import json
--import pdb
--
--import trajectory.utils as utils
--
--DATASETS = [
--	f'{env}-{buffer}'
--	for env in ['hopper', 'walker2d', 'halfcheetah', 'ant']
--	for buffer in ['medium-expert-v2', 'medium-v2', 'medium-replay-v2']
--]
--
--LOGBASE = 'logs'
--TRIAL = '*'
--EXP_NAME = 'plans/pretrained'
--
--def load_results(paths):
--	'''
--		paths : path to directory containing experiment trials
--	'''
--	scores = []
--	for i, path in enumerate(sorted(paths)):
--		score = load_result(path)
--		if score is None:
--			print(f'Skipping {path}')
--			continue
--		scores.append(score)
--
--		suffix = path.split('/')[-1]
--
--	mean = np.mean(scores)
--	err = np.std(scores) / np.sqrt(len(scores))
--	return mean, err, scores
--
--def load_result(path):
--	'''
--		path : path to experiment directory; expects `rollout.json` to be in directory
--	'''
--	fullpath = os.path.join(path, 'rollout.json')
--	suffix = path.split('/')[-1]
--
--	if not os.path.exists(fullpath):
--		return None
--
--	results = json.load(open(fullpath, 'rb'))
--	score = results['score']
--	return score * 100
--
--#######################
--######## setup ########
--#######################
--
--if __name__ == '__main__':
--
--	class Parser(utils.Parser):
--	    dataset: str = None
--
--	args = Parser().parse_args()
--
--	for dataset in ([args.dataset] if args.dataset else DATASETS):
--		subdirs = glob.glob(os.path.join(LOGBASE, dataset, EXP_NAME))
--
--		for subdir in subdirs:
--			reldir = subdir.split('/')[-1]
--			paths = glob.glob(os.path.join(subdir, TRIAL))
--
--			mean, err, scores = load_results(paths)
--			print(f'{dataset.ljust(30)} | {subdir.ljust(50)} | {len(scores)} scores \n    {mean:.2f} +/- {err:.2f}\n')
-diff --git a/halfcheetah/plotting/scores.py b/halfcheetah/plotting/scores.py
-deleted file mode 100644
-index f1917f7..0000000
---- a/halfcheetah/plotting/scores.py
-+++ /dev/null
-@@ -1,123 +0,0 @@
--means = {
--	'Trajectory\nTransformer': {
--		##
--		'halfcheetah-medium-expert-v2': 95.0,
--		'hopper-medium-expert-v2': 110.0,
--		'walker2d-medium-expert-v2': 101.9,
--		'ant-medium-expert-v2': 116.1,
--		##
--		'halfcheetah-medium-v2': 46.9,
--		'hopper-medium-v2': 61.1,
--		'walker2d-medium-v2': 79.0,
--		'ant-medium-v2': 83.1,
--		##
--		'halfcheetah-medium-replay-v2': 41.9,
--		'hopper-medium-replay-v2': 91.5,
--		'walker2d-medium-replay-v2': 82.6,
--		'ant-medium-replay-v2': 77.0,
--	},
--	'Decision\nTransformer': {
--		##
--		'halfcheetah-medium-expert-v2': 86.8,
--		'hopper-medium-expert-v2': 107.6,
--		'walker2d-medium-expert-v2': 108.1,
--		##
--		'halfcheetah-medium-v2': 42.6,
--		'hopper-medium-v2': 67.6,
--		'walker2d-medium-v2': 74.0,
--		##
--		'halfcheetah-medium-replay-v2': 36.6,
--		'hopper-medium-replay-v2': 82.7,
--		'walker2d-medium-replay-v2': 66.6,
--	},
--	'CQL': {
--		##
--		'halfcheetah-medium-expert-v2': 91.6,
--		'hopper-medium-expert-v2': 105.4,
--		'walker2d-medium-expert-v2': 108.8,
--		##
--		'halfcheetah-medium-v2': 44.0,
--		'hopper-medium-v2': 58.5,
--		'walker2d-medium-v2': 72.5,
--		##
--		'halfcheetah-medium-replay-v2': 45.5,
--		'hopper-medium-replay-v2': 95.0,
--		'walker2d-medium-replay-v2': 77.2,
--	},
--	'MOPO': {
--		##
--		'halfcheetah-medium-expert-v2': 63.3,
--		'hopper-medium-expert-v2': 23.7,
--		'walker2d-medium-expert-v2': 44.6,
--		##
--		'halfcheetah-medium-v2': 42.3,
--		'hopper-medium-v2': 28.0,
--		'walker2d-medium-v2': 17.8,
--		##
--		'halfcheetah-medium-replay-v2': 53.1,
--		'hopper-medium-replay-v2': 67.5,
--		'walker2d-medium-replay-v2':39.0,
--	},
--	'MBOP': {
--		##
--		'halfcheetah-medium-expert-v2': 105.9,
--		'hopper-medium-expert-v2': 55.1,
--		'walker2d-medium-expert-v2': 70.2,
--		##
--		'halfcheetah-medium-v2': 44.6,
--		'hopper-medium-v2': 48.8,
--		'walker2d-medium-v2': 41.0,
--		##
--		'halfcheetah-medium-replay-v2': 42.3,
--		'hopper-medium-replay-v2': 12.4,
--		'walker2d-medium-replay-v2': 9.7,
--	},
--	'BRAC': {
--		##
--		'halfcheetah-medium-expert-v2': 41.9,
--		'hopper-medium-expert-v2': 0.9,
--		'walker2d-medium-expert-v2': 81.6,
--		##
--		'halfcheetah-medium-v2': 46.3,
--		'hopper-medium-v2': 31.3,
--		'walker2d-medium-v2': 81.1,
--		##
--		'halfcheetah-medium-replay-v2': 47.7,
--		'hopper-medium-replay-v2': 0.6,
--		'walker2d-medium-replay-v2': 0.9,
--	},
--	'BC': {
--		##
--		'halfcheetah-medium-expert-v2': 59.9,
--		'hopper-medium-expert-v2': 79.6,
--		'walker2d-medium-expert-v2': 36.6,
--		##
--		'halfcheetah-medium-v2': 43.1,
--		'hopper-medium-v2': 63.9,
--		'walker2d-medium-v2': 77.3,
--		##
--		'halfcheetah-medium-replay-v2': 4.3,
--		'hopper-medium-replay-v2': 27.6,
--		'walker2d-medium-replay-v2': 36.9,
--	},
--}
--
--errors = {
--	'Trajectory\nTransformer': {
--		##
--		'halfcheetah-medium-expert-v2': 0.2,
--		'hopper-medium-expert-v2': 2.7,
--		'walker2d-medium-expert-v2': 6.8,
--		'ant-medium-expert-v2': 9.0,
--		##
--		'halfcheetah-medium-v2': 0.4,
--		'hopper-medium-v2': 3.6,
--		'walker2d-medium-v2': 2.8,
--		'ant-medium-v2': 7.3,
--		##
--		'halfcheetah-medium-replay-v2': 2.5,
--		'hopper-medium-replay-v2': 3.6,
--		'walker2d-medium-replay-v2': 6.9,
--		'ant-medium-replay-v2': 6.8,
--	},
--}
++-trajectory_aaa
++diff --git a/seaquest/readme.md b/seaquest/readme.md
++index 84e53f8..53561f9 100644
++--- a/seaquest/readme.md
+++++ b/seaquest/readme.md
++@@ -10,4 +10,4 @@ pip install git+https://github.com/takuseno/d4rl-atari
++ pip install "gym[atari, accept-rom-license]"
++ pip install pyclustering
++ pip install seaborn
++-pip install d3rlpy==1.1.1
++\ No newline at end of file
+++pip install d3rlpy==1.1.1
 \ No newline at end of file
-diff --git a/halfcheetah/plotting/table.py b/halfcheetah/plotting/table.py
+diff --git a/halfcheetah/scripts/xrl_v2.py b/halfcheetah/scripts/xrl_v2.py
 deleted file mode 100644
-index eae74e6..0000000
---- a/halfcheetah/plotting/table.py
+index 62a3d4d..0000000
+--- a/halfcheetah/scripts/xrl_v2.py
 +++ /dev/null
-@@ -1,127 +0,0 @@
--import numpy as np
--import pdb
--
--from plotting.plot import get_mean
--from plotting.scores import (
--	means as MEANS,
--	errors as ERRORS,
--)
--
--ALGORITHM_STRINGS = {
--	'Trajectory\nTransformer': 'TT (Ours)',
--	'Decision\nTransformer': 'DT',	
--}
--
--BUFFER_STRINGS = {
--	'medium-expert': 'Medium-Expert',
--	'medium': 'Medium',
--	'medium-replay': 'Medium-Replay',	
--}
--
--ENVIRONMENT_STRINGS = {
--	'halfcheetah': 'HalfCheetah',
--	'hopper': 'Hopper',
--	'walker2d': 'Walker2d',
--	'ant': 'Ant',
--}
--
--SHOW_ERRORS = ['Trajectory\nTransformer']
--
--def get_result(algorithm, buffer, environment, version='v2'):
--	key = f'{environment}-{buffer}-{version}'
--	mean = MEANS[algorithm].get(key, '-')
--	if algorithm in SHOW_ERRORS:
--		error = ERRORS[algorithm].get(key)
--		return (mean, error)
--	else:
--		return mean
--
--def format_result(result):
--	if type(result) == tuple:
--		mean, std = result
--		return f'${mean}$ \\scriptsize{{\\raisebox{{1pt}}{{$\\pm {std}$}}}}'
--	else:
--		return f'${result}$'
--
--def format_row(buffer, environment, results):
--	buffer_str = BUFFER_STRINGS[buffer]
--	environment_str = ENVIRONMENT_STRINGS[environment]
--	results_str = ' & '.join(format_result(result) for result in results)
--	row = f'{buffer_str} & {environment_str} & {results_str} \\\\ \n'
--	return row
--
--def format_buffer_block(algorithms, buffer, environments):
--	block_str = '\\midrule\n'
--	for environment in environments:
--		results = [get_result(alg, buffer, environment) for alg in algorithms]
--		row_str = format_row(buffer, environment, results)
--		block_str += row_str
--	return block_str
--
--def format_algorithm(algorithm):
--	algorithm_str = ALGORITHM_STRINGS.get(algorithm, algorithm)
--	return f'\multicolumn{{1}}{{c}}{{\\bf {algorithm_str}}}'
--
--def format_algorithms(algorithms):
--	return ' & '.join(format_algorithm(algorithm) for algorithm in algorithms)
--
--def format_averages(means, label):
--	prefix = f'\\multicolumn{{2}}{{c}}{{\\bf Average ({label})}} & '
--	formatted = ' & '.join(str(mean) for mean in means)
--	return prefix + formatted
--
--def format_averages_block(algorithms):
--	means_filtered = [np.round(get_mean(MEANS[algorithm], exclude='ant'), 1) for algorithm in algorithms]
--	means_all = [np.round(get_mean(MEANS[algorithm], exclude=None), 1) for algorithm in algorithms]
--
--	means_all = [
--		means
--		if 'ant-medium-expert-v2' in MEANS[algorithm]
--		else '$-$'
--		for algorithm, means in zip(algorithms, means_all)
--	]
--
--	formatted_filtered = format_averages(means_filtered, 'without Ant')
--	formatted_all = format_averages(means_all, 'all settings')
--
--	formatted_block = (
--		f'{formatted_filtered} \\hspace{{.6cm}} \\\\ \n'
--		f'{formatted_all} \\hspace{{.6cm}} \\\\ \n'
--	)
--	return formatted_block
--
--def format_table(algorithms, buffers, environments):
--	justify_str = 'll' + 'r' * len(algorithms)
--	algorithm_str = format_algorithms(['Dataset', 'Environment'] + algorithms)
--	averages_str = format_averages_block(algorithms)
--	table_prefix = (
--		'\\begin{table*}[h]\n'
--		'\\centering\n'
--		'\\small\n'
--		f'\\begin{{tabular}}{{{justify_str}}}\n'
--		'\\toprule\n'
--		f'{algorithm_str} \\\\ \n'
--	)
--	table_suffix = (
--		'\\midrule\n'
--		f'{averages_str}'
--		'\\bottomrule\n'
--		'\\end{tabular}\n'
--		'\\label{table:d4rl}\n'
--		'\\end{table*}'
--	)
--	blocks = ''.join(format_buffer_block(algorithms, buffer, environments) for buffer in buffers)
--	table = (
--		f'{table_prefix}'
--		f'{blocks}'
--		f'{table_suffix}'
--	)
--	return table
--
--
--algorithms =['BC', 'MBOP', 'BRAC', 'CQL',  'Decision\nTransformer', 'Trajectory\nTransformer']
--buffers = ['medium-expert', 'medium', 'medium-replay']
--environments = ['halfcheetah', 'hopper', 'walker2d', 'ant']
--
--table = format_table(algorithms, buffers, environments)
--print(table)
-diff --git a/halfcheetah/scripts/plan.py b/halfcheetah/scripts/plan.py
-deleted file mode 100644
-index f13d4cc..0000000
---- a/halfcheetah/scripts/plan.py
-+++ /dev/null
-@@ -1,124 +0,0 @@
--import json
--import pdb
--from os.path import join
--
--import trajectory.utils as utils
--import trajectory.datasets as datasets
--from trajectory.search import (
--    beam_plan,
--    make_prefix,
--    extract_actions,
--    update_context,
--)
--
--class Parser(utils.Parser):
--    dataset: str = 'halfcheetah-medium-expert-v2'
--    config: str = 'config.offline'
--
--#######################
--######## setup ########
--#######################
--
--args = Parser().parse_args('plan')
--
--#######################
--####### models ########
--#######################
--
--dataset = utils.load_from_config(args.logbase, args.dataset, args.gpt_loadpath,
--        'data_config.pkl')
--
--gpt, gpt_epoch = utils.load_model(args.logbase, args.dataset, args.gpt_loadpath,
--        epoch=args.gpt_epoch, device=args.device)
--
--#######################
--####### dataset #######
--#######################
--
--env = datasets.load_environment(args.dataset)
--print('yo')
--renderer = utils.make_renderer(args)
--timer = utils.timer.Timer()
--
--discretizer = dataset.discretizer
--discount = dataset.discount
--observation_dim = dataset.observation_dim
--action_dim = dataset.action_dim
--
--value_fn = lambda x: discretizer.value_fn(x, args.percentile)
--preprocess_fn = datasets.get_preprocess_fn(env.name)
--
--print('yo2')
--
--#######################
--###### main loop ######
--#######################
--
--observation = env.reset()
--total_reward = 0
--
--## observations for rendering
--rollout = [observation.copy()]
+@@ -1,442 +0,0 @@
+-"""
+-TODO HEADER
+-"""
 -
--## previous (tokenized) transitions for conditioning transformer
--context = []
--
--T = env.max_episode_steps
--for t in range(T):
--
--    observation = preprocess_fn(observation)
--
--    if t % args.plan_freq == 0:
--        ## concatenate previous transitions and current observations to input to model
--        prefix = make_prefix(discretizer, context, observation, args.prefix_context)
--
--        ## sample sequence from model beginning with `prefix`
--        sequence = beam_plan(
--            gpt, value_fn, prefix,
--            args.horizon, args.beam_width, args.n_expand, observation_dim, action_dim,
--            discount, args.max_context_transitions, verbose=args.verbose,
--            k_obs=args.k_obs, k_act=args.k_act, cdf_obs=args.cdf_obs, cdf_act=args.cdf_act,
--        )
--
--    else:
--        sequence = sequence[1:]
--
--    ## [ horizon x transition_dim ] convert sampled tokens to continuous trajectory
--    sequence_recon = discretizer.reconstruct(sequence)
--
--    ## [ action_dim ] index into sampled trajectory to grab first action
--    action = extract_actions(sequence_recon, observation_dim, action_dim, t=0)
--
--    ## execute action in environment
--    next_observation, reward, terminal, _ = env.step(action)
--
--    ## update return
--    total_reward += reward
--    score = env.get_normalized_score(total_reward)
--
--    ## update rollout observations and context transitions
--    rollout.append(next_observation.copy())
--    context = update_context(context, discretizer, observation, action, reward, args.max_context_transitions)
--
--    print(
--        f'[ plan ] t: {t} / {T} | r: {reward:.2f} | R: {total_reward:.2f} | score: {score:.4f} | '
--        f'time: {timer():.2f} | {args.dataset} | {args.exp_name} | {args.suffix}\n'
--    )
--
--    ## visualization
--    if t % args.vis_freq == 0 or terminal or t == T:
--
--        ## save current plan
--        renderer.render_plan(join(args.savepath, f'{t}_plan.mp4'), sequence_recon, env.state_vector())
--
--        ## save rollout thus far
--        renderer.render_rollout(join(args.savepath, f'rollout.mp4'), rollout, fps=80)
--
--    if terminal: break
--
--    observation = next_observation
--
--## save result as a json file
--json_path = join(args.savepath, 'rollout.json')
--json_data = {'score': score, 'step': t, 'return': total_reward, 'term': terminal, 'gpt_epoch': gpt_epoch}
--json.dump(json_data, open(json_path, 'w'), indent=2, sort_keys=True)
-diff --git a/halfcheetah/scripts/train.py b/halfcheetah/scripts/train.py
-deleted file mode 100644
-index 04af8d7..0000000
---- a/halfcheetah/scripts/train.py
-+++ /dev/null
-@@ -1,122 +0,0 @@
 -import os
 -import numpy as np
--import torch
--import pdb
--
--import trajectory.utils as utils
--import trajectory.datasets as datasets
--from trajectory.models.transformers import GPT
--
--
--class Parser(utils.Parser):
--    dataset: str = 'halfcheetah-medium-expert-v2'
--    config: str = 'config.offline'
--
--#######################
--######## setup ########
--#######################
--
--args = Parser().parse_args('train')
--
--#######################
--####### dataset #######
--#######################
--
--env = datasets.load_environment(args.dataset)
--
--sequence_length = args.subsampled_sequence_length * args.step
--
--dataset_config = utils.Config(
--    datasets.DiscretizedDataset,
--    savepath=(args.savepath, 'data_config.pkl'),
--    env=args.dataset,
--    N=args.N,
--    penalty=args.termination_penalty,
--    sequence_length=sequence_length,
--    step=args.step,
--    discount=args.discount,
--    discretizer=args.discretizer,
--)
--
--dataset = dataset_config()
--obs_dim = dataset.observation_dim
--act_dim = dataset.action_dim
--transition_dim = dataset.joined_dim
--
--#######################
--######## model ########
--#######################
--
--block_size = args.subsampled_sequence_length * transition_dim - 1
--print(
--    f'Dataset size: {len(dataset)} | '
--    f'Joined dim: {transition_dim} '
--    f'(observation: {obs_dim}, action: {act_dim}) | Block size: {block_size}'
--)
--
--model_config = utils.Config(
--    GPT,
--    savepath=(args.savepath, 'model_config.pkl'),
--    ## discretization
--    vocab_size=args.N, block_size=block_size,
--    ## architecture
--    n_layer=args.n_layer, n_head=args.n_head, n_embd=args.n_embd*args.n_head,
--    ## dimensions
--    observation_dim=obs_dim, action_dim=act_dim, transition_dim=transition_dim,
--    ## loss weighting
--    action_weight=args.action_weight, reward_weight=args.reward_weight, value_weight=args.value_weight,
--    ## dropout probabilities
--    embd_pdrop=args.embd_pdrop, resid_pdrop=args.resid_pdrop, attn_pdrop=args.attn_pdrop,
--)
--
--model = model_config()
--model.to(args.device)
--
--#######################
--####### trainer #######
--#######################
--
--warmup_tokens = len(dataset) * block_size ## number of tokens seen per epoch
--final_tokens = 20 * warmup_tokens
--
--trainer_config = utils.Config(
--    utils.Trainer,
--    savepath=(args.savepath, 'trainer_config.pkl'),
--    # optimization parameters
--    batch_size=args.batch_size,
--    learning_rate=args.learning_rate,
--    betas=(0.9, 0.95),
--    grad_norm_clip=1.0,
--    weight_decay=0.1, # only applied on matmul weights
--    # learning rate decay: linear warmup followed by cosine decay to 10% of original
--    lr_decay=args.lr_decay,
--    warmup_tokens=warmup_tokens,
--    final_tokens=final_tokens,
--    ## dataloader
--    num_workers=0,
--    device=args.device,
--)
--
--trainer = trainer_config()
--
--#######################
--###### main loop ######
--#######################
--
--## scale number of epochs to keep number of updates constant
--n_epochs = int(1e6 / len(dataset) * args.n_epochs_ref)
--save_freq = int(n_epochs // args.n_saves)
--
--for epoch in range(n_epochs):
--    print(f'\nEpoch: {epoch} / {n_epochs} | {args.dataset} | {args.exp_name}')
--
--    trainer.train(model, dataset)
--
--    ## get greatest multiple of `save_freq` less than or equal to `save_epoch`
--    save_epoch = (epoch + 1) // save_freq * save_freq
--    statepath = os.path.join(args.savepath, f'state_{save_epoch}.pt')
--    print(f'Saving model to {statepath}')
--
--    ## save state to disk
--    state = model.state_dict()
--    torch.save(state, statepath)
-diff --git a/halfcheetah/scripts/xrl.py b/halfcheetah/scripts/xrl.py
-deleted file mode 100644
-index 134232a..0000000
---- a/halfcheetah/scripts/xrl.py
-+++ /dev/null
-@@ -1,372 +0,0 @@
--import json
--import pdb
--from os.path import join
+-import matplotlib.pyplot as plt
+-import d3rlpy
 -
 -import trajectory.utils as utils
 -import trajectory.datasets as datasets
@@ -852,112 +21953,21 @@ index 134232a..0000000
 -)
 -from trajectory.search.sampling import forward
 -
--import gym
--import d4rl # Import required to register environments, you may need to also import the submodule
--import numpy as np
--import d3rlpy
--import math as mt
--from sklearn.cluster import KMeans
--from sklearn import datasets as skdatasets
 -from sklearn.decomposition import PCA
--
 -from pyclustering.cluster.xmeans import xmeans
 -from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer
--
 -from scipy.stats import wasserstein_distance
+-from moviepy.editor import VideoFileClip
 -
 -class Parser(utils.Parser):
--    dataset: str = 'halfcheetah-medium-expert-v2'
+-    dataset: str = 'halfcheetah-medium-v2'
 -    config: str = 'config.offline'
 -
 -# utils
--    
--class XMeans:
--    def loglikelihood(self, r, rn, var, m, k):
--        l1 = - rn / 2.0 * mt.log(2 * mt.pi)
--        l2 = - rn * m / 2.0 * mt.log(var)
--        l3 = - (rn - k) / 2.0
--        l4 = rn * mt.log(rn)
--        l5 = - rn * mt.log(r)
 -
--        return l1 + l2 + l3 + l4 + l5
+-def cluster_trajectories(trajectories, n_clusters=10):
+-    """TODO"""
 -
--    def __init__(self, X, kmax = 20):
--        self.X = X
--        self.num = np.size(self.X, axis=0)
--        self.dim = np.size(X, axis=1)
--        self.KMax = kmax
--
--    def fit(self):
--        k = 1
--        X = self.X
--        M = self.dim
--        num = self.num
--
--        while(1):
--            ok = k
--
--            #Improve Params
--            kmeans = KMeans(n_clusters=k).fit(X)
--            labels = kmeans.labels_
--            m = kmeans.cluster_centers_
--
--            #Improve Structure
--            #Calculate BIC
--            p = M + 1
--
--            obic = np.zeros(k)
--
--            for i in range(k):
--                rn = np.size(np.where(labels == i))
--                var = np.sum((X[labels == i] - m[i])**2)/float(rn - 1)
--                obic[i] = self.loglikelihood(rn, rn, var, M, 1) - p/2.0*mt.log(rn)
--
--            #Split each cluster into two subclusters and calculate BIC of each splitted cluster
--            sk = 2 #The number of subclusters
--            nbic = np.zeros(k)
--            addk = 0
--
--            for i in range(k):
--                ci = X[labels == i]
--                r = np.size(np.where(labels == i))
--
--                kmeans = KMeans(n_clusters=sk).fit(ci)
--                ci_labels = kmeans.labels_
--                sm = kmeans.cluster_centers_
--
--                for l in range(sk):
--                    rn = np.size(np.where(ci_labels == l))
--                    var = np.sum((ci[ci_labels == l] - sm[l])**2)/float(rn - sk)
--                    nbic[i] += self.loglikelihood(r, rn, var, M, sk)
--
--                p = sk * (M + 1)
--                nbic[i] -= p/2.0*mt.log(r)
--
--                if obic[i] < nbic[i]:
--                    addk += 1
--
--            k += addk
--
--            if ok == k or k >= self.KMax:
--                break
--
--
--        #Calculate labels and centroids
--        kmeans = KMeans(n_clusters=k).fit(X)
--        self.labels = kmeans.labels_
--        self.k = k
--        self.m = kmeans.cluster_centers_
--
--
--def cluster_trajectories(trajectories):
--    xmeans_instance = XMeans(trajectories, kmax=10)
--    xmeans_instance.fit()
--
--    clusters = xmeans_instance.labels
--    return clusters
--
--def cluster_trajectories_2(trajectories):
 -    # Prepare initial centers - amount of initial centers defines amount of clusters from which X-Means will
 -    # start analysis.
 -    amount_initial_centers = 2
@@ -965,7 +21975,7 @@ index 134232a..0000000
 -    
 -    # Create instance of X-Means algorithm. The algorithm will start analysis from 2 clusters, the maximum
 -    # number of clusters that can be allocated is 10.
--    xmeans_instance = xmeans(trajectories, initial_centers, 10)
+-    xmeans_instance = xmeans(trajectories, initial_centers, n_clusters)
 -    xmeans_instance.process()
 -    
 -    # Extract clustering results: clusters
@@ -978,19 +21988,27 @@ index 134232a..0000000
 -
 -    return idxs_per_cluster, np.array(clusters)
 - 
--# https://github.com/sascha-kirch/ML_Notebooks/blob/main/Softmax_Temperature.ipynb
 -def softmax(x, temp):
--    """Compute softmax values for each sets of scores in x."""
--    return np.exp(np.divide(x,temp)) / np.sum(np.exp(np.divide(x,temp)))
+-    """TODO"""
+-    max_x = np.max(x)
+-    return np.exp(np.divide(x-max_x,temp)) / np.sum(np.exp(np.divide(x-max_x,temp)))
+-
+-def generate_data_embedding(trajectory_embeddings, temperature=10000):
+-    """TODO"""
 -
--def generate_data_embedding(trajectory_embeddings, normalizing_factor=1, temperature=1):
--    embedding = np.sum(trajectory_embeddings, axis=0) / normalizing_factor
+-    embedding = np.sum(trajectory_embeddings, axis=0)
 -    embedding = softmax(embedding, temperature)
+-    
+-
 -    return embedding
 -
 -def embed_trajectory(gpt, discretizer, observations, actions, rewards, preprocess_fn):
+-    """TODO"""
+-
 -    context = []
 -
+-    output = []
+-
 -    for i in range(len(observations)):
 -        observation = observations[i]
 -        action = actions[i]
@@ -998,253 +22016,396 @@ index 134232a..0000000
 -
 -        observation = preprocess_fn(observation)
 -
--        # print(observation)
 -        prefix = make_prefix(discretizer, context, observation, True)
--        # print("prefix", prefix.shape)
 -
 -        out = forward(gpt, prefix)
--        # print("out", out.shape)
+-
+-        if len(context) >= 9:
+-            context.pop(0)
+-            if len(output) == 0:
+-                output = out.detach().numpy()[0]
+-            else:
+-                output = np.concatenate((output, out.detach().numpy()[0][217:]), axis=0)
+-
 -        context = update_context(context, discretizer, observation, action, reward, len(observations))
--        # print("cotext", context)
--    
--    emb = []
--    for context_step in context:
--        emb.append(context_step.numpy())
--    emb = np.array(emb)
--    emb = np.mean(emb, axis=0)[0]
 -
+-    emb = np.mean(output, axis=0)
 -    return emb
 -
+-def create_complementary_dataset(dataset, idxs, trajectory_length=10, inverse=False):
+-    """TODO"""
 -
--def create_complementary_dataset(dataset, idxs, trajectory_length=10):
 -    observations = []
 -    actions = []
 -    rewards = []
 -    terminals = []
--    for i in range(1000):
--        if i not in idxs:
--            observations += list(dataset.observations[1000*i:1000*i+trajectory_length])
--            actions += list(dataset.actions[1000*i:1000*i+trajectory_length])
--            rewards += list(dataset.rewards[1000*i:1000*i+trajectory_length])
--            terminals += list(dataset.terminals[1000*i:1000*i+trajectory_length])
+-
+-    n_trajs = int(1000000/trajectory_length)
+-    for i in range(n_trajs):
+-        condition = i not in idxs
+-        if inverse: condition = not condition
+-
+-        if condition:
+-            observations += list(dataset.observations[trajectory_length*i:trajectory_length*(i+1)])
+-            actions += list(dataset.actions[trajectory_length*i:trajectory_length*(i+1)])
+-            rewards += list(dataset.rewards[trajectory_length*i:trajectory_length*(i+1)])
+-
+-    terminals = np.tile([0]*(trajectory_length-1)+[1], int(len(observations)/trajectory_length))
 -
 -    new_dataset = d3rlpy.dataset.MDPDataset(
 -        observations=np.array(observations),
 -        actions=np.array(actions),
 -        rewards=np.array(rewards),
--        terminals=np.array(terminals)
+-        terminals=np.array(terminals),
 -    )
 -    return new_dataset
 -    
 -
+-def clusters_to_idxs(clusters):
+-    idxs_per_cluster = []
+-    for i in np.sort(np.unique(clusters)):
+-        idxs_per_cluster.append(list(np.argwhere(clusters == i).flatten()))
+-    
+-    return idxs_per_cluster
 -
 -
 -def main():
--    # args = Parser().parse_args('plan')
--
--    #######################
--    ####### models ########
--    #######################
--
--
--
--
 -
--    # print(args.dataset)
+-    args = Parser().parse_args('plan')
 -
--    # dataset = utils.load_from_config(args.logbase, args.dataset, args.gpt_loadpath,
--    #         'data_config.pkl')
 -
+-    ### DATASET ###
 -
--    # gpt, gpt_epoch = utils.load_model(args.logbase, args.dataset, args.gpt_loadpath,
--    #         epoch=args.gpt_epoch, device=args.device)
--
--    # env = datasets.load_environment(args.dataset)
--
--    # discretizer = dataset.discretizer
--
--    # preprocess_fn = datasets.get_preprocess_fn(env.name)
--
--    # #######################
--    # ####### dataset #######
--    # #######################
+-    dataset_d3, env = d3rlpy.datasets.get_dataset("halfcheetah-medium-v2")
 -
--    # # env = datasets.load_environment(args.dataset)
--    # discretizer = dataset.discretizer
--    # preprocess_fn = datasets.get_preprocess_fn(env.name)
+-    ### IMPORTANT DEFINITIONS XRL SCRIPT ###
 -
--    # # dataset
--    dataset_d3, env = d3rlpy.datasets.get_dataset("halfcheetah-medium-v2")
+-    load_embeddings = False
+-    load_clusters = True
+-    load_agents = True
+-    generate_human_study = False
+-    
+-    seed = 4 
+-    trajectory_length = 25 # 10 = max
+-    n_clusters = 10
+-    k = 3
+-    temperature = 10000
+-    logging_folder = f"results/v2_models_100k_{seed}"
+-    training_steps = 100000
 -
--    # env = gym.make('halfcheetah-medium-v2')
--    # dataset_d4 = d4rl.qlearning_dataset(env)
+-    d3rlpy.seed(seed)
 -
--    # # checks to see if d3rl & d4rl datasets are equal
--    # print(np.allclose(dataset_d3.actions[100], dataset_d4['actions'][100]))
+-    if load_embeddings:
+-        embeddings = np.load(f"{logging_folder}/embeddings.npy")
+-    else:
+-   
+-        ### TRAJECTORY TRANSFORMER ###
+-    
+-        dataset = utils.load_from_config(args.logbase, args.dataset, args.gpt_loadpath,
+-                'data_config.pkl')
+-        gpt, gpt_epoch = utils.load_model(args.logbase, args.dataset, args.gpt_loadpath,
+-                epoch=args.gpt_epoch, device=args.device)
+-        env = datasets.load_environment(args.dataset)
+-        discretizer = dataset.discretizer
+-        preprocess_fn = datasets.get_preprocess_fn(env.name)
+-    
+-        ### TRAJECTORY EMBEDDINGS ###
+-    
+-        embeddings = []
+-        n_trajs = int(1000000/trajectory_length)
+-        for i in range(n_trajs):
+-            observations = dataset_d3.observations[trajectory_length*i:trajectory_length*(i+1)]
+-            actions = dataset_d3.actions[trajectory_length*i:trajectory_length*(i+1)]
+-            rewards = dataset_d3.rewards[trajectory_length*i:trajectory_length*(i+1)]
+-            terminals = dataset_d3.terminals[trajectory_length*i:trajectory_length*(i+1)]
+-            emb = embed_trajectory(gpt, discretizer, observations, actions, rewards, preprocess_fn)
+-            embeddings.append(emb)
+-        embeddings = np.array(embeddings)
+-        np.save(f"{logging_folder}/embeddings.npy", embeddings)
 -
--    # # dr4rl has same trajectories, just cut off 1 element before the end
--    # for j in range(1000):
--    #     for i in range(999):
--    #         if dataset_d4['rewards'][j * 999 + i] != dataset_d3.rewards[j * 1000 + i]: print("yo", i)
+-    print("embeddings ready")
 -
--    # #######################
--    # ###### main loop ######
--    # #######################
+-    ### TRAJECTORY CLUSTERS ###
 -
--    trajectory_length = 10 # 10 = max
+-    if load_clusters:
+-        clusters = np.load(f"{logging_folder}/clusters.npy")
+-        idxs_per_cluster = clusters_to_idxs(clusters)
+-    else:
+-        idxs_per_cluster, clusters = cluster_trajectories(embeddings, n_clusters)
+-        np.save(f"{logging_folder}/clusters.npy", clusters)
 -
--    # embeddings = []
--    # for i in range(1000):
--    #     observations = dataset_d3.observations[1000*i:1000*i+trajectory_length]
--    #     actions = dataset_d3.actions[1000*i:1000*i+trajectory_length]
--    #     rewards = dataset_d3.rewards[1000*i:1000*i+trajectory_length]
--    #     terminals = dataset_d3.terminals[1000*i:1000*i+trajectory_length]
--    #     emb = embed_trajectory(gpt, discretizer, observations, actions, rewards, preprocess_fn)
--    #     embeddings.append(emb)
--    # embeddings = np.array(embeddings)
--    # np.save("embeddings.npy", embeddings)
--    # print(embeddings)
+-    print("clusters ready")
 -
--    embeddings = np.load("embeddings.npy")
+-    ### PCA (solely for visualization) ###
+- 
+-    pca_idxs = np.random.choice(len(embeddings), 500, replace=False)
 -
 -    pca = PCA(n_components=2)
--    pca = PCA(n_components=2)
--    pca_embeddings = pca.fit_transform(embeddings)
--    np.save("pca.py", pca_embeddings)
+-    pca_embeddings = pca.fit_transform(embeddings[pca_idxs])
+-    pca_clusters = clusters[pca_idxs]
 -
--    idxs_per_cluster, clusters = cluster_trajectories_2(embeddings)
--    # print(clusters)
--    # return
--    np.save("clusters.npy", clusters)
+-    print("pca ready")
 -
--    import matplotlib.pyplot as plt
+-    ### COMPLEMENTARY DATASETS & CLUSTER EMBEDDINGS (also plotting PCA) ###
+-    d_orig = generate_data_embedding(embeddings, temperature=temperature)
 -
--    d_orig = generate_data_embedding(embeddings)
 -    unique_clusters = np.unique(clusters)
--    
 -    d_j = []
 -    complementary_datasets = []
+-    cluster_datasets = []
+-    fig, ax = plt.subplots(figsize=(5,4))
 -    for j in np.sort(unique_clusters):
--        print(j)
--        d_j.append(generate_data_embedding(embeddings[clusters != j]))
--        plt.scatter(pca_embeddings[clusters == j][:,0], pca_embeddings[clusters == j][:,1], label=j)
+-        d_j.append(generate_data_embedding(embeddings[clusters != j], temperature=temperature))
+-        ax.scatter(pca_embeddings[pca_clusters == j][:,0], pca_embeddings[pca_clusters == j][:,1], label=j)
 -        complementary_datasets.append(create_complementary_dataset(dataset_d3, idxs_per_cluster[j], trajectory_length))
+-        cluster_datasets.append(create_complementary_dataset(dataset_d3, idxs_per_cluster[j], trajectory_length, inverse=True))
 -    
 -    original_dataset = create_complementary_dataset(dataset_d3, [], trajectory_length)
 -
--    print(complementary_datasets, original_dataset)
+-    ax.legend(title="$c_j$", bbox_to_anchor=(0.5, 1.2), loc="lower center", ncol=5)
+-    ax.set_xlabel("feature 1")
+-    ax.set_ylabel("feature 2")
+-    ax.spines['top'].set_visible(False)
+-    ax.spines['right'].set_visible(False)
+-    plt.title("Trajectory Clustering HalfCheetah")
+-    plt.tight_layout()
+-
+-    plt.savefig(f"{logging_folder}/pca.pdf")
+-
+-    print("complementary datasets ready")
 -
--    plt.legend()
--    plt.show()
+-    ### AGENT TRAINING (original & complementary) ###
 -
 -    agent_orig = d3rlpy.algos.SAC(
 -        actor_learning_rate=3e-4,
 -        critic_learning_rate=3e-4,
 -        temp_learning_rate=3e-4,
--        batch_size=256)
+-        batch_size=512)
 -
--    print(agent_orig)
--
--    training_steps = 1000
--
--    agent_orig.fit(original_dataset, n_steps=training_steps)
+-    if load_agents:
+-        agent_orig.build_with_dataset(original_dataset)
+-        agent_orig.load_model(f"{logging_folder}/agent_orig.pt")
+-    else:
+-        agent_orig.fit(original_dataset, n_steps=training_steps)
+-        agent_orig.save_model(f"{logging_folder}/agent_orig.pt")
 -
 -    agents_compl = []
 -
--    for dset in complementary_datasets:
+-    for i in range(len(complementary_datasets)):
 -        agent = d3rlpy.algos.SAC(
 -            actor_learning_rate=3e-4,
 -            critic_learning_rate=3e-4,
 -            temp_learning_rate=3e-4,
--            batch_size=256)
--        agent.fit(dset, n_steps=training_steps)
+-            batch_size=512)
+-        if load_agents:
+-            agent.build_with_dataset(complementary_datasets[i])
+-            agent.load_model(f"{logging_folder}/agent_compl_{i}.pt")
+-        else:
+-            agent.fit(complementary_datasets[i], n_steps=training_steps)
+-            agent.save_model(f"{logging_folder}/agent_compl_{i}.pt")
 -        agents_compl.append(agent)
 -
--    action_orig = agent_orig.predict(dataset_d3.observations[0])
+-    print("agents ready")
 -
--    actions_compl = []
--    for agent in agents_compl:
--        actions_compl.append(agent.predict(dataset_d3.observations[0]))
--    
--    action_dists = []
--    for action in actions_compl:
--        action_dists.append(np.linalg.norm(action_orig-action))
+-    ### OBSERVATION EXPLANATION (cluster assignment) ###
 -
--    k = 3
--    topk = np.argpartition(action_dists, -k)[-k:]
+-    original_state = np.random.get_state()
+-    np.random.seed(0)
+-    idxs_to_explain = np.random.choice(range(len(dataset_d3.observations)), 1000, replace=False)
+-    np.random.set_state(original_state)
 -
--    d_w = {}
--    for idx in topk:
--        d_w[idx] = wasserstein_distance(d_j[idx], d_orig)
+-    observations_to_explain = [dataset_d3.observations[i] for i in idxs_to_explain] 
 -
--    cluster_assignment = min(d_w, key=d_w.get)
--    print("explanation assigned to cluster", cluster_assignment)
+-    ISVE = []
+-    ISVE_orig = 0.
+-    LMAAVD = []
+-    ACM = []
+-    NWD = []
+-    CAF = [0] * len(d_j)
 -
--    
--def assignment_test():
--    action_orig = np.random.rand(10)
--    d_orig = np.random.rand(5)
+-    if generate_human_study:
+-        ctr = 0
+-        unrelated_idxs = [690, 1520, 3030, 6050, 7080, 8030]
+-        if not os.path.isdir(f"{logging_folder}/mp4s"): os.mkdir(f"{logging_folder}/mp4s")
+-        if not os.path.isdir(f"{logging_folder}/gifs"): os.mkdir(f"{logging_folder}/gifs")
 -
--    actions_compl = np.random.rand(6,10)
--    d_j = np.random.rand(6,5)
+-    for observation_to_explain in observations_to_explain:
+-        action_orig = agent_orig.predict([observation_to_explain])
 -
--    action_dists = []
--    for action in actions_compl:
--        action_dists.append(np.linalg.norm(action_orig-action))
+-        actions_compl = []
+-        for agent in agents_compl:
+-            actions_compl.append(agent.predict([observation_to_explain]))
 -
--    print(action_dists)
+-        action_dists = []
+-        for action in actions_compl:
+-            action_dists.append(np.linalg.norm(action_orig-action))
 -
--    k = 3
--    topk = np.argpartition(action_dists, -k)[-k:]
+-        topk = np.argpartition(action_dists, -k)[-k:]
+-
+-        d_w = {}
+-        for idx in topk:
+-            d_w[idx] = wasserstein_distance(d_j[idx], d_orig)
+-
+-        cluster_assignment = min(d_w, key=d_w.get)
+-
+-        ### OBSERVATION EXPLANATION (representing cluster with 1 trajectory) ###
+-
+-        distances_to_obs = [np.linalg.norm(observation_to_explain-obs) for obs in cluster_datasets[cluster_assignment].observations]
+-        trajectory_to_assign = np.floor(np.argmin(distances_to_obs) / trajectory_length)
+-        assigned_trajectory = np.arange(trajectory_to_assign * trajectory_length, (trajectory_to_assign+1) * trajectory_length)
 -
--    print(topk)
+-        ### OBSERVATION EXPLANATION (metrics) ###
 -
--    d_w = {}
--    for idx in topk:
--        d_w[idx] = wasserstein_distance(d_j[idx], d_orig)
+-        # Initial State Value Estimate
+- 
+-        V_s = 0.
+-        for _ in range(10):
+-            sampled_action = agent_orig.sample_action([observation_to_explain])
+-            Q_sa = agent_orig.predict_value([observation_to_explain], [sampled_action[0]])[0]
+-            V_s += Q_sa
+-        ISVE_orig += V_s/10
+- 
+-        new_ISVE = []
+-        for agent in agents_compl:
+-            V_s = 0.
+-            for _ in range(10):
+-                sampled_action = agent.sample_action([observation_to_explain])
+-                Q_sa = agent.predict_value([observation_to_explain], [sampled_action[0]])[0]
+-                V_s += Q_sa
+-            new_ISVE.append(V_s/10)
+-        ISVE.append(new_ISVE)
+- 
+-        # Local Mean Absolute Action-Value Difference
+-        Q_orig = agent_orig.predict_value([observation_to_explain], [action_orig[0]])
+-        Q_j = [agent_orig.predict_value([observation_to_explain], [ac[0]]) for ac in actions_compl]
+-        LMAAVD.append(np.abs(np.array(Q_j) - Q_orig).flatten())
+- 
+-        # Action Contrast Measure
+-        ACM.append(action_dists)
+- 
+-        # Normalized Wasserstein distance (between cluster embeddings)
+-        wasser = np.array([wasserstein_distance(d, d_orig) for d in d_j])
+-        NWD.append(list((wasser-np.min(wasser))/(np.max(wasser)-np.min(wasser))))
+- 
+-        # Cluster attribution frequency
+-        CAF[cluster_assignment] += 1
+- 
+-        if generate_human_study:
+-            ### RENDERING ###
+-            if not os.path.isdir(f"{logging_folder}/gifs/question_{ctr}"): os.mkdir(f"{logging_folder}/gifs/question_{ctr}")
 -
--    print(d_w)
+-            rollout = dataset_d3.observations[idxs_to_explain[ctr]-25:idxs_to_explain[ctr]]
+-            print(idxs_to_explain[ctr])
+-            print(rollout)
+-            renderer = utils.make_renderer(args)
+-            rollout_savepath = f"{logging_folder}/mp4s/question_{ctr}/traj_to_explain.mp4"
+-            renderer.render_rollout(rollout_savepath, rollout, fps=10)
+-            videoClip = VideoFileClip(f"{logging_folder}/mp4s/question_{ctr}/traj_to_explain.mp4")
+-            videoClip.write_gif(f"{logging_folder}/gifs/question_{ctr}/traj_to_explain.gif")
 -
--    cluster_assignment = min(d_w, key=d_w.get)
--    print("explanation assigned to cluster", cluster_assignment)
+-
+-            rollout = cluster_datasets[cluster_assignment].observations[assigned_trajectory.astype(int)]
+-            print(rollout)
+-            renderer = utils.make_renderer(args)
+-            rollout_savepath = f"{logging_folder}/mp4s/question_{ctr}/traj_assigned_cluster_attr.mp4"
+-            renderer.render_rollout(rollout_savepath, rollout, fps=10)
+-            videoClip = VideoFileClip(f"{logging_folder}/mp4s/question_{ctr}/traj_assigned_cluster_attr.mp4")
+-            videoClip.write_gif(f"{logging_folder}/gifs/question_{ctr}/traj_assigned_cluster_attr.gif")
+-
+-            random_trajs = np.random.randint(len(cluster_datasets[cluster_assignment])//25, size=3)
+-
+-            rollout = cluster_datasets[cluster_assignment].observations[random_trajs[0]*25:(random_trajs[0]+1)*25]
+-            print(rollout)
+-            renderer = utils.make_renderer(args)
+-            rollout_savepath = f"{logging_folder}/mp4s/question_{ctr}/traj_assigned_cluster_1.mp4"
+-            renderer.render_rollout(rollout_savepath, rollout, fps=10)
+-            videoClip = VideoFileClip(f"{logging_folder}/mp4s/question_{ctr}/traj_assigned_cluster_1.mp4")
+-            videoClip.write_gif(f"{logging_folder}/gifs/question_{ctr}/traj_assigned_cluster_1.gif")
+-
+-            rollout = cluster_datasets[cluster_assignment].observations[random_trajs[1]*25:(random_trajs[1]+1)*25]
+-            print(rollout)
+-            renderer = utils.make_renderer(args)
+-            rollout_savepath = f"{logging_folder}/mp4s/question_{ctr}/traj_assigned_cluster_2.mp4"
+-            renderer.render_rollout(rollout_savepath, rollout, fps=10)
+-            videoClip = VideoFileClip(f"{logging_folder}/mp4s/question_{ctr}/traj_assigned_cluster_2.mp4")
+-            videoClip.write_gif(f"{logging_folder}/gifs/question_{ctr}/traj_assigned_cluster_2.gif")
+-
+-
+-            rollout = cluster_datasets[cluster_assignment].observations[random_trajs[2]*25:(random_trajs[2]+1)*25]
+-            print(rollout)
+-            renderer = utils.make_renderer(args)
+-            rollout_savepath = f"{logging_folder}/mp4s/question_{ctr}/traj_assigned_cluster_3.mp4"
+-            renderer.render_rollout(rollout_savepath, rollout, fps=10)
+-            videoClip = VideoFileClip(f"{logging_folder}/mp4s/question_{ctr}/traj_assigned_cluster_3.mp4")
+-            videoClip.write_gif(f"{logging_folder}/gifs/question_{ctr}/traj_assigned_cluster_3.gif")
+-
+-
+-            different_cluster = 0 if cluster_assignment != 0 else 1
+-            random_trajs = np.random.randint(len(cluster_datasets[different_cluster])//25, size=3)
+-            rollout = cluster_datasets[different_cluster].observations[random_trajs[2]*25:(random_trajs[2]+1)*25]
+-            print(rollout)
+-            renderer = utils.make_renderer(args)
+-            rollout_savepath = f"{logging_folder}/mp4s/question_{ctr}/traj_different_cluster.mp4"
+-            renderer.render_rollout(rollout_savepath, rollout, fps=10)
+-            videoClip = VideoFileClip(f"{logging_folder}/mp4s/question_{ctr}/traj_different_cluster.mp4")
+-            videoClip.write_gif(f"{logging_folder}/gifs/question_{ctr}/traj_different_cluster.gif")
+-
+-            rollout = dataset_d3.observations[unrelated_idxs[ctr]-25:unrelated_idxs[ctr]]
+-            print(rollout)
+-            renderer = utils.make_renderer(args)
+-            rollout_savepath = f"{logging_folder}/mp4s/question_{ctr}/traj_unrelated.mp4"
+-            renderer.render_rollout(rollout_savepath, rollout, fps=10)
+-            videoClip = VideoFileClip(f"{logging_folder}/mp4s/question_{ctr}/traj_unrelated.mp4")
+-            videoClip.write_gif(f"{logging_folder}/gifs/question_{ctr}/traj_unrelated.gif")
+-
+-            ctr += 1
+-
+-    ### RESULTS ###
+-    ISVE_orig /= len(observations_to_explain)
+-    ISVE = np.mean(ISVE, axis=0)
+-    LMAAVD = np.mean(LMAAVD, axis=0)
+-    ACM = np.mean(ACM, axis=0)
+-    NWD = np.mean(NWD, axis=0)
+-    CAF = np.array(CAF) / np.sum(CAF)
+-
+-    print("ISVE orig:", ISVE_orig)
+-    print("ISVE:",ISVE)
+-    print("LMAAVD:",LMAAVD)
+-    print("ACM:",ACM)
+-    print("NWD:",NWD)
+-    print("CAF:",CAF)
 -
 -
 -if __name__ == "__main__":
--    # main()
--    assignment_test()
-diff --git a/halfcheetah/trajectory.egg-info/PKG-INFO b/halfcheetah/trajectory.egg-info/PKG-INFO
-index 452c6cb..2603850 100644
---- a/halfcheetah/trajectory.egg-info/PKG-INFO
-+++ b/halfcheetah/trajectory.egg-info/PKG-INFO
-@@ -1,4 +1,11 @@
- Metadata-Version: 2.1
- Name: trajectory
- Version: 0.0.0
-+Summary: UNKNOWN
-+Home-page: UNKNOWN
-+License: UNKNOWN
-+Platform: UNKNOWN
- License-File: LICENSE
-+
-+UNKNOWN
-+
+-    main()
 diff --git a/halfcheetah/trajectory.egg-info/SOURCES.txt b/halfcheetah/trajectory.egg-info/SOURCES.txt
-index 4474d85..84e8e3a 100644
+index 84e8e3a..4474d85 100644
 --- a/halfcheetah/trajectory.egg-info/SOURCES.txt
 +++ b/halfcheetah/trajectory.egg-info/SOURCES.txt
-@@ -30,4 +30,5 @@ trajectory/utils/serialization.py
+@@ -30,5 +30,4 @@ trajectory/utils/serialization.py
  trajectory/utils/setup.py
  trajectory/utils/timer.py
  trajectory/utils/training.py
 -trajectory/utils/video.py
+-trajectory_aaa/__init__.py
 \ No newline at end of file
 +trajectory/utils/video.py
-+trajectory_aaa/__init__.py
 \ No newline at end of file
 diff --git a/halfcheetah/trajectory.egg-info/top_level.txt b/halfcheetah/trajectory.egg-info/top_level.txt
-index ce65198..1d5271f 100644
+index 1d5271f..ce65198 100644
 --- a/halfcheetah/trajectory.egg-info/top_level.txt
 +++ b/halfcheetah/trajectory.egg-info/top_level.txt
-@@ -1 +1,2 @@
+@@ -1,2 +1 @@
  trajectory
-+trajectory_aaa
\ No newline at end of file
+-trajectory_aaa
+diff --git a/seaquest/readme.md b/seaquest/readme.md
+index 84e53f8..53561f9 100644
+--- a/seaquest/readme.md
++++ b/seaquest/readme.md
+@@ -10,4 +10,4 @@ pip install git+https://github.com/takuseno/d4rl-atari
+ pip install "gym[atari, accept-rom-license]"
+ pip install pyclustering
+ pip install seaborn
+-pip install d3rlpy==1.1.1
+\ No newline at end of file
++pip install d3rlpy==1.1.1
\ No newline at end of file
diff --git a/halfcheetah/scripts/xrl_v2.py b/halfcheetah/scripts/xrl_v2.py
deleted file mode 100644
index 62a3d4d..0000000
--- a/halfcheetah/scripts/xrl_v2.py
+++ /dev/null
@@ -1,442 +0,0 @@
-"""
-TODO HEADER
-"""
-
-import os
-import numpy as np
-import matplotlib.pyplot as plt
-import d3rlpy
-
-import trajectory.utils as utils
-import trajectory.datasets as datasets
-from trajectory.search import (
-    make_prefix,
-    update_context,
-)
-from trajectory.search.sampling import forward
-
-from sklearn.decomposition import PCA
-from pyclustering.cluster.xmeans import xmeans
-from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer
-from scipy.stats import wasserstein_distance
-from moviepy.editor import VideoFileClip
-
-class Parser(utils.Parser):
-    dataset: str = 'halfcheetah-medium-v2'
-    config: str = 'config.offline'
-
-# utils
-
-def cluster_trajectories(trajectories, n_clusters=10):
-    """TODO"""
-
-    # Prepare initial centers - amount of initial centers defines amount of clusters from which X-Means will
-    # start analysis.
-    amount_initial_centers = 2
-    initial_centers = kmeans_plusplus_initializer(trajectories, amount_initial_centers).initialize()
-    
-    # Create instance of X-Means algorithm. The algorithm will start analysis from 2 clusters, the maximum
-    # number of clusters that can be allocated is 10.
-    xmeans_instance = xmeans(trajectories, initial_centers, n_clusters)
-    xmeans_instance.process()
-    
-    # Extract clustering results: clusters
-    idxs_per_cluster = xmeans_instance.get_clusters()
-
-    clusters = []
-    for i in range(len(trajectories)):
-        for j in range(len(idxs_per_cluster)):
-            if i in idxs_per_cluster[j]: clusters.append(j)
-
-    return idxs_per_cluster, np.array(clusters)
- 
-def softmax(x, temp):
-    """TODO"""
-    max_x = np.max(x)
-    return np.exp(np.divide(x-max_x,temp)) / np.sum(np.exp(np.divide(x-max_x,temp)))
-
-def generate_data_embedding(trajectory_embeddings, temperature=10000):
-    """TODO"""
-
-    embedding = np.sum(trajectory_embeddings, axis=0)
-    embedding = softmax(embedding, temperature)
-    
-
-    return embedding
-
-def embed_trajectory(gpt, discretizer, observations, actions, rewards, preprocess_fn):
-    """TODO"""
-
-    context = []
-
-    output = []
-
-    for i in range(len(observations)):
-        observation = observations[i]
-        action = actions[i]
-        reward = rewards[i]
-
-        observation = preprocess_fn(observation)
-
-        prefix = make_prefix(discretizer, context, observation, True)
-
-        out = forward(gpt, prefix)
-
-        if len(context) >= 9:
-            context.pop(0)
-            if len(output) == 0:
-                output = out.detach().numpy()[0]
-            else:
-                output = np.concatenate((output, out.detach().numpy()[0][217:]), axis=0)
-
-        context = update_context(context, discretizer, observation, action, reward, len(observations))
-
-    emb = np.mean(output, axis=0)
-    return emb
-
-def create_complementary_dataset(dataset, idxs, trajectory_length=10, inverse=False):
-    """TODO"""
-
-    observations = []
-    actions = []
-    rewards = []
-    terminals = []
-
-    n_trajs = int(1000000/trajectory_length)
-    for i in range(n_trajs):
-        condition = i not in idxs
-        if inverse: condition = not condition
-
-        if condition:
-            observations += list(dataset.observations[trajectory_length*i:trajectory_length*(i+1)])
-            actions += list(dataset.actions[trajectory_length*i:trajectory_length*(i+1)])
-            rewards += list(dataset.rewards[trajectory_length*i:trajectory_length*(i+1)])
-
-    terminals = np.tile([0]*(trajectory_length-1)+[1], int(len(observations)/trajectory_length))
-
-    new_dataset = d3rlpy.dataset.MDPDataset(
-        observations=np.array(observations),
-        actions=np.array(actions),
-        rewards=np.array(rewards),
-        terminals=np.array(terminals),
-    )
-    return new_dataset
-    
-
-def clusters_to_idxs(clusters):
-    idxs_per_cluster = []
-    for i in np.sort(np.unique(clusters)):
-        idxs_per_cluster.append(list(np.argwhere(clusters == i).flatten()))
-    
-    return idxs_per_cluster
-
-
-def main():
-
-    args = Parser().parse_args('plan')
-
-
-    ### DATASET ###
-
-    dataset_d3, env = d3rlpy.datasets.get_dataset("halfcheetah-medium-v2")
-
-    ### IMPORTANT DEFINITIONS XRL SCRIPT ###
-
-    load_embeddings = False
-    load_clusters = True
-    load_agents = True
-    generate_human_study = False
-    
-    seed = 4 
-    trajectory_length = 25 # 10 = max
-    n_clusters = 10
-    k = 3
-    temperature = 10000
-    logging_folder = f"results/v2_models_100k_{seed}"
-    training_steps = 100000
-
-    d3rlpy.seed(seed)
-
-    if load_embeddings:
-        embeddings = np.load(f"{logging_folder}/embeddings.npy")
-    else:
-   
-        ### TRAJECTORY TRANSFORMER ###
-    
-        dataset = utils.load_from_config(args.logbase, args.dataset, args.gpt_loadpath,
-                'data_config.pkl')
-        gpt, gpt_epoch = utils.load_model(args.logbase, args.dataset, args.gpt_loadpath,
-                epoch=args.gpt_epoch, device=args.device)
-        env = datasets.load_environment(args.dataset)
-        discretizer = dataset.discretizer
-        preprocess_fn = datasets.get_preprocess_fn(env.name)
-    
-        ### TRAJECTORY EMBEDDINGS ###
-    
-        embeddings = []
-        n_trajs = int(1000000/trajectory_length)
-        for i in range(n_trajs):
-            observations = dataset_d3.observations[trajectory_length*i:trajectory_length*(i+1)]
-            actions = dataset_d3.actions[trajectory_length*i:trajectory_length*(i+1)]
-            rewards = dataset_d3.rewards[trajectory_length*i:trajectory_length*(i+1)]
-            terminals = dataset_d3.terminals[trajectory_length*i:trajectory_length*(i+1)]
-            emb = embed_trajectory(gpt, discretizer, observations, actions, rewards, preprocess_fn)
-            embeddings.append(emb)
-        embeddings = np.array(embeddings)
-        np.save(f"{logging_folder}/embeddings.npy", embeddings)
-
-    print("embeddings ready")
-
-    ### TRAJECTORY CLUSTERS ###
-
-    if load_clusters:
-        clusters = np.load(f"{logging_folder}/clusters.npy")
-        idxs_per_cluster = clusters_to_idxs(clusters)
-    else:
-        idxs_per_cluster, clusters = cluster_trajectories(embeddings, n_clusters)
-        np.save(f"{logging_folder}/clusters.npy", clusters)
-
-    print("clusters ready")
-
-    ### PCA (solely for visualization) ###
- 
-    pca_idxs = np.random.choice(len(embeddings), 500, replace=False)
-
-    pca = PCA(n_components=2)
-    pca_embeddings = pca.fit_transform(embeddings[pca_idxs])
-    pca_clusters = clusters[pca_idxs]
-
-    print("pca ready")
-
-    ### COMPLEMENTARY DATASETS & CLUSTER EMBEDDINGS (also plotting PCA) ###
-    d_orig = generate_data_embedding(embeddings, temperature=temperature)
-
-    unique_clusters = np.unique(clusters)
-    d_j = []
-    complementary_datasets = []
-    cluster_datasets = []
-    fig, ax = plt.subplots(figsize=(5,4))
-    for j in np.sort(unique_clusters):
-        d_j.append(generate_data_embedding(embeddings[clusters != j], temperature=temperature))
-        ax.scatter(pca_embeddings[pca_clusters == j][:,0], pca_embeddings[pca_clusters == j][:,1], label=j)
-        complementary_datasets.append(create_complementary_dataset(dataset_d3, idxs_per_cluster[j], trajectory_length))
-        cluster_datasets.append(create_complementary_dataset(dataset_d3, idxs_per_cluster[j], trajectory_length, inverse=True))
-    
-    original_dataset = create_complementary_dataset(dataset_d3, [], trajectory_length)
-
-    ax.legend(title="$c_j$", bbox_to_anchor=(0.5, 1.2), loc="lower center", ncol=5)
-    ax.set_xlabel("feature 1")
-    ax.set_ylabel("feature 2")
-    ax.spines['top'].set_visible(False)
-    ax.spines['right'].set_visible(False)
-    plt.title("Trajectory Clustering HalfCheetah")
-    plt.tight_layout()
-
-    plt.savefig(f"{logging_folder}/pca.pdf")
-
-    print("complementary datasets ready")
-
-    ### AGENT TRAINING (original & complementary) ###
-
-    agent_orig = d3rlpy.algos.SAC(
-        actor_learning_rate=3e-4,
-        critic_learning_rate=3e-4,
-        temp_learning_rate=3e-4,
-        batch_size=512)
-
-    if load_agents:
-        agent_orig.build_with_dataset(original_dataset)
-        agent_orig.load_model(f"{logging_folder}/agent_orig.pt")
-    else:
-        agent_orig.fit(original_dataset, n_steps=training_steps)
-        agent_orig.save_model(f"{logging_folder}/agent_orig.pt")
-
-    agents_compl = []
-
-    for i in range(len(complementary_datasets)):
-        agent = d3rlpy.algos.SAC(
-            actor_learning_rate=3e-4,
-            critic_learning_rate=3e-4,
-            temp_learning_rate=3e-4,
-            batch_size=512)
-        if load_agents:
-            agent.build_with_dataset(complementary_datasets[i])
-            agent.load_model(f"{logging_folder}/agent_compl_{i}.pt")
-        else:
-            agent.fit(complementary_datasets[i], n_steps=training_steps)
-            agent.save_model(f"{logging_folder}/agent_compl_{i}.pt")
-        agents_compl.append(agent)
-
-    print("agents ready")
-
-    ### OBSERVATION EXPLANATION (cluster assignment) ###
-
-    original_state = np.random.get_state()
-    np.random.seed(0)
-    idxs_to_explain = np.random.choice(range(len(dataset_d3.observations)), 1000, replace=False)
-    np.random.set_state(original_state)
-
-    observations_to_explain = [dataset_d3.observations[i] for i in idxs_to_explain] 
-
-    ISVE = []
-    ISVE_orig = 0.
-    LMAAVD = []
-    ACM = []
-    NWD = []
-    CAF = [0] * len(d_j)
-
-    if generate_human_study:
-        ctr = 0
-        unrelated_idxs = [690, 1520, 3030, 6050, 7080, 8030]
-        if not os.path.isdir(f"{logging_folder}/mp4s"): os.mkdir(f"{logging_folder}/mp4s")
-        if not os.path.isdir(f"{logging_folder}/gifs"): os.mkdir(f"{logging_folder}/gifs")
-
-    for observation_to_explain in observations_to_explain:
-        action_orig = agent_orig.predict([observation_to_explain])
-
-        actions_compl = []
-        for agent in agents_compl:
-            actions_compl.append(agent.predict([observation_to_explain]))
-
-        action_dists = []
-        for action in actions_compl:
-            action_dists.append(np.linalg.norm(action_orig-action))
-
-        topk = np.argpartition(action_dists, -k)[-k:]
-
-        d_w = {}
-        for idx in topk:
-            d_w[idx] = wasserstein_distance(d_j[idx], d_orig)
-
-        cluster_assignment = min(d_w, key=d_w.get)
-
-        ### OBSERVATION EXPLANATION (representing cluster with 1 trajectory) ###
-
-        distances_to_obs = [np.linalg.norm(observation_to_explain-obs) for obs in cluster_datasets[cluster_assignment].observations]
-        trajectory_to_assign = np.floor(np.argmin(distances_to_obs) / trajectory_length)
-        assigned_trajectory = np.arange(trajectory_to_assign * trajectory_length, (trajectory_to_assign+1) * trajectory_length)
-
-        ### OBSERVATION EXPLANATION (metrics) ###
-
-        # Initial State Value Estimate
- 
-        V_s = 0.
-        for _ in range(10):
-            sampled_action = agent_orig.sample_action([observation_to_explain])
-            Q_sa = agent_orig.predict_value([observation_to_explain], [sampled_action[0]])[0]
-            V_s += Q_sa
-        ISVE_orig += V_s/10
- 
-        new_ISVE = []
-        for agent in agents_compl:
-            V_s = 0.
-            for _ in range(10):
-                sampled_action = agent.sample_action([observation_to_explain])
-                Q_sa = agent.predict_value([observation_to_explain], [sampled_action[0]])[0]
-                V_s += Q_sa
-            new_ISVE.append(V_s/10)
-        ISVE.append(new_ISVE)
- 
-        # Local Mean Absolute Action-Value Difference
-        Q_orig = agent_orig.predict_value([observation_to_explain], [action_orig[0]])
-        Q_j = [agent_orig.predict_value([observation_to_explain], [ac[0]]) for ac in actions_compl]
-        LMAAVD.append(np.abs(np.array(Q_j) - Q_orig).flatten())
- 
-        # Action Contrast Measure
-        ACM.append(action_dists)
- 
-        # Normalized Wasserstein distance (between cluster embeddings)
-        wasser = np.array([wasserstein_distance(d, d_orig) for d in d_j])
-        NWD.append(list((wasser-np.min(wasser))/(np.max(wasser)-np.min(wasser))))
- 
-        # Cluster attribution frequency
-        CAF[cluster_assignment] += 1
- 
-        if generate_human_study:
-            ### RENDERING ###
-            if not os.path.isdir(f"{logging_folder}/gifs/question_{ctr}"): os.mkdir(f"{logging_folder}/gifs/question_{ctr}")
-
-            rollout = dataset_d3.observations[idxs_to_explain[ctr]-25:idxs_to_explain[ctr]]
-            print(idxs_to_explain[ctr])
-            print(rollout)
-            renderer = utils.make_renderer(args)
-            rollout_savepath = f"{logging_folder}/mp4s/question_{ctr}/traj_to_explain.mp4"
-            renderer.render_rollout(rollout_savepath, rollout, fps=10)
-            videoClip = VideoFileClip(f"{logging_folder}/mp4s/question_{ctr}/traj_to_explain.mp4")
-            videoClip.write_gif(f"{logging_folder}/gifs/question_{ctr}/traj_to_explain.gif")
-
-
-            rollout = cluster_datasets[cluster_assignment].observations[assigned_trajectory.astype(int)]
-            print(rollout)
-            renderer = utils.make_renderer(args)
-            rollout_savepath = f"{logging_folder}/mp4s/question_{ctr}/traj_assigned_cluster_attr.mp4"
-            renderer.render_rollout(rollout_savepath, rollout, fps=10)
-            videoClip = VideoFileClip(f"{logging_folder}/mp4s/question_{ctr}/traj_assigned_cluster_attr.mp4")
-            videoClip.write_gif(f"{logging_folder}/gifs/question_{ctr}/traj_assigned_cluster_attr.gif")
-
-            random_trajs = np.random.randint(len(cluster_datasets[cluster_assignment])//25, size=3)
-
-            rollout = cluster_datasets[cluster_assignment].observations[random_trajs[0]*25:(random_trajs[0]+1)*25]
-            print(rollout)
-            renderer = utils.make_renderer(args)
-            rollout_savepath = f"{logging_folder}/mp4s/question_{ctr}/traj_assigned_cluster_1.mp4"
-            renderer.render_rollout(rollout_savepath, rollout, fps=10)
-            videoClip = VideoFileClip(f"{logging_folder}/mp4s/question_{ctr}/traj_assigned_cluster_1.mp4")
-            videoClip.write_gif(f"{logging_folder}/gifs/question_{ctr}/traj_assigned_cluster_1.gif")
-
-            rollout = cluster_datasets[cluster_assignment].observations[random_trajs[1]*25:(random_trajs[1]+1)*25]
-            print(rollout)
-            renderer = utils.make_renderer(args)
-            rollout_savepath = f"{logging_folder}/mp4s/question_{ctr}/traj_assigned_cluster_2.mp4"
-            renderer.render_rollout(rollout_savepath, rollout, fps=10)
-            videoClip = VideoFileClip(f"{logging_folder}/mp4s/question_{ctr}/traj_assigned_cluster_2.mp4")
-            videoClip.write_gif(f"{logging_folder}/gifs/question_{ctr}/traj_assigned_cluster_2.gif")
-
-
-            rollout = cluster_datasets[cluster_assignment].observations[random_trajs[2]*25:(random_trajs[2]+1)*25]
-            print(rollout)
-            renderer = utils.make_renderer(args)
-            rollout_savepath = f"{logging_folder}/mp4s/question_{ctr}/traj_assigned_cluster_3.mp4"
-            renderer.render_rollout(rollout_savepath, rollout, fps=10)
-            videoClip = VideoFileClip(f"{logging_folder}/mp4s/question_{ctr}/traj_assigned_cluster_3.mp4")
-            videoClip.write_gif(f"{logging_folder}/gifs/question_{ctr}/traj_assigned_cluster_3.gif")
-
-
-            different_cluster = 0 if cluster_assignment != 0 else 1
-            random_trajs = np.random.randint(len(cluster_datasets[different_cluster])//25, size=3)
-            rollout = cluster_datasets[different_cluster].observations[random_trajs[2]*25:(random_trajs[2]+1)*25]
-            print(rollout)
-            renderer = utils.make_renderer(args)
-            rollout_savepath = f"{logging_folder}/mp4s/question_{ctr}/traj_different_cluster.mp4"
-            renderer.render_rollout(rollout_savepath, rollout, fps=10)
-            videoClip = VideoFileClip(f"{logging_folder}/mp4s/question_{ctr}/traj_different_cluster.mp4")
-            videoClip.write_gif(f"{logging_folder}/gifs/question_{ctr}/traj_different_cluster.gif")
-
-            rollout = dataset_d3.observations[unrelated_idxs[ctr]-25:unrelated_idxs[ctr]]
-            print(rollout)
-            renderer = utils.make_renderer(args)
-            rollout_savepath = f"{logging_folder}/mp4s/question_{ctr}/traj_unrelated.mp4"
-            renderer.render_rollout(rollout_savepath, rollout, fps=10)
-            videoClip = VideoFileClip(f"{logging_folder}/mp4s/question_{ctr}/traj_unrelated.mp4")
-            videoClip.write_gif(f"{logging_folder}/gifs/question_{ctr}/traj_unrelated.gif")
-
-            ctr += 1
-
-    ### RESULTS ###
-    ISVE_orig /= len(observations_to_explain)
-    ISVE = np.mean(ISVE, axis=0)
-    LMAAVD = np.mean(LMAAVD, axis=0)
-    ACM = np.mean(ACM, axis=0)
-    NWD = np.mean(NWD, axis=0)
-    CAF = np.array(CAF) / np.sum(CAF)
-
-    print("ISVE orig:", ISVE_orig)
-    print("ISVE:",ISVE)
-    print("LMAAVD:",LMAAVD)
-    print("ACM:",ACM)
-    print("NWD:",NWD)
-    print("CAF:",CAF)
-
-
-if __name__ == "__main__":
-    main()
diff --git a/halfcheetah/trajectory.egg-info/SOURCES.txt b/halfcheetah/trajectory.egg-info/SOURCES.txt
index 84e8e3a..4474d85 100644
--- a/halfcheetah/trajectory.egg-info/SOURCES.txt
+++ b/halfcheetah/trajectory.egg-info/SOURCES.txt
@@ -30,5 +30,4 @@ trajectory/utils/serialization.py
 trajectory/utils/setup.py
 trajectory/utils/timer.py
 trajectory/utils/training.py
-trajectory/utils/video.py
-trajectory_aaa/__init__.py
\ No newline at end of file
+trajectory/utils/video.py
\ No newline at end of file
diff --git a/halfcheetah/trajectory.egg-info/top_level.txt b/halfcheetah/trajectory.egg-info/top_level.txt
index 1d5271f..ce65198 100644
--- a/halfcheetah/trajectory.egg-info/top_level.txt
+++ b/halfcheetah/trajectory.egg-info/top_level.txt
@@ -1,2 +1 @@
 trajectory
-trajectory_aaa
diff --git a/seaquest/readme.md b/seaquest/readme.md
index 84e53f8..53561f9 100644
--- a/seaquest/readme.md
+++ b/seaquest/readme.md
@@ -10,4 +10,4 @@ pip install git+https://github.com/takuseno/d4rl-atari
 pip install "gym[atari, accept-rom-license]"
 pip install pyclustering
 pip install seaborn
-pip install d3rlpy==1.1.1
\ No newline at end of file
+pip install d3rlpy==1.1.1